# Information Criteria - Overview

Example created by Wilson Rocha Lacerda Junior

> **Looking for more details on NARMAX models?**
> For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book:
> [*Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy*](https://sysidentpy.org/book/0%20-%20Preface/)
>
> This book provides in-depth guidance to support your work with SysIdentPy.

## Comparing different information criteria methods

Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.


```python
pip install sysidentpy
```


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sysidentpy.model_structure_selection import FROLS
from sysidentpy.basis_function import Polynomial
from sysidentpy.parameter_estimation import LeastSquares
from sysidentpy.metrics import root_relative_squared_error
from sysidentpy.utils.generate_data import get_siso_data
from sysidentpy.utils.display_results import results
from sysidentpy.utils.plotting import plot_results
```

## Generating sample data
  
The data is generated by simulating the following model:
$y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}$

If *colored_noise* is set to True:

$e_{k} = 0.8\nu_{k-1} + \nu_{k}$

where $x$ is a uniformly distributed random variable and $\nu$ is a gaussian distributed variable with $\mu=0$ and $\sigma=0.1$

In the next example we will generate a data with 3000 samples with white noise and selecting 90% of the data to train the model. 


```python
x_train, x_test, y_train, y_test = get_siso_data(
    n=100, colored_noise=False, sigma=0.1, train_percentage=70
)
```

The idea is to show the impact of the information criteria to select the number of terms to compose the final model. You will se why it is an auxiliary tool and let the algorithm select the number of terms based on the minimum value is not a good idea when dealing with data highly corrupted by noise (even white noise) 

Note: You may find different results when running the examples. This is due the fact we are not setting a fixed random generator for the sample data. However, the main analysis remain.

## AIC


```python
basis_function = Polynomial(degree=2)
estimator = LeastSquares()

model = FROLS(
    order_selection=True,
    n_info_values=15,
    ylag=2,
    xlag=2,
    info_criteria="aic",
    # estimator=estimator,
    basis_function=basis_function,
    err_tol=None,
)
model.fit(X=x_train, y=y_train)

yhat = model.predict(X=x_test, y=y_test)
rrse = root_relative_squared_error(y_test, yhat)
print(rrse)

r = pd.DataFrame(
    results(
        model.final_model,
        model.theta,
        model.err,
        model.n_terms,
        err_precision=8,
        dtype="sci",
    ),
    columns=["Regressors", "Parameters", "ERR"],
)
print(r)
plot_results(y=y_test, yhat=yhat, n=1000)

xaxis = np.arange(1, model.n_info_values + 1)
plt.plot(xaxis, model.info_values)
plt.xlabel("n_terms")
plt.ylabel("Information Criteria")
```

    0.1681621129389993
           Regressors   Parameters             ERR
    0         x1(k-2)   9.2076E-01  9.41001395E-01
    1          y(k-1)   1.7063E-01  2.71018399E-02
    2   x1(k-1)y(k-1)   1.7342E-01  8.79812755E-03
    3   x1(k-1)y(k-2)  -9.7902E-02  2.75495842E-03
    4  x1(k-2)x1(k-1)   4.9319E-02  1.85339089E-03
    5        y(k-2)^2  -5.6743E-02  1.02439383E-03
    6         x1(k-1)  -2.0179E-02  6.78305323E-04



    
![png](information-criteria-overview_files/information-criteria-overview_10_1.png)
    





    Text(0, 0.5, 'Information Criteria')




    
![png](information-criteria-overview_files/information-criteria-overview_10_3.png)
    



```python
model.info_values
```




    array([-273.81858224, -311.60797635, -331.34011486, -338.49936124,
           -342.10339048, -342.27073244, -342.82764626, -342.16492383,
           -341.04704839, -339.58437034, -337.79642875, -336.20531349,
           -333.72427584, -331.48645717, -329.53042523])



As can be seen above, the minimum value make the algorithm choose a model with 4 terms. However, if you check the plot, 3 terms is the best choice. Increasing the number of terms from 3 upwards do not lead to a better model since the difference is very small.

In this case, you should run the model again with the parameters *n_terms=3*! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again!

## AICc


```python
basis_function = Polynomial(degree=2)
estimator = LeastSquares()

model = FROLS(
    order_selection=True,
    n_info_values=15,
    ylag=2,
    xlag=2,
    info_criteria="aicc",
    estimator=estimator,
    basis_function=basis_function,
)
model.fit(X=x_train, y=y_train)
yhat = model.predict(X=x_test, y=y_test)
rrse = root_relative_squared_error(y_test, yhat)
print(rrse)

r = pd.DataFrame(
    results(
        model.final_model,
        model.theta,
        model.err,
        model.n_terms,
        err_precision=8,
        dtype="sci",
    ),
    columns=["Regressors", "Parameters", "ERR"],
)
print(r)
plot_results(y=y_test, yhat=yhat, n=1000)

xaxis = np.arange(1, model.n_info_values + 1)
plt.plot(xaxis, model.info_values)
plt.xlabel("n_terms")
plt.ylabel("Information Criteria")
```

    0.1697650652880654
           Regressors   Parameters             ERR
    0         x1(k-2)   9.2659E-01  9.41001395E-01
    1          y(k-1)   1.7219E-01  2.71018399E-02
    2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03
    3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03
    4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03
    5        y(k-2)^2  -4.8117E-02  1.02439383E-03
    6         x1(k-1)  -2.4728E-02  6.78305323E-04



    
![png](information-criteria-overview_files/information-criteria-overview_14_1.png)
    





    Text(0, 0.5, 'Information Criteria')




    
![png](information-criteria-overview_files/information-criteria-overview_14_3.png)
    



```python
model.info_values
```




    array([-273.99834706, -311.49708248, -331.28179881, -338.08184328,
           -341.32119362, -341.3373076 , -341.51818541, -340.50119461,
           -339.16231408, -336.96924137, -334.34018578, -331.63849434,
           -328.92685178, -325.8181513 , -322.55039655])



As can be seen above, the minimum value make the algorithm choose a model with 4 terms. AICc, however, have major differences compared with AIC when the number of samples is small.

In this case, you should run the model again with the parameters *n_terms=3*! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again! 

## BIC 


```python
basis_function = Polynomial(degree=2)
estimator = LeastSquares()

model = FROLS(
    order_selection=True,
    n_info_values=15,
    ylag=2,
    xlag=2,
    info_criteria="bic",
    estimator=estimator,
    basis_function=basis_function,
)

model.fit(X=x_train, y=y_train)
yhat = model.predict(X=x_test, y=y_test)
rrse = root_relative_squared_error(y_test, yhat)
print(rrse)

r = pd.DataFrame(
    results(
        model.final_model,
        model.theta,
        model.err,
        model.n_terms,
        err_precision=8,
        dtype="sci",
    ),
    columns=["Regressors", "Parameters", "ERR"],
)
print(r)
plot_results(y=y_test, yhat=yhat, n=1000)

xaxis = np.arange(1, model.n_info_values + 1)
plt.plot(xaxis, model.info_values)
plt.xlabel("n_terms")
plt.ylabel("Information Criteria")
```

    0.16868631871856155
           Regressors   Parameters             ERR
    0         x1(k-2)   9.3050E-01  9.41001395E-01
    1          y(k-1)   1.7980E-01  2.71018399E-02
    2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03
    3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03
    4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03



    
![png](information-criteria-overview_files/information-criteria-overview_18_1.png)
    





    Text(0, 0.5, 'Information Criteria')




    
![png](information-criteria-overview_files/information-criteria-overview_18_3.png)
    



```python
model.info_values
```




    array([-271.83944541, -307.24268246, -324.99827569, -329.8387331 ,
           -331.19139703, -329.39731055, -327.84829815, -325.18581094,
           -322.29019301, -318.63381344, -314.63988674, -310.67712915,
           -306.81399235, -302.66957173, -298.4885502 ])



BIC did a better job in this case! The way it penalizes the model regarding the number of terms ensure that the minimum value here was exact the number of expected terms to compose the model. Good, but not always the best method!

## LILC


```python
basis_function = Polynomial(degree=2)
estimator = LeastSquares()

model = FROLS(
    order_selection=True,
    n_info_values=15,
    ylag=2,
    xlag=2,
    info_criteria="lilc",
    estimator=estimator,
    basis_function=basis_function,
)
model.fit(X=x_train, y=y_train)
yhat = model.predict(X=x_test, y=y_test)
rrse = root_relative_squared_error(y_test, yhat)
print(rrse)

r = pd.DataFrame(
    results(
        model.final_model,
        model.theta,
        model.err,
        model.n_terms,
        err_precision=8,
        dtype="sci",
    ),
    columns=["Regressors", "Parameters", "ERR"],
)
print(r)
plot_results(y=y_test, yhat=yhat, n=1000)

xaxis = np.arange(1, model.n_info_values + 1)
plt.plot(xaxis, model.info_values)
plt.xlabel("n_terms")
plt.ylabel("Information Criteria")
```

    0.16868631871856155
           Regressors   Parameters             ERR
    0         x1(k-2)   9.3050E-01  9.41001395E-01
    1          y(k-1)   1.7980E-01  2.71018399E-02
    2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03
    3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03
    4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03



    
![png](information-criteria-overview_files/information-criteria-overview_22_1.png)
    





    Text(0, 0.5, 'Information Criteria')




    
![png](information-criteria-overview_files/information-criteria-overview_22_3.png)
    



```python
model.info_values
```




    array([-273.17951619, -309.92282401, -329.01848803, -335.19901621,
           -337.89175092, -337.43773522, -337.22879359, -335.90637716,
           -334.35083001, -332.03452122, -329.3806653 , -326.75797849,
           -324.23491246, -321.43056262, -318.58961187])



LILC also includes spurious terms. Like AIC, it fails to automatically select the correct terms but you could select the right number based on the plot above!

## FPE


```python
basis_function = Polynomial(degree=2)
estimator = LeastSquares()

model = FROLS(
    order_selection=True,
    n_info_values=15,
    ylag=2,
    xlag=2,
    info_criteria="fpe",
    estimator=estimator,
    basis_function=basis_function,
)
model.fit(X=x_train, y=y_train)
yhat = model.predict(X=x_test, y=y_test)
rrse = root_relative_squared_error(y_test, yhat)
print(rrse)

r = pd.DataFrame(
    results(
        model.final_model,
        model.theta,
        model.err,
        model.n_terms,
        err_precision=8,
        dtype="sci",
    ),
    columns=["Regressors", "Parameters", "ERR"],
)
print(r)

xaxis = np.arange(1, model.n_info_values + 1)
plt.plot(xaxis, model.info_values)
plt.xlabel("n_terms")
plt.ylabel("Information Criteria")
```

    0.1697650652880654
           Regressors   Parameters             ERR
    0         x1(k-2)   9.2659E-01  9.41001395E-01
    1          y(k-1)   1.7219E-01  2.71018399E-02
    2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03
    3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03
    4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03
    5        y(k-2)^2  -4.8117E-02  1.02439383E-03
    6         x1(k-1)  -2.4728E-02  6.78305323E-04





    Text(0, 0.5, 'Information Criteria')




    
![png](information-criteria-overview_files/information-criteria-overview_26_2.png)
    



```python
model.info_values
```




    array([-274.05880892, -311.68054386, -331.65290152, -338.70751749,
           -342.27085495, -342.68306863, -343.33508312, -342.86743567,
           -342.15953986, -340.68281499, -338.85950374, -337.05732543,
           -335.3437066 , -333.33668596, -331.27985457])



FPE also failed to automatically select the right number of terms! But, as we pointed out before, Information Criteria is an auxiliary tool! If you look at the plots, all the methods allows you to choose the right numbers of terms!

## Important Note

Here we are dealing with a known model structure! Concerning real data, we do not know the right number of terms so the methods above stands as excellent tools to help you out!

If you check the metrics above, even with the models with more terms, you will see excellent metrics! But System Identification always search for the best model structure! Model Structure Selection is the core of NARMAX methods! In this respect, the examples are to show basic concepts and how the algorithms work!

{"config":{"lang":["en","pt","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"book/0-Preface/","title":"Preface","text":"<p>All the world is a nonlinear system</p> <p>He linearised to the right</p> <p>He linearised to the left</p> <p>Till nothing was right</p> <p>And nothing was left</p> <p>Stephen A. Billings - Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains</p>"},{"location":"book/0-Preface/#nonlinear-system-identification-and-forecasting-theory-and-practice-with-sysidentpy","title":"Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy","text":"<p>Welcome to our companion book on System Identification! This book is a comprehensive approach to learning about dynamic models and forecasting. The main aim of this book is to describe a comprehensive set of algorithms for the identification, forecasting and analysis of nonlinear systems.</p> <p>Our book is specifically designed for those who are interested in learning system identification and forecasting.  We will guide you through the process step-by-step using Python and the SysIdentPy package. With SysIdentPy, you will be able to apply a range of techniques for modeling dynamic systems, making predictions, and exploring different design schemes for dynamic models, from polynomial to neural networks. This book is for graduates, postgraduates, researchers, and for all people from different research areas who have data and want to find models to understand their systems better.</p> <p>The research literature is filled with books and papers covering various aspects of nonlinear system identification, including NARMAX methods. In this book, our objective isn't to replicate all the numerous algorithm variations available. Instead, we want to show you how to model your data using those algorithms with SysIdentPy. We'll mention all the specific details and different versions of the algorithms in the book, so if you're more interested in the theoretical aspects, you can explore those ideas further. We aim to focus on the fundamental techniques, explaining them in straightforward language and showing how to use them in real-world situations. While there will be some math and technical details involved, the aim is to keep it as easy to understand as possible. In essence, this book aims to be a resource that readers from various fields can use to learn how to model dynamic nonlinear systems.</p> <p>The best part about our book is that it is open source material, meaning that it is freely available for anyone to use and contribute to. We hope this brings together people who share interest for system identification and forecasting techniques, from linear to nonlinear models.</p> <p>So, whether you're a student, researcher, data scientist or practitioner, we invite you to share your knowledge and contribute with us. Let\u2019s explore system identification and forecasting with SysIdentPy!</p> <p>To follow along with the Python examples in the book, you\u2019ll need to have some packages installed. We\u2019ll cover the main ones here and let you know if any additional packages are required as we proceed.</p> <pre><code>import sysidentpy\nimport pandas as pd\nimport numpy as np\nimport torch\nimport matplotlib\nimport scipy\n</code></pre>"},{"location":"book/0-Preface/#about-the-author","title":"About the Author","text":"<p>Wilson Rocha is the Head of Data Science at RD Sa\u00fade and the creator of the SysIdentPy library. He holds a degree in Electrical Engineering and a Master's in Systems Modeling and Control, both from Federal University of S\u00e3o Jo\u00e3o del-Rei (UFSJ), Brazil. Wilson began his journey in Machine Learning by developing soccer-playing robots and continues to advance his research in the fields of Multi-objective Nonlinear System Identification and Time Series Forecasting.</p> <p>Connect with Wilson Rocha through the following social networks:</p> <ul> <li>LinkedIn</li> <li>ResearchGate</li> <li>Discord</li> </ul>"},{"location":"book/0-Preface/#referencing-this-book","title":"Referencing This Book","text":"<p>If you find this book useful, please cite it as follows:</p> <pre><code>Lacerda Junior, W.R. (2024). *Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy*. Web version. https://sysidentpy.org\n</code></pre> <p>If you use SysIdentPy on your project, please drop me a line.</p> <p>If you use SysIdentPy on your scientific publication, we would appreciate citations to the following paper: - Lacerda et al., (2020). SysIdentPy: A Python package for System Identification using NARMAX models. Journal of Open Source Software, 5(54), 2384, https://doi.org/10.21105/joss.02384</p> <pre><code>@article{Lacerda2020,\n  doi = {10.21105/joss.02384},\n  url = {https://doi.org/10.21105/joss.02384},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {54},\n  pages = {2384},\n  author = {Wilson Rocha Lacerda Junior and Luan Pascoal Costa da Andrade and Samuel Carlos Pessoa Oliveira and Samir Angelo Milani Martins},\n  title = {SysIdentPy: A Python package for System Identification using NARMAX models},\n  journal = {Journal of Open Source Software}\n}\n</code></pre>"},{"location":"book/0-Preface/#pdf-epub-and-mobi-version","title":"PDF, Epub and Mobi version","text":"<p>Download the pdf version of the book: pdf version</p> <p>Download the epub version of the book: epub version</p> <p>Download the mobi version of the book: mobi version</p>"},{"location":"book/0-Preface/#acknowledgments","title":"Acknowledgments","text":"<p>The System Identification class taught by Samir Martins  (in Portuguese) has been a great source of inspiration for this series. In this book, we will explore Dynamic Systems and learn how to master NARMAX models using Python and the SysIdentPy package. The Stephen A. Billings book, Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio - Temporal Domains, have been instrumental in showing us how powerful System Identification can be.</p> <p>In addition to these resources, we will also reference Luis Ant\u00f4nio Aguirre Introdu\u00e7\u00e3o \u00e0 Identifica\u00e7\u00e3o de Sistemas. T\u00e9cnicas Lineares e n\u00e3o Lineares Aplicadas a Sistemas. Teoria e Aplica\u00e7\u00e3o (in Portuguese), which has proven to be an invaluable tool in introducing complex dynamic modeling concepts in a straightforward way. As an open source material on System Identification and Forecasting, this book aims to provide a accessible yet rigorous approach to learning dynamic models and forecasting.</p>"},{"location":"book/0-Preface/#support-the-project","title":"Support the Project","text":"<p>The Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy is an extensive open-source resource dedicated to the science of System Identification. Our goal is to make this knowledge accessible to everyone, both financially and intellectually.</p> <p>If this book has been valuable to you, and you'd like to support our efforts, we welcome financial contributions through our Sponsor page.</p> <p>If you're not in a position to contribute financially, you can still support by helping us improve the book. We encourage you to report any typos, suggest edits, or provide feedback on sections that you found challenging. You can do this by visiting the book's repository and opening an issue. Additionally, if you enjoyed the content, please consider sharing it with others who might benefit from it, and give us a star on GitHub.</p> <p>Your support, in any form, helps us continue to enhance this project and maintain a high-quality resource for the community. Thank you for your contribution!</p>"},{"location":"book/0.1-Contents/","title":"Contents","text":"<p>Preface</p> <ol> <li>Introduction<ol> <li>Models</li> <li>System Identification</li> <li>Linear or Nonlinear System Identification<ol> <li>Linear Models</li> <li>Nonlinear Models</li> </ol> </li> <li>NARMAX Methods</li> <li>What is the Purpose of System Identification?</li> <li>Is System Identification Machine Learning?</li> <li>Nonlinear System Identification and Forecasting Applications: Case Studies</li> <li>Abbreviations</li> <li>Variables</li> <li>Book Organization</li> </ol> </li> <li>NARMAX Model Representation<ol> <li>Basis Function</li> <li>Linear Models<ol> <li>ARMAX</li> <li>ARX</li> <li>ARMA</li> <li>AR</li> <li>FIR</li> <li>Other Variants</li> </ol> </li> <li>Nonlinear Models<ol> <li>NARMAX</li> <li>NARMA</li> <li>NAR</li> <li>NFIR</li> <li>Mixed NARMAX Models</li> <li>Neural NARX Network</li> <li>General Model Set Representation</li> <li>MIMO Models</li> </ol> </li> </ol> </li> <li>Parameter Estimation<ol> <li>Least Squares</li> <li>Total Least Squares</li> <li>Recursive Least Squares</li> <li>Least Mean Squares</li> <li>Extended Least Squares Algorithms</li> </ol> </li> <li>Model Structure Selection<ol> <li>Introduction</li> <li>The Forward Regression Orthogonal Least Squares<ol> <li>Case Study</li> </ol> </li> <li>Information Criteria<ol> <li>Overview of the Information Criteria Methods<ol> <li>AIC</li> <li>AICc</li> <li>BIC</li> <li>LILC</li> <li>FPE</li> </ol> </li> </ol> </li> <li>Meta Model Structure Selection (MetaMSS)<ol> <li>Meta-heuristics</li> <li>Standard Particle Swarm Optimization (PSO)</li> <li>Standard Gravitational Search Algorithm (GSA)</li> <li>The Binary Hybrid Optimization Algorithm</li> <li>Meta-Model Structure Selection (MetaMSS): Building NARX for Regression</li> <li>Case Studies: Simulation Results</li> <li>MetaMSS vs FROLS</li> <li>Meta-MSS vs RJMCMC</li> <li>MetaMSS algorithm using SysIdentPy</li> </ol> </li> <li>Accelerated Orthogonal Least Squares</li> <li>Entropic Regression</li> </ol> </li> <li>Multiobjective Parameter Estimation<ol> <li>Introduction</li> <li>Multi-objective optimization problem</li> <li>Pareto Optimal Definition and Pareto Dominance</li> <li>Affine Information Least Squares Algorithm</li> <li>Case Study - Buck converter</li> </ol> </li> <li>Multiobjective Model Structure Selection<ol> <li>Introduction</li> <li>Multiobjective Error Reduction Ratio</li> <li>Multiobjective Meta Model Structure Selection</li> <li>Case Studies</li> <li>References</li> </ol> </li> <li>NARX Neural Network<ol> <li>Introduction</li> <li>NARX Neural Network</li> <li>NARX Neural Network vs. Recursive Neural Network</li> <li>Case Studies</li> <li>References</li> </ol> </li> <li>Severely Nonlinear Systems<ol> <li>Introduction</li> <li>Modeling Hysteresis With Polynomial NARX Model</li> <li>Continuous-time loading-unloading quasi-static signal</li> <li>Hysteresis loops in continuous time \\(\\mathcal{H}_t(\\omega)\\)</li> <li>Rate Independent Hysteresis  in polynomial NARX model</li> </ol> </li> <li>Validation<ol> <li>The <code>predict</code> Method in SysIdentPy</li> <li>Infinity-Step-Ahead Prediction</li> <li>One-step Ahead Prediction</li> <li>n-step Ahead Prediction</li> <li>Model Performance</li> <li>Metrics Available in SysIdentPy</li> <li>Case study</li> </ol> </li> <li>Case Studies: System Identification and Forecasting<ol> <li>M4 Dataset</li> <li>Coupled Eletric Device</li> <li>Wiener-Hammerstein</li> <li>Air Passenger Demand Forecasting</li> <li>System With Hysteresis - Modeling a Magneto-rheological Damper Device</li> <li>Silver box</li> <li>F-16 Ground Vibration Test Benchmark</li> <li>PV Forecasting</li> <li>Industrial Robot Identification Benchmark (coming soon)</li> <li>Two-Story Frame with Hysteretic Links (coming soon)</li> <li>Cortical Responses Evoked by Wrist Joint Manipulation (coming soon)</li> <li>Total quarterly beer production in Australia (coming soon)</li> <li>Australian Domestic Tourism Demand (coming soon)</li> <li>Electric Power Consumption (coming soon)</li> <li>Gas Rate CO2 (coming soon)</li> <li>Number of Patients Seen With Influenza-like Illness (coming soon)</li> <li>Monthly Sales of Heaters and Ice Cream (coming soon)</li> <li>Monthly Production of Milk (coming soon)</li> <li>Half-hourly Electricity Demand in England and Wales (coming soon)</li> <li>Daily Temperature in Melbourne (coming soon)</li> <li>Weekly U.S. Product Supplied of Finished Motor Gasoline (coming soon)</li> <li>Australian Total Wine Sales (coming soon)</li> <li>Quarterly Production of Woollen Yarn in Australia (coming soon)</li> <li>Hourly Nuclear Energy Generation (coming soon)</li> </ol> </li> </ol>"},{"location":"book/1-Introduction/","title":"Introduction","text":"<p>The concept of a mathematical model is fundamental in many fields of science. From engineering to sociology, models plays a central role to the study of complex systems as they allow to simulate what will happen in different scenarios and conditions, predict its output for a given input, analyse its properties and explore different design schemes. To accomplish these goals, however, it is crucial that the model is a proper representation of the system under study. The modeling of dynamic and steady-state behaviors is, therefore, fundamental to this type of analysis and depends on System Identification (SI) procedures.</p>"},{"location":"book/1-Introduction/#models","title":"Models","text":"<p>Mathematical modeling is a great way to understand and analyze different parts of our world. It gives us a clear framework to make sense of complex systems and how they behave. Whether it\u2019s for everyday tasks or big-picture issues like disease control, models are a key part of how we deal with various challenges.</p> <p>Typing efficiently on a conventional QWERTY keyboard layout is the result of a well-learned model of the QWERTY keyboard embedded in the individual cognitive processes. However, if you are faced with a different keyboard layout, such as the Dvorak or AZERTY, you will probably struggle to adapt to the new model. The system changed, so you will have to update you model.</p> <p></p> <p>QWERTY - Wikipedia - ANSI\u00a0QWERTY\u00a0keyboard layout\u00a0(US)</p> <p></p> <p>AZERTY layout used on a keyboard</p> <p>Mathematical modeling touches on many parts of our lives. Whether we're looking at economic trends, tracking how diseases spread, or figuring out consumer behavior, models are essential tools for gaining knowledge, making informed decisions, and take control over complex systems.</p> <p>In essence, mathematical models help us make sense of the world. They let us understand human behavior and the systems we deal with every day. By using these models, we can learn, adapt, and adjust our strategies to keep up with the surrounding changes.</p>"},{"location":"book/1-Introduction/#system-identification","title":"System Identification","text":"<p>System identification is a data-driven framework to model dynamical systems. Initially, scientists focused on linear system identification, but this has been changing over the past decades with more emphasis in nonlinear systems. Nonlinear system identification is widely considered to be one of the most important topics concerning the modeling of many different dynamical systems, from time-series to severally nonlinear dynamic behaviors.</p> <p>Extensive resources, including excellent textbooks covering linear system identification and time series forecasting are readily available. In this book, we revisit some known topics, but we also try to approach such subjects in a different and complementary way. We will explore the modeling of nonlinear dynamic systems  using NARMAX(Nonlinear AutoRegressive Moving Average model with eXogenous inputs) methods, which were introduced by [Stephen A. Billings] in 1981.</p>"},{"location":"book/1-Introduction/#linear-or-nonlinear-system-identification","title":"Linear or Nonlinear System Identification","text":""},{"location":"book/1-Introduction/#linear-models","title":"Linear Models","text":"<p>While most real world systems are nonlinear, you probably should give linear models a try first. Linear models usually serves as a strong baseline and can be good enough for your case, giving satisfactory performance. Astron and Murray and Glad and Ljung showed that many nonlinear systems can be well described by locally linear models. Besides, linear models are easy to fit, easy to interpret, and requires less computational resources than nonlinear models, allowing you to experiment fast and gather insights before thinking about gray box models or complex nonlinear models.</p> <p>Linear models can be very useful, even in the presence of strong nonlinearities, because it is much easier to deal with it. Moreover, the development of linear identification algorithms is still a very active and healthy research field, with many papers being released every year Sai Li, Linjun Zhang, T. Tony Cai &amp; Hongzhe Li, Maria Jaenada, Leandro Pardo, Xing Liu;\u00a0Lin Qiu, Youtong Fang;\u00a0Kui Wang;\u00a0Yongdong Li, Jose Rodr\u00edguez, Alessandro D\u2019Innocenzo and Francesco Smarra. Linear models work well most of the time and should be the first choice for many applications. However, when dealing with complex systems where linear assumptions don\u2019t hold, nonlinear models become essential.</p>"},{"location":"book/1-Introduction/#nonlinear-models","title":"Nonlinear Models","text":"<p>When linear models do not perform well enough, you should consider nonlinear models. It's important to notice, however, that changing from a linear to a nonlinear model is not always a simple task. For inexperienced users, it's common to build nonlinear models that performs worse than the linear ones. To work with nonlinear models, you must consider that characteristics such structural errors, noise, operation point, excitation signals and many others aspects of your system under study impact your modelling approach and strategy.</p> <p>As suggested by Johan Schoukens and Lennart Ljung in \"Nonlinear System Identi\ufb01cation - A User-Oriented Roadmap\", only start working with nonlinear models if there is enough evidence that linear models will not solve the problem.</p> <p>Nonlinear models are more flexible than linear models and can be built using many different mathematical representations, such as polynomial, generalized additive, neural networks, wavelet and many more (Billings, S. A.). Such flexibility, however, makes nonlinear system identification much more complex the linear ones, from the experiment design to the model selection. The user should consider that, besides the modeling complexity, moving to nonlinear models will require a revision in the road-map and computational resources defined when dealing with the linear models. In this respect, always ask yourself whether the potential benefits of nonlinear models are worth the effort.</p>"},{"location":"book/1-Introduction/#narmax-methods","title":"NARMAX Methods","text":"<p>NARMAX model is one of the most frequently employed nonlinear model representation and is widely used to represent a broad class of nonlinear systems. NARMAX methods were successfully applied in many scenarios, which include industrial processes, control systems, structural systems, economic and financial systems, biology, medicine, social systems, and much more. The NARMAX model representation and the class of systems that can be represented by it will be discussed later in the book.</p> <p>The main steps involved to build NARMAX models are (Billings, 2013):</p> <ol> <li>Model Representation: define the model mathematical representation.</li> <li>Model Structure Selection: define which terms are in the final model.</li> <li>Parameter Estimation: estimate the coefficients of each model term selected in step 1.</li> <li>Model validation: to make sure the model is unbiased and accurate;</li> <li>Model Prediction/Simulation: predict future outputs or simulate the behavior of the system given different inputs.</li> <li>Analysis: understanding the dynamical properties of the system under study</li> <li>Control: develop control design schemes based on the obtained model.</li> </ol> <p>Model Structure Selection (MSS) is the most important aspect of NARMAX methods and the most complex. Selecting the model terms is fundamental if the goal of the identification is to obtain models that can reproduce the dynamics of the original system and impacts every other aspect of the identification process. Problems related to over parameterization and numerical ill-conditioning are typical because of the limitations the identification algorithms in selecting the appropriate terms that should compose the final model (L. A. Aguirre e S. A. Billings, L. Piroddi e W. Spinelli).</p> <p>In SysIdentPy, you are allowed to interact directly with every item described in the 7 steps, except for the control one. SysIdentPy focuses on modeling, not on control design. You'll have to use some of the code bellow in every modeling task using SysIdentPy. You'll learn the details along the book, so don't worry if you are not familiar with those methods yet.</p> <pre><code>from sysidentpy.basis_function import Polynomial\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.general_estimators import NARX\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.simulation import SimulateNARMAX\n</code></pre>"},{"location":"book/1-Introduction/#what-is-the-purpose-of-system-identification","title":"What is the Purpose of System Identification?","text":"<p>Because of the Model Structure Selection problem, Billings, S. A. states that the goal of System Identification using NARMAX  methods is twofold: performance and parsimony.</p> <p>The first goal is often about approximation. Here, the main focus is to build a model that make predictions with with the lowest error possible. This approach is common in applications like weather forecasting, demand forecasting, predicting stock prices, speech recognition, target tracking, and pattern classification. In these cases, the specific form of the model isn't as critical. In other words, how the terms interact (in parametric models), the mathematical representation, the static behavior and so on are not that important; what matters most is finding a way to minimize prediction errors.</p> <p>But system identification isn't just about minimizing prediction errors. One of the main goals of System Identification is to build models that help the user to understand and interpret the system being modeled. Beyond just making accurate predictions, the goal is to develop models that truly capture the dynamic behavior of the system being studied, ideally in the simplest form possible. Science and engineering are all about understanding systems by breaking down complex behaviors into simpler ones that we can understand and control. For example, if the system's behavior can be described by a simple first-order dynamic model with a cubic nonlinear term in the input, system identification should help uncover that.</p>"},{"location":"book/1-Introduction/#is-system-identification-machine-learning","title":"Is System Identification Machine Learning?","text":"<p>First, let's take an overview of static and dynamic systems. Imagine you have an electric guitar connected to an effect processor that can apply various audio effects, such as reverb or distortion. The effect is controlled by a switch that toggles between \"on\" and \"off\". Let\u2019s consider this from the perspective of signals. The input signal represents the state of the effect switch: switch off (low level), switch on (high level). If we represent the guitar signal, we have a binary condition: effect off (original guitar sound), effect on (modified guitar sound). This is an example of a static system: the output (guitar sound) directly follows the input (state of the effect switch).</p> <p>When the effect switch is off, the output is just the clean, unaltered guitar signal. When the effect switch is on, the output is the guitar signal with the effect applied, such as amplification or distortion. In this system, the effect being on or off directly influences the guitar signal without any delay or additional processing.</p> <p>This example illustrates how a static system operates with binary control inputs, where the output directly reflects the input state, providing a straightforward mapping between the control signal and the system\u2019s response.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\nt = np.linspace(0, 30, 500, endpoint=False)\nu = signal.square(0.2*np.pi * t)\nu[u &lt; 0] = 0\n# In a static system, the output y directly follows the input u\ny = u\n\n# Plot the input and output\nplt.figure(figsize=(15, 3))\nplt.plot(t, u, label='Input (State of the Switch)', color=\"grey\", linewidth=10, alpha=0.5)\nplt.plot(t, y, label='Output (Static System Response)', color='k', linewidth=0.5)\nplt.title('Static System Response to the Input')\nplt.xlabel('Time [s]')\nplt.ylabel('y')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Static response representation. The input signal representing the state of the switch (switch off (low level), switch on (high level)), and the static response: original sound (low level), processed sound (high level).</p> <p>Now, let\u2019s consider a dynamic system: using an air conditioner to lower the room temperature. This example effectively illustrates the concepts of dynamic systems and how their output responds over time.</p> <p>Let\u2019s imagine this from the perspective of signals. The input signal represents the state of the air conditioner's control: turning the air conditioner on (high level) or turning it off (low level). When the air conditioner is turned on, it begins to cool the room. However, the room temperature does not drop instantaneously to the desired cooler level. It takes time for the air conditioner to affect the temperature, and the rate at which the temperature decreases can vary based on factors like the room size and insulation.</p> <p>Conversely, when the air conditioner is turned off, the room temperature does not immediately return to its original ambient temperature. Instead, it gradually warms up as the cooling effect diminishes.</p> <p></p> <p>Using an air conditioner to lower the room temperature as dynamic system representation.</p> <p>In this dynamic system, the output (room temperature) does not instantly follow the input (state of the air conditioner) because there is a time lag involved in both cooling and warming processes. The system has memory, meaning the current room temperature depends not only on the current state of the air conditioner but also on how long it has been running or off, and how much it has already cooled or allowed the room to warm up.</p> <p>This example highlights the nature of dynamic systems: the response to an input is gradual and affected by the system\u2019s internal dynamics. The air conditioner\u2019s effect on the room temperature exemplifies how dynamic systems have a time-dependent response, where the output changes over time and does not immediately match the input signal.</p> <p>For static systems, the output is a direct function of the input, represented by an algebraic equation:</p> \\[ y(t) = G \\cdot u(t) \\] <p>For dynamic systems, the output depends on the input and the rate of change of the input, represented by a differential equation. For example, the output \\(y(t)\\) can be modeled as:</p> \\[ y(t) = G \\cdot u(t) - \\tau \\cdot \\frac{dy(t)}{dt} \\] <p>Here, \\(G\\) is the gain, and \\(\\tau\\) is a constant that incorporates the system's memory. For discrete-time systems, we consider signals at specific and spaced intervals. The differential equation is discretized, and the derivative is approximated by a finite difference:</p> \\[ y[k] = \\alpha y[k-1] + \\beta u[k] \\] <p>where \\(\\alpha\\) and \\(\\beta\\) are constants that determine the system's response. The z-transform can be used to obtain the transfer function in the z-domain.</p> <p>In summary, static systems are modeled by algebraic equations, while dynamic systems are modeled by differential equations.</p> <p>As Luis Antonio Aguirre states in one of his classes on YouTube (in Portuguese), all physical systems are dynamic, but depending on the timescale, they can be modeled as static for simplification. For example, the transition between the effects on the guitar sound, if taken in seconds (as we did in the example), could be treated as static depending on your analysis. However, the pedal board have components like capacitors, which are dynamic electrical components, making it a dynamic system. The response, however, is so fast that we dealt with it like a static system. Therefore, representing a system as static is a modeling decision.</p> <p>Table 1 shows how this field can be categorized with respect to linear/nonlinear and static/dynamic systems.</p> System Characteristics Linear Model Nonlinear Model Static Linear Regression Machine Learning Dynamic Linear System Identification Nonlinear System Identification &gt; Table 1: Naming conventions in the System Identification field. Adapted from Oliver Nelles"},{"location":"book/1-Introduction/#nonlinear-system-identification-and-forecasting-applications-case-studies","title":"Nonlinear System Identification and Forecasting Applications: Case Studies","text":"<p>There\u2019s a lot of research out there on nonlinear system identification, including NARMAX methods. However, there are a relatively small number of books and papers showing how to apply these methods to real-life systems Instead in a way that\u2019s easy to understand. Our goal with this book is to change that. We want to make these methods practical and accessible. While we\u2019ll cover the necessary math and algorithms, we\u2019ll keep things as clear and simple as possible, making it easier for readers from all backgrounds to learn how to model dynamic nonlinear systems using SysIdentPy.</p> <p>Therefore, this book aims to fill a gap in the existing literature. In Chapter 10, we present real-world case studies to show how NARMAX methods can be applied to a variety of complex systems. Whether it\u2019s modeling a highly nonlinear system like the Bouc-Wen model, modeling a dynamic behavior in a full-scale F-16 aircraft, or working with the M4 dataset for benchmarking, we\u2019ll guide you through building NARMAX models using SysIdentPy.</p> <p>The case studies we've selected come from a wide range of fields, not just the typical timeseries or industrial examples you might expect from traditional system identification or timeseries books. Our aim is to showcase the versatility of NARMAX algorithms and SysIdentPy and illustrate the kind of in-depth analysis you can achieve with these tools.</p>"},{"location":"book/1-Introduction/#abbreviations","title":"Abbreviations","text":"Abbreviation Full Name AIC Akaike Information Criterion AICC Corrected Akaike Information Criterion AOLS Accelerated Orthogonal Least Squares ANN Artificial Neural Network AR AutoRegressive ARMAX AutoRegressive Moving Average with eXogenous Input ARARX AutoRegressive AutoRegressive with eXogenous Input ARX AutoRegressive with eXogenous Input BIC Bayesian Information Criterion ELS Extended Least Squares ER Entropic Regression ERR Error Reduction Ratio FIR Finite Impulse Response FPE Final Prediction Error FROLS Forward Regression Orthogonal Least Squares GLS Generalized Least Squares LMS Least Mean Square LS Least Squares LSTM Long Short-Term Memory MA Moving Average MetaMSS Meta Model Structure Selection MIMO Multiple Input Multiple Output MISO Multiple Input Single Output MLP Multilayer Perceptron MSE Mean Squared Error MSS Model Structure Selection NARMAX Nonlinear AutoRegressive Moving Average with eXogenous Input NARX Nonlinear AutoRegressive with eXogenous Input NFIR Nonlinear Finite Impulse Response NIIR Nonlinear Infinite Impulse Response NLS Nonlinear Least Squares NN Neural Network OBF Orthonormal Basis Function OE Output Error OLS Orthogonal Least Squares RBF Radial Basis Function RELS Recursive Extended Least Squares RLS Recursive Least Squares RMSE Root Mean Squared Error SI System Identification SISO Single Input Single Output SVD Singular Value Decomposition WLS Weighted Least Squares"},{"location":"book/1-Introduction/#variables","title":"Variables","text":"Variable Name Description \\(f(\\cdot)\\) function to be approximated \\(k\\) discrete time \\(m\\) dynamic order \\(x\\) system inputs \\(y\\) system output \\(\\hat{y}\\) model predicted output \\(\\lambda\\) regularization strength \\(\\sigma\\) standard deviation \\(\\theta\\) parameter vector \\(N\\) number of data points \\(\\Psi(\\cdot)\\) Information Matrix \\(n_{m^r}\\) Number of potential regressors for MIMO models \\(\\mathcal{F}\\) Arbitrary mathematical representation \\(\\Omega_{y^p x^m}\\) Term cluster of polynomial NARX \\(\\ell\\) nonlinearity degree of the model \\(\\hat{\\Theta}\\) Estimated Parameter Vector \\(\\hat{y}_k\\) model predicted output at discrete time \\(k\\) \\(\\mathbf{X}_k\\) Column vector of multiple system inputs at discrete-time \\(k\\) \\(\\mathbf{Y}_k\\) Column vector of multiple system outputs at discrete-time \\(k\\) \\(\\mathcal{H}_t(\\omega)\\) Hysteresis loop of the system in continuous-time \\(\\mathcal{H}\\) Bounding structure that delimits the system hysteresis loop \\(\\rho\\) Tolerance value \\(\\sum_{y^p x^m}\\) Cluster coefficients of polynomial NARX \\(e_k\\) error vector at discrete-time \\(k\\) \\(n_r\\) Number of potential regressors for SISO models \\(n_x\\) maximum lag of the input regressor \\(n_y\\) maximum lag of the output regressor \\(n\\) number of observations in a sample \\(x_k\\) system input at discrete-time \\(k\\) \\(y_k\\) system output at discrete-time \\(k\\)"},{"location":"book/1-Introduction/#book-organization","title":"Book Organization","text":"<p>This book focuses on making concepts easy to understand, emphasizing clear explanations and practical connections between different methods. We avoid excessive formalism and complex equations, opting instead to illustrate core ideas with plenty of hands-on examples. Written with a System Identification perspective, the book offers practical implementation details throughout the chapters.</p> <p>The goals of this book are to help you:</p> <ul> <li>Understand the advantages, drawbacks, and areas of application of different NARMAX models and algorithms.</li> <li>Choose the right approach for your specific problem.</li> <li>Adjust all hyperparameters properly.</li> <li>Interpret and comprehend the obtained results.</li> <li>valuate the reliability and limitations of your models.</li> </ul> <p>Many chapters include real-world examples and data, guiding you on how to apply these methods using SysIdentPy in practice.</p>"},{"location":"book/10-Case-Studies/","title":"10. Case Studies","text":""},{"location":"book/10-Case-Studies/#m4-dataset","title":"M4 Dataset","text":"<p>The M4 dataset is a well known resource for time series forecasting, offering a wide range of data series used to test and improve forecasting methods. Created for the M4 competition organized by Spyros Makridakis, this dataset has driven many advancements in forecasting techniques.</p> <p>The M4 dataset includes 100,000 time series from various fields such as demographics, finance, industry, macroeconomics, and microeconomics, which were selected randomly from the ForeDeCk database. The series come in different frequencies (yearly, quarterly, monthly, weekly, daily, and hourly), making it a comprehensive collection for testing forecasting methods.</p> <p>In this case study, we will focus on the hourly subset of the M4 dataset. This subset consists of time series data recorded hourly, providing a detailed and high-frequency look at changes over time. Hourly data presents unique challenges due to its granularity and the potential for capturing short-term fluctuations and patterns.</p> <p>The M4 dataset provides a standard benchmark to compare different forecasting methods, allowing researchers and practitioners to evaluate their models consistently. With series from various domains and frequencies, the M4 dataset represents real-world forecasting challenges, making it valuable for developing robust forecasting techniques. The competition and the dataset itself have led to the creation of new algorithms and methods, significantly improving forecasting accuracy and reliability.</p> <p>We will present a end to end walkthrough using the M4 hourly dataset to demonstrate the capabilities of SysIdentPy. SysIdentPy offers a range of tools and techniques designed to effectively handle the complexities of time series data, but we will focus on fast and easy setup for this case. We will cover model selection and evaluation metrics specific to the hourly dataset.</p> <p>By the end of this case study, you will have a solid understanding of how to use SysIdentPy for forecasting with the M4 hourly dataset, preparing you to tackle similar forecasting challenges in real-world scenarios.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\ndatasetsforecast==0.0.8\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\ns3fs==2024.6.1\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import SettingWithCopyWarning\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error, symmetric_mean_absolute_percentage_error\nfrom sysidentpy.utils.plotting import plot_results\n\nfrom datasetsforecast.m4 import M4, M4Evaluation\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n</code></pre> <p>The following plots provide a visualization of the training data for a small subset of the time series. The plot shows the raw data, giving you an insight into the patterns and behaviors inherent in each series.</p> <p>By observing the data, you can get a sense of the variety and complexity of the time series we are working with. The plots can reveal important characteristics such as trends, seasonal patterns, and potential anomalies within the time series. Understanding these elements is crucial for the development of accurate forecasting models.</p> <p>However, when dealing with a large number of different time series, it is common to start with broad assumptions rather than detailed individual analysis. In this context, we will adopt a similar approach. Instead of going into the specifics of each dataset, we will make some general assumptions and see how SysIdentPy handles them.</p> <p>This approach provides a practical starting point, demonstrating how SysIdentPy can manage different types of time series data without too much work. As you become more familiar with the tool, you can refine your models with more detailed insights. For now, let's focus on using SysIdentPy to create the forecasts based on these initial assumptions.</p> <p>Our first assumption is that there is a 24-hour seasonal pattern in the series. By examining the plots below, this seems reasonable. Therefore, we'll begin building our models with <code>ylag=24</code>.</p> <pre><code>ax = train[train[\"unique_id\"]==\"H10\"].reset_index(drop=True)[\"y\"].plot(figsize=(15, 2), title=\"H10\")\nxcoords = [a for a in range(24, 24*30, 24)]\n\nfor xc in xcoords:\n\u00a0 \u00a0 plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Let's check build a model for the <code>H20</code> group before we extrapolate the settings for every group. Because there are no input features, we will be using a <code>NAR</code> model type in SysIdentPy. To keep things simple and fast, we will start with Polynomial basis function with degree \\(1\\).</p> <pre><code>unique_id = \"H20\"\ny_id = train[train[\"unique_id\"]==unique_id][\"y\"].values.reshape(-1, 1)\ny_val = test[test[\"unique_id\"]==unique_id][\"y_test\"].values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    order_selection=True,\n    ylag=24,\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\n\nmodel.fit(y=y_id)\ny_val = np.concatenate([y_id[-model.max_lag :], y_val])\ny_hat = model.predict(y=y_val, forecast_horizon=48)\nsmape = symmetric_mean_absolute_percentage_error(y_val[model.max_lag::], y_hat[model.max_lag::])\n\nplot_results(y=y_val[model.max_lag :], yhat=y_hat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Group: {unique_id} - SMAPE {round(smape, 4)}\")\n</code></pre> <p></p> <p>Probably, the result are not optimal and will not work for every group. However, let's check how this setting performs against the winner model M4 time series competition: the Exponential Smoothing with Recurrent Neural Networks (ESRNN).</p> <pre><code>esrnn_url = 'https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-118.zip'\nesrnn_forecasts = M4Evaluation.load_benchmark('data', 'Hourly', esrnn_url)\nesrnn_evaluation = M4Evaluation.evaluate('data', 'Hourly', esrnn_forecasts)\n\nesrnn_evaluation\n</code></pre> SMAPE MASE OWA Hourly 9.328 0.893 0.440 &gt; Table 1. ESRNN SOTA results <p>The following code took only 49 seconds to run on my machine (AMD Ryzen 5 5600x processor, 32GB RAM at 3600MHz). Because of its efficiency, I didn't create a parallel version. By the end of this use case, you will see how SysIdentPy can be both fast and effective, delivering good results without too much optimization.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n\u00a0 \u00a0 y_id = data[\"y\"].values.reshape(-1, 1)\n\u00a0 \u00a0 basis_function = Polynomial(degree=1)\n\u00a0 \u00a0 model = FROLS(\n        ylag=24,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        n_info_values=25,\n    )\n\u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 model.fit(y=y_id)\n\u00a0 \u00a0 \u00a0 \u00a0 y_val = y_id[-model.max_lag :].reshape(-1, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 y_hat = model.predict(y=y_val, forecast_horizon=48)\n\u00a0 \u00a0 \u00a0 \u00a0 r.append(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u_id*len(y_hat[model.max_lag::]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ds_test,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y_hat[model.max_lag::].ravel()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 except Exception:\n\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(['unique_id', 'ds', 'NARMAX_1'])\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)#.clip(lower=10)\npivot_df = results_1.pivot(index='unique_id', columns='ds', values='NARMAX_1')\nresults = pivot_df.to_numpy()\n\nM4Evaluation.evaluate('data', 'Hourly', results)\n</code></pre> SMAPE MASE OWA Hourly 16.034196 0.958083 0.636132 Table 2. First test with SysIdentPy <p>The initial results are reasonable, but they don't quite match the performance of <code>ESRNN</code>. These results are based solely on our first assumption. To better understand the performance, let\u2019s examine the groups with the worst results.</p> <p></p> <p>The following plot illustrates two such groups, <code>H147</code> and <code>H136</code>. Both exhibit a 24-hour seasonal pattern.</p> <p></p> <p></p> <p>However, a closer look reveals an additional insight: in addition to the daily pattern, these series also show a weekly pattern. Observe how the data looks like when we split the series into weekly segments.</p> <p></p> <pre><code>xcoords = list(range(0, 168*5, 168))\nfiltered_train = train[train[\"unique_id\"] == \"H147\"].reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(10, 1.5 * len(xcoords[1:])))\nfor i, start in enumerate(xcoords[:-1]):\n\u00a0 \u00a0 end = xcoords[i + 1]\n\u00a0 \u00a0 ax = fig.add_subplot(len(xcoords[1:]), 1, i + 1)\n\u00a0 \u00a0 filtered_train[\"y\"].iloc[start:end].plot(ax=ax)\n\u00a0 \u00a0 ax.set_title(f'H147 -&gt; Slice {i+1}: Hour {start} to {end-1}')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Therefore, we will build models setting <code>ylag=168</code>.</p> <p>Note that this is a very high number for lags, so be careful if you want to try it with higher polynomial degrees because the time to run the models can increase significantly. I tried some configurations with polynomial degree equal to 2 and only took \\(6\\) minutes to run (even less, using <code>AOLS</code>), without making the code run in parallel. As you can see, SysIdentPy can be very fast, and you can make it faster by applying parallelization.</p> <pre><code># this took 2min to run on my computer.\nr = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n\u00a0 \u00a0 y_id = data[\"y\"].values.reshape(-1, 1)\n\u00a0 \u00a0 basis_function = Polynomial(degree=1)\n\u00a0 \u00a0 model = FROLS(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ylag=168,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_type=\"NAR\",\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 model.fit(y=y_id)\n\u00a0 \u00a0 \u00a0 \u00a0 y_val = y_id[-model.max_lag :].reshape(-1, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 y_hat = model.predict(y=y_val, forecast_horizon=48)\n\u00a0 \u00a0 \u00a0 \u00a0 r.append(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u_id*len(y_hat[model.max_lag::]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ds_test,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y_hat[model.max_lag::].ravel()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 except Exception:\n\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(['unique_id', 'ds', 'NARMAX_1'])\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)#.clip(lower=10)\npivot_df = results_1.pivot(index='unique_id', columns='ds', values='NARMAX_1')\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate('data', 'Hourly', results)\n</code></pre> SMAPE MASE OWA Hourly 10.475998 0.773749 0.446471 &gt; Table 3. Improved results using SysIdentPy <p>Now, the results are much closer to those of the <code>ESRNN</code> model! While the Symmetric Mean Absolute Percentage Error (<code>SMAPE</code>) is slightly worse, the Mean Absolute Scaled Error (<code>MASE</code>) is better when comparing against <code>ESRNN</code>, leading to a very similar Overall Weighted Average (<code>OWA</code>) metric. Remarkably, these results are achieved using only simple <code>AR</code> models. Next, let's see if the <code>AOLS</code> method can provide even better results.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n\u00a0 \u00a0 y_id = data[\"y\"].values.reshape(-1, 1)\n\u00a0 \u00a0 basis_function = Polynomial(degree=1)\n\u00a0 \u00a0 model = AOLS(\n        ylag=168,\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        # due to high lag settings, k was increased to 6 as an initial guess\n        k=6,\n    )\n\u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 model.fit(y=y_id)\n\u00a0 \u00a0 \u00a0 \u00a0 y_val = y_id[-model.max_lag :].reshape(-1, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 y_hat = model.predict(y=y_val, forecast_horizon=48)\n\u00a0 \u00a0 \u00a0 \u00a0 r.append(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u_id*len(y_hat[model.max_lag::]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ds_test,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y_hat[model.max_lag::].ravel()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 except Exception:\n\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(['unique_id', 'ds', 'NARMAX_1'])\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)#.clip(lower=10)\npivot_df = results_1.pivot(index='unique_id', columns='ds', values='NARMAX_1')\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate('data', 'Hourly', results)\n</code></pre> SMAPE MASE OWA Hourly 9.951141 0.809965 0.439755 &gt; Table 4. SysIdentPy results using AOLS algorithm <p>The Overall Weighted Average (<code>OWA</code>) is even better than that of the <code>ESRNN</code> model! Additionally, the <code>AOLS</code> method was incredibly efficient, taking only 6 seconds to run. This combination of high performance and rapid execution makes <code>AOLS</code> a compelling alternative for time series forecasting in cases with multiple series.</p> <p>Before we finish, let's verify how the performance of the <code>H147</code> model has improved with the <code>ylag=168</code> setting.</p> <p></p> <p>Based on the M4 benchmark paper, we could also clip the predictions lower than 10 to 10 and the results would be slightly better. But this is left to the user.</p> <p>We could achieve even better performance with some fine-tuning of the model configuration. However, I\u2019ll leave exploring these alternative adjustments as an exercise for the user. However, keep in mind that experimenting with different settings does not always guarantee improved results. A deeper theoretical knowledge can often lead you to better configurations and, hence, better results.</p>"},{"location":"book/10-Case-Studies/#coupled-eletric-device","title":"Coupled Eletric Device","text":"<p>The CE8 coupled electric drives dataset - Nonlinear Benchmark presents a compelling use case for demonstrating the performance of SysIdentPy. This system involves two electric motors driving a pulley with a flexible belt, creating a dynamic environment ideal for testing system identification tools.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p>"},{"location":"book/10-Case-Studies/#system-overview","title":"System Overview","text":"<p>The CE8 system, illustrated in Figure 1, features: - Two Electric Motors: These motors independently control the tension and speed of the belt, providing symmetrical control around zero. This enables both clockwise and counterclockwise movements. - Pulley Mechanism: The pulley is supported by a spring, introducing a lightly damped dynamic mode that adds complexity to the system. - Speed Control Focus: The primary focus is on the speed control system. The pulley\u2019s angular speed is measured using a pulse counter, which is insensitive to the direction of the velocity.</p> <p></p> <p>Figure 1. CE8 system design.</p>"},{"location":"book/10-Case-Studies/#sensor-and-filtering","title":"Sensor and Filtering","text":"<p>The measurement process involves: - Pulse Counter: This sensor measures the angular speed of the pulley without regard to the direction. - Analogue Low Pass Filtering: This reduces high-frequency noise, followed by antialiasing filtering to prepare the signal for digital processing. The dynamic effects are mainly influenced by the electric drive time constants and the spring, with the low pass filtering having a minimal impact on the output.</p>"},{"location":"book/10-Case-Studies/#sota-results","title":"SOTA Results","text":"<p>SysIdentPy can be used to build robust models for identifying and modeling the complex dynamics of the CE8 system. The performance will be compared against a benchmark provided by Max D. Champneys, Gerben I. Beintema, Roland T\u00f3th, Maarten Schoukens, and Timothy J. Rogers -\u00a0Baselines for Nonlinear Benchmarks,\u00a0Workshop on Nonlinear System Identification Benchmarks, 2024.</p> <p></p> <p>The benchmark evaluate the average metric between the two experiments. That's why the SOTA method do not have the better metric for <code>test 1</code>, but it is still the best overall.  The goal of this case study is not only to showcase the robustness of SysIdentPy but also provides valuable insights into its practical applications in real-world dynamic systems.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_1","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_1","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the CE8 coupled electric drives dataset. This example showcases the robust performance of SysIdentPy in modeling and identifying complex dynamic systems. The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for CE8 system.</p> <p>This practical example will help users understand how to effectively utilize SysIdentPy for their own system identification tasks, leveraging its advanced features to handle the complexities of real-world dynamic systems. Let's dive into the code and explore the capabilities of SysIdentPy.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.CED(atleast_2d=True)\ndata_train_1, data_train_2 = train_val\ndata_test_1, data_test_2 = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation GerbenBeintema - nonlinear_benchmarks: The official data load for nonlinear benchmark datasets to check the details of how to use it.</p> <p>The following plot detail the training and testing data of both experiments. Here we are trying to get two models, one for each experiment, that have a better performance than the mentioned baselines.</p> <pre><code>plt.plot(data_train_1.u)\nplt.plot(data_train_1.y)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(data_test_1.u)\nplt.plot(data_test_1.y)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(data_train_2.u)\nplt.plot(data_train_2.y)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(data_test_2.u)\nplt.plot(data_test_2.y)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"book/10-Case-Studies/#results","title":"Results","text":"<p>First, we will set the exactly same configuration to built models for both experiments. We can have better models by optimizing the configurations individually, but we will start simple.</p> <p>A basic configuration of FROLS using a polynomial basis function with degree equal 2 is defined. The information criteria will be the default one, the <code>aic</code>. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example.</p> <p>Model for experiment 1:</p> <pre><code>y_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 n_info_values=120\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <p>Model for experiment 2: <pre><code>y_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 n_info_values=120\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre></p> <p></p> <p>The first configuration for experiment 1 is already better than the LTI ARX, LTI SS, GRU, LSTM, MLP NARX, MLP FIR, OLSTM, and the SOTA models shown in the benchmark table. Better than 8 out 11 models shown in the benchmark. For experiment 2, its better than LTI ARX, LTI SS, GRU, RNN, LSTM, OLSTM, and pNARX (7 out 11). It's a good start, but let's check if the performance improves if we set a higher lag for both <code>xlag</code> and <code>ylag</code>.</p> <p>The average metric is \\((0.1131 + 0.1059)/2 = 0.1095\\), which is very good, but worse than the SOTA (\\(0.0945\\)). We will now increase the lags for <code>x</code> and <code>y</code> to check if we get a better model. Before increasing the lags, the information criteria is shown:</p> <p><pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> </p> <p>It can be observed that after 22 regressors, adding new regressors do not improve the model performance (considering the configuration defined for that model). Because we want to try models with higher lags and higher nonlinearity degree, the stopping criteria will be changed to <code>err_tol</code> instead of information criteria. This will made the algorithm runs considerably faster.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=22,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aicc\",\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=22,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <p>In the first experiment, the model showed a slight improvement, while the performance of the second experiment experienced a minor decline. Increasing the lag settings with these configurations did not result in significant changes. Therefore, let's set the polynomial degree to \\(3\\) and increase the number of terms to build the model to <code>n_terms=40</code> if the <code>err_tol</code> is not reached. It's important to note that these values are chosen empirically. We could also adjust the parameter estimation technique, the <code>err_tol</code>, the model structure selection algorithm, and the basis function, among other factors. Users are encouraged to employ hyperparameter tuning techniques to find the optimal combinations of hyperparameters.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aicc\",\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <p>As shown in the plot, we have surpassed the state-of-the-art (SOTA) results with an average metric of \\((0.0969 + 0.0731)/2 = 0.0849\\). Additionally, the metric for the first experiment matches the best model in the benchmark, and the metric for the second experiment slightly exceeds the benchmark's best model. Using the same configuration for both models, we achieved the best overall results!</p>"},{"location":"book/10-Case-Studies/#wiener-hammerstein","title":"Wiener-Hammerstein","text":"<p>The description content primarily derives from the benchmark website - Nonlinear Benchmark and associated paper - Wiener-Hammerstein benchmark with process noise. For a detailed description, readers are referred to the linked references.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p> <p>This benchmark focuses on a Wiener-Hammerstein electronic circuit where process noise plays a significant role in distorting the output signal.</p> <p>The Wiener-Hammerstein structure is a well-known block-oriented system which contains a static nonlinearity sandwiched between two Linear Time-Invariant (LTI) blocks (Figure 2). This arrangement presents a challenging identification problem due to the presence of these LTI blocks.</p> <p></p> <p>Figure 2: the Wiener-Hammerstein system</p> <p>In Figure 2, the Wiener-Hammerstein system is illustrated with process noise \\(e_x(t)\\) entering before the static nonlinearity \\(f(x)\\), sandwiched between LTI blocks represented by \\(R(s)\\) and \\(S(s)\\) at the input and output, respectively. Additionally, small, negligible noise sources \\(e_u(t)\\) and \\(e_y(t)\\) affect the measurement channels. The measured input and output signals are denoted as \\(u_m(t)\\) and \\(y_m(t)\\).</p> <p>The first LTI block \\(R(s)\\) is effectively modeled as a third-order lowpass filter. The second LTI subsystem \\(S(s)\\) is configured as an inverse Chebyshev filter with a stop-band attenuation of \\(40 dB\\) and a cutoff frequency of \\(5 kHz\\). Notably, \\(S(s)\\) includes a transmission zero within the operational frequency range, complicating its inversion.</p> <p>The static nonlinearity \\(f(x)\\) is implemented using a diode-resistor network, resulting in saturation nonlinearity. Process noise \\(e_x(t)\\) is introduced as filtered white Gaussian noise, generated from a discrete-time third-order lowpass Butterworth filter followed by zero-order hold and analog low-pass reconstruction filtering with a cutoff of \\(20 kHz\\).</p> <p>Measurement noise sources \\(e_u(t)\\) and \\(e_y(t)\\) are minimal compared to \\(e_x(t)\\). The system's inputs and process noise are generated using an Arbitrary Waveform Generator (AWG), specifically the Agilent/HP E1445A, sampling at \\(78125 Hz\\), synchronized with an acquisition system (Agilent/HP E1430A) to ensure phase coherence and prevent leakage errors. Buffering between the acquisition cards and the system's inputs and outputs minimizes measurement equipment distortion.</p> <p>The benchmark provides two standard test signals through the benchmarking website: a random phase multi sine and a sine-sweep signal. Both signals have a \\(rms\\) value of \\(0.71 Vrms\\) and cover frequencies from DC to \\(15 kHz\\) (excluding DC). The sine-sweep spans this frequency range at a rate of \\(4.29 MHz/min\\). These test sets serve as targets for evaluating the model's performance, emphasizing accurate representation under varied conditions.</p> <p>The Wiener-Hammerstein benchmark highlights three primary nonlinear system identification challenges:</p> <ol> <li>Process Noise: Significant in the system, influencing output fidelity.</li> <li>Static Nonlinearity: Indirectly accessible from measured data, posing identification challenges.</li> <li>Output Dynamics: Complex inversion due to transmission zero presence in \\(S(s)\\).</li> </ol> <p>The goal of this benchmark is to develop and validate robust models using separate estimation data, ensuring accurate characterization of the Wiener-Hammerstein system's behavior.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_2","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_2","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Wiener-Hammerstein system dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for Wiener-Hammerstein system.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares, BoundedVariableLeastSquares, NonNegativeLeastSquares, LeastSquaresMinimalResidual\n\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.WienerHammerBenchMark(atleast_2d=True)\nx_train, y_train = train_val\nx_test, y_test = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plot_n = 800\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_train[:plot_n])\nplt.plot(y_train[:plot_n])\nplt.title(\"Experiment: training data\")\nplt.legend([\"x_train\", \"y_train\"])\nplt.show()\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_test[:plot_n])\nplt.plot(y_test[:plot_n])\nplt.title(\"Experiment: testing data\")\nplt.legend([\"x_test\", \"y_test\"])\nplt.show()\n</code></pre> <p></p> <p></p> <p>The goal of this benchmark it to get a model that have a better performance than the SOTA model provided in the benchmarking paper.</p> <p></p> <p>State of the art results presented in the benchmarking paper. In this section we are only working with the Wiener-Hammerstein results, which are presented in the \\(W-H\\)  column.</p>"},{"location":"book/10-Case-Studies/#results_1","title":"Results","text":"<p>We will start with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=50</code>. This means the FROLS algorithm will not include all regressors when calculating the information criteria used to determine the model order. While this approach might result in a suboptimal model, it is a reasonable starting point for our first attempt.</p> <pre><code>n = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nrmse_sota = rmse/y_test.std()\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=1000, title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\")\n</code></pre> <p></p> <p>The first configuration is already better than the SOTA models shown in the benchmark table! We started using <code>xlag=ylag=7</code> to have an idea of how well SysIdentPy would handle this dataset, but the results are pretty good already! However, the benchmarking paper indicates  that they used higher lags for their models. Let's check what happens if we set <code>xlag=ylag=10</code>.</p> <pre><code>x_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=10,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nrmse_sota = rmse/y_test.std()\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=1000, title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\")\n</code></pre> <p></p> <p>The performance is even better now! For now, we are not worried about the model complexity (even in this case where we are comparing to a deep state neural network...). However, if we check the model order and the <code>AIC</code> plot, we see that the model have 50 regressors , but the <code>AIC</code> values do not change much after each added regression.</p> <pre><code>plt.plot(model.info_values)\n</code></pre> <p></p> <p>So, what happens if we set a model with half of the regressors?</p> <pre><code>x_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=10,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 n_info_values=50,\n\u00a0 \u00a0 n_terms=25,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nrmse_sota = rmse/y_test.std()\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=1000, title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\")\n</code></pre> <p></p> <p>As shown in the figure above, the results still outperform the SOTA models presented in the benchmarking paper. The SOTA results from the paper could likely be improved as well. Users are encouraged to explore the deepsysid package, which can be used to build deep state neural networks.</p> <p>This basic configuration can serve as a starting point for users to develop even better models using SysIdentPy. Give it a try!</p>"},{"location":"book/10-Case-Studies/#air-passenger-demand-forecasting-a-benchmarking","title":"Air Passenger Demand Forecasting - A Benchmarking","text":"<p>In this case study, we explore the capabilities of SysIdentPy by applying it to the Air Passenger dataset, a classic time series dataset widely used for evaluating time series forecasting methods. The primary goal of this analysis is to demonstrate that SysIdentPy can serve as a strong alternative for time series modeling, rather than to assert that one library is superior to another.</p>"},{"location":"book/10-Case-Studies/#dataset-overview","title":"Dataset Overview","text":"<p>The Air Passenger dataset consists of monthly totals of international airline passengers from 1949 to 1960. This dataset is characterized by its strong seasonal patterns, trend components, and variability, making it an ideal benchmark for evaluating various time series forecasting methods. Specifically, the dataset includes:</p> <ul> <li>Total Monthly Passengers: The number of passengers (in thousands) for each month.</li> <li>Time Period: From January 1949 to December 1960, providing 144 data points.</li> </ul> <p>The dataset exhibits clear seasonal fluctuations and a trend, which poses a significant challenge for forecasting methods. It serves as a well-known benchmark for assessing the performance of different time series models due to its inherent complexity and well-documented behavior.</p>"},{"location":"book/10-Case-Studies/#comparison-with-other-libraries","title":"Comparison with Other Libraries","text":"<p>We will compare the performance of SysIdentPy with other popular time series modeling libraries, focusing on the following tools:</p> <ul> <li>sktime: An extensive library for time series analysis in Python, offering various modeling techniques. For this case study, we will use:</li> <li><code>AutoARIMA</code>: Automatically selects the best ARIMA model based on the data.</li> <li><code>BATS</code> (Bayesian Structural Time Series): A model that captures complex seasonal patterns and trends.</li> <li><code>TBATS</code> (Trigonometric, Box-Cox, ARMA, Trend, and Seasonal): A model designed to handle multiple seasonal patterns.</li> <li><code>Exponential Smoothing</code>: A method that applies weighted averages to forecast future values.</li> <li><code>Prophet</code>: Developed by Facebook, it is particularly effective for capturing seasonality and holiday effects.</li> <li> <p><code>AutoETS</code> (Automatic Exponential Smoothing): Selects the best exponential smoothing model for the data.</p> </li> <li> <p>SysIdentPy: A library designed for system identification and time series modeling. We will focus on:</p> </li> <li><code>MetaMSS</code> (Meta-heuristic Model Structure Selection): Uses metaheuristic algorithms to select the best model structure.</li> <li><code>AOLS</code> (Accelerated Orthogonal Least Squares): A method for selecting relevant regressors in a model.</li> <li><code>FROLS</code> (Forward Regression with Orthogonal Least Squares, using polynomial base functions): A regression technique for model structure selection with polynomial terms.</li> <li><code>NARXNN</code> (Nonlinear Auto-Regressive model with Exogenous Inputs using Neural Networks): A flexible method for modeling nonlinear time series with external inputs.</li> </ul>"},{"location":"book/10-Case-Studies/#objective","title":"Objective","text":"<p>The objective of this case study is to evaluate and compare the performance of these methods on the Air Passenger dataset. We aim to assess how well each library handles the complex seasonal and trend components of the data and to showcase SysIdentPy as a viable option for time series forecasting.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_3","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npystan==2.19.1.1\nholidays==0.11.2\nfbprophet==0.7.1\nneuralprophet==0.2.7\npandas==1.3.2\nnumpy==1.23.3\nmatplotlib==3.8.4\npmdarima==1.8.3\nscikit-learn==0.24.2\nscipy==1.9.1\nsktime==0.8.0\nstatsmodels==0.12.2\ntbats==1.1.0\ntorch==1.12.1\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions. This practice isolates your project\u2019s dependencies and prevents version conflicts with other projects or system-wide packages. Additionally, be aware that some packages, such as <code>sktime</code> and <code>neuralprophet</code>, may install several dependencies automatically during their installation. Setting up a virtual environment helps manage these dependencies more effectively and keeps your project environment clean and reproducible.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul> <p>Let's begin by importing the necessary packages and setting up the environment for this analysis.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.signal.signaltools\n\ndef _centered(arr, newsize):\n\u00a0 \u00a0 # Return the center newsize portion of the array.\n\u00a0 \u00a0 # this is needed due a conflict error using the versions of the packages defined\n\u00a0 \u00a0 # for this example\n\u00a0 \u00a0 newsize = np.asarray(newsize)\n\u00a0 \u00a0 currsize = np.array(arr.shape)\n\u00a0 \u00a0 startind = (currsize - newsize) // 2\n\u00a0 \u00a0 endind = startind + newsize\n\u00a0 \u00a0 myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n\u00a0 \u00a0 return arr[tuple(myslice)]\n\n\nscipy.signal.signaltools._centered = _centered\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom torch import nn\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.ets import AutoETS\nfrom sktime.forecasting.arima import ARIMA, AutoARIMA\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.fbprophet import Prophet\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.bats import BATS\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.performance_metrics.forecasting import mean_squared_error\nfrom sktime.utils.plotting import plot_series\n\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n%matplotlib inline\nloss = mean_squared_error\n</code></pre> <p>We use the <code>sktime</code> method to load the data. Besides, 23 samples is used as test data, following the definitions in the <code>sktime</code> examples.</p> <pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23) \u00a0# 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</code></pre> <p>The following image shows the data of the system to be modeled.</p> <p></p>"},{"location":"book/10-Case-Studies/#results_2","title":"Results","text":"<p>Because we have several different models to test, the results are summarized in the following table. The user you will see that no hyperparameter tuning was made for SysIdentPy model. The idea here is to show how simple it can be to build good models in SysIdentPy.</p> No. Package Mean Squared Error 1 SysIdentPy (Neural Model) 316.54 2 SysIdentPy (MetaMSS) 450.99 3 SysIdentPy (AOLS) 476.64 4 NeuralProphet 501.24 5 SysIdentPy (FROLS) 805.95 6 Exponential Smoothing 910.52 7 Prophet 1186.00 8 AutoArima 1714.47 9 Manual Arima 2085.42 10 ETS 2590.05 11 BATS 7286.64 12 TBATS 7448.43"},{"location":"book/10-Case-Studies/#sysidentpy-frols","title":"SysIdentPy: FROLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 ylag=13, \u00a0# the lags for all models will be 13\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NAR\",\n)\n\nsysidentpy.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\nyhat = sysidentpy.predict(y=y_test, forecast_horizon=23)\nfrols_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(frols_loss)\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n&gt;&gt;&gt; 805.95\n</code></pre>"},{"location":"book/10-Case-Studies/#sysidentpy-aols","title":"SysIdentPy: AOLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\ndf_train, df_test = temporal_train_test_split(y, test_size=23)\ndf_train = df_train.reset_index()\ndf_train.columns = [\"ds\", \"y\"]\ndf_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"].astype(str))\ndf_test = df_test.reset_index()\ndf_test.columns = [\"ds\", \"y\"]\ndf_test[\"ds\"] = pd.to_datetime(df_test[\"ds\"].astype(str))\n\nsysidentpy_AOLS = AOLS(\n\u00a0 \u00a0 ylag=13, k=2, L=1, model_type=\"NAR\", basis_function=basis_function\n)\n\nsysidentpy_AOLS.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\nyhat = sysidentpy_AOLS.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\n\naols_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\n\nprint(aols_loss)\nplot_results(y=y_test[sysidentpy_AOLS.max_lag :], yhat=yhat[sysidentpy_AOLS.max_lag :])\n&gt;&gt;&gt; 476.64\n</code></pre>"},{"location":"book/10-Case-Studies/#sysidentpy-metamss","title":"SysIdentPy: MetaMSS","text":"<pre><code>set_random_seed(42)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nsysidentpy_metamss = MetaMSS(\n\u00a0 \u00a0 basis_function=basis_function, ylag=13, model_type=\"NAR\", test_size=0.17\n)\n\nsysidentpy_metamss.fit(y=y_train)\n\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\nyhat = sysidentpy_metamss.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\n\nmetamss_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\n\nprint(metamss_loss)\nplot_results(\n\u00a0 \u00a0 y=y_test[sysidentpy_metamss.max_lag :], yhat=yhat[sysidentpy_metamss.max_lag :]\n)\n\n&gt;&gt;&gt; 450.99\n</code></pre>"},{"location":"book/10-Case-Studies/#sysidentpy-neural-narx","title":"SysIdentPy: Neural NARX","text":"<p>The network architecture is just the same as the one used in to show how to build a Neural NARX model in SysIdentPy docs.</p> <pre><code>import torch\ntorch.manual_seed(42)\n\ny = load_airline()\n# the split here will use 36 as test size just because the network will use the first values as initial conditions. It could be done like the others methods by concatenating the values\ny_train, y_test = temporal_train_test_split(y, test_size=36)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nx_train = np.zeros_like(y_train)\nx_test = np.zeros_like(y_test)\n\nclass NARX(nn.Module):\n\u00a0 \u00a0 def __init__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin = nn.Linear(13, 20)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin2 = nn.Linear(20, 20)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin3 = nn.Linear(20, 20)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin4 = nn.Linear(20, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 self.relu = nn.ReLU()\n\n\n\u00a0 \u00a0 def forward(self, xb):\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin(xb)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.relu(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin2(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.relu(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin3(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.relu(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin4(z)\n\u00a0 \u00a0 \u00a0 \u00a0 return z\n\nnarx_net = NARXNN(\n\u00a0 \u00a0 net=NARX(),\n\u00a0 \u00a0 ylag=13,\n\u00a0 \u00a0 model_type=\"NAR\",\n\u00a0 \u00a0 basis_function=Polynomial(degree=1),\n\u00a0 \u00a0 epochs=900,\n\u00a0 \u00a0 verbose=False,\n\u00a0 \u00a0 learning_rate=2.5e-02,\n\u00a0 \u00a0 optim_params={}, \u00a0# optional parameters of the optimizer\n)\n\nnarx_net.fit(y=y_train)\nyhat = narx_net.predict(y=y_test, forecast_horizon=23)\n\nnarxnet_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[narx_net.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[narx_net.max_lag :]),\n)\n\nprint(narxnet_loss)\nplot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :])\n</code></pre> <p></p>"},{"location":"book/10-Case-Studies/#sktime-models","title":"sktime models","text":"<p>The following models are the ones available in the sktime package.</p> <pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23) \u00a0# 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-exponential-smoothing","title":"sktime: Exponential Smoothing","text":"<pre><code>es = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nes.fit(y_train)\ny_pred_es = es.predict(fh)\nplot_series(y_test, y_pred_es, labels=[\"y_test\", \"y_pred\"])\nes_loss = loss(y_test, y_pred_es)\nes_loss\n&gt;&gt;&gt; 910.46\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-autoets","title":"sktime: AutoETS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nets = AutoETS(auto=True, sp=12, n_jobs=-1)\nets.fit(y_train)\ny_pred_ets = ets.predict(fh)\nplot_series(y_test, y_pred_ets, labels=[\"y_test\", \"y_pred\"])\nets_loss = loss(y_test, y_pred_ets)\nets_loss\n&gt;&gt;&gt; 1739.11\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-autoarima","title":"sktime: AutoArima","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\n\nauto_arima = AutoARIMA(sp=12, suppress_warnings=True)\nauto_arima.fit(y_train)\ny_pred_auto_arima = auto_arima.predict(fh)\nplot_series(y_test, y_pred_auto_arima, labels=[\"y_test\", \"y_pred\"])\nautoarima_loss = loss(y_test, y_pred_auto_arima)\nautoarima_loss\n&gt;&gt;&gt; 1714.47\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-arima","title":"sktime: Arima","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nmanual_arima = ARIMA(\n\u00a0 \u00a0 order=(13, 1, 0), suppress_warnings=True\n) \u00a0# seasonal_order=(0, 1, 0, 12)\nmanual_arima.fit(y_train)\ny_pred_manual_arima = manual_arima.predict(fh)\nplot_series(y_test, y_pred_manual_arima, labels=[\"y_test\", \"y_pred\"])\nmanualarima_loss = loss(y_test, y_pred_manual_arima)\nmanualarima_loss\n&gt;&gt;&gt; 2085.42\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-bats","title":"sktime: BATS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nbats = BATS(sp=12, use_trend=True, use_box_cox=False)\nbats.fit(y_train)\ny_pred_bats = bats.predict(fh)\nplot_series(y_test, y_pred_bats, labels=[\"y_test\", \"y_pred\"])\nbats_loss = loss(y_test, y_pred_bats)\nbats_loss\n&gt;&gt;&gt; 7286.64\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-tbats","title":"sktime: TBATS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ntbats = TBATS(sp=12, use_trend=True, use_box_cox=False)\ntbats.fit(y_train)\ny_pred_tbats = tbats.predict(fh)\nplot_series(y_test, y_pred_tbats, labels=[\"y_test\", \"y_pred\"])\ntbats_loss = loss(y_test, y_pred_tbats)\ntbats_loss\n&gt;&gt;&gt; 7448.43\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-prophet","title":"sktime: Prophet","text":"<pre><code>set_random_seed(42)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nz = y.copy()\nz = z.to_timestamp(freq=\"M\")\nz_train, z_test = temporal_train_test_split(z, test_size=23)\nprophet = Prophet(\n\u00a0 \u00a0 seasonality_mode=\"multiplicative\",\n\u00a0 \u00a0 n_changepoints=int(len(y_train) / 12),\n\u00a0 \u00a0 add_country_holidays={\"country_name\": \"Germany\"},\n\u00a0 \u00a0 yearly_seasonality=True,\n\u00a0 \u00a0 weekly_seasonality=False,\n\u00a0 \u00a0 daily_seasonality=False,\n)\nprophet.fit(z_train)\ny_pred_prophet = prophet.predict(fh.to_relative(cutoff=y_train.index[-1]))\ny_pred_prophet.index = y_test.index\nplot_series(y_test, y_pred_prophet, labels=[\"y_test\", \"y_pred\"])\nprophet_loss = loss(y_test, y_pred_prophet)\nprophet_loss\n&gt;&gt;&gt; 1186.00\n</code></pre>"},{"location":"book/10-Case-Studies/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\ndf = pd.read_csv(r\".\\datasets\\air_passengers.csv\")\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\ndf_train = df.iloc[:-23, :].copy()\ndf_test = df.iloc[-23:, :].copy()\n\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\nmetrics = m.fit(df_train, freq=\"MS\")\nfuture = m.make_future_dataframe(\n\u00a0 \u00a0 df_train, periods=23, n_historic_predictions=len(df_train)\n)\nforecast = m.predict(future)\nplt.plot(forecast[\"yhat1\"].values[-23:])\nplt.plot(df_test[\"y\"].values)\nneuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values)\nneuralprophet_loss\n&gt;&gt;&gt; 501.24\n</code></pre> <p>The final results can be summarized as follows, resulting in the table presented in the beginning of this case study:</p> <pre><code>results = {\n\u00a0 \u00a0 \"Exponential Smoothing\": es_loss,\n\u00a0 \u00a0 \"ETS\": ets_loss,\n\u00a0 \u00a0 \"AutoArima\": autoarima_loss,\n\u00a0 \u00a0 \"Manual Arima\": manualarima_loss,\n\u00a0 \u00a0 \"BATS\": bats_loss,\n\u00a0 \u00a0 \"TBATS\": tbats_loss,\n\u00a0 \u00a0 \"Prophet\": prophet_loss,\n\u00a0 \u00a0 \"SysIdentPy (Polynomial Model)\": frols_loss,\n\u00a0 \u00a0 \"SysIdentPy (Neural Model)\": narxnet_loss,\n\u00a0 \u00a0 \"SysIdentPy (AOLS)\": aols_loss,\n\u00a0 \u00a0 \"SysIdentPy (MetaMSS)\": metamss_loss,\n\u00a0 \u00a0 \"NeuralProphet\": neuralprophet_loss,\n}\nsorted(results.items(), key=lambda result: result[1])\n</code></pre>"},{"location":"book/10-Case-Studies/#system-with-hysteresis-modeling-a-magneto-rheological-damper-device","title":"System With Hysteresis - Modeling a Magneto-rheological Damper Device","text":"<p>The memory effects between quasi-static input and output make the modeling of hysteretic systems very difficult. Physics-based models are often used to describe the hysteresis loops, but these models usually lack the simplicity and efficiency required in practical applications involving system characterization, identification, and control. As detailed in Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models, NARX models have proven to be a feasible choice to describe the hysteresis loops. See Chapter 8 for a detailed background. However, even considering the sufficient conditions for rate independent hysteresis representation, classical structure selection algorithms fails to return a model with decent performance and the user needs to set a multi-valued function to ensure the occurrence of the bounding structure \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models).</p> <p>Even though some progress has been made, previous work has been limited to models with a single equilibrium point. The present case study aims to present new prospects in the model structure selection of hysteretic systems regarding the cases where the models have multiple inputs, and it is not restricted concerning the number of equilibrium points. For that, the MetaMSS algorithm will be used to build a model for a magneto-rheological damper (MRD) considering the mentioned sufficient conditions.</p>"},{"location":"book/10-Case-Studies/#a-brief-description-of-the-bouc-wen-model-of-magneto-rheological-damper-device","title":"A Brief description of the Bouc-Wen model of magneto-rheological damper device","text":"<p>The data used in this study-case is the Bouc-Wen model (Bouc, R - Forced Vibrations of a Mechanical System with Hysteresis), (Wen, Y. X. - Method for Random Vibration of Hysteretic Systems) of an MRD whose schematic diagram is shown in the figure below.</p> <p></p> <p>The model for a magneto-rheological damper proposed by Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback.</p> <p>The general form of the Bouc-Wen model can be described as (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback):</p> \\[ \\begin{equation} \\dfrac{dz}{dt} = g\\left[x,z,sign\\left(\\dfrac{dx}{dt}\\right)\\right]\\dfrac{dx}{dt}, \\end{equation} \\] <p>where \\(z\\) is the hysteretic model output, \\(x\\) the input and \\(g[\\cdot]\\) a nonlinear function of \\(x\\), \\(z\\) and \\(sign (dx/dt)\\). (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback) proposed the following phenomenological model for the aforementioned device:</p> \\[ \\begin{align} f&amp;= c_1\\dot{\\rho}+k_1(x-x_0),\\nonumber\\\\ \\dot{\\rho}&amp;=\\dfrac{1}{c_0+c_1}[\\alpha z+c_0\\dot{x}+k_0(x-\\rho)],\\nonumber\\\\ \\dot{z}&amp;=-\\gamma|\\dot{x}-\\dot{\\rho}|z|z|^{n-1}-\\beta(\\dot{x}-\\dot{\\rho})|z|^n+A(\\dot{x}-\\dot{\\rho}),\\nonumber\\\\ \\alpha&amp;=\\alpha_a+\\alpha_bu_{bw},\\nonumber\\\\ c_1&amp;=c_{1a}+c_{1b}u_{bw},\\nonumber\\\\ c_0&amp;=c_{0a}+c_{0b}u_{bw},\\nonumber\\\\ \\dot{u}_{bw}&amp;=-\\eta(u_{bw}-E). \\end{align} \\] <p>where \\(f\\) is the damping force, \\(c_1\\) and \\(c_0\\) represent the viscous coefficients, \\(E\\) is the input voltage, \\(x\\) is the displacement and \\(\\dot{x}\\) is the velocity of the model. The parameters of the system (see table below) were taken from Leva, A. and Piroddi, L. - NARX-based technique for the modelling of magneto-rheological damping devices.</p> Parameter Value Parameter Value \\(c_{0_a}\\) \\(20.2 \\, N \\, s/cm\\) \\(\\alpha_{a}\\) \\(44.9 \\, N/cm\\) \\(c_{0_b}\\) \\(2.68 \\, N \\, s/cm \\, V\\) \\(\\alpha_{b}\\) \\(638 \\, N/cm\\) \\(c_{1_a}\\) \\(350 \\, N \\, s/cm\\) \\(\\gamma\\) \\(39.3 \\, cm^{-2}\\) \\(c_{1_b}\\) \\(70.7 \\, N \\, s/cm \\, V\\) \\(\\beta\\) \\(39.3 \\, cm^{-2}\\) \\(k_{0}\\) \\(15 \\, N/cm\\) \\(n\\) \\(2\\) \\(k_{1}\\) \\(5.37 \\, N/cm\\) \\(\\eta\\) \\(251 \\, s^{-1}\\) \\(x_{0}\\) \\(0 \\, cm\\) \\(A\\) \\(47.2\\) <p>For this particular study, both displacement and voltage inputs, \\(x\\) and \\(E\\), respectively, were generated by filtering a white Gaussian noise sequence using a Blackman-Harris FIR filter with \\(6\\)Hz cutoff frequency. The integration step-size was set to \\(h = 0.002\\), following the procedures described in Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models. These procedures are for identification purposes only since the inputs of a MRD could have several different characteristics.</p> <p>The data used in this example is provided by the Professor Samir Angelo Milani Martins.</p> <p>The challenges are:</p> <ul> <li>it possesses a nonlinearity featuring memory, i.e. a dynamic nonlinearity;</li> <li>the nonlinearity is governed by an internal variable z(t), which is not measurable;</li> <li>the nonlinear functional form in the Bouc Wen equation is nonlinear in the parameter;</li> <li>the nonlinear functional form in the Bouc Wen equation does not admit a finite Taylor series expansion because of the presence of absolute values</li> </ul>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_4","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nscikit-learn==1.4.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_3","title":"SysIdentPy Configuration","text":"<pre><code>import numpy as np\nfrom sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\ndf = pd.read_csv(\"boucwen_histeretic_system.csv\")\nscaler_x = MaxAbsScaler()\nscaler_y = MaxAbsScaler()\n\ninit = 400\nx_train = df[[\"E\", \"v\"]].iloc[init:df.shape[0]//2, :]\nx_train[\"sign_v\"] = np.sign(df[\"v\"])\nx_train = scaler_x.fit_transform(x_train)\n\nx_test = df[[\"E\", \"v\"]].iloc[df.shape[0]//2 + 1:df.shape[0] - init, :]\nx_test[\"sign_v\"] = np.sign(df[\"v\"])\nx_test = scaler_x.transform(x_test)\n\ny_train = df[[\"f\"]].iloc[init:df.shape[0]//2, :].values.reshape(-1, 1)\ny_train = scaler_y.fit_transform(y_train)\n\ny_test = df[[\"f\"]].iloc[df.shape[0]//2 + 1:df.shape[0] - init, :].values.reshape(-1, 1)\ny_test = scaler_y.transform(y_test)\n\n# Plotting the data\nplt.figure(figsize=(10, 8))\nplt.suptitle('Identification (training) data', fontsize=16)\n\nplt.subplot(221)\nplt.plot(y_train, 'k')\nplt.ylabel('Force - Output')\nplt.xlabel('Samples')\nplt.title('y')\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(222)\nplt.plot(x_train[:, 0], 'k')\nplt.ylabel('Control Voltage')\nplt.xlabel('Samples')\nplt.title('x_1')\nplt.grid()\nplt.axis([0, 1500, 0, 1])\n\nplt.subplot(223)\nplt.plot(x_train[:, 1], 'k')\nplt.ylabel('Velocity')\nplt.xlabel('Samples')\nplt.title('x_2')\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(224)\nplt.plot(x_train[:, 2], 'k')\nplt.ylabel('sign(Velocity)')\nplt.xlabel('Samples')\nplt.title('x_3')\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n</code></pre> <p>Let's check how is the hysteretic behavior considering each input: <pre><code>plt.plot(x_train[:, 0], y_train)\nplt.xlabel(\"x1 - Voltage\")\nplt.ylabel(\"y - Force\")\n</code></pre></p> <p></p> <pre><code>plt.plot(x_train[:, 1], y_train)\nplt.xlabel(\"x2 - Velocity\")\nplt.ylabel(\"y - Force\")\n</code></pre> <p></p> <pre><code>plt.plot(x_train[:, 2], y_train)\nplt.xlabel(\"u3 - sign(Velocity)\")\nplt.ylabel(\"y - Force\")\n</code></pre> <p></p> <p>Now, we can just build a NARX model:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=[[1], [1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"FROLS: sign(v) and MaxAbsScaler\")\n&gt;&gt;&gt; 0.0450\n</code></pre> <p></p> <p>If we remove the <code>sign(v)</code> input and try to build a NARX model using the same configuration, the model diverge, as can be seen in the following figure:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=[[1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"FROLS: MaxAbsScaler, discarding sign(v)\")\n&gt;&gt;&gt; nan\n</code></pre> <p></p> <p>If we use the <code>MetaMSS</code> algorithm instead, the results are better.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\nbasis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n\u00a0 \u00a0 xlag=[[1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 random_state=42,\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"MetaMSS: MaxAbsScaler, discarding sign(v)\")\n&gt;&gt;&gt; 0.24\n</code></pre> <p></p> <p>However, when the output of the system reach its minimum value, the model oscillate</p> <pre><code>plot_results(y=y_test[1100 : 1200], yhat=yhat[1100 : 1200], n=10000, title=\"Unstable region\")\n</code></pre> <p></p> <p>If we add the <code>sign(v)</code> input again and use <code>MetaMSS</code>, the results are very close to the <code>FROLS</code> algorithm with all inputs</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n\u00a0 \u00a0 xlag=[[1], [1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"MetaMSS: sign(v) and MaxAbsScaler\")\n&gt;&gt;&gt; 0.0554\n</code></pre> <p></p> <p>This case will also highlight the significance of data scaling. Previously, we used the <code>MaxAbsScaler</code> method, which resulted in great models when using the <code>sign(v)</code> inputs, but also resulted in unstable models when removing that input feature. When scaling is applied using <code>MinMaxScaler</code>, however, the overall stability of the results improves, and the model does not diverge, even when the <code>sign(v)</code> input is removed, using the <code>FROLS</code> algorithm.</p> <p>The user can get the results bellow by just changing the data scaling method using</p> <pre><code>scaler_x = MinMaxScaler()\nscaler_y = MinMaxScaler()\n</code></pre> <p>and running each model again. That is the only change to improve the results.</p> <p></p> <p>FROLS: with <code>sign(v)</code> and <code>MinMaxScaler</code>. RMSE: 0.1159</p> <p> FROLS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1639</p> <p></p> <p>MetaMSS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1762</p> <p></p> <p>MetaMSS: including <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.0694</p> <p>In contrast, the MetaMSS method returned the best model overall, but not better than the best <code>FROLS</code> method using <code>MaxAbsScaler</code>.</p> <p>Here is the predicted hysteretic loop: <pre><code>plt.plot(x_test[:, 1], yhat)\n</code></pre></p> <p></p>"},{"location":"book/10-Case-Studies/#silver-box","title":"Silver box","text":"<p>The description content mainly derives (copy and paste) from the associated paper - Three free data sets for development and benchmarking in nonlinear system identification. For a detailed description, readers are referred to the linked reference.</p> <p>The Silverbox system can be seen as an electronic implementation of the Duffing oscillator. It is build as a 2<sup>nd</sup> order linear time-invariant system with a 3<sup>rd</sup> degree polynomial static nonlinearity around it in feedback. This type of dynamics are, for instance, often encountered in mechanical systems Nonlinear Benchmark - Silverbox.</p> <p>In this case study, we will create a NARX model for the Silver box benchmark. The Silver box represents a simplified version of mechanical oscillating processes, which are a critical category of nonlinear dynamic systems. Examples include vehicle suspensions, where shock absorbers and progressive springs play vital roles. The data generated by the Silver box provides a simplified representation of such combined components. The electrical circuit generating this data closely approximates, but does not perfectly match, the idealized models described below.</p> <p>As described in the original paper, the system was excited using a general waveform generator (HPE1445A). The input signal begins as a discrete-time signal \\(r(k)\\), which is converted to an analog signal \\(r_c(t)\\) using zero-order-hold reconstruction. The actual excitation signal \\(u_0(t)\\) is then obtained by passing \\(r_c(t)\\) through an analog low-pass filter \\(G(p)\\) to eliminate high-frequency content around multiples of the sampling frequency. Here, \\(p\\) denotes the differentiation operator. Thus, the input is given by:</p> \\[ u_0(t) = G(p) r_c(t). \\] <p>The input and output signals were measured using HP1430A data acquisition cards, with synchronized clocks for the acquisition and generator cards. The sampling frequency was:</p> \\[ f_s = \\frac{10^7}{2^{14}} = 610.35 \\, \\text{Hz}. \\] <p>The silver box uses analog electrical circuitry to generate data representing a nonlinear mechanical resonating system with a moving mass \\(m\\), viscous damping \\(d\\), and a nonlinear spring \\(k(y)\\). The electrical circuit is designed to relate the displacement \\(y(t)\\) (the output) to the force \\(u(t)\\) (the input) by the following differential equation:</p> \\[ m \\frac{d^2 y(t)}{dt^2} + d \\frac{d y(t)}{dt} + k(y(t)) y(t) = u(t). \\] <p>The nonlinear progressive spring is described by a static, position-dependent stiffness:</p> \\[ k(y(t)) = a + b y^2(t). \\] <p>The signal-to-noise ratio is sufficiently high to model the system without accounting for measurement noise. However, measurement noise can be included by replacing \\(y(t)\\) with the artificial variable \\(x(t)\\) in the equation above, and introducing disturbances \\(w(t)\\) and \\(e(t)\\) as follows:</p> \\[ \\begin{align} &amp; m \\frac{d^2 x(t)}{dt^2} + d \\frac{d x(t)}{dt} + k(x(t)) x(t) = u(t) + w(t), \\\\ &amp; k(x(t)) = a + b x^2(t), \\\\ &amp; y(t) = x(t) + e(t). \\end{align} \\]"},{"location":"book/10-Case-Studies/#required-packages-and-versions_5","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_4","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.Silverbox(atleast_2d=True)\n\nx_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_multisine.u, test_multisine.y\n\nn = test_multisine.state_initialization_window_length\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation - GerbenBeintema/nonlinear_benchmarks: The official data load for http://www.nonlinearbenchmark.org/ (github.com) to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plt.plot(x_train)\nplt.plot(y_train, alpha=0.3)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(x_test)\nplt.plot(y_test, alpha=0.3)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(test_arrow_full.u)\nplt.plot(test_arrow_full.y, alpha=0.3)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(test_arrow_no_extrapolation.u)\nplt.plot(test_arrow_no_extrapolation.y, alpha=0.2)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Important Note</p> <p>The goal of this benchmark is to develop a model that outperforms the state-of-the-art (SOTA) model presented in the benchmarking paper. However, the results in the paper differ from those provided in the  GitHub repository.</p> nx Set NRMS RMS (mV) 2 Train 0.10653 5.8103295 2 Validation 0.11411 6.1938068 2 Test 0.19151 10.2358533 2 Test (no extra) 0.12284 5.2789727 4 Train 0.03571 1.9478290 4 Validation 0.03922 2.1286373 4 Test 0.12712 6.7943448 4 Test (no extra) 0.05204 2.2365904 8 Train 0.03430 1.8707026 8 Validation 0.03732 2.0254112 8 Test 0.10826 5.7865255 8 Test (no extra) 0.04743 2.0382715 &gt; Table: results presented in the github. <p>It appears that the values shown in the paper actually represent the training time, not the error metrics. I will contact the authors to confirm this information. According to the Nonlinear Benchmark website, the information is as follows:</p> <p></p> <p>where the values in the \"Training time\" column matches the ones presented as error metrics in the paper.</p> <p>While we await confirmation of the correct values for this benchmark, we will demonstrate the performance of SysIdentPy. However, we will refrain from making any comparisons or attempting to improve the model at this stage.</p>"},{"location":"book/10-Case-Studies/#results_3","title":"Results","text":"<p>We will start (as we did in every other case study) with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=40</code>. Because we're dealing with a large training dataset, we will use the <code>err_tol</code> instead of information criteria to have a faster performance. We will also set <code>n_terms=40</code>, which means that the search will stop if the <code>err_tol</code> is reached or 40 regressors is tested in the <code>ERR</code> algorithm. While this approach might result in a suboptimal model, it is a reasonable starting point for our first attempt. There are three different experiments: multi sine, arrow (full), and arrow (no extrapolation).</p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_multisine.u, test_multisine.y\n\nn = test_multisine.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.999,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nnrmse = rmse/y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=300, figsize=(15, 4), title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\n&gt; 0.1423804033714937\n&gt; 7.727682109791501\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_full.u, test_arrow_full.y\n\nn = test_arrow_full.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9999,\n\u00a0 \u00a0 n_terms=80,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\n# we will not concatente the last values from train data to use as initial condition here because\n# this test data have a very different behavior.\n# However, if you want you can do that and you will see that the model will still perform\n# great after a few iterations\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nnrmse = rmse/y_test.std()\nrmse_mv = 1000 * rmse\n\nprint(nrmse, rmse_mv)\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=300, figsize=(15, 4), title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_no_extrapolation.u, test_arrow_no_extrapolation.y\n\nn = test_arrow_no_extrapolation.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9999,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nnrmse = rmse/y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Arrow (no extrapolation). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=300, figsize=(15, 4), title=f\"Free Run simulation. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n</code></pre> <p></p> <p></p>"},{"location":"book/10-Case-Studies/#f-16-ground-vibration-test-benchmark","title":"F-16 Ground Vibration Test Benchmark","text":"<p>The following examples are intended to demonstrate the application of SysIdentPy on a real-world dataset. Please note that these examples are not aimed at replicating the results presented in the cited manuscripts. The model parameters, such as <code>ylag</code> and <code>xlag</code>, as well as the size of the identification and validation data sets, differ from those used in the original studies. Additionally, adjustments related to sampling rates and other data preparation steps are not covered in this notebook.</p> <p>For a comprehensive reference regarding the F-16 Ground Vibration Test benchmark, please visit the nonlinear benchmark website.</p> <p>Note: This notebook serves as a preliminary demonstration of SysIdentPy's performance on the F-16 dataset. A more detailed analysis will be provided in a future publication. The nonlinear benchmark website offers valuable resources and references related to system identification and machine learning, and readers are encouraged to explore the papers and information available there.</p>"},{"location":"book/10-Case-Studies/#benchmark-overview","title":"Benchmark Overview","text":"<p>The F-16 Ground Vibration Test benchmark is a notable experiment in the field of system identification and nonlinear dynamics. It involves a high-order system with clearance and friction nonlinearities at the mounting interfaces of payloads on a full-scale F-16 aircraft.</p> <p>Experiment Details: - Event: Siemens LMS Ground Vibration Testing Master Class - Date: September 2014 - Location: Saffraanberg military base, Sint-Truiden, Belgium</p> <p>During the test, two dummy payloads were mounted on the wing tips of the F-16 to simulate the mass and inertia of real devices typically equipped on the aircraft during flight. Accelerometers were installed on the aircraft structure to capture vibration data. A shaker was placed under the right wing to apply input signals. The key source of nonlinearity in the system was identified as the mounting interfaces of the payloads, particularly the right-wing-to-payload interface, which exhibited significant nonlinear distortions.</p> <p>Data and Resources: - Data Availability: The dataset, including detailed system descriptions, estimation and test data sets, and setup images, is available for download in both .csv and .mat file formats. - Reference: For in-depth information on the F-16 benchmark, refer to: J.P. No\u00ebl and M. Schoukens, \"F-16 aircraft benchmark based on ground vibration test data,\" 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> <p>The goal of this notebook is to illustrate how SysIdentPy can be applied to such complex datasets, showcasing its capabilities in modeling and analysis. For a thorough exploration of the benchmark and its methodologies, please consult the provided resources and references.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_6","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_5","title":"SysIdentPy Configuration","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n\u00a0 \u00a0 compute_residues_autocorrelation,\n\u00a0 \u00a0 compute_cross_correlation,\n)\n</code></pre>"},{"location":"book/10-Case-Studies/#procedure","title":"Procedure","text":"<p><pre><code>f_16 = pd.read_csv(r\"examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"])\nf_16.shape\nf_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n\n&gt;&gt;&gt; (32768, 3)\n</code></pre> </p> <pre><code>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</code></pre> <p></p> <p>The following code is to split the dataset into training and test sets</p> <pre><code>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n\u00a0 \u00a0 16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n\u00a0 \u00a0 16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n\u00a0 \u00a0 16384::\n].values.reshape(-1, 1)\n</code></pre> <p>We will set the lags for both inputs as</p> <pre><code>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\n</code></pre> <p>and build a NARX model as follows</p> <pre><code>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=39,\n\u00a0 \u00a0 ylag=20,\n\u00a0 \u00a0 xlag=[x1lag, x2lag],\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\ny_hat = model.predict(X=x_val, y=y_val)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> <p>The RRSE is \\(0.2910\\)</p> Regressors Parameters ERR y(k-1) 1.8387E+00 9.43378253E-01 y(k-2) -1.8938E+00 1.95167599E-02 y(k-3) 1.3337E+00 1.02432261E-02 y(k-6) -1.6038E+00 8.03485985E-03 y(k-9) 2.6776E-01 9.27874557E-04 x2(k-7) -2.2385E+01 3.76837313E-04 x1(k-1) 8.2709E+00 6.81508210E-04 x2(k-3) 1.0587E+02 1.57459800E-03 x1(k-8) -3.7975E+00 7.35086279E-04 x2(k-1) 8.5725E+01 4.85358786E-04 y(k-7) 1.3955E+00 2.77245281E-04 y(k-5) 1.3219E+00 8.64120037E-04 y(k-10) -2.9306E-01 8.51717688E-04 y(k-4) -9.5479E-01 7.23623116E-04 y(k-8) -7.1309E-01 4.44988077E-04 y(k-12) -3.0437E-01 1.49743148E-04 y(k-11) 4.8602E-01 3.34613282E-04 y(k-13) -8.2442E-02 1.43738964E-04 y(k-15) -1.6762E-01 1.25546584E-04 x1(k-2) -8.9698E+00 9.76699739E-05 y(k-17) 2.2036E-02 4.55983807E-05 y(k-14) 2.4900E-01 1.10314107E-04 y(k-19) -6.8239E-03 1.99734771E-05 x2(k-9) -9.6265E+01 2.98523208E-05 x2(k-8) 2.2620E+02 2.34402543E-04 x2(k-2) -2.3609E+02 1.04172323E-04 y(k-20) -5.4663E-02 5.37895336E-05 x2(k-6) -2.3651E+02 2.11392628E-05 x2(k-4) 1.7378E+02 2.18396315E-05 x1(k-7) 4.9862E+00 2.03811842E-05 <pre><code>plot_results(y=y_val, yhat=y_hat, n=1000)\nee = compute_residues_autocorrelation(y_val, y_hat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0])\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"book/10-Case-Studies/#pv-forecasting","title":"PV Forecasting","text":"<p>In this case study, we evaluate SysIdentPy's capabilities for forecasting solar irradiance data, which can serve as a proxy for solar photovoltaic (PV) production. The objective is to demonstrate that SysIdentPy provides a competitive alternative for time series modeling, rather than claiming superiority over other libraries.</p>"},{"location":"book/10-Case-Studies/#dataset-overview_1","title":"Dataset Overview","text":"<p>The dataset used in this analysis consists of solar irradiance measurements, which are crucial for predicting solar PV production. Solar irradiance refers to the power of solar radiation received per unit area at the Earth's surface, typically measured in watts per square meter (W/m\u00b2). Accurate forecasting of solar irradiance is essential for optimizing energy production and managing grid stability in solar power systems.</p> <p>Dataset Details: - Source: The dataset can be accessed from the NeuralProphet GitHub repository. - Time Frame: The dataset covers a continuous period with frequent measurements. - Variables: Solar irradiance values over time, which will be used to model and forecast future irradiance levels.</p>"},{"location":"book/10-Case-Studies/#comparison-with-other-libraries_1","title":"Comparison with Other Libraries","text":"<p>To assess the effectiveness of SysIdentPy, we will compare its performance with the NeuralProphet library. NeuralProphet is known for its flexibility and ability to capture complex seasonal patterns and trends, making it a suitable benchmark for this task.</p> <p>For the comparison, we will use the following methods:</p> <ul> <li>NeuralProphet:</li> <li> <p>The configuration for NeuralProphet models will be based on examples provided in the NeuralProphet documentation. This library employs advanced techniques for capturing temporal patterns and forecasting.</p> </li> <li> <p>SysIdentPy:</p> </li> <li>MetaMSS (Meta-heuristic Model Structure Selection): Utilizes metaheuristic algorithms to determine the optimal model structure.</li> <li>AOLS (Accelerated Orthogonal Least Squares): A method designed for selecting relevant regressors in a model.</li> <li>FROLS (Forward Regression with Orthogonal Least Squares, using polynomial base functions): A regression technique that incorporates polynomial terms to enhance model selection.</li> </ul>"},{"location":"book/10-Case-Studies/#objective_1","title":"Objective","text":"<p>The goal of this case study is to compare the performance of SysIdentPy's forecasting methods with NeuralProphet. We will specifically focus on:</p> <ul> <li>1-Step Ahead Forecasting: Evaluating the models' ability to predict the next time step in the series based on historical data.</li> </ul> <p>We will train our models on 80% of the dataset and reserve the remaining 20% for validation purposes. This setup ensures that we test the models' performance on unseen data.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_7","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npystan==2.19.1.1\nholidays==0.11.2\nfbprophet==0.7.1\nneuralprophet==0.2.7\npandas==1.3.2\nnumpy==1.23.3\nmatplotlib==3.8.4\npmdarima==1.8.3\nscikit-learn==0.24.2\nscipy==1.9.1\nsktime==0.8.0\nstatsmodels==0.12.2\ntbats==1.1.0\ntorch==1.12.1\n</code></pre> <p>Then, install the packages using:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions. This practice isolates your project\u2019s dependencies and prevents version conflicts with other projects or system-wide packages. Additionally, be aware that some packages, such as <code>sktime</code> and <code>neuralprophet</code>, may install several dependencies automatically during their installation. Setting up a virtual environment helps manage these dependencies more effectively and keeps your project environment clean and reproducible.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#procedure_1","title":"Procedure","text":"<ol> <li>Data Preparation: Load and preprocess the solar irradiance dataset.</li> <li>Model Training: Apply the chosen methods from SysIdentPy and NeuralProphet to the training data.</li> <li>Evaluation: Assess the forecasting accuracy of each model on the validation set.</li> </ol> <p>By comparing these approaches, we aim to showcase SysIdentPy as a viable option for time series forecasting, highlighting its strengths and versatility in practical applications.</p> <p>Let\u2019s start by importing the necessary libraries and setting up the environment for this analysis.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.metrics import mean_squared_error\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n%matplotlib inline\n\nloss = mean_squared_error\ndata_location = r\".\\datasets\"\n</code></pre>"},{"location":"book/10-Case-Studies/#neural-prophet_1","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n\u00a0 \u00a0 n_lags=24,\n\u00a0 \u00a0 ar_sparsity=0.5,\n)\n\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\n\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n\nplt.plot(forecast[\"y\"][-104:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-104:], \"k*-\")\n</code></pre> <p>The error is \\(MSE=4642.23\\) and will be used as baseline in this case. Let's check how SysIdentPy methods handle this data.</p>"},{"location":"book/10-Case-Studies/#frols","title":"FROLS","text":"<pre><code>files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 ylag=24,\n\u00a0 \u00a0 xlag=24,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 estimator=LeastSquares(),\n)\n\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\n\nprint(sysidentpy_loss)\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <p>The \\(MSE=3869.34\\) for this case.</p> <p></p>"},{"location":"book/10-Case-Studies/#metamss","title":"MetaMSS","text":"<pre><code>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nsysidentpy_metamss = MetaMSS(\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 xlag=24,\n\u00a0 \u00a0 ylag=24,\n\u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 maxiter=10,\n\u00a0 \u00a0 steps_ahead=1,\n\u00a0 \u00a0 n_agents=15,\n\u00a0 \u00a0 loss_func=\"metamss_loss\",\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 random_state=42,\n)\n\nsysidentpy_metamss.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\nyhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1)\nmetamss_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\n\nprint(metamss_loss)\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <p>The MetaMSS algorithm was able to select a better model in this case, as can be observed in the error metric, \\(MSE=2157.77\\).</p> <p></p>"},{"location":"book/10-Case-Studies/#aols","title":"AOLS","text":"<pre><code>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(\n\u00a0 \u00a0 ylag=24, xlag=24, k=2, L=1, model_type=\"NARMAX\", basis_function=basis_function\n)\n\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <p>The error now is \\(MSE=2361.56\\).</p> <p></p>"},{"location":"book/2-NARMAX-Model-Representation/","title":"2. NARMAX Model Representation","text":"<p>There are several NARMAX model representations, including polynomial, Fourier, generalized additive, neural networks, and wavelet (Billings, S. A, Aguirra, L. A). This book focuses on the model representations available in SysIdentPy and we\u2019ll keep things updated as new methods are added to the package. If a particular representation is mentioned but is not available in SysIdentPy, it will be explicitly mentioned.</p> <p>To reproduce the codes presented in this section, make sure you have these packages installed:</p> <pre><code>sysidentpy, scikit-learn, scipy, pytorch, matplotlib\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#basis-function","title":"Basis Function","text":"<p>In System Identification, understanding the concept of basis functions is crucial for effectively modeling complex systems. Basis functions are predefined mathematical functions used to transform the input data into a new space, where the relationships within the data can be more easily modeled. By expressing the original data in terms of these basis functions, we can build nonlinear models in respect to its structure while keeping it linear in the parameters, allowing the usage of straightforward parameter estimation methods.</p> <p>Basis functions commonly used in system identification:</p> <ol> <li> <p>Polynomial Basis Functions: These functions are powers of the input variables. They are useful for capturing simple nonlinear relationships.</p> </li> <li> <p>Fourier Basis Functions: These sinusoidal functions (sine and cosine) are ideal for representing periodic patterns within the data.</p> </li> <li> <p>Wavelet Basis Functions: These functions are localized in both time and frequency, making them suitable for analyzing data with varying frequency components. Not available in SysIdentPy yet.</p> </li> </ol> <p>In SysIdentPy you can define the basis function you want to use in your model by just import them:</p> <pre><code>from sysidentpy.basis_function import Polynomial, Fourier, Bernstein\n</code></pre> <p>To keep things simple for now, we will show simple examples of how basis function can be used in a modeling task. We will show a simple polynomial basis functions, a triangular basis function, a radial basis function and a rectangular basis function.</p> <p>SysIdentPy does not currently include Vandermonde or any of the other basis functions defined below. These functions are provided solely as examples to illustrate the significance of the basis functions. The examples are based on Fredrik Bagge Carlson's PhD thesis, which I highly recommended for anyone interested in Nonlinear System Identification.</p> <p>Although Vandermonde and Radial Basis Functions (RBF) are planned for inclusion as native basis functions in SysIdentPy version 1.0, users can already create and use their own custom basis functions with SysIdentPy. An example of how to do this is available on the SysIdentPy documentation page.</p>"},{"location":"book/2-NARMAX-Model-Representation/#example-vandermonde-matrix","title":"Example: Vandermonde Matrix","text":"<p>The polynomial basis functions used in this example is defined as:</p> \\[ \\phi_i(x) = x^i \\tag{2.1} \\] <p>where \\(i\\) is the degree of the polynomial and \\(x\\) is the input variable.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n\n# Polynomial basis function\ndef poly_basis(x, degree):\n\u00a0 \u00a0 return np.vander(x, degree + 1, increasing=True)\n\n# Create polynomial features\ndegree = 2\nX_poly = poly_basis(x, degree)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\ny_pred = model.predict(X_poly)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the polynomial approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the polynomial basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(degree + 1):\n\u00a0 \u00a0 plt.plot(x, poly_basis(x, degree)[:, i], linewidth=0.5, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 1. Approximation using Vandermode Matrix. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2-NARMAX-Model-Representation/#example-rectangular-basis-functions","title":"Example: Rectangular Basis Functions","text":"<p>The rectangular basis functions are defined as:</p> \\[ \\phi_{i}(x) = \\begin{cases} 1 &amp; \\text{if } c_i - \\frac{w}{2} \\leq x &lt; c_i + \\frac{w}{2} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\tag{2.2} \\] <p>where \\(c_i\\) represents the center of the basis function, \\(w\\) is the width, and \\(x\\) is the input variable.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n# Rectangular basis function\ndef rectangular_basis(x, centers, width):\n\u00a0 \u00a0 return np.column_stack([(np.abs(x - c) &lt; width).astype(float) for c in centers])\n\n# Create rectangular features\ncenters = np.linspace(-3, 3, 6)\nwidth = 3\nX_rect = rectangular_basis(x, centers, width)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_rect, y)\ny_pred = model.predict(X_rect)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the rectangular approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the rectangular basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(len(centers)):\n\u00a0 \u00a0 plt.plot(x, rectangular_basis(x, centers, width)[:, i], linewidth=1, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 2. Approximation using Rectangular Basis Function. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2-NARMAX-Model-Representation/#example-triangular-basis-functions","title":"Example: Triangular Basis Functions","text":"<p>The triangular basis functions are defined as:</p> \\[ \\phi_{i}(x) = \\max \\left(0, 1 - \\frac{|x - c_i|}{w} \\right) \\tag{2.3} \\] <p>where \\(c_i\\) is the center of the basis function, \\(w\\) is the width, and \\(x\\) is the input variable.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n# Triangular basis function\ndef triangular_basis(x, centers, width):\n\u00a0 \u00a0 return np.column_stack([np.maximum(0, 1 - np.abs((x - c) / width)) for c in centers])\n\n# Create triangular features\ncenters = np.linspace(-3, 3, 6)\nwidth = 1.5\nX_tri = triangular_basis(x, centers, width)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_tri, y)\ny_pred = model.predict(X_tri)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the triangular approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the triangular basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(len(centers)):\n\u00a0 \u00a0 plt.plot(x, triangular_basis(x, centers, width)[:, i], linewidth=1, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 3. Approximation using a Triangular Basis Function. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2-NARMAX-Model-Representation/#example-radial-basis-function-rbf-gaussian","title":"Example: Radial Basis Function (RBF) - Gaussian","text":"<p>The Gaussian Radial Basis Function is defined as:</p> \\[ \\phi(x; c, \\sigma) = \\exp\\left(- \\frac{(x - c)^2}{2 \\sigma^2}\\right) \\tag{2.4} \\] <p>where: - \\(x\\) is the input variable. - \\(c\\) is the center of the RBF. - \\(\\sigma\\) is the spread (or scale) of the RBF.</p> <p>This function measures the distance between \\(x\\) and the center \\(c\\), and it decays exponentially based on the width \\(\\sigma\\). The smaller the \\(\\sigma\\), the more localized the basis function is around the center \\(c\\).</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200) \u00a0# More points for a smoother curve\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape) \u00a0# Quadratic polynomial with noise\n# RBF centers and sigma\ncenters = np.linspace(-3, 3, 6) \u00a0# More centers for better coverage\nsigma = 0.5 \u00a0# Spread of the RBF\n# RBF basis function\ndef rbf_basis(x, c, sigma):\n\u00a0 \u00a0 return np.exp(- (x - c) ** 2 / (2 * sigma ** 2))\n\n# Create RBF features\nX_rbf = np.column_stack([rbf_basis(x, c, sigma) for c in centers])\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_rbf, y)\ny_pred = model.predict(X_rbf)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Basis function colors\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nn_colors = len(basis_colors)\n# Plot the basis functions\nfor i, c in enumerate(centers):\n\u00a0 \u00a0 color = basis_colors[i % n_colors]\n\u00a0 \u00a0 plt.plot(x, rbf_basis(x, c, sigma), linewidth=1, color=color, label=f'RBF Center {c:.2f}')\n\n# Plot the approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 4. Approximation using the Radial Basis Function. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2-NARMAX-Model-Representation/#linear-models","title":"Linear Models","text":""},{"location":"book/2-NARMAX-Model-Representation/#armax","title":"ARMAX","text":"<p>You may have noticed the similarity between the acronym NARMAX with the well-known models ARX, ARMAX, etc., which are widely used for forecasting time series. And this resemblance is not by chance. The AutoRegressive models with Moving Average and Exogenous Input (ARMAX) and their variations AR, ARX, ARMA (to name just a few) are one of the most used mathematical representations for identifying linear systems. The ARMAX can be expressed as:</p> \\[ y_k= \\mathcal{\\phi}[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\tag{2.5} \\] <p>where \\(n_y\\in \\mathbb{N}\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) , are the maximum lags for the system output, input and noise regressors (representing the moving average part), respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{\\phi}\\) is some linear function of the input and output regressors and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>If \\(\\mathcal{F}\\) is a polynomial, we have a polynomial ARMAX model</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.6} \\] <p>where \\(\\sum\\nolimits_{0}\\), \\(\\Theta_{y}^{i}\\), \\(\\Theta_{e}^{j}\\), and \\(\\Theta_{x}^{m}\\) are constant parameters.</p> <p>The following example is a polynomial ARMAX model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139x_{k-1} -0.1691x_{k-1} + 0.2245e_{k-1} \\end{align} \\tag{2.7} \\] <p>You can easily build a polynomial ARMAX model using SysIdentPy: <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True)\n)\n</code></pre></p> <p>In the example above, we define the linear polynomial basis function by importing the Polynomial basis and setting the degree equal to 1 (this ensures that we do not have a nonlinear combination of the regressors). Don't worry about the <code>FROLS</code> and <code>LeastSquares</code> yet. We'll talk about them in chapters 3 and 4, respectively.</p> <p>For Figure 4, we conducted 10 separate simulations to analyse the effects of different noise process generation on the ARMAX system's behavior. Each simulation uses a unique sample of noise to observe how variations in this random component influence the overall system output. To illustrate this, we highlight one specific simulation while the others are displayed with less emphasis.</p> <p>It's important to notice that all simulations, whether highlighted or not, are governed by the same underlying model. The deterministic part of the model equation explains the behavior of all the signals shown. The noticeable differences among the signals arise solely from the distinct noise samples used in each simulation. Despite these variations, the core dynamics of the signal remain consistent and are described by the model's deterministic component.</p> <p>Most of the code presented in this chapter is intended to illustrate fundamental concepts rather than demonstrating how to use SysIdentPy specifically. Many examples are implemented using pure Python to help you better understand the underlying concepts, replicate the examples, and adapt them as needed. SysIdentPy itself will be introduced and be used in the examples in the following chapter.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, u, nu):\n\u00a0 \u00a0 yk = 0.9*y[0] - 0.24*y[1] + 0.92*u[0] + 0.92*nu[0] + nu[1]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nu = np.random.normal(size=(random_samples,), scale=1)\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [u[i - 1]], [nu[i - 1], nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot \"nicer\"\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARMAX model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 4. Simulations to show the effects of different noise process generation on the ARMAX model's behavior.</p>"},{"location":"book/2-NARMAX-Model-Representation/#arx","title":"ARX","text":"<p>If we do not include noise terms \\(e_{k-n_e}\\)  in equation (1), we have ARX models.</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.8} \\] <p>The following example is a polynomial ARX model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139x_{k-1} -0.1691x_{k-1} \\end{align} \\tag{2.9} \\] <p>The only difference in SysIdentPy is setting the <code>unbiased=False</code></p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False)\n)\n</code></pre> <p>The following example shows 10 separate simulations to analyse the effects of different noise process generation on the ARX system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, u, nu):\n\u00a0 \u00a0 yk = 0.9*y[0] - 0.24*y[1] + 0.92*u[0] + nu[0]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nu = np.random.normal(size=(random_samples,), scale=1)\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [u[i - 1]], [nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARX model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 5. Simulations to show the effects of different noise process generation on the ARX model's behavior.</p>"},{"location":"book/2-NARMAX-Model-Representation/#arma","title":"ARMA","text":"<p>if we do not include input terms in equation (1), it turns to ARMA model</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + e_k \\tag{2.10} \\] <p>The following example is a polynomial ARMA model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139y_{k-3} -0.1691y_{k-4} + 0.2245e_{k-1} \\end{align} \\tag{2.11} \\] <p>Since the model representation do not have inputs, we have to set the model type to <code>NAR</code> and set <code>unbiased=True</code> again in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True),\n    model_type=\"NAR\"\n)\n</code></pre> <p>The figure bellow shows 10 separate simulations to analyse the effects of different noise process generation on the ARX system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, nu):\n\u00a0 \u00a0 yk = 0.5*y[0] - 0.4*y[1] + 0.8*nu[0] + nu[1]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [nu[i - 1], nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARMA model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 6. Simulations to show the effects of different noise process generation on the ARMA model's behavior.</p>"},{"location":"book/2-NARMAX-Model-Representation/#ar","title":"AR","text":"<p>if we do not include input terms and noise terms in equation (1), it turns to AR model</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + e_k \\tag{2.12} \\] <p>The following example is a polynomial AR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139y_{k-3} -0.1691y_{k-4} \\end{align} \\tag{2.13} \\] <p>In this case, we have to set the model type to <code>NAR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NAR\"\n)\n</code></pre> <p>The figure bellow shows 10 separate simulations to analyse the effects of different noise process generation on the AR system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, nu):\n\u00a0 \u00a0 yk = 0.5*y[0] - 0.3*y[1] + nu[0]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an AR model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 7. Simulations to show the effects of different noise process generation on the AR model's behavior.</p>"},{"location":"book/2-NARMAX-Model-Representation/#fir","title":"FIR","text":"<p>if we only keep input terms in equation (1), it turns to NFIR model</p> \\[ y_k = \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.14} \\] <p>The following example is a polynomial FIR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213x_{k-1}-0.5692x_{k-2}+0.1139x_{k-3} -0.1691x_{k-4} \\end{align} \\tag{2.15} \\] <p>In this case, we have to set the model type to <code>NFIR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NFIR\"\n)\n</code></pre> <p>The figure bellow shows 10 separate simulations to analyse the effects of different noise process generation on the FIR system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(u, nu):\n\u00a0 \u00a0 yk = 0.28*u[0] - 0.34*u[1] + nu[0]\n\u00a0 \u00a0 return yk\n\nu = np.random.normal(size=(random_samples,), scale=1)\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([0.1*u[i - 1], u[i - 2]], [nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an FIR model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 8. Simulations to show the effects of different noise process generation on the FIR model's behavior.</p> <p>We didn't set the model_type for ARMAX and ARX because the default is <code>NARMAX</code>. SysIdentPy allows three different model types: <code>NARMAX</code>, <code>NAR</code>, and <code>NFIR</code>. Because ARMAX, ARX and others linear variants are subsets of NARMAX models, there is no need for specific <code>ARMAX</code> model type. The idea is to have model types for model with input and output regressors; models with only output regressors; and models with only input regressors.</p>"},{"location":"book/2-NARMAX-Model-Representation/#other-variants","title":"Other Variants","text":"<p>For the sake of simplicity, we defined Equation 2.5 and only approach the polynomial representations. However, you can extend the representations to other basis functions, like the Fourier. If you set \\(\\mathcal{F}\\) as the Fourier extension</p> \\[ \\mathcal{F}(x) = [\\cos(\\pi x), \\sin(\\pi x), \\cos(2\\pi x), \\sin(2\\pi x), \\ldots, \\cos(N\\pi x), \\sin(N\\pi x)] \\tag{2.16} \\] <p>In this case, the Fourier ARX representation will be:</p> \\[ \\begin{aligned} y_k = &amp;\\Big[ \\cos(\\pi y_{k-1}), \\sin(\\pi y_{k-1}), \\cos(2\\pi y_{k-1}), \\sin(2\\pi y_{k-1}), \\ldots, \\cos(N\\pi y_{k-1}), \\sin(N\\pi y_{k-1}), \\\\ &amp;\\ \\ \\cos(\\pi y_{k-2}), \\sin(\\pi y_{k-2}), \\ldots, \\cos(N\\pi y_{k-n_y}), \\sin(N\\pi y_{k-n_y}), \\\\ &amp;\\ \\ \\cos(\\pi x_{k-1}), \\sin(\\pi x_{k-1}), \\cos(2\\pi x_{k-1}), \\sin(2\\pi x_{k-1}), \\ldots, \\cos(N\\pi x_{k-n_x}), \\sin(N\\pi x_{k-n_x}) \\Big] \\\\ &amp;\\ \\ + e_k \\end{aligned} \\tag{2.17} \\] <p>To do that in SysIdentPy, just import the Fourier basis instead of the Polynomial</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Fourier(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NARMAX\"\n)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#nonlinear-models","title":"Nonlinear Models","text":""},{"location":"book/2-NARMAX-Model-Representation/#narmax","title":"NARMAX","text":"<p>The NARMAX model was proposed by  Stephen A. Billings and I.J. Leontaritis in 1981, (Billings, S. A. - Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains), and can be described as</p> \\[ \\begin{equation} y_k= \\mathcal{F}[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k, \\end{equation} \\tag{2.18} \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) , are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) represents uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}\\) is some nonlinear function of the input and output regressors and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>You can notice that the difference between Equation 2.5 and Equation 2.18 if the function representing the system. For NARMAX models, \\(\\mathcal{F}\\) can be any nonlinear function, while for Equation 2.5 only linear functions are allowed. Although there are many possible approximations of \\(\\mathcal{F}(\\cdot)\\) (e.g., Neural Networks, Fuzzy, Wavelet, Radial Basis Function), the power-form Polynomial NARMAX model is the most commonly used (Billings, S. A.; Khandelwal, D. and Schoukens, M. and Toth, R.):</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j}\\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.19} \\] <p>where \\(p\\) is the number of regressors, \\(\\Theta_i\\) are the model parameters, and \\(a_i, m\\), \\(b_i, j\\) and \\(d_i, l \\in \\mathbb{N}\\) are the exponents of the output, input and noise terms, respectively.</p> <p>The Equation 2.20 describes a polynomial NARMAX model where the nonlinearity degree is equal to \\(2\\), identified from experimental data of a DC motor/generator with no prior knowledge of the model form, taken from Lacerda Junior, W. R., Almeida, V. M., &amp; Martins, S. A. M. (2017):</p> \\[ \\begin{align}   y_k =&amp; 1.7813y_{k-1}-0.7962y_{k-2}+0.0339x_{k-1} -0.1597x_{k-1} y_{k-1} +0.0338x_{k-2} + \\\\   &amp; + 0.1297x_{k-1}y_{k-2} - 0.1396x_{k-2}y_{k-1}+ 0.1086x_{k-2}y_{k-2}+0.0085y_{k-2}^2 + 0.0247e_{k-1}e_{k-2} \\end{align} \\tag{2.20} \\] <p>The \\(\\Theta\\) values are the coefficients of each term of the polynomial equation.</p> <p>Polynomial basis functions are one of the most used representations of NARMAX models due to several interesting attributes, such as (Billings, S. A.):</p> <ul> <li>All polynomial functions are smooth in \\(\\mathbb{R}\\).</li> <li>The Weierstrass approximation theorem states that any continuous real-valued function defined on a closed and bounded space \\([a,b]\\) can be uniformly approximated using a polynomial on that interval.</li> <li>They can describe several nonlinear dynamical systems, including industrial processes, control systems, structural systems, economic and financial systems, biology, medicine, and social systems (some examples are detailed in Lacerda Junior, W. R. and Martins, S. A. M. and Nepomuceno, E. G. and Lacerda, Marcio J. ; Fung, E. H. K. and Wong, Y. K. and Ho, H. F. and Mignolet, M. P.; Kukreja, S. L. and Galiana, H. L. and Kearney, R. E.; Billings, S. A; Aguirre, L. A.; and many others).</li> <li>Several algorithms have been developed for structure selection and parameter estimation of polynomial NARMAX models, and it remains an active area of research.</li> <li>Polynomial NARMAX models are versatile and can be used both for prediction and inference. The structure of polynomial NARMAX models are easy to interpret and can be related to the underlying system, which is much harder to achieve with neural networks or wavelet functions, for instance.</li> </ul> <p>You can easily build a polynomial NARMAX model using SysIdentPy. Note that the difference for ARMAX, in this case, is the degree of the polynomial function.</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True)\n)\n</code></pre> <p>One could think that is a simple change, but in nonlinear scenarios the course of dimensionality becomes a real problem. The number of candidate regressors, \\(n_r\\), of polynomial NARX can be defined as Korenberg, M. L., Billings, S. A., Liu, Y. P., and McIlroy, P. J. - Orthogonal parameter estimation algorithm for non-linear stochastic systems:</p> \\[ \\begin{equation}     n_r = M+1, \\end{equation} \\tag{2.21} \\] <p>where</p> \\[ \\begin{align}     M = &amp; \\sum_{i=1}^{\\ell}n_i \\\\     n_i = &amp; \\frac{n_{i-1}(n_y+n_x+i-1)}{i}, n_{0} = 1. \\end{align} \\tag{2.22} \\] <p>As we mentioned in the Introduction of the book, NARMAX methods aims to build the simplest models possible. The idea is to reproduce a wide range of behaviors using a small subset of terms from the vast search space formed by candidate regressors.</p> <p>Let's use SysIdentPy to see how the search space grows in the linear versus the nonlinear scenario. The <code>count_model_regressors</code> method available in <code>narmax_tools</code> can be used the check how many regressors exists in the search space given the number of inputs, the delays of <code>y</code> and <code>x</code> regressors and the basis function. We will use <code>xlag=ylag=10</code> and the polynomial basis function. The user can simulate different scenarios by setting different parameters.</p> <pre><code>from sysidentpy.utils.information_matrix import count_model_regressors\nfrom sysidentpy.basis_function import Polynomial\nimport numpy as np\n</code></pre> <p>For the linear case with 1 input we have 21 regressors: <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=1)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=10,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 21\n</code></pre></p> <p>For the linear case with 2 inputs, the number of regressors jumps to 31:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=1)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=xlag,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 31\n</code></pre> <p>If we consider a nonlinear case with 1 input by just changing the degree to 2, we have 231 regressors.</p> <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=2)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=10,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 231\n</code></pre> <p>If we set the degree to 3, the number of terms increases significantly to 1771 regressors.</p> <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=3)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=10,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 1771\n</code></pre> <p>If you have 2 inputs in the nonlinear scenario with <code>degree=2</code>, the number of regressors is 496:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=2)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=xlag,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 496\n</code></pre> <p>If you have 2 inputs in the nonlinear scenario with <code>degree=3</code>, the number jumps to 5456 regressors:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=3)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=xlag,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 5456\n</code></pre> <p>As you can notice, the number of regressors increases significantly as the degree of the polynomial and the number of inputs increases. That makes the model structure selection much more complex! In the linear case with 10 inputs we have <code>2^31=2.15e+09</code> possible model combinations. When <code>degree=2</code> with 2 inputs we have <code>2^496=2.05e+149</code> possible combinations! Try to get the number of possible model combinations when <code>degree=3</code> with 2 inputs. Moreover, try that with more inputs and higher nonlinear degree and see how the course of dimensionality is a big problem.</p> <p>As you can see, getting a simple model in such a large search space is complex model structure selection task. To select the most significant terms from a huge dictionary of possible terms is not an easy task. And it is hard not only because the complex combinatorial problem and the uncertainty concerning the model order. Identifying the most significant terms in a nonlinear scenario is very difficult because depends on the type of the nonlinearity (sparse singularity or near-singular behavior, memory or dumping effects and many others), dynamical response (spatial-temporal systems, time-dependent), the steady-state response,  frequency of the data, the noise and many more.</p> <p>Because of the model structure selection algorithms developed for NARMAX models, even linear models like ARMAX can have different performance when obtained using SysIdentPy when compared to other libraries, like Statsmodels. We have a case study showing exactly that in Chapter 10.</p>"},{"location":"book/2-NARMAX-Model-Representation/#narx","title":"NARX","text":"<p>If we do not include noise terms \\(e_{k-n_e}\\)  in Equation (2.19), we have NARX models.</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.23} \\] <p>The Equation 2.24 describes a simple polynomial NARX model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^2+0.1139y_{k-1}x_{k-1} \\end{align} \\tag{2.24} \\] <p>The only difference in SysIdentPy is setting the <code>unbiased=False</code></p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False)\n)\n</code></pre> <p>The user can use the codes provided for linear models to analyse the nonlinear models with different noise realizations.</p>"},{"location":"book/2-NARMAX-Model-Representation/#narma","title":"NARMA","text":"<p>if we do not include input terms in Equation 2.19, it turns to NARMA model</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times\\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.25} \\] <p>The following example is a polynomial NARMA model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^3+0.1139y_{k-3}y_{k-4} + 0.2245e_{k-1} \\end{align} \\tag{2.26} \\] <p>Since the model representation do not have inputs, we have to set the model type to <code>NAR</code> and set <code>unbiased=True</code> again in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True),\n    model_type=\"NAR\"\n)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#nar","title":"NAR","text":"<p>if we do not include input terms and noise terms in Equation 2.19, it turns to AR model</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.27} \\] <p>The following example is a polynomial NAR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^2+0.1139y_{k-3}^3 -0.1691y_{k-4}y_{k-5} \\end{align} \\tag{2.28} \\] <p>In this case, we have to set the model type to <code>NAR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NAR\"\n)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#nfir","title":"NFIR","text":"<p>If we only keep input terms in Equation 2.19, it becomes a NFIR model</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j} \\end{align} \\tag{2.29} \\] <p>The following example is a polynomial NFIR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213x_{k-1}-0.5692x_{k-2}^2+0.1139x_{k-3}x_{k-4} -0.1691x_{k-4}^3 \\end{align} \\tag{2.30} \\] <p>In this case, we have to set the model type to <code>NFIR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NFIR\"\n)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#mixed-narmax-models","title":"Mixed NARMAX Models","text":"<p>In some applications, using a single basis functions cannot provide a satisfactory description for the relationship between the input (or independent) variables and the output (or response) variable. In order to improve the performance of the model, it has been proposed to use a linear combination of a set of nonlinear functions to replace the linear counterparts.</p> <p>You can achieve that in SysIdentPy using ensembles in basis functions. You can build a Fourier model where terms have interactions. You can also build a model with mixed basis functions, using terms expanded by polynomial basis and Fourier basis or any other basis function available is the package.</p> <p>You can only mix a basis function with the polynomial basis for now in SysIdentPy. You can mix Fourier with Polynomial, but you can't mix Fourier with Bernstein.</p> <p>To mix Fourier or Bernstein basis with Polynomial, the user just have to set <code>ensamble=True</code> in the basis function definition</p> <pre><code>from sysidentpy.basis_function import Fourier\n\nbasis_function = Fourier(degree=2, ensemble=True)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#neural-narx-network","title":"Neural NARX Network","text":"<p>Neural networks are models composed of interconnected layers of nodes (neurons) designed for tasks like classification and regression. Each neuron is a basic unit within these networks. Mathematically, a neuron is represented by a function \\(f\\) that takes an input vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]\\) and generates an output \\(y\\). This function usually involves a weighted sum of the inputs, an optional bias term \\(b\\), and an activation function \\(\\phi\\):</p> \\[ y = \\phi \\left( \\sum_{i=1}^{n} w_i x_i + b \\right) \\tag{2.31} \\] <p>where \\(\\mathbf{w} = [w_1, w_2, \\ldots, w_n]\\) are the weights associated with the inputs. The activation function \\(\\phi\\) introduces nonlinearity into the model, allowing the network to learn complex patterns. Common activation functions include:</p> <ul> <li> <p>Sigmoid: \\(\\phi(z) = \\frac{1}{1 + e^{-z}}\\)   Produces outputs between 0 and 1, making it useful for binary classification.</p> </li> <li> <p>Hyperbolic Tangent (tanh): \\(\\phi(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\)   Outputs values between -1 and 1, often used to center data around zero.</p> </li> <li> <p>Rectified Linear Unit (ReLU): \\(\\phi(z) = \\max(0, z)\\)   Outputs zero for negative values and the input value itself for positive values, helping to mitigate the vanishing gradient problem.</p> </li> <li> <p>Leaky ReLU: \\(\\phi(z) = \\max(0.01z, z)\\)   A variant of ReLU that allows a small, non-zero gradient when the input is negative, addressing the problem of dying neurons.</p> </li> <li> <p>Softmax: \\(\\phi(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\\)   Converts logits into probabilities for multi-class classification, ensuring that the outputs sum to 1.</p> </li> </ul> <p>Each activation function has its own advantages and is chosen based on the specific needs of the neural network and the task at hand.</p> <p>As mentioned, neural network is composed of multiple layers, each consisting of several neurons. In this respect, the layers can be categorized into:</p> <ul> <li>Input Layer: The layer that receives the input data.</li> <li>Hidden Layers: Intermediate layers that process the inputs through weighted connections and activation functions.</li> <li>Output Layer: The final layer that produces the output of the network.</li> </ul> <p>The network itself therefore has a very simple architecture. The terminology used in neural networks is also slightly different from the standard notation that is universal in system identification and statistics. So, instead of talking about model parameters, the term network weights is used, and instead of estimation, the term learning is used. This terminology was no doubt introduced to make it appear that something completely new was being discussed, whereas some of the problems addressed are quite traditional - Stephen A. Billings</p> <p>Notice that the network itself is simply a collection of nonlinear activation units \\(\\phi(\\cdot)\\)  that are simple static functions. There are no dynamics within the network. This is fine for applications such as pattern recognition, but to use the network in system identification lagged inputs and outputs are necessary and these have to be supplied as inputs either explicitly or through a recurrent procedure. In this respect, if we set \\(\\mathcal{F}\\) as a neural function, we can adapt it to create a neural NARX model by transforming the neural architecture into a NARX architecture. The neural NARX, however, is not linear in the parameters like the NARMAX models based on basis functions. So, algorithms like Orthogonal Least Squares are not adequate to estimate the weights of the model.</p> <p>SysIdentPy\u00a0support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier. We convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction.</p> <p>Series-Parallel allows us to use <code>pytorch</code> directly for training, so SysIdentPy uses <code>pytorch</code> in the backend for neural NARX along with auxiliary methods available only in SysIdentPy.</p> <p>A simple neural NARX model can be represented as a Multi-Layer Perceptron neural network with autoregressive component along with delayed inputs.</p> <p></p> <p>Figure 9. Parallel and series-parallel neural network architectures for modeling the dynamic system \\(\\mathbf{y}[k]=\\mathbf{F}(\\mathbf{y}[k-1], \\mathbf{y}[k-2], \\mathbf{u}[k-1], \\mathbf{u}[k-2])\\). The delay operator \\(q^{-1}\\) is such that \\(\\mathbf{y}[k-1]=q^{-1} \\mathbf{y}[k]\\). Reference: Antonio H. Ribeiro and Luis A. Aguirre</p> <p>Neural NARX is not the same model as Recurrent Neural Networks (RNN). The user is referred to the following paper for more details A Note on the Equivalence of NARX and RNN</p> <p>To build a Neural NARX network in SysIdentPy, the user must use <code>pytorch</code>. We use <code>pytorch</code> to make the definition of the network architecture flexible. However, this requires that the user have a better understanding of how a neural networks. See the script bellow of how to build a simple Neural NARX model in SysIdentPy</p> <pre><code>from torch import nn\nimport torch\n\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.narmax_tools import regressor_code\n\n# simulated data\nx_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre> <p>The user can use <code>cuda</code> following the same approach when build a neural network in pytorch</p> <pre><code>torch.cuda.is_available()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</code></pre> <p>The user can create a NARXNN object and choose the maximum lag of both input and output for building the regressor matrix to serve as input of the network. In addition, you can choose the loss function, the optimizer, the optional parameters of the optimizer, the number of epochs.</p> <p>Because we built this feature on top of Pytorch, you can choose any of the loss function of the torch.nn.functional. Click here for a list of the loss functions you can use. You just need to pass the name of the loss function you want.</p> <p>Similarly, you can choose any of the optimizer of the torch.optim. Click here for a list of optimizer available.</p> <pre><code>basis_function = Polynomial(degree=1)\nnarx_net = NARXNN(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 loss_func=\"mse_loss\",\n\u00a0 \u00a0 optimizer=\"Adam\",\n\u00a0 \u00a0 epochs=2000,\n\u00a0 \u00a0 verbose=False,\n\u00a0 \u00a0 device=device,\n\u00a0 \u00a0 optim_params={\n\u00a0 \u00a0 \u00a0 \u00a0 \"betas\": (0.9, 0.999),\n\u00a0 \u00a0 \u00a0 \u00a0 \"eps\": 1e-05,\n\u00a0 \u00a0 }, \u00a0# optional parameters of the optimizer\n)\n</code></pre> <p>Because the NARXNN model were defined using \\(ylag=2\\), \\(xlag=2\\) and a polynomial basis function with \\(degree=1\\), we have a regressor matrix with 4 features. We need the size of the regressor matrix to build the layers of our network. Our input data(<code>x_train</code>) have only one feature, but since we are creating an NARX network, a regressor matrix is built behind the scenes with new features based on the <code>xlag</code> and <code>ylag</code>.</p> <p>If you need help finding how many regressors are created behind the scenes you can use the <code>narmax_tools</code> function <code>regressor_code</code> and take the size of the regressor code generated:</p> <pre><code>basis_function = Polynomial(degree=1)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=2,\n    ylag=2,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=True,\n)\nn_regressors\n&gt;&gt;&gt; 4\n</code></pre> <p>The configuration of your network follows exactly the same pattern of a network defined in Pytorch. The following representing our NARX neural network.</p> <pre><code>class NARX(nn.Module):\n\u00a0 \u00a0 def __init__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin = nn.Linear(n_regressors, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin2 = nn.Linear(30, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin3 = nn.Linear(30, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 self.tanh = nn.Tanh()\n\n\n\u00a0 \u00a0 def forward(self, xb):\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin(xb)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin2(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin3(z)\n\u00a0 \u00a0 \u00a0 \u00a0 return z\n</code></pre> <p>The user have to pass the defined network to our NARXNN estimator and set <code>cuda</code> if available (or needed):</p> <pre><code>narx_net.net = NARX()\n\nif device == \"cuda\":\n\u00a0 \u00a0 narx_net.net.to(torch.device(\"cuda\"))\n</code></pre> <p>Because we have a fit (for training) and predict function for Polynomial NARMAX, we create the same pattern for the NARX net. So, you only have to fit and predict using the following:</p> <pre><code>narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid)\nyhat = narx_net.predict(X=x_valid, y=y_valid)\n</code></pre> <p>If the net configuration is built before calling the NARXNN, just pass the model to the NARXNN as follows:</p> <pre><code>class NARX(nn.Module):\n\u00a0 \u00a0 def __init__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin = nn.Linear(n_regressors, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin2 = nn.Linear(30, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin3 = nn.Linear(30, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 self.tanh = nn.Tanh()\n\n\n\u00a0 \u00a0 def forward(self, xb):\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin(xb)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin2(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin3(z)\n\u00a0 \u00a0 \u00a0 \u00a0 return z\n\n\nnarx_net2 = NARXNN(\n\u00a0 \u00a0 net=NARX(),\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 loss_func=\"mse_loss\",\n\u00a0 \u00a0 optimizer=\"Adam\",\n\u00a0 \u00a0 epochs=2000,\n\u00a0 \u00a0 verbose=False,\n\u00a0 \u00a0 optim_params={\n\u00a0 \u00a0 \u00a0 \u00a0 \"betas\": (0.9, 0.999),\n\u00a0 \u00a0 \u00a0 \u00a0 \"eps\": 1e-05,\n\u00a0 \u00a0 }, \u00a0# optional parameters of the optimizer\n)\n\nnarx_net2.fit(X=x_train, y=y_train)\nyhat = narx_net2.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#general-model-set-representation","title":"General Model Set Representation","text":"<p>Based on the idea of transforming a static neural network in a neural NARX model, we can extend the method for basically any model class. SysIdentPy do not aim to implement every model class that exists in literature. However, we created a functionality that allows the usage of any other machine learning package that follows a <code>fit</code> and <code>predict</code> API inside SysIdentPy to convert such models to NARX versions of them.</p> <p>Let's take XGBoost (eXtreme Gradient Boosting) Algorithm as an example. XGBoost is a well known model class used for regression tasks. XGBoost, however, are not a common choice when you are dealing with a dynamical system identification task because they are originally made for modeling static systems. You can easily transform XGBoost into a NARX model using SysIdentPy.</p> <p>Scikit-learn, for example, is another great example. You can transform any Scikit-learn model into NARX models using SysIdentPy. We will see such applications in detail at Chapter 11, but you can see how easy it in the script bellow</p> <pre><code>from sysidentpy.general_estimators import NARX\nfrom sysidentpy.basis_function import Polynomial\nfrom sklearn.linear_model import BayesianRidge\nimport xgboost as xgb\n\nbasis_function = Fourier(degree=1)\n# define the scikit estimator\nscikit_estimator = BayesianRidge()\n# transform scikit_estimator into NARX model\ngb_narx = NARX(\n    base_estimator=scikit_estimator,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\ngb_narx.fit(X=x_train, y=y_train)\nyhat = gb_narx.predict(X=x_valid, y=y_valid)\n\n# XGboost examples\nxgb_estimator = xgb.XGBRegressor()\nxgb_narx = NARX(\n    base_estimator=xgb_estimator,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nxgb_narx.fit(X=x_train, y=y_train)\nyhat = xgb_narx.predict(X=x_valid, y=y_valid)\n</code></pre> <p>You can use any other model by just changing the model class and passing it to the <code>base_estimator</code> in <code>NARX</code> functionality.</p>"},{"location":"book/2-NARMAX-Model-Representation/#mimo-models","title":"MIMO Models","text":"<p>To keep things simple, only SISO models were represented in previous sections. However,  the NARMAX  models can effortlessly be extended to MIMO case (Billings, S. A. and Chen, S. and Korenberg, M. J.):</p> \\[ \\begin{align}      y_{{_i}k}=&amp; F_{{_i}}^\\ell \\bigl[y_{{_1}k-1},  \\dotsc, y_{{_1}k-n^i_{y{_1}}},\\dotsc, y_{{_s}k-1},  \\dotsc, y_{{_s}k-n^i_{y{_s}}}, x_{{_1}k-d}, \\\\      &amp; x_{{_1}k-d-1}, \\dotsc, x_{{_1}k-d-n^i_{x{_1}}}, \\dotsc, x_{{_r}k-d}, x_{{_r}k-d-1}, \\dotsc, x_{{_r}k-d-n^i_{x{_r}}}\\bigr] + \\xi_{{_i}k}, \\end{align} \\tag{2.32} \\] <p>where for \\(i = 1, \\dotsc, s\\), each linear in the parameter sub-model can change regarding different maximum lags. More generally, considering</p> \\[ \\begin{align}     Y_k = \\begin{bmatrix}     y_{{_1}k} \\\\     y_{{_2}k} \\\\     \\vdots \\\\     y_{{_s}k}     \\end{bmatrix},     X_k = \\begin{bmatrix}     x_{{_1}k} \\\\     x_{{_2}k} \\\\     \\vdots \\\\     x_{{_r}k}     \\end{bmatrix},     \\Xi_k = \\begin{bmatrix}     \\xi_{{_1}k} \\\\     \\xi_{{_2}k} \\\\     \\vdots \\\\     \\xi_{{_r}k}     \\end{bmatrix}, \\end{align} \\tag{2.33} \\] <p>the MIMO model can be denoted as</p> \\[ \\begin{equation}              Y_k= F^\\ell[Y_{k-1},  \\dotsc, Y_{k-n_y},X_{k-d}, X_{k-d-1}, \\dotsc, X_{k-d-n_x}] + \\Xi_k, \\end{equation} \\tag{2.34} \\] <p>where \\(Xk ~= \\{x_{{_1}k}, x_{{_2}k}, \\dotsc, x_{{_r}k}\\}\\in \\mathbb{R}^{n^i_{x{_r}}}\\) and \\(Yk~= \\{y_{{_1}k}, y_{{_2}k}, \\dotsc, y_{{_s}k}\\}\\in \\mathbb{R}^{n^i_{y{_s}}}\\). The number of possibles terms of MIMO NARX model given the \\(i\\)-th polynomial degree, \\(\\ell_i\\), is:</p> \\[ \\begin{equation}     n_{{_{m}}r} = \\sum_{j = 0}^{\\ell_i}n_{ij}, \\end{equation} \\tag{2.35} \\] <p>where</p> \\[ \\begin{align}     n_{ij} = \\frac{ n_{ij-1} \\biggl[ \\sum\\limits_{k=1}^{s} n^i_{y_k} + \\sum\\limits_{k=1}^{r} n^i_{x_k} + j - 1 \\biggr]}{j}, \\qquad n_{i0}=1, j=1, \\dotsc, \\ell_i. \\end{align} \\tag{2.36} \\] <p>If \\(s=1\\), we have a MISO model that can be represented by a single polynomial function. Additionally, a MIMO model can be decomposed into MISO models, as presented in the following figure:</p> <p></p> <p>Figure 10. A MIMO model split into individual MISO models.</p> <p>SysIdentPy do not support MIMO models yet, only MISO models. You can, however, decompose a MIMO system as presented in Figure 9 and use SysIdentPy to create models for each subsystem.</p>"},{"location":"book/3-Parameter-Estimation/","title":"3. Parameter Estimation","text":""},{"location":"book/3-Parameter-Estimation/#least-squares","title":"Least Squares","text":"<p>Consider the NARX model described in a generic form as</p> \\[ \\begin{equation}     y_k = \\psi^\\top_{k-1}\\hat{\\Theta} + \\xi_k, \\end{equation} \\tag{3.1} \\] <p>where \\(\\psi^\\top_{k-1} \\in \\mathbb{R}^{n_r \\times n}\\) is the information matrix, also known as the regressors' matrix. The information matrix is the input and output transformation based in a basis function and \\(\\hat{\\Theta}~\\in \\mathbb{R}^{n_{\\Theta}}\\) the vector of estimated parameters. The model above can also be represented in a matrix form as:</p> \\[ \\begin{equation}     y = \\Psi\\hat{\\Theta} + \\Xi, \\end{equation} \\tag{3.2} \\] <p>where</p> \\[ \\begin{align}     Y = \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_n     \\end{bmatrix},     \\Psi = \\begin{bmatrix}     \\psi_{{_1}} \\\\     \\psi_{{_2}} \\\\     \\vdots \\\\     \\psi_{{_{n_{\\Theta}}}}     \\end{bmatrix}^\\top=     \\begin{bmatrix}     \\psi_{{_1}1} &amp; \\psi_{{_2}1} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}1} \\\\     \\psi_{{_1}2} &amp; \\psi_{{_2}2} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}2} \\\\     \\vdots &amp; \\vdots &amp;       &amp; \\vdots \\\\     \\psi_{{_1}n} &amp; \\psi_{{_2}n} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}n} \\\\     \\end{bmatrix},     \\hat{\\Theta} = \\begin{bmatrix}     \\hat{\\Theta}_1 \\\\     \\hat{\\Theta}_2 \\\\     \\vdots \\\\     \\hat{\\Theta}_{n_\\Theta}     \\end{bmatrix},     \\Xi = \\begin{bmatrix}     \\xi_1 \\\\     \\xi_2 \\\\     \\vdots \\\\     \\xi_n     \\end{bmatrix}. \\end{align} \\tag{3.3} \\] <p>We will consider the polynomial basis function to keep the examples straightforward, but the methods here will work for any other basis function.</p> <p>The parametric NARX model is linear in the parameters \\(\\Theta\\), so we can use well known algorithms, like the linear Least Squares algorithm developed by Gauss in \\(1795\\), to estimate the model parameters. The idea is to find the parameter vector that minimizes the \\(l2\\)-norm, also known as the residual sum of squares, described as</p> \\[ \\begin{equation}     J_{\\hat{\\Theta}} = \\Xi^\\top \\Xi = (y - \\Psi\\hat{\\Theta})^\\top(y - \\Psi\\hat{\\Theta}) = \\lVert y - \\Psi\\hat{\\Theta} \\rVert^2. \\end{equation} \\tag{3.4} \\] <p>In Equation 3.4 , \\(\\Psi\\hat{\\Theta}\\) is the one-step ahead prediction of \\(y_k\\), expressed as</p> \\[ \\begin{equation}     \\hat{y}_{1_k} = g(y_{k-1}, u_{k-1}\\lvert ~\\Theta), \\end{equation} \\tag{3.5} \\] <p>where \\(g\\) is some unknown polynomial function. If the gradient of \\(J_{\\Theta}\\) with respect to \\(\\Theta\\) is equal to zero, then we have the normal equation and the Least Squares estimate is expressed as</p> \\[ \\begin{equation}     \\hat{\\Theta}  = (\\Psi^\\top\\Psi)^{-1}\\Psi^\\top y, \\end{equation} \\tag{3.6} \\] <p>where \\((\\Psi^\\top\\Psi)^{-1}\\Psi^\\top\\) is called the pseudo-inverse of the matrix \\(\\Psi\\), denoted \\(\\Psi^+ \\in \\mathbb{R}^{n \\times n_r}\\).</p> <p>In order to have a bias-free estimator, the following are the basic assumptions needed for the least-squares method: - A1 - There is no correlation between the error vector, \\(\\Xi\\), and the matrix of regressors, \\(\\Psi\\). Mathematically: - \\(\\mathrm{E}\\{[(\\Psi^\\top\\Psi)^{-1}\\Psi^\\top] \\Xi\\} = \\mathrm{E}[(\\Psi^\\top\\Psi)^{-1}\\Psi^\\top] \\mathrm{E}[\\Xi]; \\tag{3.7}\\) - A2 - The error vector \\(\\Xi\\) is a zero mean white noise sequence: - \\(\\mathrm{E}[\\Xi] = 0; \\tag{3.8}\\) - A3 - The covariance matrix of the error vector is - \\(\\mathrm{Cov}[\\hat{\\Theta}] = \\mathrm{E}[(\\Theta - \\hat{\\Theta})(\\Theta - \\hat{\\Theta})^\\top] = \\sigma^2(\\Psi^\\top\\Psi); \\tag{3.9}\\) - A4 - The matrix of regressors, \\(\\Psi\\), is full rank.</p> <p>The aforementioned assumptions are needed to guarantee that the Least Squares algorithm produce a unbiased final model.</p>"},{"location":"book/3-Parameter-Estimation/#example","title":"Example","text":"<p>Let's see a practical example. Consider the model</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k} \\tag{3.10} \\] <p>We can generate the input <code>X</code>  and output <code>y</code> using SysIdentPy. Before getting in the details, let run a simple model using SysIdentPy. Because we know a priori that the system we are trying to have is not linear (the simulated system have an interaction term \\(0.1y_{k-1}x_{k-1}\\)) and the order is 2 (the maximum lag of the input and output), we will set the hyperparameters accordingly. Note that this a simulated scenario, and you'll not have such information a priori in a real identification task. But don't worry, the idea, for now, is just show how things works and we will develop some real models along the book.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\n\nx_train, x_test, y_train, y_test = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    n_info_values=3,\n    ylag=1,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\n# print the identified model\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nRegressors   Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.56885108E-01\n1         y(k-1)  2.0000E-01  3.96313039E-02\n2  x1(k-1)y(k-1)  1.0001E-01  3.48355000E-03\n</code></pre> <p>As you can see, the final model have the same 3 regressors of the simulated system and the parameters are very close the ones used to simulate the system. This shows us that the Least Squares performed well for this data.</p> <p>In this example, however, we are applying a Model Structure Selection algorithm (FROLS), which we will see in chapter 6. That's why the final model have only 3 regressors. The parameter estimation algorithm do not choose which terms to include in the model, so if we have a expanded basis function with 6 regressors, it will estimate the parameter for each one of the regressors.</p> <p>To check how this work, we can use SysIdentPy without Model Structure Selection by generating the information matrix and applying the parameter estimation algorithm directly.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils import build_lagged_matrix\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\ntheta = LeastSquares().optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[-4.1511e-06]\n [ 2.0002e-01]\n [ 1.1237e-05]\n [ 1.0068e-05]\n [ 8.9997e-01]\n [-6.3216e-05]\n [ 1.3298e-04]\n [ 1.0008e-01]\n [ 6.3118e-05]\n [-5.6031e-05]\n [-1.9073e-05]\n [-1.8223e-04]\n [ 1.1307e-04]\n [-1.6601e-04]\n [-8.5068e-05]]\n</code></pre> <p>In this case, we have 15 model parameters. If we take a look in the basis function expansion where the degree of the polynomial is equal to 2 and the lags for <code>y</code> and <code>x</code> are set to 2, we have</p> <pre><code>from sysidentpy.utils.narmax_tools import regressor_code\nbasis_function = Polynomial(degree=2)\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"Polynomial\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nregressors\n\narray([[ 0, 0],\n   [1001, 0],\n   [1002, 0],\n   [2001, 0],\n   [2002, 0],\n   [1001, 1001],\n   [1002, 1001],\n   [2001, 1001],\n   [2002, 1001],\n   [1002, 1002],\n   [2001, 1002],\n   [2002, 1002],\n   [2001, 2001],\n   [2002, 2001],\n   [2002, 2002]]\n   )\n</code></pre> <p>The regressors is how SysIdentPy encode the polynomial basis function following this  codification pattern:</p> <ul> <li>\\(0\\) is the constant term,\\n\",</li> <li>\\([1001] = y_{k-1}\\)</li> <li>\\([100n] = y_{k-n}\\)</li> <li>\\([200n] = x1_{k-n}\\)</li> <li>\\([300n] = x2_{k-n}\\)</li> <li>\\([1011, 1001] = y_{k-11} \\\\times y_{k-1}\\)</li> <li>\\([100n, 100m] = y_{k-n} \\times y_{k-m}\\)</li> <li>\\([12001, 1003, 1001] = x11_{k-1} \\times y_{k-3} \\times y_{k-1}\\),</li> <li>and so on</li> </ul> <p>So, if you take a look at the parameters, we can see that the Least Squares algorithm estimation for the terms that belongs to the simulated system are very close to the real values.</p> <pre><code>[1001, 0] -&gt; [ 2.00002486e-01]\n[2002, 0] -&gt; [ 8.99927332e-01]\n[2001, 1001] -&gt; [ 1.00062340e-01]\n</code></pre> <p>Moreover, the parameters estimated for the other regressors are considerably lower values than the ones estimated for the correct terms, indicating that the other might not be relevant to the model.</p> <p>You can start thinking that we only need to define a basis function and apply some parameter estimation technique to build NARMAX models. However, as mentioned before, the main goal of the NARMAX methods is to build the best model possible while keeping it simple. And that's true for the case where we applied the FROLS algorithm. Besides, when dealing with system identification we want to recover the dynamics of the system under study, so adding more terms than necessary can lead to unexpected behaviors, poor performance and unstable models. Remember, this is only a toy example, so in real cases the model structure selection is fundamental.</p> <p>You can implement Least Squares method as simple as</p> <pre><code>import numpy as np\n\ndef simple_least_squares(psi, y):\n\u00a0 \u00a0 return np.linalg.pinv(psi.T @ psi) @ psi.T @ y\n\n# use the psi and y data created in previous examples or\n# create them again here to run the example.\ntheta = simple_least_squares(psi, y_train[max_lag:, :])\n\ntheta\n\narray(\n    [\n       [-1.08377785e-05],\n       [ 2.00002486e-01],\n       [ 1.73422294e-05],\n       [-3.50957931e-06],\n       [ 8.99927332e-01],\n       [ 2.04427279e-05],\n       [-1.47542408e-04],\n       [ 1.00062340e-01],\n       [ 4.53379771e-05],\n       [ 8.90006341e-05],\n       [ 1.15234873e-04],\n       [ 1.57770755e-04],\n       [ 1.58414037e-04],\n       [-3.09236444e-05],\n       [-1.60377753e-04]\n    ]\n)\n</code></pre> <p>As you can see, the estimated parameters are very close. However, be careful when using such approach in under-, well-, or over-determined systems. We recommend to use the numpy or scipy <code>lstsq</code> methods.</p>"},{"location":"book/3-Parameter-Estimation/#total-least-squares","title":"Total Least Squares","text":"<p>This section is based on the Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least squares methods. Signal Processing..</p> <p>The Total Least Squares (TLS) algorithm, is a statistical method used to find the best-fitting linear relationship between variables when both the input and output signals present white noise perturbation. Unlike ordinary least squares (OLS), which assumes that only the dependent variable is subject to error, TLS considers errors in all measured variables, providing a more robust solution in many practical applications. The algorithm was proposed by Golub and Van Loan.</p> <p>In TLS, we assume errors in both \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), denoted as \\(\\Delta \\mathbf{X}\\) and \\(\\Delta \\mathbf{Y}\\), respectively. The true model becomes:</p> \\[ \\mathbf{Y} + \\Delta \\mathbf{Y} = (\\mathbf{X} + \\Delta \\mathbf{X}) \\mathbf{B} \\tag{3.11} \\] <p>Rearranging, we get:</p> \\[ \\Delta \\mathbf{Y} = \\Delta \\mathbf{X} \\mathbf{B} \\tag{3.12} \\]"},{"location":"book/3-Parameter-Estimation/#objective-function","title":"Objective Function","text":"<p>The TLS solution minimizes the Frobenius norm of the total perturbations in \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\):</p> \\[ \\min_{\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}} \\|[\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}]\\|_F \\tag{3.13} \\] <p>subject to:</p> \\[ (\\mathbf{X} + \\Delta \\mathbf{X}) \\mathbf{B} = \\mathbf{Y} + \\Delta \\mathbf{Y} \\tag{3.14} \\] <p>where \\(\\| \\cdot \\|_F\\) denotes the Frobenius norm.</p>"},{"location":"book/3-Parameter-Estimation/#classical-solution","title":"Classical Solution","text":"<p>The classical approach to solve the TLS problem is by using Singular Value Decomposition (SVD). The augmented matrix \\([\\mathbf{X}, \\mathbf{Y}]\\) is decomposed as:</p> \\[ [\\mathbf{X}, \\mathbf{Y}] = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\tag{3.15} \\] <p>where \\(\\mathbf{U}\\) is an \\(n \\times n\\) orthogonal matrix, \\(\\Sigma=\\operatorname{diag}\\left(\\sigma_1, \\ldots, \\sigma_{n+d}\\right)\\) is a diagonal matrix of singular values; and \\(\\mathbf{V}\\) is an orthogonal matrix defined as</p> \\[ V:=\\left[\\begin{array}{cc} V_{11} &amp; V_{12} \\\\ V_{21} &amp; V_{22} \\end{array}\\right] \\quad \\begin{aligned} \\end{aligned} \\quad \\text { and } \\quad \\Sigma:=\\left[\\begin{array}{cc} \\Sigma_1 &amp; 0 \\\\ 0 &amp; \\Sigma_2 \\end{array}\\right] \\begin{gathered} \\end{gathered} . \\tag{3.16} \\] <p>A total least squares solution exists if and only if \\(V_{22}\\) is non-singular. In addition, it is unique if and only if \\(\\sigma_n \\neq \\sigma_{n+1}\\). In the case when the total least squares solution exists and is unique, it is given by</p> \\[ \\widehat{X}_{\\mathrm{tls}}=-V_{12} V_{22}^{-1} \\tag{3.17} \\] <p>and the corresponding total least squares correction matrix is</p> \\[ \\Delta C_{\\mathrm{tls}}:=\\left[\\begin{array}{ll} \\Delta A_{\\mathrm{tls}} &amp; \\Delta B_{\\mathrm{tls}} \\end{array}\\right]=-U \\operatorname{diag}\\left(0, \\Sigma_2\\right) V^{\\top} . \\tag{3.18} \\] <p>This is implemented in SysIdentPy as follows:</p> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using Total Least Squares method.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    \"\"\"\n    check_linear_dependence_rows(psi)\n    full = np.hstack((psi, y))\n    n = psi.shape[1]\n    _, _, v = np.linalg.svd(full, full_matrices=True)\n    theta = -v.T[:n, n:] / v.T[n:, n:]\n    return theta.reshape(-1, 1)\n</code></pre> <p>To use it in the modeling task, just import it like we did in the Least Squares example.</p> <p>From now on the examples will not include the Model Structure Selection step. The goal here is to focus on the parameter estimation methods. However, we already provided an example including MSS in the Least Squares section, so you will not have any problem to test that with other parameter estimation algorithms.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import TotalLeastSquares\nfrom sysidentpy.utils import build_lagged_matrix\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\ntheta = TotalLeastSquares().optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[ 1.3321e-04]\n [ 2.0014e-01]\n [-1.1771e-04]\n [ 5.8085e-05]\n [ 9.0011e-01]\n [-1.5490e-04]\n [-1.3517e-05]\n [ 9.9824e-02]\n [ 8.2326e-05]\n [-2.2814e-04]\n [-7.0837e-05]\n [-5.4319e-05]\n [-1.7472e-04]\n [-2.0396e-04]\n [ 1.7416e-05]]\n</code></pre>"},{"location":"book/3-Parameter-Estimation/#recursive-least-squares","title":"Recursive Least Squares","text":"<p>Consider the regression model</p> \\[ y_k = \\mathbf{\\Psi}_k^T \\theta_k + \\epsilon_k \\tag{3.19}\\] <p>where: - \\(y_k\\) is the observed output at time $ k $. - \\(\\mathbf{\\Psi}_k\\) is the information matrix at time \\(k\\). - \\(\\theta_k\\) is the parameter vector to be estimated at time \\(k\\). - \\(\\epsilon_k\\) is the noise at time \\(k\\).</p> <p>The Recursive Least Squares (RLS) algorithm updates the parameter estimate \\(\\theta_k\\) recursively as new data points \\((\\mathbf{x}_k, y_k)\\) become available, minimizing a weighted linear least squares cost function relating to the information matrix in a sequential manner. RLS is particularly useful in real-time applications where the data arrives sequentially and the model needs continuous updating or for modeling time varying systems (if the forgetting factor is included).</p> <p>Because it's a recursive estimation, it is useful to relate \\(\\hat{\\Theta}_k\\) to \\(\\hat{\\Theta}_{k-1}\\). In other words, the new \\(\\hat{\\Theta}_k\\) depends on the last estimated value (k). Moreover, to estimate \\(\\hat{\\Theta}_k\\), we need to incorporate the current information present in \\(y_k\\).</p> <p>Aguirre BOOK defines the Recursive Least Squares estimator with forgetting factor \\(\\lambda\\) as</p> \\[ \\left\\{\\begin{array}{c} K_k= Q_k\\psi_k = \\frac{P_{k-1} \\psi_k}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda} \\\\ \\hat{\\theta}_k=\\hat{\\theta}_{k-1}+K_k\\left[y(k)-\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1}\\right] \\\\ P_k=\\frac{1}{\\lambda}\\left(P_{k-1}-\\frac{P_{k-1} \\psi_k \\psi_k^{\\mathrm{T}} P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda}\\right) \\end{array}\\right. \\tag{3.20} \\] <p>where \\(K_k\\) is the gain vector calculation (also known as Kalman gain), \\(P_k\\) is the covariance matrix update, and \\(y_k - \\mathbf{\\Psi}_k^T \\theta_{k-1}\\) is the a priori estimation error. The forgetting factor \\(\\lambda\\) (\\(0 &lt; \\lambda \\leq 1\\)) is usually defined between \\(0.94\\) and \\(0.99\\). If you set \\(\\lambda = 1\\) you will be using the traditional recursive algorithm. The equation above consider that the regressor vector \\(\\psi(k-1)\\) has been rewritten as \\(\\psi_k\\), since this vector is updated at iteration \\(k\\) and contains information up to time instant \\(k-1\\). We can  Initialize the parameter estimate \\(\\theta_0\\) as</p> \\[ \\theta_0 = \\mathbf{0} \\tag{3.21}\\] <p>and Initialize the inverse of the covariance matrix \\(\\mathbf{P}_0\\) with a large value:</p> \\[ \\mathbf{P}_0 = \\frac{\\mathbf{I}}{\\delta} \\tag{3.22}\\] <p>where \\(\\delta\\) is a small positive constant, and \\(\\mathbf{I}\\) is the identity matrix.</p> <p>The forgetting factor \\(\\lambda\\) controls how quickly the algorithm forgets past data: - \\(\\lambda = 1\\) means no forgetting, and all past data are equally weighted. - \\(\\lambda &lt; 1\\) means that when new data is available, all weights are multiplied by \\(\\lambda\\), which can be interpreted as the ratio between consecutive weights for the same data.</p> <p>You can access the source code to check how SysIdentPy implements the RLS algorithm. The following example present how you can use it in SysIdentPy.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.utils import build_lagged_matrix\nimport matplotlib.pyplot as plt\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n = 1000, colored_noise = False, sigma = 0.001, train_percentage = 90\n)\n\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\nestimator = RecursiveLeastSquares(lam=0.99)\ntheta = estimator.optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[-1.1778e-04]\n [ 1.9988e-01]\n [-9.3114e-05]\n [ 2.5119e-04]\n [ 9.0006e-01]\n [ 1.8339e-04]\n [-1.1943e-04]\n [ 9.9957e-02]\n [-4.6181e-05]\n [ 1.3155e-04]\n [ 3.4535e-04]\n [ 1.3843e-04]\n [-3.5454e-05]\n [ 1.5669e-04]\n [ 2.4311e-04]]\n</code></pre> <p>You can plot the evolution of the estimated parameters over time by accessing the <code>theta_evolution</code> values <pre><code># plotting only the first 50 values\nplt.plot(estimator.theta_evolution.T[:50, :])\nplt.xlabel(\"iterations\")\nplt.ylabel(\"theta\")\n</code></pre> </p> <p>Figure 1. Evolution of the estimated parameters over time using the RLS algorithm.</p>"},{"location":"book/3-Parameter-Estimation/#least-mean-squares","title":"Least Mean Squares","text":"<p>The Least Mean Squares (LMS) adaptive filter is a popular stochastic gradient algorithm developed by Widrow and Hoff in 1960. The LMS adaptive filter aims to adaptively change its filter coefficients to achieve the best possible filtering of a signal. This is done by minimizing the error between the desired signal \\(d(n)\\) and the filter output \\(y(n)\\). We can derive the LMS algorithm from the RLS formulation.</p> <p>In RLS, the \\(\\lambda\\) is related to the minimization of the sum of weighted squares of the innovation</p> \\[ J_k = \\sum^k_{j=1}\\lambda^{k-j}e^2_j. \\tag{3.23} \\] <p>The \\(Q_k\\) in Equation 3.20, defined as</p> \\[ Q_k = \\frac{P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda} \\\\ \\tag{3.24} \\] <p>is derived from the general form the Kalman Filter (KF) algorithm.</p> \\[ Q_k = \\frac{P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+v_0} \\\\ \\tag{3.25} \\] <p>where \\(v_0\\) is the variance of the noise in the definition of the KF, in which the cost function is defined as the sum of squares of the innovation (noise). You can check the details in the Billings, S. A. - Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains.</p> <p>If we change \\(Q_k\\) in Equation 3.25 to scaled identity matrix</p> \\[ Q_k = \\frac{\\mu}{\\Vert \\psi_k \\Vert^2}I \\tag{3.26} \\] <p>where \\(\\mu \\in \\mathbb{R}^+\\), the \\(Q_k\\) and \\(\\hat{\\theta}_k\\) in Equation 3.20 becomes</p> \\[ \\hat{\\theta}_k=\\hat{\\theta}_{k-1}+\\frac{\\mu\\left[y(k)-\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1}\\right]}{\\Vert \\psi_k \\Vert^2}\\psi_k \\tag{3.27} \\] <p>where \\(\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1} = \\hat{y}_k\\), which is known as the LMS algorithm.</p>"},{"location":"book/3-Parameter-Estimation/#convergence-and-step-size","title":"Convergence and Step-Size","text":"<p>The step-size parameter \\(\\mu\\) plays a crucial role in the performance of the LMS algorithm. If \\(\\mu\\) is too large, the algorithm may become unstable and fail to converge. If \\(\\mu\\) is too small, the algorithm will converge slowly. The choice of \\(\\mu\\) is typically:</p> \\[ 0 &lt; \\mu &lt; \\frac{2}{\\lambda_{\\max}} \\tag{3.28} \\] <p>where \\(\\lambda_{\\max}\\) is the largest eigenvalue of the input signal\u2019s autocorrelation matrix.</p> <p>In SysIdentPy, you can use several variants of the LMS algorithm:</p> <ol> <li>LeastMeanSquareMixedNorm</li> <li>LeastMeanSquares</li> <li>LeastMeanSquaresFourth</li> <li>LeastMeanSquaresLeaky</li> <li>LeastMeanSquaresNormalizedLeaky</li> <li>LeastMeanSquaresNormalizedSignRegressor</li> <li>LeastMeanSquaresNormalizedSignSign</li> <li>LeastMeanSquaresSignError</li> <li>LeastMeanSquaresSignSign</li> <li>AffineLeastMeanSquares</li> <li>NormalizedLeastMeanSquares</li> <li>NormalizedLeastMeanSquaresSignError</li> <li>LeastMeanSquaresSignRegressor</li> </ol> <p>To use any one on the methods above, you just need to import it and set the <code>estimator</code> using the option you want:</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastMeanSquares\nfrom sysidentpy.utils import build_lagged_matrix\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n = 1000, colored_noise = False, sigma = 0.001, train_percentage = 90\n)\n\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\nestimator = LeastMeanSquares(mu=0.1)\ntheta = estimator.optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[ 1.5924e-04]\n [ 1.9950e-01]\n [ 3.2137e-04]\n [ 1.7824e-04]\n [ 8.9951e-01]\n [ 2.7314e-04]\n [ 3.3538e-04]\n [ 1.0062e-01]\n [ 3.5219e-04]\n [ 1.3544e-04]\n [ 3.4149e-04]\n [ 5.6315e-04]\n [-4.6664e-04]\n [ 2.2849e-04]\n [ 1.0536e-04]]\n</code></pre>"},{"location":"book/3-Parameter-Estimation/#extended-least-squares-algorithm","title":"Extended Least Squares Algorithm","text":"<p>Let's show an example of the effect of a biased parameter estimation. To make things simple,The data is generated by simulating the following model:</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k} \\] <p>In this case, we know the values of the true parameters, so it will be easier to understand how they are affected by a biased estimation. The data is generated using a method from SysIdentPy. If colored_noise is set to True in the method, a colored noise is added to the data:</p> \\[e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\] <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma\\) is defined by the user.</p> <p>We will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n</code></pre> <p>First we will train a model without the Extended Least Squares Algorithm for comparison purpose.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=False)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=False,\n\u00a0 \u00a0 n_terms=3,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR x1(k-2) 9.0442E-01 7.55518391E-01 y(k-1) 2.7405E-01 7.57565084E-02 x1(k-1)y(k-1) 9.8757E-02 3.12896171E-03 <p>Clearly we have something wrong with the obtained model. The estimated parameters differs from the true one defined in the equation that generated the data. As we can observe above, the model structure is exact the same the one that generate the data. You can se that the ERR ordered the terms in the correct way. And this is an important note regarding the ERR algorithm: it is very robust to colored noise!!</p> <p>That is a great feature! However, although the structure is correct, the model parameters are not correct! Here we have a biased estimation! For instance, the real parameter for \\(y_{k-1}\\) is \\(0.2\\), not \\(0.274\\).</p> <p>In this case, we are actually modeling using a NARX model, not a NARMAX. The MA part exists to allow an unbiased estimation of the parameters. To achieve a unbiased estimation of the parameters we have the Extend Least Squares algorithm.</p> <p>Before applying the Extended Least Squares Algorithm we will run several NARX models to check how different the estimated parameters are from the real ones.</p> <pre><code>parameters = np.zeros([3, 50])\nfor i in range(50):\n\u00a0 \u00a0 x_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 \u00a0 \u00a0 n=3000, colored_noise=True, train_percentage=90\n\u00a0 \u00a0 )\n\u00a0 \u00a0 model.fit(X=x_train, y=y_train)\n\u00a0 \u00a0 parameters[:, i] = model.theta.flatten()\n\n# Set the theme for seaborn (optional)\nsns.set_theme()\nplt.figure(figsize=(14, 4))\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label='Parameter 1')\nsns.kdeplot(parameters.T[:, 1], label='Parameter 2')\nsns.kdeplot(parameters.T[:, 2], label='Parameter 3')\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color='k', linestyle='--', label='Real Value 0.1')\nplt.axvline(x=0.2, color='k', linestyle='--', label='Real Value 0.2')\nplt.axvline(x=0.9, color='k', linestyle='--', label='Real Value 0.9')\nplt.xlabel('Parameter Value')\nplt.ylabel('Density')\nplt.title('Kernel Density Estimate of Parameters')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Figure 2.: Kernel Density Estimates (KDEs) of the estimated parameters obtained from 50 NARX models realizations, each fitted to data with colored noise. The vertical dashed lines indicate the true parameter values used to generate the data. While the model structure is correctly identified, the estimated parameters are biased due to the omission of the Moving Average (MA) component, highlighting the need for the Extended Least Squares algorithm to achieve unbiased parameter estimation</p> <p>As shown in figure above, we have a problem to estimate the parameter for \\(y_{k-1}\\). Now we will use the Extended Least Squares Algorithm. In SysIdentPy, just set <code>unbiased=True</code> in the parameter estimation definition and the ELS algorithm will be applied.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\nfor i in range(50):\n\u00a0 \u00a0 x_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 \u00a0 \u00a0 n=3000, colored_noise=True, train_percentage=90\n\u00a0 \u00a0 )\n\u00a0 \u00a0 model = FROLS(\n\u00a0 \u00a0 \u00a0 \u00a0 order_selection=False,\n\u00a0 \u00a0 \u00a0 \u00a0 n_terms=3,\n\u00a0 \u00a0 \u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 \u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 \u00a0 \u00a0 elag=2,\n\u00a0 \u00a0 \u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 \u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 \u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 )\n\n\u00a0 \u00a0 model.fit(X=x_train, y=y_train)\n\u00a0 \u00a0 parameters[:, i] = model.theta.flatten()\n\nplt.figure(figsize=(14, 4))\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label='Parameter 1')\nsns.kdeplot(parameters.T[:, 1], label='Parameter 2')\nsns.kdeplot(parameters.T[:, 2], label='Parameter 3')\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color='k', linestyle='--', label='Real Value 0.1')\nplt.axvline(x=0.2, color='k', linestyle='--', label='Real Value 0.2')\nplt.axvline(x=0.9, color='k', linestyle='--', label='Real Value 0.9')\nplt.xlabel('Parameter Value')\nplt.ylabel('Density')\nplt.title('Kernel Density Estimate of Parameters')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Figure 3. Kernel Density Estimates (KDEs) of the estimated parameters obtained from 50 NARX models using the Extended Least Squares (ELS) algorithm with unbiased estimation. The vertical dashed lines indicate the true parameter values used to generate the data.</p> <p>Unlike the previous biased estimation, these KDEs in Figure 3 show that the estimated parameters are now closely aligned with the true values, demonstrating the effectiveness of the ELS algorithm in achieving unbiased parameter estimation, even in the presence of colored noise.</p> <p>The Extended Least Squares algorithm is iterative by nature. In SysIdentPy, the default number of iterations is set to 30 (<code>uiter=30</code>). However, the literature suggests that the algorithm typically converges quickly, often within 10 to 20 iterations. Therefore, you may want to test different numbers of iterations to find the optimal balance between convergence speed and computational efficiency.</p>"},{"location":"book/4-Model-Structure-Selection/","title":"4. Model Structure Selection","text":""},{"location":"book/4-Model-Structure-Selection/#introduction","title":"Introduction","text":"<p>This section is taken mainly from my master thesis, which was based on Billings, S. A.</p> <p>Selecting the model structure is crucial to develop models that can correctly reproduce the system behavior. If some prior information about the system are known, e.g., the dynamic order and degree of nonlinearity, determining the terms and then estimate the parameters is trivial. In real life scenarios, however, usually there is no information about what terms should be included in the model and the correct regressors has to be selected in the identification framework. If the MSS is not performed with the necessary concerns, the scientific law that describes the system may will not be revealed and resulting in misleading interpretations about the system. To illustrate this scenario, consider the following example.</p> <p>Let \\(\\mathcal{D}\\) denote an arbitrary dataset</p> \\[ \\begin{equation}     \\mathcal{D} = \\{(x_k, y_k), k = 1, 2, \\dotsc, n\\}, \\end{equation} \\tag{1} \\] <p>where \\(x_k \\in \\mathbb{R}^{n_x}\\) and \\(y_k\\in \\mathbb{R}^{n_y}\\) are the input and output of an unknown system and \\(n\\) is the number of samples in the dataset. The following are two polynomial NARX models built to describe that system:</p> \\[ \\begin{align}     y_{ak} &amp;= 0.7077y_{ak-1} + 0.1642u_{k-1} + 0.1280u_{k-2} \\end{align} \\tag{2} \\] \\[ \\begin{align}     y_{bk} &amp;= 0.7103y_{bk-1} + 0.1458u_{k-1} + 0.1631u_{k-2} \\\\            &amp;\\quad - 1467y_{bk-1}^3 + 0.0710y_{bk-2}^3 + 0.0554y_{bk-3}^2u_{k-3}. \\end{align} \\tag{3} \\] <p>Figure 1 shows the predicted values of each model and the real data. As can be observed, the nonlinear model 2 seems to fit the data better than the linear model 1. The original system under consideration is an RLC circuit, consisting of a resistor (R), inductor (L), and capacitor (C) connected in series with a voltage source. It is well known that the behavior of such an RLC series circuit can be accurately described by a linear second-order differential equation that relates the current \\(I(t)\\) and the applied voltage \\(V(t)\\):</p> \\[ L\\frac{d^2I(t)}{dt^2} + R\\frac{dI(t)}{dt} + \\frac{1}{C}I(t) = \\frac{dV(t)}{dt} \\tag{4} \\] <p>Given this linear relationship, an adequate model for the RLC circuit should reflect this second-order linearity. While Model 2, which includes nonlinear terms, may provide a closer fit to the data, it is clearly over-parameterized. Such over-parameterization can introduce spurious nonlinear effects, often referred to as \"ghost\" nonlinearities, which do not correspond to the actual dynamics of the system. Therefore, these models need to be interpreted with caution, as the use of an overly complex model could obscure the true linear nature of the system and lead to incorrect conclusions about its behavior.</p> <p></p> <p>Figure 1.Results for two polynomial NARX models fitted to data from an unknown system. Model 1 (left) is a linear model, while Model 2 (right) includes nonlinear terms. The figure illustrates that Model 2 provides a closer fit to the data compared to Model 1. However, since the original system is a linear RLC circuit known to have a second-order linear behavior, the improved fit of Model 2 may be misleading due to over-parameterization. This highlights the importance of considering the physical characteristics of the system when interpreting model results to avoid misinterpretation of artificial nonlinearities. Reference: Meta Model Structure Selection: An Algorithm For Building Polynomial NARX Models For Regression And Classification</p> <p>Correctly identifying the structure of a model is crucial for accurately analyzing the system's dynamics. A well-chosen model structure ensures that the model reflects the true behavior of the system, allowing for consistent and meaningful analysis. In this respect, several algorithms have been developed to select the appropriate terms for constructing a polynomial NARX model. The primary goal of model structure selection (MSS) algorithms is to reveal the system's characteristics by producing the simplest model that adequately describes the data. While some systems may indeed require more complex models, it is essential to strike a balance between simplicity and accuracy. As Einstein aptly put it:</p> <p>A model should be as simple as possible, but not simpler.</p> <p>This principle emphasizes the importance of avoiding unnecessary complexity while ensuring that the model still captures the essential dynamics of the system.</p> <p>We see at chapter 2 that regressors selection, however, is not a simple task. If the nonlinear degree, the order of the model and the number inputs increases, the number of candidate models becomes too large for brute force approach. Considering the MIMO case, this problem is far worse than the SISO one if many inputs and outputs are required. The number of all different models can be calculated as</p> \\[ \\begin{align}     n_m =     \\begin{cases}     2^{n_r} &amp; \\text{for SISO models}, \\\\     2^{n_{{_{m}}r}} &amp; \\text{for MIMO models},     \\end{cases} \\end{align} \\tag{5} \\] <p>where \\(n_r\\) and \\(n_{{_{m}}r}\\) are the values computed using the equations presented in Chapter 2.</p> <p>A classical solution to regressors selection problem is the Forward Regression Orthogonal Least Squares (FROLS) algorithm associated with Error Reduction Ratio (ERR) algorithm. This technique is based on the Prediction Error Minimization framework and, one at time, select the most relevant regressor by using a step-wise regression. The FROLS method adapt the set of regressors in the search space into a set of orthogonal vectors, which ERR evaluates the individual contribution to the desired output variance.</p>"},{"location":"book/4-Model-Structure-Selection/#the-forward-regression-orthogonal-least-squares-algorithm","title":"The Forward Regression Orthogonal Least Squares Algorithm","text":"<p>Consider the general NARMAX model defined in Equation 2.23 described in a generic form as</p> \\[ \\begin{equation}     y_k = \\psi^\\top_{k-1}\\hat{\\Theta} + \\xi_k, \\end{equation} \\tag{6} \\] <p>where \\(\\psi^\\top_{k-1} \\in \\mathbb{R}^{n_r \\times n}\\) is a vector of some combinations of the regressors and \\(\\hat{\\Theta} \\in \\mathbb{R}^{n_{\\Theta}}\\) the vector of estimated parameters. In a more compact form, the NARMAX model can be represented in a matrix form as:</p> \\[ \\begin{equation}     y = \\Psi\\hat{\\Theta} + \\Xi, \\end{equation} \\tag{7} \\] <p>where</p> \\[ \\begin{align}     Y = \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_n     \\end{bmatrix},     \\Psi = \\begin{bmatrix}     \\psi_{{_1}} \\\\     \\psi_{{_2}} \\\\     \\vdots \\\\     \\psi_{{_{n_{\\Theta}}}}     \\end{bmatrix}^\\top=     \\begin{bmatrix}     \\psi_{{_1}1} &amp; \\psi_{{_2}1} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}1} \\\\     \\psi_{{_1}2} &amp; \\psi_{{_2}2} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}2} \\\\     \\vdots &amp; \\vdots &amp;       &amp; \\vdots \\\\     \\psi_{{_1}n} &amp; \\psi_{{_2}n} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}n} \\\\     \\end{bmatrix},     \\hat{\\Theta} = \\begin{bmatrix}     \\hat{\\Theta}_1 \\\\     \\hat{\\Theta}_2 \\\\     \\vdots \\\\     \\hat{\\Theta}_{n_\\Theta}     \\end{bmatrix},     \\Xi = \\begin{bmatrix}     \\xi_1 \\\\     \\xi_2 \\\\     \\vdots \\\\     \\xi_n     \\end{bmatrix}. \\end{align} \\tag{8} \\] <p>The parameters in equation above could be estimated as a result of a Least Squares-based algorithm, but this would require to optimize all parameters at the same time on account of the fact of interaction between regressors due to non-orthogonality characteristic. Consequently, the computational demand becomes impractical for high number of regressors. In this respect, the FROLS transforms the non-orthogonal model presented in the equation above into a orthogonal one.</p> <p>The regressor matrix \\(\\Psi\\) can be orthogonally decomposed as</p> \\[ \\begin{equation}     \\Psi = QA, \\end{equation} \\tag{9} \\] <p>where \\(A \\in \\mathbb{R}^{n_{\\Theta}\\times n_{\\Theta}}\\) is an unit upper triangular matrix according to</p> \\[ \\begin{align} A =     \\begin{bmatrix}     1       &amp; a_{12} &amp; a_{13} &amp; \\dotsc &amp; a_{1n_{\\Theta}} \\\\     0       &amp;   1    &amp; a_{23} &amp; \\dotsc &amp; a_{2n_{\\Theta}} \\\\     0       &amp;   0    &amp;   1    &amp; \\dotsc &amp;     \\vdots       \\\\     \\vdots  &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; a_{n_{\\Theta}-1n_{\\Theta}} \\\\     0       &amp;  0     &amp;  0     &amp;  0     &amp; 1     \\end{bmatrix}, \\end{align} \\tag{10} \\] <p>and \\(Q \\in \\mathbb{R}^{n\\times n_{\\Theta}}\\) is a matrix with orthogonal columns \\(q_i\\), described as</p> \\[ \\begin{equation}     Q =         \\begin{bmatrix}         q_{{_1}} &amp; q_{{_2}} &amp; q_{{_3}} &amp; \\dotsc &amp; q_{{_{n_{\\Theta}}}}         \\end{bmatrix}, \\end{equation} \\tag{11} \\] <p>such that \\(Q^\\top Q = \\Lambda\\) and \\(\\Lambda\\) is diagonal with entry \\(d_i\\) and can be expressed as:</p> \\[ \\begin{align}     d_i = q_i^\\top q_i = \\sum^{k=1}_{n}q_{{_i}k}q_{{_i}k}, \\qquad 1\\leq i \\leq n_{\\Theta}. \\end{align} \\] <p>Because the space spanned by the orthogonal basis \\(Q\\) (Equation 11) is the same as that spanned by the basis set \\(\\Psi\\) (Equation 8) (i.e, contains every linear combination of elements of such subspace), we can define the Equation 7 as</p> \\[ \\begin{equation}     Y = \\underbrace{(\\Psi A^{-1})}_{Q}\\underbrace{(A\\Theta)}_{g}+ \\Xi = Qg+\\Xi, \\end{equation} \\tag{12} \\] <p>where \\(g\\in \\mathbb{R}^{n_\\Theta}\\) is an auxiliary parameter vector. The solution of the model described in Equation 12 is given by</p> \\[ \\begin{equation}     g = \\left(Q^\\top Q\\right)^{-1}Q^\\top Y = \\Lambda^{-1}Q^\\top Y \\end{equation} \\tag{13} \\] <p>or</p> \\[ \\begin{equation}     g_{{_i}} = \\frac{q_{{_i}}^\\top Y}{q_{{_i}}^\\top q_{{_i}}}. \\end{equation} \\tag{14} \\] <p>Since the parameter \\(\\Theta\\) and \\(g\\) satisfies the triangular system \\(A\\Theta = g\\), any orthogonalization method like Householder, Gram-Schmidt, modified Gram-Schmidt or Givens transformations can be used to solve the equation and estimate the original parameters. Assuming that \\(E[\\Psi^\\top \\Xi] = 0\\), the output variance can be derived by multiplying Equation 12 with itself and dividing by \\(n\\), resulting in</p> \\[ \\begin{equation}     \\frac{1}{n}Y^\\top Y = \\underbrace{\\frac{1}{n}\\sum^{i = 1}_{n_{\\Theta}}g_{{_i}}^2q^\\top_{{_i}}q_{{_i}}}_{\\text{{output explained by the regressors}}} + \\underbrace{\\frac{1}{n}\\Xi^\\top \\Xi}_{\\text{{unexplained variance}}}. \\end{equation} \\tag{15} \\] <p>Thus, the ERR due to the inclusion of the regressor \\(q_{{_i}}\\) is expressed as:</p> \\[ [\\text{ERR}]_i = \\frac{g_{i}^2 \\cdot q_{i}^\\top q_{i}}{Y^\\top Y}, \\qquad \\text{for } i=1,2,\\dotsc, n_\\Theta. \\] <p>There are many ways to terminate the algorithm. An approach often used is stop the algorithm if the model output variance drops below some predetermined limit \\(\\varepsilon\\):</p> \\[ \\begin{equation}     1 - \\sum_{i = 1}^{n_{\\Theta}}\\text{ERR}_i \\leq \\varepsilon, \\end{equation} \\tag{17} \\]"},{"location":"book/4-Model-Structure-Selection/#keep-it-simple","title":"Keep it simple","text":"<p>For the sake of simplicity, let's present the FROLS along with simple examples to make the intuition clear. First, let define the ERR calculation and then explain the idea of the FROLS in simple terms.</p>"},{"location":"book/4-Model-Structure-Selection/#orthogonal-case","title":"Orthogonal case","text":"<p>Consider the case where we have a set of inputs defined as \\(x_1, x_2, \\ldots, x_n\\) and an output called \\(y\\). These inputs are orthogonal vectors.</p> <p>Lets suppose that we want to create a model to approximate \\(y\\)  using \\(x_1, x_2, \\ldots, x_n\\), as follows:</p> \\[ y=\\hat{\\theta}_1 x_1+\\hat{\\theta}_2 x_2+\\ldots+\\hat{\\theta}_n x_n+e \\tag{18} \\] <p>where \\(\\hat{\\theta}_1, \\hat{\\theta}_2, \\ldots, \\hat{\\theta}_n\\) are parameters and \\(e\\) is white noise and independent of \\(x\\) and \\(y\\) (remember the  \\(E[\\Psi^\\top \\Xi] = 0\\), in previous section). In this case, we can rewrite the equation above as</p> \\[ y = \\hat{\\theta} x \\tag{19} \\] <p>so</p> \\[ \\left\\langle x, y\\right\\rangle = \\left\\langle \\hat{\\theta} x, x\\right\\rangle = \\hat{\\theta} \\left\\langle x, x\\right\\rangle \\tag{20} \\] <p>Which implies that</p> \\[ \\hat{\\theta} = \\frac{\\left\\langle x, y\\right\\rangle}{\\left\\langle x, x\\right\\rangle} \\tag{21} \\] <p>Therefore we can show that</p> \\[ \\begin{align} &amp; \\left\\langle x_1, y\\right\\rangle=\\hat{\\theta}_1\\left\\langle x_1, x_1\\right\\rangle \\Rightarrow \\hat{\\theta}_1=\\frac{\\left\\langle x_1, y\\right\\rangle}{\\left\\langle x_1, x_1\\right\\rangle}=\\frac{x_1^T y}{x_1^T x_1} \\\\ &amp; \\left\\langle x_2, y\\right\\rangle=\\hat{\\theta}_2\\left\\langle x_2, x_2\\right\\rangle \\Rightarrow \\hat{\\theta}_2=\\frac{\\left\\langle x_2, y\\right\\rangle}{\\left\\langle x_2, x_2\\right\\rangle}=\\frac{x_2^T y}{x_2^T x_2}, \\ldots \\\\ &amp; \\left\\langle x_n, y\\right\\rangle=\\hat{\\theta}_n\\left\\langle x_n, x_n\\right\\rangle \\Rightarrow \\hat{\\theta}_n=\\frac{\\left\\langle x_n, y\\right\\rangle}{\\left\\langle x_n, x_n\\right\\rangle}=\\frac{x_n^T y}{x_n^T x_n}, \\end{align} \\tag{22} \\] <p>Following the same idea, we can also show that</p> \\[ \\langle y, y\\rangle=\\hat{\\theta}_1^2\\left\\langle x_1, x_1\\right\\rangle+\\hat{\\theta}_2^2\\left\\langle x_2, x_2\\right\\rangle+\\ldots+\\hat{\\theta}_n^2\\left\\langle x_n, x_n\\right\\rangle+\\langle e, e\\rangle \\tag{23} \\] <p>which can be described as</p> \\[ y^T y=\\hat{\\theta}_1^2 x_1^T x_1+\\hat{\\theta}_2^2 x_2^T x_2+\\ldots+\\hat{\\theta}_n^2 x_n^T x_n+e^T e \\tag{24} \\] <p>or</p> \\[ \\|y\\|^2=\\hat{\\theta}_1^2\\left\\|x_1\\right\\|^2+\\hat{\\theta}_2^2\\left\\|x_2\\right\\|^2+\\ldots+\\hat{\\theta}_n^2\\left\\|x_n\\right\\|^2+\\|e\\|^2 \\tag{25} \\] <p>So, dividing both sides of the equation by \\(y\\) and rearranging the equation, we have</p> \\[ \\frac{\\|e\\|^2}{\\|y\\|^2}=1-\\hat{\\theta}_1^2 \\frac{\\left\\|x_1\\right\\|^2}{\\|y\\|^2}-\\hat{\\theta}_2^2 \\frac{\\left\\|x_2\\right\\|^2}{\\|y\\|^2}-\\ldots-\\hat{\\theta}_n^2 \\frac{\\left\\|x_n\\right\\|^2}{\\|y\\|^2} \\tag{26} \\] <p>Because \\(\\hat{\\theta}_k=\\frac{x_k^T y}{x_k^T x_k}=\\frac{x_k^T y}{\\left\\|x_k\\right\\|^2}, k=1,2, . ., n\\), we have</p> \\[ \\begin{align} \\frac{\\|e\\|^2}{\\|y\\|^2} &amp; =1-\\left(\\frac{x_1^T y}{\\left\\|x_1\\right\\|^2}\\right)^2 \\frac{\\left\\|x_1\\right\\|^2}{\\|y\\|^2}-\\left(\\frac{x_2^T y}{\\left\\|x_2\\right\\|^2}\\right)^2 \\frac{\\left\\|x_2\\right\\|^2}{\\|y\\|^2}-\\ldots-\\left(\\frac{x_n^T y}{\\left\\|x_n\\right\\|^2}\\right)^2 \\frac{\\left\\|x_n\\right\\|^2}{\\|y\\|^2} \\\\ &amp; =1-\\frac{\\left(x_1^T y\\right)^2}{\\left\\|x_1\\right\\|\\left\\|^2\\right\\| y \\|^2}-\\frac{\\left(x_2^T y\\right)^2}{\\left\\|x_2\\right\\|^2\\|y\\|^2}-\\cdots-\\frac{\\left(x_n^T y\\right)^2}{\\left\\|x_n\\right\\|^2\\|y\\|^2} \\\\ &amp; =1-ERR_1 \\quad-ERR_2-\\cdots-ERR_n \\end{align} \\tag{27} \\] <p>where \\(\\operatorname{ERR}_k(k=1,2 \\ldots, n)\\) is the Error Reduction Ratio defined in previous section.</p> <p>Check the example bellow using the fundamental basis</p> <pre><code>import numpy as np\n\ny = np.array([3, 7, 8])\n# Orthogonal Basis\nx1 = np.array([1, 0, 0])\nx2 = np.array([0, 1, 0])\nx3 = np.array([0, 0, 1])\n\ntheta1 = (x1.T@y)/(x1.T@x1)\ntheta2 = (x2.T@y)/(x2.T@x2)\ntheta3 = (x3.T@y)/(x3.T@x3)\n\nsquared_y = y.T @ y\nerr1 = (x1.T@y)**2/((x1.T@x1) * squared_y)\nerr2 = (x2.T@y)**2/((x2.T@x2) * squared_y)\nerr3 = (x3.T@y)**2/((x3.T@x3) * squared_y)\n\nprint(f\"x1 represents {round(err1*100, 2)}% of the variation in y, \\n x2 represents {round(err2*100, 2)}% of the variation in y, \\n x3 represents {round(err3*100, 2)}% of the variation in y\")\n\nx1 represents 7.38% of the variation in y,\nx2 represents 40.16% of the variation in y,\nx3 represents 52.46% of the variation in y\n</code></pre> <p>Lets see what happens in a non-orthogonal scenario.</p> <p><pre><code>y = np.array([3, 7, 8])\nx1 = np.array([1, 2, 2])\nx2 = np.array([-1, 0, 2])\nx3 = np.array([0, 0, 1])\n\ntheta1 = (x1.T@y)/(x1.T@x1)\ntheta2 = (x2.T@y)/(x2.T@x2)\ntheta3 = (x3.T@y)/(x3.T@x3)\n\nsquared_y = y.T @ y\nerr1 = (x1.T@y)**2/((x1.T@x1) * squared_y)\nerr2 = (x2.T@y)/((x2.T@x2) * squared_y)\nerr3 = (x3.T@y)**2/((x3.T@x3) * squared_y)\n\nprint(f\"x1 represents {round(err1*100, 2)}% of the variation in y, \\n x2 represents {round(err2*100, 2)}% of the variation in y, \\n x3 represents {round(err3*100, 2)}% of the variation in y\")\n\n&gt;&gt;&gt; x1 represents 99.18% of the variation in y,\n&gt;&gt;&gt; x2 represents 2.13% of the variation in y,\n&gt;&gt;&gt; x3 represents 52.46% of the variation in y\n</code></pre> In this case, \\(x1\\) have the highest \\(err\\) value, so we have chosen it to be the first orthogonal vector.</p> <pre><code>q1 = x1.copy()\n\nv1 = x2 - (q1.T@x2)/(q1.T@q1)*q1\nerrv1 = (v1.T@y)**2/((v1.T@v1) * squared_y)\n\nv2 = x3 - (q1.T@x3)/(q1.T@q1)*q1\nerrv2 = (v2.T@y)**2/((v2.T@v2) * squared_y)\n\nprint(f\"v1 represents {round(errv1*100, 2)}% of the variation in y, \\n v2 represents {round(errv2*100, 2)}% of the variation in y\")\n\n&gt;&gt;&gt; v1 represents 0.82% of the variation in y,\n&gt;&gt;&gt; v2 represents 0.66% of the variation in y\n</code></pre> <p>So, in this case, when we sum the err values of the first two orthogonal vectors, \\(x1\\) and \\(v1\\), we get \\(err_3 + errv1 = 100\\%\\). Then there is no need to keep the iterations looking for more terms. The model with this two terms already explain all the variance in the data.</p> <p>That's the idea of the FROLS algorithm. We calculate the ERR, choose the vector with the highest ERR to be the first orthogonal vector, orthogonalize every vector but the one we choose in the first step, calculate the ERR for each one of them, choose the vector with the highest ERR value and keep doing that until we reach some criteria.</p> <p>In SysIdentPy, we have 2 hyperparameters called <code>n_terms</code> and <code>err_tol</code>. Both of them can be used to stop the iterations. The first one will iterate until <code>n_terms</code> are chosen. The second one iterate until the \\(\\sum ERR_i &gt; err_{tol}\\) . If you set both, the algorithm stop when any of the conditions is true.</p> <pre><code>model = FROLS(\n\u00a0 \u00a0 \u00a0 \u00a0 n_terms=50,\n\u00a0 \u00a0 \u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 \u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 \u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 \u00a0 \u00a0 err_tol=0.98\n\u00a0 \u00a0 )\n</code></pre> <p>SysIdentPy apply the Golub -Householder method for the orthogonal decomposition. A more detailed discussion about Householder and orthogonalization procedures in general can be found in Chen, S. and Billings, S. A. and Luo, W.</p>"},{"location":"book/4-Model-Structure-Selection/#case-study","title":"Case Study","text":"<p>An example using real data will be described using SysIdentPy. In this example, we will build models linear and nonlinear models to describe the behavior of a DC motor operating as generator. Details of the experiment used to generate this data can be found in the paper (in Portuguese) IDENTIFICA\u00c7\u00c3O DE UM MOTOR/GERADOR CC POR MEIO DE MODELOS POLINOMIAIS AUTORREGRESSIVOS E REDES NEURAIS ARTIFICIAIS</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n\ndf1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n\n# checking the ouput\ndf2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <p></p> <p>Figure 2. Output of the electromechanical system.</p> <p>In this example, we will decimate the data using \\(d = 500\\). The rationale behind decimation here is that the data is oversampled due to the experimental setup. A future section will provide a detailed explanation of how to handle oversampled data in the context of system identification. For now, consider this approach as the most appropriate solution.</p> <pre><code>x_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre> <p>In this case, we will build a NARX model. In SysIdentPy, this means setting <code>unbiased=False</code> in the <code>LeastSquares</code> definition. We'll use a <code>Polynomial</code> basis function and set the maximum lag for both input and output to 2. This configuration results in 15 terms in the information matrix, so we'll set <code>n_terms=15</code>. This specification is necessary because, in this example, <code>order_selection</code> is set to <code>False</code>. We will discuss <code>order_selection</code> in more detail in the Information Criteria section later on.</p> <p><code>order_selection</code> is <code>True</code> by default in SysIdentPy. When <code>order_selection=False</code> the user must pass a values to <code>n_terms</code> because it is an optional argument and its default value is <code>None</code>. If we set <code>n_terms=5</code>, for exemple, the FROLS will stop after choosing the first 5 regressors. We do not want that in this case because we want the FROLS stop only when <code>e_tol</code> is reached.</p> <pre><code>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 e_tol=0.9999\n\u00a0 \u00a0 n_terms=15\n)\n</code></pre> <p>SysIdentPy aims to simplify the use of algorithms like <code>FROLS</code> for the user. Building, training, or fitting a model is made straightforward through a simple interface called <code>fit</code>. By using this method, the entire process is handled internally, requiring no further interaction from the user.</p> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <p>SysIdentPy also offers a method to retrieve detailed information about the fitted model. Users can check the terms included in the model, the estimated parameters, the Error Reduction Ratio (ERR) values, and more.</p> <p>We're using <code>pandas</code> here only to make the output more readable, but it's optional.</p> <pre><code>r = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR y(k-1) 1.0998E+00 9.86000384E-01 x1(k-1)^2 1.0165E+02 7.94805130E-03 y(k-2)^2 -1.9786E-05 2.50905908E-03 x1(k-1)y(k-1) -1.2138E-01 1.43301039E-03 y(k-2) -3.2621E-01 1.02781443E-03 x1(k-1)y(k-2) 5.3596E-02 5.35200312E-04 x1(k-2) 3.4655E+02 2.79648078E-04 x1(k-2)y(k-1) -5.1647E-02 1.12211942E-04 x1(k-2)x1(k-1) -8.2162E+00 4.54743448E-05 y(k-2)y(k-1) 4.0961E-05 3.25346101E-05 &gt;Table 1 <p>The table above shows that 10 regressors (out of the 15 available) were needed to reach the defined <code>e_tol</code>, with the sum of the ERR for the selected regressors being \\(0.99992\\).</p> <p>Next, let's evaluate the model's performance using the test data. Similar to the <code>fit</code> method, SysIdentPy provides a <code>predict</code> method. To obtain the predicted values and plot the results, simply follow these steps:</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n</code></pre> <p></p> <p>Figure 3. Free run simulation (or infinity-steps ahead prediction) of the fitted model.</p>"},{"location":"book/4-Model-Structure-Selection/#information-criteria","title":"Information Criteria","text":"<p>We said that there are many ways to terminate the algorithm and select the model terms, but only ERR criteria was defined in previous section. Different ways to terminate the algorithm is by using some information criteria, e.g, Akaike Information Criteria (AIC). For Least Squares based regression analysis, the AIC indicates the number of regressors by minimizing the objective function (Akaike, H. - A new look at the statistical model identification):</p> \\[ \\begin{equation}     J_{\\text{AIC}} = \\underbrace{n\\log\\left(Var[\\xi_k]\\right)}_{\\text{first component}}+\\underbrace{2n_{\\Theta}}_{\\text{{second component}}}. \\end{equation} \\tag{28} \\] <p>It is important to note that the equation above illustrates a trade-off between model fit and model complexity. Specifically, this trade-off involves balancing the model's ability to accurately fit the data (the first component) against its complexity, which is related to the number of parameters included (the second component). As additional terms are included in the model, the Akaike Information Criterion (AIC) value initially decreases, reaching a minimum that represents an optimal balance between model complexity and predictive accuracy. However, if the number of parameters becomes excessive, the penalty for complexity outweighs the benefit of a better fit, causing the AIC value to increase. The AIC and many others variants have been extensively used for linear and nonlinear system identification. Check Wei, H. and Zhu, D. and Billings, S. A. and Balikhin, M. A. - Forecasting the geomagnetic activity of the Dst index using multiscale radial basis function networks, Martins, S. A. M. and Nepomuceno, E. G. and Barroso, M. F. S. - Improved Structure Detection For Polynomial NARX Models Using a Multiobjective Error Reduction Ratio, Hafiz, F. and Swain, A. and Mendes, E. M. A. M. and Patel, N. - Structure Selection of Polynomial NARX Models Using Two Dimensional (2D) Particle Swarms, Gu, Y. and Wei, H. and Balikhin, M. M. - Nonlinear predictive model selection and model averaging using information criteria and references therein.</p> <p>Despite their effectiveness in many linear model selection scenarios, information criteria such as AIC can struggle to select an appropriate number of parameters when dealing with systems exhibiting significant nonlinear behavior. Additionally, these criteria may lead to suboptimal models if the search space does not encompass all the necessary terms required to accurately represent the true model. Consequently, in highly nonlinear systems or when critical model components are missing, information criteria might not provide reliable guidance, resulting in models that exhibit poor performance.</p> <p>Besides AIC, SysIdentPy provides other four different information criteria: Bayesian Information Criteria (BIC), Final Prediction Error (FPE), Low of Iterated Logarithm Criteria (LILC), and Corrected Akaike Information Criteria (AICc), which can be described respectively as</p> \\[ \\begin{align} \\operatorname{FPE}\\left(n_\\theta\\right) &amp; =N \\ln \\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]+N \\ln \\left[\\frac{N+n_\\theta}{N-n_\\theta}\\right] \\\\ B I C\\left(n_\\theta\\right) &amp; =N \\ln \\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]+n_\\theta \\ln N \\\\ A I C c &amp;=A I C+2 n_p * \\frac{n_p+1}{N-n_p-1} \\\\ LILC &amp;= 2n_{\\theta}\\ln(\\ln(N)) + N \\ln(\\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]) \\end{align} \\tag{29} \\] <p>To use any information criteria in SysIdentPy, set <code>order_selection=True</code> (as said before, the default value is already <code>True</code>). Besides <code>order_selection</code>, you can define how many regressors you want to evaluate before stopping the algorithm by using the <code>n_info_values</code> hyperparameter. The default value is \\(15\\), but the user should increase it based on how many regressors exists given the <code>ylag</code>, <code>xlag</code> and the degree of the basis function.</p> <p>Using information Criteria can take a long time depending on how many regressors you are evaluating and the number of samples. To calculate the criteria, the ERR algorithm is executed <code>n</code> times, where <code>n</code> is the number defined in <code>n_info_values</code>. Make sure to understand how it works to define whether you have to use it or not.</p> <p>Running the same example, but now using the BIC information criteria to select the order of the model, we have</p> <pre><code>model = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function\n)\nmodel.fit(X=x_train, y=y_train)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR y(k-1) 1.3666E+00 9.86000384E-01 x1(k-1)^2 1.0500E+02 7.94805130E-03 y(k-2)^2 -5.8577E-05 2.50905908E-03 x1(k-1)y(k-1) -1.2427E-01 1.43301039E-03 y(k-2) -5.1414E-01 1.02781443E-03 x1(k-1)y(k-2) 5.3001E-02 5.35200312E-04 x1(k-2) 3.1144E+02 2.79648078E-04 x1(k-2)y(k-1) -4.8013E-02 1.12211942E-04 x1(k-2)x1(k-1) -8.0561E+00 4.54743448E-05 x1(k-2)y(k-2) 4.1381E-03 3.25346101E-05 1 -5.6653E+01 7.54107553E-06 y(k-2)y(k-1) 1.5679E-04 3.52002717E-06 y(k-1)^2 -9.0164E-05 6.17373260E-06 &gt;Table 2 <p>In this case, instead of 8 regressors, the final model have 13 terms.</p> <p>Currently, the number of regressors is determined by identifying the index of the last value where the difference between the current and previous value is less than 0. To inspect these values, you can use the following approach:</p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <p></p> <p>Figure 4. The plot shows the Information Criterion values (BIC) as a function of the number of terms included in the model. The model selection process, using the BIC criterion, iteratively adds regressors until the BIC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the BIC value stops decreasing marks the optimal number of terms, resulting in a final model with 13 terms.</p> <p>The model prediction in this case is shown in Figure 5</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n</code></pre> <p></p> <p>Figure 5. Free run simulation (or infinity-steps ahead prediction) of the fitted model using BIC.</p>"},{"location":"book/4-Model-Structure-Selection/#overview-of-the-information-criteria-methods","title":"Overview of the Information Criteria Methods","text":"<p>In this section, simulated data are used to provide users with a clearer understanding of the information criteria available in SysIdentPy.</p> <p>Here, we're working with a known model structure, which allows us to focus on how different information criteria perform. When dealing with real data, the correct number of terms in the model is unknown, making these methods invaluable for guiding model selection.</p> <p>If you review the metrics below, you'll notice excellent performance across all models. However, it's crucial to remember that System Identification is about finding the optimal model structure. Model Structure Selection is at the heart of NARMAX methods!</p> <p>The data is generated by simulating the following model:</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_k \\tag{30} \\] <p>If <code>colored_noise</code> is set to <code>True</code>, the noise term is defined as:</p> \\[ e_k = 0.8\\nu_{k-1} + \\nu_k \\tag{31} \\] <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a Gaussian-distributed variable with \\(\\mu = 0\\) and \\(\\sigma = 0.1\\).</p> <p>In the next example, we will generate data with 100 samples, using white noise, and select 70% of the data to train the model.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 n=100, colored_noise=False, sigma=0.1, train_percentage=70\n)\n</code></pre> <p>The idea is to show the impact of the information criteria to select the number of terms to compose the final model. You will se why it is an auxiliary tool and let the algorithm select the number of terms based on the minimum value is not always a good idea when dealing with data highly corrupted by noise (even white noise).</p>"},{"location":"book/4-Model-Structure-Selection/#aic","title":"AIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <p>The regressors, the free run simulation and the AIC values are detailed bellow.</p> Regressors Parameters ERR x1(k-2) 9.4236E-01 9.26094341E-01 y(k-1) 2.4933E-01 3.35898283E-02 x1(k-1)y(k-1) 1.3001E-01 2.35736200E-03 x1(k-1) 8.4024E-02 4.11741791E-03 x1(k-1)^2 7.0807E-02 2.54231877E-03 x1(k-2)^2 -9.1138E-02 1.39658893E-03 y(k-1)^2 1.1698E-01 1.70257419E-03 x1(k-2)y(k-2) 8.3745E-02 1.11056684E-03 y(k-2)^2 -4.1946E-02 1.01686239E-03 x1(k-2)x1(k-1) 5.9034E-02 7.47435512E-04 &gt;Table 3 <p></p> <p>Figure 5. Free run simulation (or infinity-steps ahead prediction) of the fitted model using AIC.</p> <p></p> <p>Figure 6. The plot shows the Information Criterion values (AIC) as a function of the number of terms included in the model. The model selection process, using the AIC criterion, iteratively adds regressors until the AIC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the AICc value stops decreasing marks the optimal number of terms, resulting in a final model with 10 terms.</p> <p>For this case, we have a model with 10 terms. We know that the correct number is 3 because of the simulated system we are using as example.</p>"},{"location":"book/4-Model-Structure-Selection/#aicc","title":"AICc","text":"<p>The only change we have to do to use AICc instead of AIC is changing the information criteria hyperparameter: <code>information_criteria=\"aicc\"</code></p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aicc\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.2282E-01 9.26094341E-01 y(k-1) 2.4294E-01 3.35898283E-02 x1(k-1)y(k-1) 1.2753E-01 2.35736200E-03 x1(k-1) 6.9597E-02 4.11741791E-03 x1(k-1)^2 7.0578E-02 2.54231877E-03 x1(k-2)^2 -1.0523E-01 1.39658893E-03 y(k-1)^2 1.0949E-01 1.70257419E-03 x1(k-2)y(k-2) 7.1821E-02 1.11056684E-03 y(k-2)^2 -3.9756E-02 1.01686239E-03 &gt;Table 4 <p></p> <p>Figure 7. Free run simulation (or infinity-steps ahead prediction) of the fitted model using AICc.</p> <p></p> <p>Figure 8. The plot shows the Information Criterion values (AICc) as a function of the number of terms included in the model. The model selection process, using the AIC criterion, iteratively adds regressors until the AICc reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the AICc value stops decreasing marks the optimal number of terms, resulting in a final model with 9 terms.</p> <p>This time we have a model with 9 regressors.</p>"},{"location":"book/4-Model-Structure-Selection/#bic","title":"BIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1726E-01 9.26094341E-01 y(k-1) 1.8670E-01 3.35898283E-02 &gt;Table 5 <p>Figure 9. Free run simulation (or infinity-steps ahead prediction) of the fitted model using BIC.</p> <p></p> <p>Figure 10. The plot shows the Information Criterion values (BIC) as a function of the number of terms included in the model. The model selection process, using the BIC criterion, iteratively adds regressors until the BIC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the BIC value stops decreasing marks the optimal number of terms, resulting in a final model with 2 terms.</p> <p>BIC returned a model with only 2 regressors!</p>"},{"location":"book/4-Model-Structure-Selection/#lilc","title":"LILC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"lilc\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1160E-01 9.26094341E-01 y(k-1) 2.3178E-01 3.35898283E-02 x1(k-1)y(k-1) 1.2080E-01 2.35736200E-03 x1(k-1) 6.3113E-02 4.11741791E-03 x1(k-1)^2 5.4088E-02 2.54231877E-03 x1(k-2)^2 -9.0683E-02 1.39658893E-03 y(k-1)^2 8.2157E-02 1.70257419E-03 &gt;Table 6 <p>Figure 11. Free run simulation (or infinity-steps ahead prediction) of the fitted model using LILC.</p> <p></p> <p>Figure 12. The plot shows the Information Criterion values (LILC) as a function of the number of terms included in the model. The model selection process, using the LILC criterion, iteratively adds regressors until the LILC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the LILC value stops decreasing marks the optimal number of terms, resulting in a final model with 7 terms.</p> <p>LILC returned a model with 7 regressors.</p>"},{"location":"book/4-Model-Structure-Selection/#fpe","title":"FPE","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"fpe\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.4236E-01 9.26094341E-01 y(k-1) 2.4933E-01 3.35898283E-02 x1(k-1)y(k-1) 1.3001E-01 2.35736200E-03 x1(k-1) 8.4024E-02 4.11741791E-03 x1(k-1)^2 7.0807E-02 2.54231877E-03 x1(k-2)^2 -9.1138E-02 1.39658893E-03 y(k-1)^2 1.1698E-01 1.70257419E-03 x1(k-2)y(k-2) 8.3745E-02 1.11056684E-03 y(k-2)^2 -4.1946E-02 1.01686239E-03 x1(k-2)x1(k-1) 5.9034E-02 7.47435512E-04 &gt;Table 7 <p>Figure 13. Free run simulation (or infinity-steps ahead prediction) of the fitted model using FPE.</p> <p></p> <p>Figure 14. The plot shows the Information Criterion values (FPE) as a function of the number of terms included in the model. The model selection process, using the FPE criterion, iteratively adds regressors until the FPE reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the FPE value stops decreasing marks the optimal number of terms, resulting in a final model with 10 terms.</p> <p>FPE returned a model with 10 regressors.</p>"},{"location":"book/4-Model-Structure-Selection/#meta-model-structure-selection-metamss","title":"Meta Model Structure Selection (MetaMSS)","text":"<p>This section largely reflects content from a paper I published on ArXiv titled \"Meta-Model Structure Selection: Building Polynomial NARX Models for Regression and Classification.\" This paper was initially written for journal publication based on the results of my master's thesis. However, as I transitioned into a Data Scientist role and considering the lengthy journal submission process and academic delays, I decided not to pursue journal publication at this time. Thus, the paper remains available only on ArXiv.</p> <p>The work extends a previous paper I presented at a Brazilian conference (in Portuguese), where part of the results were initially shared.</p> <p>This section introduces a meta-heuristic approach for selecting the structure of polynomial NARX models in regression tasks. The proposed method considers both the complexity of the model and the contribution of each term to construct parsimonious models through a novel cost function formulation. The robustness of this new algorithm is evaluated using various simulated and experimental systems with different nonlinear characteristics. The results demonstrate that the algorithm effectively identifies the correct model when the true structure is known and produces parsimonious models for experimental data, even in cases where traditional and contemporary methods often fail. The new approach is compared against classical methods such as FROLS and recent randomized techniques.</p> <p>We mentioned that selecting the appropriate model terms is crucial for accurately capturing the dynamics of the original system. Challenges such as over-parameterization and numerical ill-conditioning often arise due to the limitations of existing identification algorithms in selecting the right terms for the final model. Check Aguirre, L. A. and Billings, S. A. - Dynamical effects of overparametrization in nonlinear models, Piroddi, L. and Spinelli, W. - An identification algorithm for polynomial NARX models based on simulation error minimization. We also mentioned that one of the most traditionally algorithms for structure selection of polynomial NARMAX is the ERR algorithm. Numerous variants of FROLS algorithm has been developed to improve the model selection performance such as Billings, S. A., Chen, S., and Korenberg, M. J. - Identification of MIMO non-linear systems using a forward-regression orthogonal estimator, Farina, M. and Piroddi, L. - Simulation Error Minimization\u2013Based Identification of Polynomial Input\u2013Output Recursive Models, Guo, Y., Guo, L. Z., Billings, S. A., and Wei, H. - A New Iterative Orthogonal Forward Regression Algorithm, Mao, K. Z. and Billings, S. A. - VARIABLE SELECTION IN NON-LINEAR SYSTEMS MODELLING. The drawbacks of the FROLS have been extensively reviewed in the literature, e.g., in Billings, S. A. and Aguirre, L. A., Palumbo, P. and Piroddi, L., Falsone, A., Piroddi, L., and Prandini, M.. Most of these weak points are related to (i) the Prediction Error Minimization (PEM) framework; (ii) the inadequacy of the ERR index in measuring the absolute importance of regressors; (iii) the use of information criteria such as AIC, FPE and the BIC, to select the model order. Regarding the information criteria, although these techniques work well for linear models, in a nonlinear context no simple relation between model size and accuracy can be established Falsone, A., Piroddi, L., and Prandini, M. - A randomized algorithm for nonlinear model structure selection, Chen, S., Hong, X., and Harris, C. J. - Sparse kernel regression modeling using combined locally regularized orthogonal least squares and D-optimality experimental design.</p> <p>Due to the limitations of Ordinary Least Squares (OLS)-based algorithms, recent research has presented solutions that diverged from the classical FROLS approach. New methods have reformulated the Model Structure Selection (MSS) process within a probabilistic framework and employed random sampling techniques Falsone, A., Piroddi, L., and Prandini, M. - A randomized algorithm for nonlinear model structure selection, Tempo, R., Calafiore, G., and Dabbene, F. - Randomized Algorithms for Analysis and Control of Uncertain Systems: With Applications, Baldacchino, T., Anderson, S. R., and Kadirkamanathan, V. - Computational system identification for Bayesian NARMAX modelling, Rodriguez-Vazquez, K., Fonseca, C. M., and Fleming, P. J. - Identifying the structure of nonlinear dynamic systems using multiobjective genetic programming, Severino, A. G. V. and Araujo, F. M. U. de. Despite their advancements, these meta-heuristic and probabilistic approaches exhibit certain shortcomings. In particular, these methods often rely on information criteria such as AIC, FPE, and BIC to define the cost function for optimization, which frequently leads to over-parameterized models.</p> <p>Consider \\(\\mathcal{F}\\) as a class of bounded functions \\(\\phi: \\mathbf{R} \\mapsto \\mathbf{R}\\). If the properties of \\(\\phi(x)\\) satisfy</p> \\[ \\begin{align}     &amp;\\lim\\limits_{x \\to \\infty} \\phi(x) = \\alpha \\nonumber \\\\     &amp;\\lim\\limits_{x \\to -\\infty} \\phi(x) = \\beta \\quad \\text{with } \\alpha &gt; \\beta,  \\nonumber \\end{align} \\tag{32} \\] <p>the function is called sigmoidal.</p> <p>In this particular case and following definition Equation 32 with \\(alpha = 0\\) and \\(\\beta = 1\\), we write a \"S\" shaped curve as</p> \\[ \\begin{equation}     \\varsigma(x) = \\frac{1}{1+e^{-a(x-c)}}. \\end{equation} \\tag{33} \\] <p>In that case, we can specify \\(a\\), the rate of change. If \\(a\\) is close to zero, the sigmoid function will be gradual. If \\(a\\) is large, the sigmoid function will have an abrupt or sharp transition. If \\(a\\) is negative, the sigmoid will go from \\(1\\) to zero. The parameter \\(c\\) corresponds to the x value where \\(y = 0.5\\).</p> <p>The Sigmoid Linear Unit Function (SiLU) is defined by the sigmoid function multiplied by its input</p> \\[ \\begin{equation}     \\text{silu}(x) = x \\varsigma(x), \\end{equation} \\tag{34} \\] <p>which can be viewed as a steeper sigmoid function with overshoot.</p>"},{"location":"book/4-Model-Structure-Selection/#meta-heuristics","title":"Meta-heuristics","text":"<p>Over the past two decades, nature-inspired optimization algorithms have gained prominence due to their flexibility, simplicity, versatility, and ability to avoid local optima in real-world applications.</p> <p>Meta-heuristic algorithms are characterized by two fundamental features: exploitation and exploration Blum, C. and Roli, A. - Metaheuristics in combinatorial optimization: Overview and conceptual comparison. Exploitation focuses on utilizing local information to refine the search around the current best solution, improving the quality of nearby solutions. Conversely, exploration aims to search a broader area of the solution space to discover potentially superior solutions and prevent the algorithm from getting trapped in local optima.</p> <p>Despite the lack of a universal consensus on the definitions of exploration and exploitation in evolutionary computing, as highlighted by Eiben, Agoston E and Schippers, Cornelis A, it is generally agreed that these concepts function as opposing forces that are challenging to balance. To address this challenge, hybrid metaheuristics combine multiple algorithms to leverage both exploitation and exploration, resulting in more robust optimization methods.</p>"},{"location":"book/4-Model-Structure-Selection/#the-binary-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-bpsogsa-algorithm","title":"The Binary hybrid Particle Swarm Optimization and Gravitational Search Algorithm (BPSOGSA) algorithm","text":"<p>Achieving a balance between exploration and exploitation is a significant challenge in most meta-heuristic algorithms. For this method, we enhance performance and flexibility in the search process by employing a hybrid approach that combines Binary Particle Swarm Optimization (BPSO) with Gravitational Search Algorithm (GSA), as proposed by Mirjalili, S. and Hashim, S. Z. M.. This hybrid method incorporates a low-level co-evolutionary heterogeneous technique originally introduced by Talbi, E. G..</p> <p>The BPSOGSA approach leverages the strengths of both algorithms: the Particle Swarm Optimization (PSO) component is known to be good at exploring the entire search space to identify the global optimum, while the Gravitational Search Algorithm (GSA) component effectively refines the search by focusing on local solutions within a binary space. This combination aims to provide a more comprehensive and effective optimization strategy, ensuring a better balance between exploration and exploitation.</p>"},{"location":"book/4-Model-Structure-Selection/#standard-particle-swarm-optimization-pso","title":"Standard Particle Swarm Optimization (PSO)","text":"<p>In Particle Swarm Optimization (PSO) Kennedy, J. and Eberhart, R. C., Kennedy, J., each particle represents a candidate solution and is characterized by two components: its position in the search space, denoted as \\(\\vec{x}_{\\,np,d} \\in \\mathbb{R}^{np \\times d}\\), and its velocity, \\(\\vec{v}_{\\,np,d} \\in \\mathbb{R}^{np \\times d}\\). Here, \\(np = 1, 2, \\ldots, n_a\\) where \\(n_a\\) is the size of the swarm, and \\(d\\) is the dimensionality of the problem. The initial population is represented as follows:</p> \\[ \\vec{x}_{\\,np,d} = \\begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,d} \\\\ x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,d} \\\\ \\vdots  &amp; \\vdots  &amp; \\ddots &amp; \\vdots \\\\ x_{n_a,1} &amp; x_{n_a,2} &amp; \\cdots &amp; x_{n_a,d} \\end{bmatrix} \\tag{35} \\] <p>At each iteration \\(t\\), the position and velocity of a particle are updated using the following equations:</p> \\[ v_{np,d}^{t+1} = \\zeta v_{np,d}^{t} + c_1 \\kappa_1 (pbest_{np}^{t} - x_{np,d}^{t}) + c_2 \\kappa_2 (gbest_{np}^{t} - x_{np,d}^{t}), \\tag{36} \\] <p>where \\(\\kappa_j \\in \\mathbb{R}\\) for \\(j = [1,2]\\) are continuous random variables in the interval \\([0,1]\\), \\(\\zeta \\in \\mathbb{R}\\) is the inertia factor that controls the influence of the previous velocity on the current one and represents a trade-off between exploration and exploitation, \\(c_1\\) is the cognitive factor associated with the personal best position \\(pbest\\), and \\(c_2\\) is the social factor associated with the global best position \\(gbest\\). The velocity \\(\\vec{v}_{\\,np,d}\\) is typically constrained within the range \\([v_{min}, v_{max}]\\) to prevent particles from moving outside the search space. The updated position is then computed as:</p> \\[ x_{np,d}^{t+1} = x_{np,d}^{t} + v_{np,d}^{t+1}. \\tag{37} \\]"},{"location":"book/4-Model-Structure-Selection/#standard-gravitational-search-algorithm-gsa","title":"Standard Gravitational Search Algorithm (GSA)","text":"<p>In the Gravitational Search Algorithm (GSA) Rashedi, Esmat, Nezamabadi-Pour, Hossein, and Saryazdi, Saeid - GSA: A Gravitational Search Algorithm, agents are represented by masses, where the magnitude of each mass is proportional to the fitness value of the agent. These masses interact through gravitational forces, attracting each other towards locations closer to the global optimum. Heavier masses (agents with better fitness) move more slowly, while lighter masses (agents with poorer fitness) move more rapidly. Each mass in GSA has four properties: position, inertial mass, active gravitational mass, and passive gravitational mass. The position of a mass represents a candidate solution to the problem, and its gravitational and inertial masses are derived from the fitness function.</p> <p>Consider a population of agents as described by the following equations. At a specific time \\(t\\), the velocity and position of each agent are updated as follows:</p> \\[ \\begin{align}     v_{i,d}^{t+1} &amp;= \\kappa_i \\times v_{i,d}^t + a_{i,d}^t, \\\\     x_{i,d}^{t+1} &amp;= x_{i,d}^t + v_{i,d}^{t+1}. \\end{align} \\tag{38} \\] <p>Here, \\(\\kappa_i\\) introduces stochastic characteristics to the search process. The acceleration \\(a_{i,d}^t\\) is computed according to the law of motion Rashedi, Esmat and Nezamabadi-Pour, Hossein and Saryazdi, Saeid:</p> \\[ \\begin{equation}     a_{i,d}^t = \\frac{F_{i,d}^t}{M_{ii}^{t}}, \\end{equation} \\tag{39} \\] <p>where \\(M_{ii}^{t}\\) is the inertial mass of agent \\(i\\) and \\(F_{i,d}^t\\) represents the gravitational force acting on agent \\(i\\) in the \\(d\\)-dimensional space. The detailed process for calculating and updating \\(F_{i,d}\\) and \\(M_{ii}\\) can be found in Rashedi, Esmat and Nezamabadi-Pour, Hossein and Saryazdi, Saeid.</p>"},{"location":"book/4-Model-Structure-Selection/#the-binary-hybrid-optimization-algorithm","title":"The Binary Hybrid Optimization Algorithm","text":"<p>The combination of algorithms follows the approach described in Mirjalili, S. and Hashim, S. Z. M. - A new hybrid PSOGSA algorithm for function optimization:</p> \\[ \\begin{align}     v_{i}^{t+1} = \\zeta \\times v_i^t + \\mathrm{c}'_{1} \\times \\kappa \\times a_i^t + \\mathrm{c}'_2 \\times \\kappa \\times (gbest - x_i^t), \\end{align} \\tag{40} \\] <p>where \\(\\mathrm{c}'_j \\in \\mathbb{R}\\) are acceleration coefficients. This formulation accelerates the exploitation phase by incorporating the best mass location found so far. However, this method may negatively impact the exploration phase. To address this issue, Mirjalili, S., Mirjalili, S. M., and Lewis, A. - Grey Wolf Optimizer proposed adaptive values for \\(\\mathrm{c}'_j\\), as described in Mirjalili, S., Wang, Gai-Ge, and Coelho, L. dos S. - Binary optimization using hybrid particle swarm optimization and gravitational search algorithm:</p> \\[ \\begin{align}     \\mathrm{c}_1' &amp;= -2 \\times \\frac{t^3}{\\max(t)^3} + 2, \\\\     \\mathrm{c}_2' &amp;= 2 \\times \\frac{t^3}{\\max(t)^3} + 2. \\end{align} \\tag{41} \\] <p>In each iteration, the positions of particles are updated according to the following rules, with continuous space mapped to discrete solutions using a transfer function Mirjalili, S. and Lewis, A. - S-shaped versus V-shaped transfer functions for binary Particle Swarm Optimization:</p> \\[ \\begin{equation}     S(v_{ik}) = \\left|\\frac{2}{\\pi}\\arctan\\left(\\frac{\\pi}{2}v_{ik}\\right)\\right|. \\end{equation} \\tag{42} \\] <p>With a uniformly distributed random number \\(\\kappa \\in (0,1)\\), the positions of the agents in the binary space are updated as follows:</p> \\[ \\begin{equation}     x_{np,d}^{t+1} =     \\begin{cases}         (x_{np,d}^{t})^{-1}, &amp; \\text{if } \\kappa &lt; S(v_{ik}^{t+1}), \\\\         x_{np,d}^{t}, &amp; \\text{if } \\kappa \\geq S(v_{ik}^{t+1}).     \\end{cases} \\end{equation} \\tag{43} \\]"},{"location":"book/4-Model-Structure-Selection/#meta-model-structure-selection-metamss-building-narx-for-regression","title":"Meta-Model Structure Selection (MetaMSS): Building NARX for Regression","text":"<p>In this section, we explore the meta-heuristic approach for selecting the structure of NARX models using BPSOGSA proposed in my master's thesis. This method searches for the optimal model structure within a decision space defined by a predefined dictionary of regressors. The objective function for this optimization problem is based on the root mean squared error (RMSE) of the free-run simulation output, augmented by a penalty factor that accounts for the model's complexity and the contribution of each regressor to the final model.</p>"},{"location":"book/4-Model-Structure-Selection/#encoding-scheme","title":"Encoding Scheme","text":"<p>The process of using BPSOGSA for model structure selection involves defining the dimensions of the test function. Specifically, \\(n_y\\), \\(n_x\\), and \\(\\ell\\) are set to cover all possible regressors, and a general matrix of regressors, \\(\\Psi\\), is constructed. The number of columns in \\(\\Psi\\) is denoted as \\(noV\\), and the number of agents, \\(N\\), is specified. A binary \\(noV \\times N\\) matrix, referred to as \\(\\mathcal{X}\\), is then randomly generated to represent the position of each agent in the search space. Each column of \\(\\mathcal{X}\\) represents a potential solution, which is essentially a candidate model structure to be evaluated in each iteration. In this matrix, a value of 1 indicates that the corresponding column of \\(\\Psi\\) is included in the reduced matrix of regressors, while a value of 0 indicates exclusion.</p> <p>For example, consider a case where all possible regressors are defined with \\(\\ell = 1\\) and \\(n_y = n_u = 2\\). The matrix \\(\\Psi\\) is:</p> \\[ \\begin{align} [ \\text{constant} \\quad y(k-1) \\quad y(k-2) \\quad u(k-1) \\quad u(k-2) ] \\end{align} \\tag{44} \\] <p>With 5 possible regressors, \\(noV = 5\\). Assuming \\(N = 5\\), \\(\\mathcal{X}\\) might be represented as:</p> \\[ \\begin{equation}     \\mathcal{X} =     \\begin{bmatrix}         0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\         1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\         0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\         0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1 &amp; 0     \\end{bmatrix} \\end{equation} \\tag{45} \\] <p>Each column of \\(\\mathcal{X}\\) is transposed to generate a candidate solution. For example, the first column of \\(\\mathcal{X}\\) yields:</p> \\[ \\begin{equation*}     \\mathcal{X} =     \\begin{bmatrix}         \\text{constant} &amp; y(k-1) &amp; y(k-2) &amp; u(k-1) &amp; u(k-2) \\\\         1 &amp; 1 &amp; 1 &amp; 0 &amp; 1     \\end{bmatrix} \\end{equation*} \\tag{46} \\] <p>In this scenario, the first model to be evaluated is \\(\\alpha y(k-1) + \\beta u(k-2)\\), where \\(\\alpha\\) and \\(\\beta\\) are parameters estimated using some parameter estimation method. The process is repeated for each subsequent column of \\(\\mathcal{X}\\).</p>"},{"location":"book/4-Model-Structure-Selection/#formulation-of-the-objective-function","title":"Formulation of the objective function","text":"<p>For each candidate model structure randomly defined, the linear-in-the-parameters system can be solved directly using the Least Squares algorithm or any other method available in SysIdentPy. The variance of the estimated parameters can be calculated as:</p> \\[ \\hat{\\sigma}^2 = \\hat{\\sigma}_e^2 V_{jj}, \\tag{47} \\] <p>where \\(\\hat{\\sigma}_e^2\\) is the estimated noise variance, given by:</p> \\[ \\hat{\\sigma}_e^2 = \\frac{1}{N-m} \\sum_{k=1}^{N} (y_k - \\psi_{k-1}^\\top \\hat{\\Theta}), \\tag{48} \\] <p>and \\(V_{jj}\\) is the \\(j\\)th diagonal element of \\((\\Psi^\\top \\Psi)^{-1}\\).</p> <p>The estimated standard error of the \\(j\\)th regression coefficient \\(\\hat{\\Theta}_j\\) is the positive square root of the diagonal elements of \\(\\hat{\\sigma}^2\\):</p> \\[ \\mathrm{se}(\\hat{\\Theta}_j) = \\sqrt{\\hat{\\sigma}^2_{jj}}. \\tag{49} \\] <p>To assess the statistical relevance of each regressor, a penalty test considers the standard error of the regression coefficients. In this case, the \\(t\\)-test is used to perform a hypothesis test on the coefficients, evaluating the significance of individual regressors in the multiple linear regression model. The hypothesis statements are:</p> \\[ \\begin{align*}    H_0 &amp;: \\Theta_j = 0, \\\\    H_a &amp;: \\Theta_j \\neq 0. \\end{align*} \\tag{50} \\] <p>The \\(t\\)-statistic is computed as:</p> \\[ T_0 = \\frac{\\hat{\\Theta}_j}{\\mathrm{se}(\\hat{\\Theta}_j)}, \\tag{51} \\] <p>which measures how many standard deviations \\(\\hat{\\Theta}_j\\) is from zero. More precisely, if:</p> \\[ -t_{\\alpha/2, N-m} &lt; T_0 &lt; t_{\\alpha/2, N-m}, \\tag{52} \\] <p>where \\(t_{\\alpha/2, N-m}\\) is the \\(t\\) value obtained considering \\(\\alpha\\) as the significance level and \\(N-m\\) as the degrees of freedom, then if \\(T_0\\) falls outside this acceptance region, the null hypothesis \\(H_0: \\Theta_j = 0\\) is rejected. This implies that \\(\\Theta_j\\) is significant at the \\(\\alpha\\) level. Otherwise, if \\(T_0\\) lies within the acceptance region, \\(\\Theta_j\\) is not significantly different from zero, and the null hypothesis cannot be rejected.</p>"},{"location":"book/4-Model-Structure-Selection/#penalty-value-based-on-the-derivative-of-the-sigmoid-linear-unit-function","title":"Penalty value based on the Derivative of the Sigmoid Linear Unit function","text":"<p>We propose a penalty value based on the derivative of the sigmoid function, defined as:</p> \\[ \\dot{\\varsigma}(x(\\varrho)) = \\varsigma(x) [1 + (a(x - c))(1 - \\varsigma(x))]. \\tag{53} \\] <p>In this formulation, the parameters are defined as follows: \\(x\\) has the dimension of \\(noV\\); \\(c = noV / 2\\); and \\(a\\) is set as the ratio of the number of regressors in the current test model to \\(c\\). This approach results in a distinct curve for each model, with the slope of the sigmoid curve becoming steeper as the number of regressors increases. The penalty value, \\(\\varrho\\), corresponds to the \\(y\\) value of the sigmoid curve for the given number of regressors in \\(x\\). Since the derivative of the sigmoid function can return negative values, we normalize \\(\\varsigma\\) as:</p> \\[ \\varrho = \\varsigma - \\mathrm{min}(\\varsigma), \\tag{54} \\] <p>ensuring that \\(\\varrho \\in \\mathbb{R}^{+}\\).</p> <p>However, two different models with the same number of regressors can yield significantly different results due to the varying importance of each regressor. To address this, we use the \\(t\\)-student test to assess the statistical relevance of each regressor and incorporate this information into the penalty function. Specifically, we calculate \\(n_{\\Theta, H_{0}}\\), the number of regressors that are not significant for the model. The penalty value is then adjusted based on the model size:</p> \\[ \\mathrm{model\\_size} = n_{\\Theta} + n_{\\Theta, H_{0}}. \\tag{55} \\] <p>The objective function, which integrates the relative root mean squared error of the model with \\(\\varrho\\), is defined as:</p> \\[ \\mathcal{F} = \\frac{\\sqrt{\\sum_{k=1}^{n} (y_k - \\hat{y}_k)^2}}{\\sqrt{\\sum_{k=1}^{n} (y_k - \\bar{y})^2}} \\times \\varrho. \\tag{56} \\] <p>This approach ensures that even if models have the same number of regressors, those with redundant regressors are penalized more heavily.</p>"},{"location":"book/4-Model-Structure-Selection/#case-studies-simulation-results","title":"Case Studies: Simulation Results","text":"<p>In this section, six simulation examples are presented to illustrate the effectiveness of the MetaMSS algorithm. An analysis of the algorithm's performance has been conducted, considering various tuning parameters. The selected systems are generally used as benchmarks for model structure algorithms and were taken from the following sources: Wei, H. and Billings, S. A., \"Model structure selection using an integrated forward orthogonal search algorithm assisted by squared correlation and mutual information\", Falsone, A. and Piroddi, L. and Prandini, M., \"A randomized algorithm for nonlinear model structure selection\", Baldacchino, T. and Anderson, S. R. and Kadirkamanathan, V., \"Computational system identification for Bayesian NARMAX modelling\", Piroddi, L. and Spinelli, W., \"An identification algorithm for polynomial NARX models based on simulation error minimization\", Guo, Y. and Guo, L. Z. and Billings, S. A. and Wei, H., \"A New Iterative Orthogonal Forward Regression Algorithm\", Bonin, M. and Seghezza, V. and Piroddi, L., \"NARX model selection based on simulation error minimization and LASSO\", and Aguirre, L. A. and Barbosa, B. H. G. and Braga, A. P., \"Prediction and simulation errors in parameter estimation for nonlinear systems\". Finally, a comparative analysis has been performed with respect to the Randomized Model Structure Selection (RaMSS), \"A randomized algorithm for nonlinear model structure selection\", the FROLS, and the Reversible-jump Markov chain Monte Carlo (RJMCMC), \"Computational system identification for Bayesian NARMAX modelling\" algorithms to evaluate the effectiveness of the proposed method.</p> <p>The simulation models are described as:</p> \\[ \\begin{align}     &amp; S_1: \\quad y_k = -1.7y_{k-1} - 0.8y_{k-2} + x_{k-1} + 0.81x_{k-2} + e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-2, 2) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.01^2); \\nonumber \\\\     &amp; S_2: \\quad y_k = 0.8y_{k-1} + 0.4x_{k-1} + 0.4x_{k-1}^2 + 0.4x_{k-1}^3 + e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{N}(0, 0.3^2) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.01^2). \\nonumber \\\\     &amp; S_3: \\quad y_k = 0.2y_{k-1}^3 + 0.7y_{k-1}x_{k-1} + 0.6x_{k-2}^2 \\nonumber \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 -0.5y_{k-2}+ e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-1, 1) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.01^2). \\nonumber \\\\     &amp; S_4: \\quad y_k = 0.7y_{k-1}x_{k-1} - 0.5y_{k-2} + 0.6x_{k-2}^2 \\nonumber \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 + e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-1, 1) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.04^2). \\nonumber \\\\     &amp; S_5: \\quad y_k = 0.7y_{k-1}x_{k-1} - 0.5y_{k-2} + 0.6x_{k-2}^2 \\nonumber \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 + 0.2e_{k-1} \\nonumber \\\\     &amp; \\qquad \\quad - 0.3x_{k-1}e_{k-2} + e_k,\\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-1, 1) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.02^2); \\nonumber \\\\     &amp; S_6: \\quad y_k = 0.75y_{k-2} + 0.25x_{k-2} - 0.2y_{k-2}x_{k-2} + e_k \\nonumber \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{N}(0, 0.25^2) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.02^2); \\nonumber \\end{align} \\tag{57} \\] <p>where \\(\\mathcal{U}(a, b)\\) are samples evenly distributed over~\\([a, b]\\), and \\(\\mathcal{N}(\\eta, \\sigma^2)\\) are samples with a Gaussian distribution with mean \\(\\eta\\) and standard deviation \\(\\sigma\\). All realizations of the systems are composed of a total of \\(500\\) input-output data samples. Also, the same random seed is used to reproducibility purpose.</p> <p>All tests shown in this section are based on the original implementation and are took from the results of my master thesis. At the time, the algorithm was performed in Matlab \\(2018\\)a environment, on a Dell Inspiron \\(5448\\) Core i\\(5-5200\\)U CPU \\(2.20\\)GHz with \\(12\\)GB of RAM. However, it is not a hard task to adapt them to SysIdentPy.</p> <p>Following the aforementioned studies, the maximum lags for the input and output are chosen to be, respectively, \\(n_u=n_y=4\\) and the nonlinear degree is \\(\\ell = 3\\). The parameters related to the BPSOGSA are detailed on Table 8.</p> Parameters \\(n_u\\) \\(n_y\\) \\(\\ell\\) p-value max_iter n_agents \\(\\alpha\\) \\(G_0\\) Values \\(4\\) \\(4\\) \\(3\\) \\(0.05\\) \\(30\\) \\(10\\) \\(23\\) \\(100\\) &gt;Table 8. Parameters used in MetaMSS <p>\\(300\\) runs of the Meta-MSS algorithm have been executed for each model, aiming to compare some statistics about the algorithm performance. The elapsed time, the time required to obtain the final model, and correctness, the percentage of exact model selections, are analyzed.</p> <p>The results in Table 9 are obtained with the parameters configured accordingly to Table 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_5\\) \\(S_6\\) Correct model 100\\% 100\\% 100\\% 100\\% 100\\% 100\\% Elapsed time (mean) 5.16s 3.90s 3.40s 2.37s 1.40s 3.80s &gt;Table 9. Overall performance of the MetaMSS <p>Table 9 shows that all the model terms are correctly selected using the Meta-MSS. It is worth to notice that even the model \\(S_5\\), which have an autoregressive noise, was correctly selected using the proposed algorithm. This result resides in the evaluation of all regressors individually, and the ones considered redundant are removed from the model.</p> <p>Figure 15 presents the convergence of each execution of Meta-MSS. It is noticeable that the majority of executions converges to the correct model structures with \\(10\\) or fewer iterations. The reason for this relies on the maximum number of iterations and the number of search agents. The first one is related to the acceleration coefficient, which boosts the exploration phase of the algorithm, while the latter increases the number of candidate models to be evaluated. Intuitively, one can see that both parameters influence the elapsed time and, more importantly, the model structure selected to compose the final model. Consequently, an inappropriate choice of one of them may results in sub/over-parameterized models, since the algorithm can converge to a local optimum. The next subsection presents an analysis of the max_iter and n_agents influence in the algorithm performance.</p> <p></p> <p>Figure 15. Convergence of Meta-MSS for different model structures. The figure illustrates the convergence behavior of the Meta-MSS algorithm across multiple executions. Each curve represents the convergence trajectory for a specific model structure from \\(S_1\\) to \\(S_6\\) over a maximum of 30 iterations.</p>"},{"location":"book/4-Model-Structure-Selection/#influence-of-the-max_iter-and-n_agents-parameters","title":"Influence of the \\(max\\_iter\\) and \\(n\\_agents\\) parameters","text":"<p>The simulation models are used to show the performance of the Meta-MSS considering different tuning for <code>max_iter</code> and <code>n_agents</code> parameters. First, we set and uphold the <code>max_iter=30</code> while the <code>n_agents</code> are changed. Then, we set and uphold the <code>n_agents</code> while the <code>max_iter</code> is modified. The results detailed in this section have been obtained by setting the remaining parameters according to Table 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_5\\) \\(S_6\\) max_iter = 30, n_agents = 1 Correct model \\(65\\%\\) \\(55.66\\%\\) \\(14\\%\\) \\(14\\%\\) \\(7.3\\%\\) \\(20.66\\%\\) Elapsed time (mean) \\(0.26\\)s \\(0.19\\)s \\(0.15\\)s \\(0.11\\)s \\(0.13\\)s \\(0.13\\)s max_iter = 30, n_agents = 5 Correct model \\(100\\%\\) \\(100\\%\\) \\(99\\%\\) \\(98\\%\\) \\(91.66\\%\\) \\(98.33\\%\\) Elapsed time (mean) \\(2.08\\)s \\(1.51\\)s \\(1.41\\)s \\(0.99\\)s \\(0.59\\)s \\(1.13\\)s max_iter = 30, n_agents = 20 Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(12.88\\)s \\(9.10\\)s \\(8.77\\)s \\(5.70\\)s \\(3.37\\)s \\(9.50\\)s max_iter = 5, n_agents = 10 Correct model \\(96.33\\%\\) \\(99\\%\\) \\(86\\%\\) \\(93.66\\%\\) \\(93\\%\\) \\(97.33\\%\\) Elapsed time (mean) \\(0.92\\)s \\(0.73\\)s \\(0.72\\)s \\(0.52\\)s \\(0.29\\)s \\(0.64\\)s max_iter = 15, n_agents = 10 Correct model \\(100\\%\\) \\(100\\%\\) \\(99\\%\\) \\(99\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(2.80\\)s \\(2.33\\)s \\(2.25\\)s \\(1.60\\)s \\(0.90\\)s \\(2.30\\)s max_iter = 50, n_agents = 10 Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(7.38\\)s \\(5.44\\)s \\(4.56\\)s \\(3.01\\)s \\(2.10\\)s \\(4.52\\)s &gt;Table 10. <p>The aggregated results in Table 10 confirms the expected performance regarding the elapsed time and percentage of correct models. Indeed, both metrics increases significantly as the number of agents and the maximum number of iteration increases. The number of agents is very relevant because it yields a broader exploration of the search space. All system are affected by the increase in the number of agents and the maximum number of iterations.</p> <p>Regarding all tested systems, it is straightforward to notice that the more extensive exploration dramatically impacts on the exactitude of the selection procedure. If only a few agents are assigned, the performance of Meta-MSS algorithm deteriorates significantly, especially for systems \\(S_3, S_4\\) and \\(S_5\\). The maximum number of iteration empowers agents to explore, globally and locally, the space around the candidate models tested so far. In this sense, as the number of iterations increases, more the agents can explore the search space and examine different regressors.</p> <p>If these parameters are improperly chosen, the algorithm might fail to select the best model structure. In this respect, the results presented here concerns only the selected systems. The larger the search space, the larger the number of agents and iterations should be. Although the computational effort increases with larger values for n_agents and max_iteration, the algorithm remains very efficient regarding the elapsed time for all tuning configurations that ensured the selection of the exact model structures.</p>"},{"location":"book/4-Model-Structure-Selection/#selection-of-over-and-sub-parameterized-models","title":"Selection of over and sub-parameterized models","text":"<p>Regardless of the successful selection of all models structures by the Meta-Structure Selection Algorithm, one can ask how the models differs from the true ones in the cases presented in Table 10 where the algorithm failed to ensure \\(100\\%\\) of correctness. Figure 16 depicts the distribution of terms number selected in each case. It is evident that the number of over-parameterized models selected is higher than the sub-parameterized in overall. Regarding the cases where the number of search agents are low, due to low exploration and exploitation capacity, the algorithm converged early and resulted in models with a high number of spurious regressors. In respect to \\(S_2\\) and \\(S_5\\), for example, with n_agents\\(=1\\), the algorithm ends up selecting models with more than \\(20\\) terms. One can say this was a extreme scenario for comparison purpose. However, a suitable choice for the parameters is intrinsically related to the dimension of the search space. Referring to cases where n_agents\\(\\geq 5\\), the number of spurious terms decreased significantly where the algorithm failed to select the true models.</p> <p>Furthermore, it is interesting to point out the importance of tuning the parameters properly because since the exploration and exploitation phase of the algorithm are strongly dependent on them. A premature convergence of the algorithm may result in models with the factual number of terms, but with wrong ones. This happened with all cases with <code>n_agents=1</code>. For example, the algorithm generates models with correct number of terms in \\(33.33\\%\\) of the cases analyzed regarding \\(S_3\\). However, Table 10 shows that only \\(14\\%\\) are, in fact, equivalent to the true model.</p> <p></p> <p>Figure 16. The distribution of terms number selected for each simulated models concerning the variation of the <code>max_iter</code> and <code>n_agents</code>.</p>"},{"location":"book/4-Model-Structure-Selection/#selection-of-over-and-sub-parameterized-models_1","title":"Selection of over and sub-parameterized models","text":"<p>Regardless of the successful selection of all models structures by the MetaMSS, one can ask how the models differ from the true ones in the cases presented in Table 10 where the algorithm failed to ensure \\(100\\%\\) of correctness. Figure 16 depicts the distribution of terms number selected in each case. It is evident that the number of over-parameterized models selected is higher than the sub-parameterized in overall. Regarding the cases where the number of search agents is low, due to low exploration and exploitation capacity, the algorithm converged early and resulted in models with a high number of spurious regressors. In respect to \\(S_2\\) and \\(S_5\\), for example, with <code>n_agents=1</code>, the algorithm ends up selecting models with more than \\(20\\) terms. One can say this was an extreme scenario for comparison purpose. However, a suitable choice for the parameters is intrinsically related to the dimension of the search space. By referring to cases where <code>n_agents</code>\\(\\geq 5\\), the number of spurious terms decreased significantly where the algorithm failed to select the true models.</p> <p>Furthermore, it is interesting to point out the importance of tuning the parameters properly because since the exploration and exploitation phase of the algorithm is strongly dependent on them. A premature convergence of the algorithm may result in models with the actual number of terms, but with wrong ones. This issue happened with all cases with <code>n_agents=1</code>. For example, the algorithm generates models with the correct number of terms in \\(33.33\\%\\) of the cases analyzed regarding \\(S_3\\). However, Table 10 shows that only \\(14\\%\\) are, in fact, equivalent to the true model.</p> <p>The systems \\(S_1\\), \\(S_2\\), \\(S_3\\), \\(S_4\\) and \\(S_6\\) has been used as benchmark by Bianchi, F., Falsone, A., Prandini, M. and Piroddi, L., so we can compare directly our results with those reported by the author in his thesis. All techniques used \\(n_y=n_u=4\\) and \\(\\ell = 3\\). The RaMSS and the RaMSS with Conditional Linear Family (C-RaMSS) used the following configuration for the tuning parameters: \\(K=1\\), \\(\\alpha = 0.997\\), \\(NP = 200\\) and \\(v=0.1\\). The Meta-Structure Selection Algorithm was tuned according to Table 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_6\\) Meta-MSS Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(5.16\\)s \\(3.90\\)s \\(3.40\\)s \\(2.37\\)s \\(3.80\\)s RaMSS- \\(NP=100\\) Correct model \\(90.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(66\\%\\) Elapsed time (mean) \\(3.27\\)s \\(1.24\\)s \\(2.59\\)s \\(1.67\\)s \\(6.66\\)s RaMSS- \\(NP=200\\) Correct model \\(78.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(82\\%\\) Elapsed time (mean) \\(6.25\\)s \\(2.07\\)s \\(4.42\\)s \\(2.77\\)s \\(9.16\\)s C-RaMSS Correct model \\(93.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(18\\)s \\(10.50\\)s \\(16.96\\)s \\(10.56\\)s \\(48.52\\)s &gt; Table 11. Comparative analysis between MetaMSS, RaMSS, and C-RaMSS <p>In terms of correctness, the MetaMSS outperforms (or at least equals) the RaMSS and C-RaMSS for all analyzed systems as shown in Table 11. Regarding \\(S_6\\), the correctness rate increased by \\(18\\%\\) when compared with RaMSS and the elapsed time required for C-RaMSS obtain \\(100\\%\\) of correctness is \\(1276.84\\%\\) higher than the MetaMSS. Furthermore, the MetaMSS is notably more computationally efficient than C-RaMSS and similar to RaMSS.</p>"},{"location":"book/4-Model-Structure-Selection/#metamss-vs-frols","title":"MetaMSS vs FROLS","text":"<p>The FROLS algorithm was applied to all tested systems, with the results summarized in Table 12. The algorithm successfully selected the correct model terms for \\(S_2\\) and \\(S_6\\). However, it failed to identify two out of four regressors for \\(S_1\\). For \\(S_3\\), FROLS included \\(y_{k-1}\\) instead of the correct term \\(y_{k-1}^3\\). Similarly, \\(S_4\\) incorrectly included \\(y_{k-4}\\) rather than the required term \\(y_{k-2}\\). Additionally, for \\(S_5\\), the algorithm produced an incorrect model structure by including the spurious term \\(y_{k-4}\\).</p> Meta-MSS Regressor Correct FROLS Regressor Correct \\(S_1\\) \\(y_{k-1}\\) yes \\(y_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-2}\\) yes \\(x_{k-4}\\) no \\(S_2\\) \\(y_{k-1}\\) yes \\(y_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}^2\\) yes \\(x_{k-1}^2\\) yes \\(x_{k-1}^3\\) yes \\(x_{k-1}^3\\) yes \\(S_3\\) \\(y_{k-1}^3\\) yes \\(y_{k-1}\\) no \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}\\) yes \\(y_{k-2}\\) yes \\(S_4\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(S_5\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(S_6\\) \\(y_{k-2}\\) yes \\(y_{k-2}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(y_{k-2}x_{k-2}\\) yes \\(y_{k-2}x_{k-1}\\) yes &gt; Table 12. Comparative analysis between MetaMSS and FROLS"},{"location":"book/4-Model-Structure-Selection/#meta-mss-vs-rjmcmc","title":"Meta-MSS vs RJMCMC","text":"<p>The \\(S_4\\) model is taken from Baldacchino, Anderson, and Kadirkamanathan's work (Computational System Identification for Bayesian NARMAX Modelling). In their study, the maximum lags for the input and output are \\(n_y = n_u = 4\\), and the nonlinear degree is \\(\\ell = 3\\). The authors ran the RJMCMC algorithm 10 times on the same input-output data. The RJMCMC method successfully identified the true model structure 7 out of 10 times. In contrast, the MetaMSS algorithm consistently identified the true model structure in all runs. These results are summarized in Table 13.</p> <p>Additionally, the RJMCMC method has notable drawbacks that are addressed by the MetaMSS algorithm. Specifically, RJMCMC is computationally intensive, requiring \\(30,000\\) iterations to achieve results. Furthermore, it relies on various probability distributions to simplify the parameter estimation process, which can complicate the computations. In contrast, MetaMSS offers a more efficient and straightforward approach, avoiding these issues.</p> Meta-MSS Model Correct RJMCMC Model 1 (\\(7\\times\\)) RJMCMC Model 2 RJMCMC Model 3 RJMCMC Model 4 Correct \\(S_4\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-2}\\) \\(y_{k-2}\\) \\(y_{k-2}\\) \\(y_{k-2}\\) yes \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) yes - - - \\(y_{k-3}x_{k-3}\\) \\(x_{k-4}^2\\) \\(x_{k-1}x_{k-3}^2\\) no &gt; Table 13. Comparative analysis between MetaMSS and RJMCMC."},{"location":"book/4-Model-Structure-Selection/#metamss-algorithm-using-sysidentpy","title":"MetaMSS algorithm using SysIdentPy","text":"<p>Consider the same data used in the Overview of the Information Criteria Methods.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\n\nbasis_function = Polynomial(degree=2)\nmodel = MetaMSS(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 random_state=42,\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p>The MetaMSS algorithm does not rely on information criteria methods such as ERR for model structure selection, which is why it does not involve those hyperparameters. This is also true for the AOLS and ER algorithms. For more details on how to use these methods and their associated hyperparameters, please refer to the documentation.</p> <p>When it comes to parameter estimation, SysIdentPy allows the use of any available method, regardless of the model structure selection algorithm. Users can select from a range of parameter estimation methods to apply to their chosen model structure. This flexibility enables users to explore various modeling approaches and customize their system identification process. While the examples provided use the default parameter estimation method, users are encouraged to experiment with different options to find the best fit for their needs.</p> <p>The results of the MetaMSS are</p> Regressors Parameters ERR y(k-1) 1.8004E-01 0.00000000E+00 x1(k-2) 8.9747E-01 0.00000000E+00 <p></p> <p>Figure 17. Free Run Simulation for the model fitted using MetaMSS.</p> <p>The <code>results</code> method brings ERR as 0 for every regressor because, as mentioned, ERR algorithm is not executed in this case.</p>"},{"location":"book/4-Model-Structure-Selection/#accelerated-orthogonal-least-squares-aols-and-entropic-regression-er","title":"Accelerated Orthogonal Least Squares (AOLS) and Entropic Regression (ER)","text":"<p>In addition to FROLS and MetaMSS, SysIdentPy includes two other methods for model structure selection: Accelerated Orthogonal Least Squares (AOLS) and Entropic Regression (ER). While I won't delve into the details of these methods in this section as I have with FROLS and MetaMSS, I will provide an overview and references for further reading:</p> <ul> <li>Accelerated Orthogonal Least Squares (AOLS): For an in-depth exploration of AOLS, refer to the original paper here.</li> <li>Entropic Regression (ER): Detailed information about ER can be found in the original paper here.</li> </ul> <p>For now, I will demonstrate how to use these methods within SysIdentPy.</p>"},{"location":"book/4-Model-Structure-Selection/#accelerated-orthogonal-least-squares","title":"Accelerated Orthogonal Least Squares","text":"<pre><code>from sysidentpy.model_structure_selection import AOLS\n\nbasis_function = Polynomial(degree=2)\nmodel = AOLS(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1542E-01 0.00000000E+00 <p>Figure 18. Free Run Simulation for the model fitted using AOLS algorithm.</p>"},{"location":"book/4-Model-Structure-Selection/#entropic-regression","title":"Entropic Regression","text":"<pre><code>from sysidentpy.model_structure_selection import ER\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> Regressors Parameters ERR 1 -2.4554E-02 0.00000000E+00 x1(k-2) 9.0273E-01 0.00000000E+00 <p>Figure 19. Free Run Simulation for the model fitted using Entropic Regression algorithm.</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/","title":"5. Multiobjective Parameter Estimation","text":"<p>Multiobjective parameter estimation represents a fundamental paradigm shift in the way we approach the parameter tuning problem for NARMAX models. Instead of seeking a single set of parameter values that optimally fits the model to the data, multiobjective approaches aim to identify a set of parameter solutions, known as the Pareto front, that provide a trade-off between competing objectives. These objectives often encompass a spectrum of model performance criteria, such as goodness-of-fit, model complexity, and robustness.</p> <p>What does that mean? It means that when we are modeling a dynamical system we are, most of the time, building models that are only good to represent the dynamical behavior of the system under study. Well, that is valid most of the time because we are building dynamical models, so if it doesn't perform well in static scenarios, it won't be a problem. However, that's not always the case and we might build a model that is good in both dynamical and static behavior. In such cases, the methods for purely dynamical systems are not adequate and multiobjective algorithms can help us in such task.</p> <p>The main idea in multiobjective parameter estimation is the inclusion of the affine information. The affine information is auxiliary information that can be defined a priori such as the static gain and the static function of the system. Formally, the affine information can be defined as follows:</p> <p>Let the parameter vector \\(\\Theta \\in \\mathbb{R}^{n_{\\Theta}}\\), a vector \\(\\mathrm{v}\\in \\mathbb{R}^p\\) and a matrix \\(\\mathrm{G}\\in \\mathbb{R}^{n_{\\Theta}\\times p}\\) where \\(\\mathrm{v}\\) and \\(\\mathrm{G}\\) are assumed to be accessible. Suppose \\(\\mathrm{G}\\Theta\\) be an estimate of \\(\\mathrm{v}\\). Hence, \\(\\mathrm{v} = \\mathrm{G}\\Theta + \\xi\\). Then, \\([\\mathrm{v}, \\mathrm{G}]\\) is a pair of affine information of the system.</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#multi-objective-optimization-problem","title":"Multi-objective optimization problem","text":"<p>Let's define what is a multiobjective problem. Given \\(m\\) objective functions</p> \\[ \\begin{equation}     \\mathrm{J}(\\hat{\\Theta}) = [J_1(\\hat{\\Theta}), J_2(\\hat{\\Theta}), \\cdots, J_m(\\hat{\\Theta})]^\\top, \\end{equation} \\tag{5.1} \\] <p>where \\(\\mathrm{J}(\\cdot):\\mathbb{R}^n \\mapsto \\mathbb{R}^m\\), a multi-objective optimization problem can be generally stated as (A. Baykasoglu, S. Owen, e N. Gindy)</p> \\[ \\begin{equation}     \\begin{aligned}         &amp; \\underset{\\Theta}{\\text{minimize}} &amp; &amp; \\mathrm{J}(\\Theta) \\\\ &amp; \\text{subject to} &amp; &amp; \\Theta \\in \\mathrm{S} = \\left\\{\\Theta \\mid \\Theta \\in \\mathrm{A}^n, g_i(\\Theta) \\leq a_i, h_j(\\Theta) = b_j \\right\\}, \\\\ &amp; &amp; &amp; i = 1, \\ldots, m, \\quad j = 1, \\ldots, n     \\end{aligned} \\end{equation} \\tag{5.2} \\] <p>where \\(\\Theta\\) is an \\(n\\)-dimensional vector of the decision variables, \\(\\mathrm{S}\\) is the set of feasible solutions bounded by \\(m\\) inequality constraints (\\(g_i\\)) and \\(n\\) equality constraints (\\(h_j\\)), and \\(a_i\\) and \\(b_j\\) are constants. For continuous variables \\(A = \\mathbb{R}\\) while \\(A\\) contains the set of permissible values for discrete variables.</p> <p>Usually problems with \\(1 &lt; m &lt; 4\\) are called multiobjective optimization problems. When there are more objectives (\\(m\\geq 4\\)), it is referred as many-objective optimization problems, an emergence class of multi-objective problems for solving complex modern real-world tasks. More details can be found in (Fleming, P. J., Purshouse, R. C., and Lygoe, R. J., \"Many-Objective Optimization: An Engineering Design Perspective\"), (Li, B., Li, J., Tang, K., and Yao, X., \"A survey on multi-objective evolutionary algorithms for many-objective problems\").</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#pareto-optimal-definition-and-pareto-dominance","title":"Pareto Optimal Definition and Pareto Dominance","text":"<p>Consider \\([y^{(1)}, y^{(2)}] \\in \\mathbb{R}^m\\) two vectors in the objective space. If and only if \\(\\forall \\in \\{1, \\ldots, m \\}: y_i^{(1)}\\leq y_i^{(2)}\\) and \\(\\exists j \\in \\{1, \\ldots, m \\}: y_j^{(1)} &lt; y_j^{(2)}\\) one can say \\(y^{(1)} \\prec y^{(2)}\\) (P. L. Yu, \"Cone convexity, cone extreme points, and non dominated solutions in decision problems with multiobjectives\").</p> <p>The concept of Pareto optimality is generally used to describe the trade-off among the minimization of different objectives. Following the pareto definition: the pareto optimal is any parameter vector representing an efficient solution  where no objective function can be improved without  making at least one objective function worse off will be referred to as a Pareto-model.</p> <p>In the system identification field, that means to find a model where you can't get a better dynamic performance without making the static performance worse.</p> <p>A hypothetical Pareto set is shown in Figure 1.</p> <p></p> <p>Figure 1. The figure illustrates the concept of Pareto optimality, where each point in the objective space represents a solution. The Pareto front is depicted as a curve, demonstrating the trade-off between two conflicting objectives. Points on the front cannot be improved in one objective without worsening the other, highlighting the balance in optimal solutions.</p> <p>In this case the model structure is assumed to be known and therefore there is a one-to-one correspondence between each parameter vector on the Pareto optimal solution and a model (Nepomuceno, E. G., Takahashi, R. H. C., and Aguirre, L. A., \"Multiobjective parameter estimation for non-linear systems: affine information and least-squares formulation\"). One can build a Pareto set by applying the Weighted Sum Method, where a set of objectives are scalarized into a single objective by adding each objective multiplied by a user supplied weight. Consider</p> \\[ \\begin{equation}     \\mathrm{W} = \\Bigg\\{ w|w \\in \\mathbb{R}^m, w_j\\geq 0 \\quad \\textrm{and} \\quad \\sum^{m}_{j=1}w_j=1 \\Bigg\\} \\end{equation} \\tag{5.3} \\] <p>as non-negative weights. Then, the convex optimization problem can be stated as</p> \\[ \\begin{equation} \\begin{aligned} \\Theta^* &amp;= \\underset{\\Theta}{\\text{argmin}} \\, \\langle w, \\mathrm{J}(\\Theta) \\rangle \\end{aligned} \\end{equation} \\tag{5.4} \\] <p>where \\(w\\) is a combination of weights to the different objectives functions. Therefore, the Pareto-set is associated to the set of realizations of \\(w \\in \\mathrm{W}\\). An efficient single-step computational strategy was presented by (Nepomuceno, E. G., Takahashi, R. H. C., and Aguirre, L. A., \"Multiobjective parameter estimation for non-linear systems: affine information and least-squares formulation\") for solving Equation 5.4 by means of a Least Squares formulation, which is presented in the following section.</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#affine-information-least-squares-algorithm","title":"Affine Information Least Squares Algorithm","text":"<p>Consider the \\(m\\) affine information pairs \\([\\mathrm{v}_i \\in \\mathbb{R}^{pi}, \\mathrm{G}_i \\mathbb{R}^{pi\\times n}]\\) with \\(i = 1, \\ldots, m\\). Assume there is exist a full column rank \\(\\mathrm{G}_i\\) and let \\(M\\) be a model of the form</p> \\[ y = \\Psi\\Theta + \\epsilon. \\tag{5.5} \\] <p>Then the \\(m\\) affine information pairs can be considered in the parameter estimation by solving</p> \\[ \\begin{equation} \\begin{aligned} \\Theta^* &amp;= \\underset{\\Theta}{\\text{argmin}} \\sum_{i=1}^{m} w_i (\\mathrm{v}_i - \\mathrm{G}_i \\Theta)^\\top (\\mathrm{v}_i - \\mathrm{G}_i \\Theta) \\end{aligned} \\end{equation} \\tag{5.6} \\] <p>with \\(w = [w_i, \\ldots, w_m]^\\top \\in \\mathrm{W}\\). The solution of equation above is given by</p> \\[ \\begin{equation}     \\Theta^* = \\left[\\sum^{m}_{i=1}w_i\\mathrm{G}_i^\\top\\mathrm{G}_i\\right]^{-1}  \\left[\\sum^{m}_{i=1}w_i\\mathrm{G}_i^\\top\\mathrm{v}_i\\right]. \\end{equation} \\tag{5.7} \\] <p>If there exists only one information, the problem reduces to the mono-objective Least Squares solution.</p> <p>To make things straightforward, lets check a detailed case study.</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#case-study-buck-converter","title":"Case Study - Buck converter","text":"<p>A buck converter is a type of DC/DC converter that decreases the voltage (while increasing the current) from its input (power supply) to its output (load). It is similar to a boost converter (elevator) and is a type of switched-mode power supply (SMPS) that typically contains at least two semiconductors (a diode and a transistor, although modern buck converters replace the diode with a second transistor used for synchronous rectification) and at least one energy storage element, a capacitor, inductor or both combined.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.multiobjective_parameter_estimation import AILS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.narmax_tools import set_weights\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#dynamic-behavior","title":"Dynamic Behavior","text":"<pre><code>df_train = pd.read_csv(r\"datasets/buck_id.csv\")\ndf_valid = pd.read_csv(r\"datasets/buck_valid.csv\")\n\n# Plotting the measured output (identification and validation data)\nplt.figure(1)\nplt.title(\"Output\")\nplt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5)\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Voltage\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Plotting the measured input (identification and validation data)\nplt.figure(2)\nplt.title(\"Input\")\nplt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5)\nplt.ylim(2.1, 2.6)\nplt.ylabel(\"u\")\nplt.xlabel(\"Samples\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#buck-converter-static-function","title":"Buck Converter Static Function","text":"<p>The duty cycle, represented by the symbol \\(D\\), is defined as the ratio of the time the system is on (\\(T_{on}\\)) to the total operation cycle time (\\(T\\)). Mathematically, this can be expressed as \\(D=\\frac{T_{on}}{T}\\). The complement of the duty cycle, represented by \\(D'\\), is defined as the ratio of the time the system is off (\\(T_{off}\\)) to the total operation cycle time (\\(T\\)) and can be expressed as \\(D'=\\frac{T_{off}}{T}\\).</p> <p>The load voltage (\\(V_o\\)) is related to the source voltage (\\(V_d\\)) by the equation \\(V_o = D\u22c5V_d = (1\u2212D')\u22c5V_d\\). For this particular converter, it is known that \\(D\u2032=\\frac{\\bar{u}-1}{3}\\), which means that the static function of this system can be derived from theory to be:</p> \\[ V_o = \\frac{4V_d}{3} - \\frac{V_d}{3}\\cdot \\bar{u} \\] <p>If we assume that the source voltage \\(V_d\\) is equal to 24 V, then we can rewrite the above expression as follows:</p> \\[ V_o = (4 - \\bar{u})\\cdot 8 \\] <pre><code># Static data\nVd = 24\nUo = np.linspace(0, 4, 50)\nYo = (4 - Uo) * Vd / 3\nUo = Uo.reshape(-1, 1)\nYo = Yo.reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Curve\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{y}$\")\nplt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\")\nplt.show()\n</code></pre> <p></p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#buck-converter-static-gain","title":"Buck converter Static Gain","text":"<p>The gain of a Buck converter is a measure of how its output voltage changes in response to changes in its input voltage. Mathematically, the gain can be calculated as the derivative of the converter\u2019s static function, which describes the relationship between its input and output voltages.</p> <p>In this case, the static function of the Buck converter is given by the equation:</p> \\[ V_o = (4 - \\bar{u})\\cdot 8 \\] <p>Taking the derivative of this equation with respect to \\(\\hat{u}\\), we find that the gain of the Buck converter is equal to \u22128. In other words, for every unit increase in the input voltage \\(\\hat{u}\\), the output voltage Vo will decrease by 8 units, so</p> \\[ gain=V_o'=-8 \\] <pre><code># Defining the gain\ngain = -8 * np.ones(len(Uo)).reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Gain\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{gain}$\")\nplt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#building-a-dynamic-model-using-the-mono-objective-approach","title":"Building a dynamic model using the mono-objective approach","text":"<pre><code>x_train = df_train.input.values.reshape(-1, 1)\ny_train = df_train.y.values.reshape(-1, 1)\nx_valid = df_valid.input.values.reshape(-1, 1)\ny_valid = df_valid.y.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=8,\n\u00a0 \u00a0 extended_least_squares=False,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 estimator=\"least_squares\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#affine-information-least-squares-algorithm-ails","title":"Affine Information Least Squares Algorithm (AILS)","text":"<p>AILS is a multiobjective parameter estimation algorithm, based on a set of affine information pairs. The multiobjective approach proposed in the mentioned paper and implemented in SysIdentPy leads to a convex multiobjective optimization problem, which can be solved by AILS. AILS is a LeastSquares-type non-iterative scheme for finding the Pareto-set solutions for the multiobjective problem.</p> <p>So, with the model structure defined (we will be using the one built using the dynamic data above), one can estimate the parameters using the multiobjective approach.</p> <p>The information about static function and static gain, besides the usual dynamic input/output data, can be used to build the pair of affine information to estimate the parameters of the model. We can model the cost function as:</p> \\[ \\gamma(\\hat\\theta) = w_1\\cdot J_{LS}(\\hat{\\theta})+w_2\\cdot J_{SF}(\\hat{\\theta})+w_3\\cdot J_{SG}(\\hat{\\theta}) \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#multiobjective-parameter-estimation-considering-3-different-objectives-the-prediction-error-the-static-function-and-the-static-gain","title":"Multiobjective parameter estimation considering 3 different objectives: the prediction error, the static function and the static gain","text":"<pre><code># you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work\nmo_estimator = AILS(final_model=model.final_model)\n# setting the log-spaced weights of each objective function\nw = set_weights(static_function=True, static_gain=True)\n# you can also use something like\n# w = np.array(\n# \u00a0 \u00a0 [\n# \u00a0 \u00a0 \u00a0 \u00a0 [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01],\n# \u00a0 \u00a0 \u00a0 \u00a0 [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01],\n# \u00a0 \u00a0 \u00a0 \u00a0 [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98],\n# \u00a0 \u00a0 ]\n# )\n\n# to set the weights. Each row correspond to each objective\n</code></pre> <p>AILS has an <code>estimate</code> method that returns the cost functions (J), the Euclidean norm of the cost functions (E), the estimated parameters referring to each weight (theta), the regressor matrix of the gain and static_function affine information HR and QR, respectively.</p> <pre><code>J, E, theta, HR, QR, position = mo_estimator.estimate(\n\u00a0 \u00a0 X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n\u00a0 \u00a0 \"w1\": w[0, :],\n\u00a0 \u00a0 \"w2\": w[2, :],\n\u00a0 \u00a0 \"w3\": w[1, :],\n\u00a0 \u00a0 \"J_ls\": J[0, :],\n\u00a0 \u00a0 \"J_sg\": J[1, :],\n\u00a0 \u00a0 \"J_sf\": J[2, :],\n\u00a0 \u00a0 \"||J||:\": E,\n}\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf \\(\\lVert J \\rVert\\) 0.006842 0.003078 0.990080 0.999970 1.095020e-05 0.000013 0.245244 0.007573 0.002347 0.990080 0.999938 2.294665e-05 0.000016 0.245236 0.008382 0.001538 0.990080 0.999885 6.504913e-05 0.000018 0.245223 0.009277 0.000642 0.990080 0.999717 4.505541e-04 0.000021 0.245182 0.006842 0.098663 0.894495 1.000000 7.393246e-08 0.000015 0.245251 ... ... ... ... ... ... ... 0.659632 0.333527 0.006842 0.995896 3.965699e-04 1.000000 0.244489 0.730119 0.263039 0.006842 0.995632 5.602981e-04 0.972842 0.244412 0.808139 0.185020 0.006842 0.995364 8.321071e-04 0.868299 0.244300 0.894495 0.098663 0.006842 0.995100 1.364999e-03 0.660486 0.244160 0.990080 0.003078 0.006842 0.992584 9.825987e-02 0.305492 0.261455 <p>Now we can set theta related to any weight results</p> <pre><code>model.theta = theta[-1, :].reshape(\n\u00a0 \u00a0 -1, 1\n) \u00a0# setting the theta estimated for the last combination of the weights\n\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\n\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 1 2.2930E+00 9.999E-01 y(k-1) 2.3307E-01 2.042E-05 y(k-2) 6.3209E-01 1.108E-06 x1(k-1) -5.9333E-01 4.688E-06 y(k-1)^2 2.7673E-01 3.922E-07 y(k-2)y(k-1) -5.3228E-01 8.389E-07 x1(k-1)y(k-1) 1.6667E-02 5.690E-07 y(k-2)^2 2.5766E-01 3.827E-06"},{"location":"book/5-Multiobjective-Parameter-Estimation/#the-dynamic-results-for-that-chosen-theta-is","title":"The dynamic results for that chosen theta is","text":"<pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#the-static-gain-result-is","title":"The static gain result is","text":"<pre><code>plt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#the-static-function-result-is","title":"The static function result is","text":"<pre><code>plt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#getting-the-best-weight-combination-based-on-the-norm-of-the-cost-function","title":"Getting the best weight combination based on the norm of the cost function","text":"<p>The variable <code>position</code> returned in <code>estimate</code> method give the position of the best weight combination. The model structure is exactly the same, but the order of the regressors is changed in estimate method. That's why you have to change the model.final_model. The dynamic, static gain, and the static function  results for that chosen theta is shown below.</p> <pre><code>model.theta = theta[position, :].reshape(\n\u00a0 \u00a0 -1, 1\n) \u00a0# setting the theta estimated for the best combination of the weights\n\n# changing the model.final_model\n\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\n# dynamic results\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\n# static gain\nplt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n\n# static function\nplt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> Regressors Parameters ERR 1 1.5405E+00 9.999E-01 y(k-1) 2.9687E-01 2.042E-05 y(k-2) 6.4693E-01 1.108E-06 x1(k-1) -4.1302E-01 4.688E-06 y(k-1)^2 2.7671E-01 3.922E-07 y(k-2)y(k-1) -5.3474E-01 8.389E-07 x1(k-1)y(k-1) 4.0624E-03 5.690E-07 y(k-2)^2 2.5832E-01 3.827E-06 <p></p> <p></p> <p></p> <p>You can also plot the pareto-set solutions</p> <pre><code>plt.figure(6)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Pareto-set solutions\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#detailing-ails","title":"Detailing AILS","text":"<p>The polynomial NARX model built using the mono-objective approach has the following structure:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>The, the goal when using the static function and static gain information in the multiobjective scenario is to estimate the vector \\(\\hat{\\theta}\\) based on:</p> \\[ \\theta = [w_1\\Psi^T\\Psi + w_2(HR)^T(HR) + w_3(QR)(QR)^T]^{-1} [w_1\\Psi^T y + w_2(HR)^T\\overline{g}+w_3(QR)^T\\overline{y}] \\] <p>The \\(\\Psi\\) matrix is built using the usual mono-objective dynamic modeling approach in SysIdentPy. However, it is still necessary to find the Q, H and R matrices. AILS have the methods to compute all of those matrices. Basically, to do that, \\(q_i^T\\) is first estimated:</p> \\[ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_1} &amp; \\overline{y_i}^2 &amp; \\cdots &amp; \\overline{y_i}^l &amp; F_{yu} &amp; \\overline{u_i}^2 &amp; \\cdots &amp; \\overline{u_i}^l \\end{bmatrix} \\] <p>where \\(F_{yu}\\) stands for all non-linear monomials in the model that are related to \\(y(k)\\) and \\(u(k)\\), \\(l\\) is the largest non-linearity in the model for input and output terms. For a model with a degree of nonlinearity equal to 2, we can obtain:</p> \\[ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_i} &amp; \\overline{y_i}^2 &amp; \\overline{u_i}\\:\\overline{y_i} &amp; \\overline{u_i}^2 \\end{bmatrix} \\] <p>It is possible to encode the \\(q_i^T\\) matrix so that it follows the model encoding defined in SysIdentPy. To do this, 0 is considered as a constant, \\(y_i\\) equal to 1 and \\(u_i\\) equal to 2. The number of columns indicates the degree of nonlinearity of the system and the number of rows reflects the number of terms:</p> \\[ q_i = \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\ 2 &amp; 2\\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y_i}\\\\ \\overline{u_i}\\\\ \\overline{y_i}^2\\\\ \\overline{u_i}\\:\\overline{y_i}\\\\ \\overline{u_i}^2\\\\ \\end{bmatrix} \\] <p>Finally, the result can be easily obtained using the \u2018regressor_space\u2019 method of SysIdentPy</p> <pre><code>from sysidentpy.narmax_base import RegressorDictionary\n\nobject_qit = RegressorDictionary(xlag=1, ylag=1)\nR_example = object_qit.regressor_space(n_inputs=1) // 1000\nprint(f\"R = {R_example}\")\n</code></pre> \\[ R = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ 2 &amp; 2 \\end{bmatrix} \\] <p>such that:</p> \\[ \\overline{y_i} = q_i^T R\\theta \\] <p>and:</p> \\[ \\overline{g_i} = H R\\theta \\] <p>where \\(R\\) is the linear mapping of the static regressors represented by \\(q_i^T\\). In addition, the \\(H\\) matrix holds affine information regarding \\(\\overline{g_i}\\), which is equal to \\(\\overline{g_i} = \\frac{d\\overline{y}}{d\\overline{u}}{\\big |}_{(\\overline{u_i}\\:\\overline{y_i})}\\).</p> <p>From now on, we will begin to apply the parameter estimation in a multiobjective manner. This will be done with the NARX polynomial model of the BUCK converter in mind. In this context, \\(q_i^T\\) will be generic and will assume a specific format for the problem at hand. For this task, the \\(R_qit\\) method will be used, whose objective is to return the \\(q_i^T\\) related to the model and the matrix of the linear mapping \\(R\\):</p> <pre><code>R, qit = mo_estimator.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> \\[ R = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} \\] <p>and</p> \\[ qit = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>So</p> \\[ q_i = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y} \\\\ \\overline{u} \\\\ \\overline{y^2} \\\\ \\overline{u} \\cdot \\overline{y} \\\\ \\end{bmatrix} \\] <p>You can notice that the method produces outputs consistent with what is expected:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>and:</p> \\[ R = \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ \\overline{y^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\\\ \\overline{y}\\:\\overline{u} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\end{bmatrix} \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#validation","title":"Validation","text":"<p>The following model structure will be used to validate the approach:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[ \\therefore \\] \\[ final\\_model = \\begin{bmatrix} 1001 &amp; 0\\\\ 1002 &amp; 0\\\\ 0 &amp; 0\\\\ 2001 &amp; 0\\\\ 2001 &amp; 2001\\\\ 2002 &amp; 2001\\\\ 2002 &amp; 0\\\\ 2002 &amp; 2002 \\end{bmatrix} \\] <p>defining in code:</p> <pre><code>final_model = np.array(\n\u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 [1001, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [1002, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [0, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [2001, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [2001, 2001],\n\u00a0 \u00a0 \u00a0 \u00a0 [2002, 2001],\n\u00a0 \u00a0 \u00a0 \u00a0 [2002, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [2002, 2002],\n\u00a0 \u00a0 ]\n)\nfinal_model\n</code></pre> 1001 0 1002 0 0 0 2001 0 2001 2001 2002 2001 2002 0 2002 2002 <pre><code>mult2 = AILS(final_model=final_model)\n\ndef psi(X, Y):\n\u00a0 \u00a0 PSI = np.zeros((len(X), 8))\n\u00a0 \u00a0 for k in range(2, len(Y)):\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 0] = Y[k - 1]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 1] = Y[k - 2]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 2] = 1\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 3] = X[k - 1]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 4] = X[k - 1] ** 2\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 5] = X[k - 2] * X[k - 1]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 6] = X[k - 2]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 7] = X[k - 2] ** 2\n\u00a0 \u00a0 return np.delete(PSI, [0, 1], axis=0)\n</code></pre> <p>The value of theta with the lowest mean squared error obtained with the same code implemented in Scilab was:</p> \\[ W_{LS} = 0.3612343 \\] <p>and:</p> \\[ W_{SG} = 0.3548699 \\] <p>and:</p> \\[ W_{SF} = 0.3548699 \\] <pre><code>PSI = psi(x_train, y_train)\nw = np.array([[0.3612343], [0.2838959], [0.3548699]])\nJ, E, theta, HR, QR, position = mult2.estimate(\n\u00a0 \u00a0 y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n\u00a0 \u00a0 \"w1\": w[0, :],\n\u00a0 \u00a0 \"w2\": w[2, :],\n\u00a0 \u00a0 \"w3\": w[1, :],\n\u00a0 \u00a0 \"J_ls\": J[0, :],\n\u00a0 \u00a0 \"J_sg\": J[1, :],\n\u00a0 \u00a0 \"J_sf\": J[2, :],\n\u00a0 \u00a0 \"||J||:\": E,\n}\n\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf \\(\\lVert J \\rVert\\) 0.361234 0.35487 0.283896 1.0 1.0 1.0 1.0 The order of the weights is different because the way we implemented in Python, but the results are very close as expected."},{"location":"book/5-Multiobjective-Parameter-Estimation/#dynamic-results","title":"Dynamic results","text":"<pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = mult2.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 1 1.4287E+00 9.999E-01 y(k-1) 5.5147E-01 2.042E-05 y(k-2) 4.0449E-01 1.108E-06 x1(k-1) -1.2605E+01 4.688E-06 x1(k-2) 1.2257E+01 3.922E-07 x1(k-1)^2 8.3274E+00 8.389E-07 x1(k-2)x1(k-1) -1.1416E+01 5.690E-07 x1(k-2)^2 3.0846E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#static-gain","title":"Static gain","text":"<pre><code>plt.figure(7)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\n\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#static-function","title":"Static function","text":"<pre><code>plt.figure(8)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#pareto-set-solutions","title":"Pareto-set solutions","text":"<pre><code>plt.figure(9)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Optimum pareto-curve\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p>The following table show the results reported in \u2018IniciacaoCientifica2007\u2019 and the ones obtained with SysIdentPy implementation</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.5514725 0.549144 \\(\\theta_2\\) 0.40449005 0.408028 \\(\\theta_3\\) 1.42867821 1.45097 \\(\\theta_4\\) -12.60548863 -12.55788 \\(\\theta_5\\) 8.32740057 8.1516315 \\(\\theta_6\\) -11.41574116 -11.09728 \\(\\theta_7\\) 12.25729955 12.215782 \\(\\theta_8\\) 3.08461195 2.9319577 <p>where:</p> \\[ E_{Scilab} = \u00a0 \u00a017.426613 \\] <p>and:</p> \\[ E_{Python} = 17.474865 \\] <p>Note: as mentioned before, the order of the regressors in the model change, but it is the same structure. The tables shows the respective regressor parameter concerning <code>SysIdentPy</code> and <code>IniciacaoCientifica2007</code>, but the order \\(\\Theta_1\\), \\(\\Theta_2\\) and so on are not the same of the ones in <code>model.final_model</code></p> <pre><code>R, qit = mult2.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> \\[ R = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>and</p> \\[ qit = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 2 \\\\ \\end{bmatrix} \\] <p>model's structure that will be utilized (\u2018IniciacaoCientifica2007\u2019):</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[ q_i = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 2 &amp; 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y} \\\\ \\overline{u} \\\\ \\overline{u^2} \\end{bmatrix} \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#biobjective-optimization","title":"Biobjective optimization","text":""},{"location":"book/5-Multiobjective-Parameter-Estimation/#an-use-case-applied-to-buck-converter-cc-cc-using-as-objectives-the-static-curve-information-and-the-prediction-error-dynamic","title":"An use case applied to Buck converter CC-CC using as objectives the static curve information and the prediction error (dynamic)","text":"<pre><code>bi_objective = AILS(\n\u00a0 \u00a0 static_function=True, static_gain=False, final_model=final_model, normalize=True\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> \\[ W_{LS} = 0.9931126 \\] <p>and:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective.estimate(\n\u00a0 \u00a0 y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\n\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\n\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg \\(\\lVert J \\rVert\\) 0.990080 0.009920 0.990863 1.000000 0.990939 0.987127 0.012873 0.990865 0.987032 0.990939 0.984182 0.015818 0.990867 0.974307 0.990939 0.981247 0.018753 0.990870 0.961803 0.990940 0.978320 0.021680 0.990873 0.949509 0.990941 ... ... ... ... ... 0.002555 0.997445 0.999993 0.000072 0.999993 0.002547 0.997453 0.999994 0.000072 0.999994 0.002540 0.997460 0.999996 0.000071 0.999996 0.002532 0.997468 0.999998 0.000071 0.999998 0.002525 0.997475 1.000000 0.000070 1.000000 <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nr\n</code></pre> Regressors Parameters ERR 0 1 1.3873E+00 9.999E-01 1 y(k-1) 5.4941E-01 2.042E-05 2 y(k-2) 4.0804E-01 1.108E-06 3 x1(k-1) -1.2515E+01 4.688E-06 4 x1(k-2) 1.2227E+01 3.922E-07 5 x1(k-1)^2 8.1171E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1047E+01 5.690E-07 7 x1(k-2)^2 2.9043E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(10)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Static Curve Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>where the best estimated \\(\\Theta\\) is</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54940883 0.5494135 \\(\\theta_2\\) 0.40803995 0.4080312 \\(\\theta_3\\) 1.38725684 3.3857601 \\(\\theta_4\\) -12.51466378 -12.513688 \\(\\theta_5\\) 8.11712897 8.116575 \\(\\theta_6\\) -11.04664789 -11.04592 \\(\\theta_7\\) 12.22693907 12.227184 \\(\\theta_8\\) 2.90425844 2.9038468 <p>where:</p> \\[ E_{Scilab} = 17.408934 \\] <p>and:</p> \\[ E_{Python} = 17.408947 \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#multiobjective-parameter-estimation","title":"Multiobjective parameter estimation","text":""},{"location":"book/5-Multiobjective-Parameter-Estimation/#use-case-considering-2-different-objectives-the-prediction-error-and-the-static-gain","title":"Use case considering 2 different objectives: the prediction error and the static gain","text":"<pre><code>bi_objective_gain = AILS(\n\u00a0 \u00a0 static_function=False, static_gain=True, final_model=final_model, normalize=False\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> \\[ W_{LS} = 0.9931126 \\] <p>and:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(0, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective_gain.estimate(\n\u00a0 \u00a0 X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\n\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\n\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg \\(\\lVert J \\rVert\\) 1.000000 0.000000 17.407256 3.579461e+01 39.802849 0.997012 0.002988 17.407528 2.109260e-01 17.408806 0.994033 0.005967 17.407540 2.082067e-01 17.408785 0.991063 0.008937 17.407559 2.056636e-01 17.408774 0.988102 0.011898 17.407585 2.031788e-01 17.408771 ... ... ... ... ... 0.002555 0.997445 17.511596 3.340081e-07 17.511596 0.002547 0.997453 17.511596 3.320125e-07 17.511596 0.002540 0.997460 17.511597 3.300289e-07 17.511597 0.002532 0.997468 17.511598 3.280571e-07 17.511598 0.002525 0.997475 17.511599 3.260972e-07 17.511599 <pre><code># Writing the results\nmodel.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective_gain.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4853E+00 9.999E-01 1 y(k-1) 5.4940E-01 2.042E-05 2 y(k-2) 4.0806E-01 1.108E-06 3 x1(k-1) -1.2581E+01 4.688E-06 4 x1(k-2) 1.2210E+01 3.922E-07 5 x1(k-1)^2 8.1686E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1122E+01 5.690E-07 7 x1(k-2)^2 2.9455E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(12)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\n\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Gain Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>being the selected \\(\\theta\\):</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54939785 0.54937289 \\(\\theta_2\\) 0.40805603 0.40810168 \\(\\theta_3\\) 1.48525190 1.48663719 \\(\\theta_4\\) -12.58066084 -12.58127183 \\(\\theta_5\\) 8.16862622 8.16780294 \\(\\theta_6\\) -11.12171897 -11.11998621 \\(\\theta_7\\) 12.20954849 12.20927355 \\(\\theta_8\\) 2.94548501 2.9446532 <p>where:</p> \\[ E_{Scilab} = \u00a017.408997 \\] <p>and:</p> \\[ E_{Python} = 17.408781 \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#additional-information","title":"Additional Information","text":"<p>You can also access the matrix Q and H using the following methods</p> <p>Matrix Q:</p> <pre><code>bi_objective_gain.build_static_function_information(Uo, Yo)[1]\n</code></pre> <p>Matrix H+R:</p> <pre><code>bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1]\n</code></pre>"},{"location":"book/6-Multiobjective-Model-Structure-Selection/","title":"6. Multiobjective Model Structure Selection","text":"<p>Coming soon</p>"},{"location":"book/7-NARX-Neural-Network/","title":"7. NARX Neural Network","text":"<p>Coming soon</p>"},{"location":"book/8-Severely-Nonlinear-System-Identification/","title":"8. Severely Nonlinear System Identification","text":"<p>We have categorized systems into two different classes for now: linear systems and nonlinear systems. As mentioned, linear systems has been extensively studied with several different well-established methods available, while nonlinear systems is a very active field with several problems that are still open for research. Besides linear and nonlinear systems, there are the ones called Severely Nonlinear Systems. Severely Nonlinear Systems are the ones that exhibit highly complex and exotic dynamic behaviors like sub-harmonics, chaotic behavior and hysteresis. For now, we will focus on system with hysteresis.</p>"},{"location":"book/8-Severely-Nonlinear-System-Identification/#modeling-hysteresis-with-polynomial-narx-model","title":"Modeling Hysteresis With Polynomial NARX Model","text":"<p>Hysteresis nonlinearity is a severely nonlinear behavior commonly found in electromagnetic devices, sensors, semiconductors, intelligent materials, and many more, which have memory effects between quasi-static input and output (Visintin, A., \"Differential Models of Hysteresis\"), (Ahmad, I., \"Two Degree-of-Freedom Robust Digital Controller Design With Bouc-Wen Hysteresis Compensator for Piezoelectric Positioning Stage\"). A hysteretic system is one that exhibits a path-dependent behavior, meaning its response depends not only on its current state but also on its history.  In a hysteretic system, when you apply an input, the system's response (like displacement or stress) doesn't follow the same path to the starting point when you remove the input. Instead, it forms a loop-like pattern called a hysteresis loop. This is because the system have the ability to preserve a deformation caused by an input, characterizing a memory effect.</p> <p>The identification of hysteretic systems using polynomial NARX models is typically an intriguing task because the traditional Model Structure Selection algorithms do not work properly (Martins, S. A. M. and Aguirre, L. A., \"Sufficient conditions for rate-independent hysteresis in autoregressive identified models\", Leva, A. and Piroddi, L., \"NARX-based technique for the modelling of magneto-rheological damping devices\"). Martins, S. A. M. and Aguirre, L. A. presented the sufficient conditions to describe hysteresis using polynomial models by providing the concept of bounding structure \\(\\mathcal{H}\\). Polynomial NARX models with a single equilibrium can be used in a full characterization of the hysteresis behavior adopting the bounding structure concept.</p> <p>The following are some of the essential concepts and formal definitions for understanding how NARX model can be used to describe systems with hysteresis.</p>"},{"location":"book/8-Severely-Nonlinear-System-Identification/#continuous-time-loading-unloading-quasi-static-signal","title":"Continuous-time loading-unloading quasi-static signal","text":"<p>One important characteristic to model hysteretic systems is the input signal. A loading-unloading quasi-static signal is a periodic continuous time signal \\(x_t\\) with period \\(T = (t_f - t_i)\\) and frequency \\(\\omega = 2\\pi f\\) where \\(x_t\\) increases monotonically from \\(x_{min}\\) to \\(x_{max}\\), considering \\(t_i \\leq t \\leq t_m\\) (loading) and decreases monotonically from \\(x_{max}\\) to \\(x_{min}\\), considering \\(t_m \\leq t \\leq t_f\\) (unloading). If the loading-unloading signal changes with \\(\\omega \\rightarrow 0\\), the signal is also called a quasi-static signal. Visually, this is much more simple to understand. The following image shows a continuous-time loading-unloading quasi-static signal.</p> <p></p> <p>Figure 1. Continuous-time loading-unloading quasi-static signal, demonstrating the periodic increase and decrease of the input signal.</p> <p>In this respect, Martins, S. A. M. and Aguirre, L. A. also presented the idea of transforming the inputs of the system using multi-valued functions.</p> <p>Multivalued functions - Let \\(\\phi (\\Delta x_{k}): \\mathbb{R} \\rightarrow \\mathbb{R}\\). If~\\(\\Delta x_{k}=x_k-x_{k-1}\\), \\(\\phi (\\Delta x_{k})\\) is a multivalued function if:</p> \\[ \\begin{equation}     \\phi (\\Delta x_{k})=     \\begin{cases}         \\phi_1, &amp; if \\ \\Delta x_{k} &gt; \\epsilon; \\\\         \\phi_2, &amp; if \\ \\Delta x_{k} &lt; \\epsilon; \\\\         \\phi_3, &amp; if \\ \\Delta x_{k} = \\epsilon; \\\\     \\end{cases} \\end{equation} \\tag{1} \\] <p>where \\(\\epsilon \\in \\mathbb{R}\\), \\(\\phi_1 \\neq \\phi_2 \\neq \\phi_3\\). For some inputs  \\(\\Delta x_{k}\\neq \\epsilon, \\ \\forall{k} \\in \\mathbb{N}\\) , and the last value in equation above is not used.</p> <p>A frequently used multivalued function is the sign\\((\\cdot): \\mathbb{R} \\rightarrow \\mathbb{R}\\):</p> \\[  \\begin{equation}  sign(x)=     \\begin{cases}         1, &amp; if \\ x &gt; 0; \\\\         -1, &amp; if \\ x &lt; 0; \\\\         0, &amp; if \\ x = 0. \\\\     \\end{cases} \\end{equation} \\tag{2} \\]"},{"location":"book/8-Severely-Nonlinear-System-Identification/#hysteresis-loops-in-continuous-time-mathcalh_tomega","title":"Hysteresis loops in continuous time \\(\\mathcal{H}_t(\\omega)\\)","text":"<p>Let \\(x_t\\) be a continuous-time loading-unloading quasi-static signal applied to a continuous-time system and \\(y_t\\) is the system output. \\(\\mathcal{H}_t(\\omega)\\) denotes a closed loop in the \\(x_t - y_t\\) plane, which shape depend on \\(\\omega\\). If the system presents hysteretic nonlinearity, \\(\\mathcal{H}_t(\\omega)\\) is denoted as:</p> \\[ \\begin{equation} \\mathcal{H}_t(\\omega) =     \\begin{cases}         \\mathcal{H}_t(\\omega)^{+}, \\ for \\ t_i \\ \\leq \\ t \\ \\leq \\ t_m, \\\\         \\mathcal{H}_t(\\omega)^{-}; \\ for \\ t_m \\ \\leq \\ t \\ \\leq \\ t_f, \\\\     \\end{cases} \\end{equation} \\tag{3} \\] <p>where \\(\\mathcal{H}_t(\\omega)^{+} \\neq \\mathcal{H}_t(\\omega)^{-}\\), \\(\\forall t \\neq t_m\\). \\(t_i \\leq t \\leq t_m\\) and~\\(t_m \\leq t \\leq t_f\\) correspond to the regime when \\(x_t\\) is loading and unloading, respectively. \\(\\mathcal{H}_t(\\omega)^{+}\\) corresponds to the part of the loop formed in the \\(x_t - y_t\\) plane, while \\(t_i \\leq t \\leq t_m\\) (when \\(x_t\\) is loading) whereas \\(\\mathcal{H}_t(\\omega)^{-}\\) is the part of the loop formed in the~\\(x_t - y_t\\) plane for~\\(t_m \\leq t \\leq t_f\\) (when \\(x_t\\) is unloading), as shown in the Figure 2:</p> <p></p> <p>Figure 2. Example of a hysteresis curve.</p> <p>Rate Independent Hysteresis (RIH) (Visintin, A., \"Differential Models of Hysteresis\") - The hysteresis behavior is called to be rate independent if the path \\(ABCD\\), which depends on pair \\(x(t), y(t)\\), is invariant with respect to any increasing diffeomorphism~\\(\\varphi : [0,T] \\rightarrow [0,T]\\), i.e.:</p> \\[ \\begin{align}         F(u \\ o \\ \\varphi, y^{0}) = F(u,y^0)\\ o \\ \\varphi &amp; \\ em \\ [0,T]. \\end{align} \\tag{4} \\] <p>This means that at any instant \\(t\\), \\(y(t)\\) depends only on \\(u:[0,T] \\rightarrow \\mathbb{R}\\) and on the order in which values have been attained before \\(t\\). In other words, the memory effect is not affected by the frequency of the input.</p>"},{"location":"book/8-Severely-Nonlinear-System-Identification/#rate-independent-hysteresis-in-polynomial-narx-model","title":"Rate Independent Hysteresis  in polynomial NARX model","text":"<p>Martins, S. A. M. and Aguirre, L. A. presented the sufficient conditions for NARX model to represent hysteresis. One of the developed concepts in the Bounding Structure \\(\\mathcal{H}\\).</p> <p>Bounding Structure \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A.) - Let \\(\\mathcal{H}_t(\\omega)\\) be the system hysteresis. \\(\\mathcal{H}= \\lim_{\\omega \\to 0} \\mathcal{H}_t(\\omega)\\) is defined as the bounding structure that delimits \\(\\mathcal{H}_t(\\omega)\\).</p> <p>Now, consider a polynomial NARX excited by a loading-unloading quasi-static signal. If the model has one real and stable equilibrium point whose location depends on input and loading/unloading regime, the polynomial will exhibit a Rate Independent Hysteresis loop \\(\\mathcal{H}_t(\\omega)\\) in the \\(x-y\\) plane.</p> <p>Here is an example. Let \\(y_k  =  0.8y_{k-1} + 0.4\\phi_{k-1} + 0.2x_{k-1}\\), where \\(\\phi_{k} = \\rm{sign}(\\Delta(x_{k}))\\) and \\(x_{k} = sin(\\omega k)\\) and \\(\\omega\\) is the frequency of the input signal \\(x\\). The equilibria of this model is given by:</p> \\[ \\begin{equation}     \\overline{y}(\\overline{\\phi},\\overline{x})=     \\begin{cases}         \\frac{0.6+0.2\\overline{x}}{1-0.8} \\ = 3 \\ + \\ \\overline{x} \\ , &amp; for \\ loading; \\\\         \\frac{-0.6+0.2\\overline{x}}{1-0.8} \\ = -3 \\ + \\ \\overline{x} \\ , &amp; for \\ unloading; \\\\     \\end{cases} \\end{equation} \\tag{5} \\] <p>where \\(\\overline {x}\\) is a loading-unloading quasi-static input signal. Since the equilibrium points are asymptotically stable, the output converges to \\(\\mathcal{H}_k (w)\\) in the \\(x-y\\) plane. Note that for a constant input value \\(x ~ = ~ 1 ~ = ~ \\overline{x}\\), the equilibrium lies in \\(\\overline{y} ~ = ~ 3\\) for loading regime and \\(\\overline {y} ~ = ~ -1\\) for unloading regime. Analogously, for \\(\\overline {x} ~ = ~ -1\\), the equilibrium lies in \\(\\overline {y} ~ = ~ 1\\) for loading regime and \\(\\overline {y} ~ = ~ -3\\) for unloading regime, as shown in the figure below:</p> <p></p> <p>Figure 3. Example of a bounding structure \\(\\mathcal{H}\\). The black dots are on \\(\\mathcal{H}_{k}(\\omega)\\) for model \\(y_k  =  0.8y_{k-1} + 0.4\\phi_{k-1} + 0.2x_{k-1}\\). The bounding structure \\(\\mathcal{H}\\), in red, confines \\(\\mathcal{H}_{k}(\\omega)\\).}</p> <p>As can be observed in the Figure 3, in we guarantee the sufficient conditions proposed by Martins, S. A. M. and Aguirre, L. A., a NARX model can reproduce a hysteretic behavior. Chapter 10 presents a case study of a system with hysteresis.</p> <p>The following code can be used to reproduce the behavior shown in Figure 3. Change <code>w</code> from \\(1\\) to \\(0.1\\) to see how the bounded structure \\(\\mathcal{H}\\) converge to the equilibria of the system.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nw = 1\nt = np.arange(0, 60.1, 0.1)\ny = np.zeros(len(t))\nx = np.sin(w * t)\n\n# Initialize y and fi\nfi = np.zeros(len(t))\n# Iterate over the time array to calculate y\nfor k in range(1, len(t)):\n\u00a0 \u00a0 fi[k] = np.sign(x[k] - x[k-1])\n\u00a0 \u00a0 y[k] = 0.8 * y[k-1] + 0.2 * x[k-1] + 0.4 * fi[k-1]\n\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Example')\nplt.show()\n</code></pre> <p></p> <p>Figure 4.  Reproduction of a bounding structure \\(\\mathcal{H}\\) using python.</p>"},{"location":"book/9-Validation/","title":"9. Validation","text":""},{"location":"book/9-Validation/#the-predict-method-in-sysidentpy","title":"The <code>predict</code> Method in SysIdentPy","text":"<p>Before getting into the validation process in System Identification, it's essential to understand how the <code>predict</code> method works in SysIdentPy.</p>"},{"location":"book/9-Validation/#using-the-predict-method","title":"Using the <code>predict</code> Method","text":"<p>A typical usage of the <code>predict</code> method in SysIdentPy looks like this:</p> <pre><code>yhat = model.predict(X=x_test, y=y_test)\n</code></pre> <p>SysIdentPy users often have two common questions about this method:</p> <ol> <li>Why do we need to pass the test data, <code>y_test</code>, as an argument in the <code>predict</code> method?</li> <li>Why are the initial predicted values identical to the values in the test data?</li> </ol> <p>To address these questions, let\u2019s first explain the concepts of infinity-step-ahead prediction, n-step-ahead prediction, and one-step-ahead prediction in dynamic systems.</p>"},{"location":"book/9-Validation/#infinity-step-ahead-prediction","title":"Infinity-Step-Ahead Prediction","text":"<p>Infinity-step-ahead prediction, also known as free run simulation, refers to making predictions using previously predicted values, \\(\\hat{y}_{k-n_y}\\), in the prediction loop.</p> <p>For example, consider the following test input and output data:</p> \\[ x_{test} = [1, 2, 3, 4, 5, 6, 7] \\] \\[ y_{test} = [8, 9, 10, 11, 12, 13, 14] \\] <p>Suppose we want to validate a model \\(m\\) defined by:</p> \\[ m \\rightarrow y_k = 1*y_{k-1} + 2*x_{k-1} \\] <p>To predict the first value, we need access to both \\(y_{k-1}\\) and \\(x_{k-1}\\). This requirement explains why you need to pass <code>y_test</code> as an argument in the <code>predict</code> method. It also answers the second question: SysIdentPy requires the user to provide the initial conditions explicitly. The <code>y_test</code> data passed in the <code>predict</code> method is not used entirely; only the initial values needed for the model\u2019s lag structure are used.</p> <p>In this example, the model's maximum lag is 1, so we need only 1 initial condition. The predicted values, <code>yhat</code>, are then calculated as follows:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*10 + 2*2 = 14\nyhat(3) = 1*14 + 2*3 = 20\nyhat(4) = 1*20 + 2*4 = 28\n</code></pre> <p>As shown, the first value of <code>yhat</code> matches the first value of <code>y_test</code> because it serves as the initial condition. Another key point is that the prediction loop uses the previously predicted values, not the actual <code>y_test</code> values, which is why it's called infinity-step-ahead or free run simulation.</p> <p>In system identification, we often aim for models that perform well in infinity-step-ahead predictions. Since the prediction error propagates over time, a model that shows good performance in free run simulation is considered a robust model.</p> <p>In SysIdentPy, users only need to pass the initial conditions when performing an infinity-step-ahead prediction. If you pass only the initial conditions, the results will be the same! Therefore</p> <pre><code>yhat = model.predict(X=x_test, y=y_test)\n</code></pre> <p>is actually the same as</p> <pre><code>yhat = model.predict(X=x_test, y=y_test[:model.max_lag].reshape(-1, 1))\n</code></pre> <p><code>model.max_lag</code> can be accessed after we fit the model using the code below.</p> <pre><code>model = FROLS(\n    order_selection=False,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 e_tol=0.9999\n\u00a0 \u00a0 n_terms=15\n)\nmodel.fit(X=x, y=y)\nmodel.max_lag\n</code></pre> <p>Its important to mention that, in current version of SysIdentPy, the maximum lag considered is actually the maximum lag between <code>xlag</code> and <code>ylag</code> definition. This is important because you can pass <code>ylag = xlag = 10</code> and the final model, after the model structure selection, select terms where the maximum lag is 3. You have to pass 10 initial conditions, but internally the calculations are done using the correct regressors. This is necessary due the way the regressors are created after that the model is fitted. Therefore, is recommended to use the <code>model.max_lag</code> to be sure.</p>"},{"location":"book/9-Validation/#1-step-ahead-prediction","title":"1-step ahead prediction","text":"<p>The difference between 1 step-ahead prediction and infinity-steps ahead prediction is that the model take the previous real <code>y_test</code> test values in the loop instead of the predicted <code>yhat</code> values. And that is a huge and important difference. Let's do prediction using 1-step ahead method:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*9 + 2*2 = 13\nyhat(3) = 1*10 + 2*3 = 16\nyhat(4) = 1*11 + 2*4 = 19\nand so on\n</code></pre> <p>The model uses real values in the loop and only predict the next value. The prediction error, in this case, is always corrected because we are not propagating the error using the predicted values in the loop.</p> <p>SysIdentPy's <code>predict</code> method allow the user to perform a 1-step ahead prediction by setting <code>steps_ahead=1</code></p> <pre><code>yhat = model.predict(X=x_test, y=y_test, steps_ahead=1)\n</code></pre> <p>In this case, as you can imagine, we need to pass the all the <code>y_test</code> data because the method have to access the real values at each iteration. If you pass only the initial conditions, <code>yhat</code> will have only the initial conditions plus 1 more sample, that is the 1-step ahead prediction. To predict another point, you would need to pass the new initial conditions again and so on. SysIdentPy already to everything for you, so just pass all the data you want to validate using the 1-step ahead method.</p>"},{"location":"book/9-Validation/#n-steps-ahead-prediction","title":"n-steps ahead prediction","text":"<p>The n-steps ahead prediction is almost the same as the 1-step ahead, but here you can define the number of steps ahead you want to test your model. If you set <code>steps_ahead=5</code>, for example, it means that the first 5 values will be predicted using <code>yhat</code> in the loop, but then the process is restarted by feeding the real values in <code>y_test</code> in the next iteration, then performing other 5 predictions using the <code>yhat</code> and so on. Let's check the example considering <code>steps_ahead=2</code>:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*10 + 2*2 = 14\nyhat(3) = 1*10 + 2*3 = 16\nyhat(4) = 1*16 + 2*4 = 24\nand so on\n</code></pre>"},{"location":"book/9-Validation/#model-performance","title":"Model Performance","text":"<p>Model validation is one of the most crucial part in system identification. As we mentioned before, in system identification we are trying the model the dynamic of the process without for task like control design. In such cases, we can not only rely on regression metrics, but also ensuring that residuals are unpredictable across various combinations of past inputs and outputs (Billings, S. A. and Voon, W. S. F., \"Structure detection and model validity tests in the identification of nonlinear systems\"). One often used statistical tests is the normalized RMSE, called RRSE, which can be expressed by</p> \\[ \\begin{equation}         \\textrm{RRSE}= \\frac{\\sqrt{\\sum\\limits_{k=1}^{n}(y_k-\\hat{y}_k)^2}}{\\sqrt{\\sum\\limits_{k=1}^{n}(y_k-\\bar{y})^2}}, \\end{equation} \\tag{1} \\] <p>where \\(\\hat{y}_k \\in \\mathbb{R}\\) the model predicted output and \\(\\bar{y} \\in \\mathbb{R}\\) the mean of the measured output \\(y_k\\). The RRSE gives some indication regarding the quality of the model, but concluding about the best model by evaluating only this quantity may lead to an incorrect interpretation, as shown in following example.</p> <p>Consider the models</p> \\[ y_{{_a}k} = 0.7077y_{{_a}k-1} + 0.1642u_{k-1} + 0.1280u_{k-2} \\] <p>and</p> \\[y_{{_b}k}=0.7103y_{{_b}k-1} + 0.1458u_{k-1} + 0.1631u_{k-2} -1467y^3_{{_b}k-1} + 0.0710y^3_{{_b}k-2} +0.0554y^2_{{_b}k-3}u_{k-3}\\] <p>defined in the Meta Model Structure Selection: An Algorithm For Building Polynomial NARX Models For Regression And Classification. The former results in a \\(RRSE = 0.1202\\) while the latter gives \\(RRSE~=0.0857\\). Although the model \\(y_{{_b}k}\\) fits the data better, it is only a biased representation to one piece of data and not a good description of the entire system.</p> <p>The RRSE (or any other metric) shows that validations test might be performed carefully. Another traditional practice is split the data set in two parts. In this respect, one can test the models obtained from the estimation part of the data using a specific data for validation. However, the one-step-ahead performance of NARX models generally results in misleading interpretations because even strongly biased models may fit the data well. Therefore, a free run simulation approach usually allows a better interpretation if the model is adequate or not (Billings, S. A.).</p> <p>Statistical tests for SISO models based on the correlation functions were proposed in (Billings, S. A. and Voon, W. S. F., \"A prediction-error and stepwise-regression estimation algorithm for non-linear systems\"), (Model validity tests for non-linear signal processing applications). The tests are:</p> \\[ \\begin{align}     \\phi_{_{\\xi \\xi}\\tau} &amp;= E\\{\\xi_k \\xi_{k-\\tau}\\} = \\delta_{\\tau}, \\\\     \\phi_{_{\\xi x}\\tau} &amp;= E\\{\\xi_k x_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{\\xi \\xi x}\\tau} &amp;= E\\{\\xi_k \\xi_{k-\\tau} x_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{x^2 \\xi}\\tau} &amp;= E\\{(u^2_k - E\\{x^2_k\\})\\xi_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{x^2 \\xi^2}\\tau} &amp;= E\\{(u^2_k - E\\{x^2_k\\})\\xi^2_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{(y\\xi) x^2}\\tau} &amp;= E\\{(y_k\\xi_k - E\\{y_k\\xi_k\\})(x^2_{k-\\tau} - E\\{x^2_k\\})\\} = 0 \\forall \\tau, \\end{align} \\tag{2} \\] <p>where \\(\\delta\\) is the Dirac delta function and the cross-correlation function \\(\\phi\\) is denoted by (Billings, S. A. and Voon, W. S. F.):</p> \\[ \\begin{equation} \\phi_{{_{ab}}\\tau} = \\frac{\\frac{1}{n}\\sum\\limits_{k=1}^{n-\\tau}(a_k - \\hat{a})(b_{k+\\tau}-\\hat{b})}{\\sqrt{\\frac{1}{n}\\sum\\limits_{k=1}^{n}(a_k-\\hat{a})^2} \\sqrt{\\frac{1}{n}\\sum\\limits_{k=1}^{n}(b_k-\\hat{b})^2}} = \\frac{\\sum\\limits_{k=1}^{n-\\tau}(a_k - \\hat{a})(b_{k+\\tau}-\\hat{b})}{\\sqrt{\\sum\\limits_{k=1}^{n}(a_k-\\hat{a})^2} \\sqrt{\\sum\\limits_{k=1}^{n}(b_k-\\hat{b})^2}}, \\end{equation} \\tag{3} \\] <p>where \\(a\\) and \\(b\\) are two signal sequences. If the tests are true, then the model residues can be considered as white noise.</p>"},{"location":"book/9-Validation/#metrics-available-in-sysidentpy","title":"Metrics Available in SysIdentPy","text":"<p>SysIdentPy provides the following regression metrics out of the box:</p> <ul> <li>forecast_error</li> <li>mean_forecast_error</li> <li>mean_squared_error</li> <li>root_mean_squared_error</li> <li>normalized_root_mean_squared_error</li> <li>root_relative_squared_error</li> <li>mean_absolute_error</li> <li>mean_squared_log_error</li> <li>median_absolute_error</li> <li>explained_variance_score</li> <li>r2_score</li> <li>symmetric_mean_absolute_percentage_error</li> </ul> <p>To use them, the user only need to import the desired metric using, for example</p> <pre><code>from sysidentpy.metrics import root_relative_squared_error\n</code></pre> <p>SysIdentPy also provides methods for calculate and analyse the residues correlation</p> <pre><code>from sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n\u00a0 \u00a0 compute_residues_autocorrelation,\n\u00a0 \u00a0 compute_cross_correlation,\n)\n</code></pre> <p>Lets check the metrics of the eletro mechanical system modeled in Chapter 4.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n\u00a0 \u00a0 compute_residues_autocorrelation,\n\u00a0 \u00a0 compute_cross_correlation,\n)\nfrom sysidentpy.metrics import root_relative_squared_error\n\ndf1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p> <p>The RRSE is 0.0800, which is a very good metric. However, we can see that the residues have somo high auto-correlations anda with the input. This mean that our model maybe is not good enough as it could be.</p> <p>Lets check what happens if we increase <code>xlag</code>, <code>ylag</code> and change the parameter estimation algorithm from Least Squares to the Recursive Least Squares</p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=50,\n\u00a0 \u00a0 ylag=5,\n\u00a0 \u00a0 xlag=5,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=RecursiveLeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>Now the RRSE is 0.0568 and we have a better residual correlation!</p> <p></p> <p></p> <p></p> <p>In the end of day, the best model will be the model that satisfy the user needs. However, it's important to understand how to analyse the models so you can have an idea if you can get some improvements without too much work.</p> <p>For the sake of curiosity, lets check how the model perform if we run a 1-step ahead prediction. We don't need to fit the model again, just make another prediction using the 1-step option.</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid, steps_ahead=1)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>The same model, but evaluating the 1-step ahead prediction, now return a RRSE\\(= 0.02044\\) and the residues are even better. But remember, that is expected, as explained in the previous section.</p> <p></p> <p></p> <p></p>"},{"location":"changelog/changelog/","title":"Changes in SysIdentPy","text":""},{"location":"changelog/changelog/#v060","title":"v0.6.0","text":""},{"location":"changelog/changelog/#contributors","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>oliveira-mark</li> </ul>"},{"location":"changelog/changelog/#changes","title":"CHANGES","text":"<p>This release introduces significant improvements focused on improving code organization, readability, and compliance with PEP8 standards. It also includes a new foundational class for Error Reduction Ratio (ERR) based algorithms, expanded testing, and the discontinuation of Python 3.7 support.</p>"},{"location":"changelog/changelog/#new-features","title":"New Features","text":"<ul> <li>Introduced the <code>OFRBase</code> class, encapsulating common methods essential for ERR-based algorithms.</li> <li>Refactored the <code>FROLS</code> class to inherit from <code>OFRBase</code>, focusing it solely on the ERR algorithm.</li> <li>Implemented the Ultra Orthogonal Forward Regression (UOFR) algorithm, also inheriting from <code>OFRBase</code>.</li> </ul>"},{"location":"changelog/changelog/#api-changes","title":"API Changes","text":"<ul> <li>BREAKING CHANGE: Fixed a typo in Bernstein Basis Function. Previously it was defined as Bersntein.</li> <li>Refactoring and Modularization:<ul> <li>Extracted the <code>InformationMatrix</code> class from <code>narmax_base</code> into a new module: <code>utils.information_matrix</code>.</li> <li>Moved specific methods to newly created modules: <code>utils.lags</code> and <code>utils.simulation</code>, promoting better separation of concerns.</li> <li>Add deprecation message for RidgeRegression <code>solver</code> argument.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#code-quality-improvements","title":"Code Quality Improvements","text":"<ul> <li>Renamed variables and methods for better readability and PEP8 compliance, including changing uppercase variable names to lowercase.</li> <li>Updated imports to use new utility modules, reducing redundancy and improving maintainability.</li> <li>Removed unused imports and redundant parentheses, streamlining the codebase.</li> <li>Change python version in deploy action.</li> </ul>"},{"location":"changelog/changelog/#testing-enhancements","title":"Testing Enhancements","text":"<ul> <li>Added comprehensive tests for basis functions: Bernstein, Bilinear, Hermite (normalized), Laguerre, and Legendre.</li> <li>Introduced tests for utility methods, including <code>narmax_tools</code>, <code>save_load</code>, and the new simulation utilities.</li> <li>Increased test coverage to 92%, ensuring robustness and reliability.</li> </ul>"},{"location":"changelog/changelog/#validation-and-error-handling","title":"Validation and Error Handling","text":"<ul> <li>Implemented a validation check for <code>train_percentage</code>, raising an error for values exceeding 100%.</li> <li>Adapted methods and tests following the removal of the <code>InformationMatrix</code> class to ensure consistency across the codebase.</li> </ul>"},{"location":"changelog/changelog/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>Launched a redesigned frontend page featuring a modern UI and improved responsiveness.</li> <li>Restructured multiple sections for better organization and clarity.</li> <li>Overhauled key guides, including <code>quick_start</code>, <code>developer_guide</code>, and <code>user_guide</code>.</li> <li>Added new examples, including the Lorenz System and Chaotic Map.</li> <li>Enhanced grammar and readability across documentation.</li> <li>Updated dependencies related to <code>mkdocs</code> for better performance and compatibility.</li> <li>Improved Google Analytics integration.</li> <li>Fixed broken links for the Nubank and Estatidados blogs, and refined link formatting in the book.</li> <li>Updated class docstrings to align with the latest changes.</li> <li>Standardized docstrings and method signatures to use lowercase variable names, following PEP8 guidelines.</li> <li>Revised contribution examples to reflect the latest <code>sysidentpy</code> version.</li> <li>Integrated book examples into traditional documentation, with direct links to the book section.</li> <li>Adjusted structure, titles, and links across various docs and examples for better navigation.</li> <li>Removed dataset files; datasets are now hosted in a dedicated repository, <code>sysidentpy-data</code>.</li> <li>Acknowledged JetBrains support and collaboration in the README and sponsor page.</li> <li>Fix edit uri when clicking to edit doc page in doc website.</li> <li>Now every example loads the data from <code>sysidentpy-data</code> repository.</li> </ul>"},{"location":"changelog/changelog/#python-version-support-update","title":"Python Version Support Update","text":"<ul> <li>Support for Python 3.7 has been discontinued. This aligns with the official end of support for Python 3.7 and resolves compatibility issues with newer dependencies.</li> <li>Certain parameter estimation algorithms, such as Bounded Variable Least Squares, require newer versions of SciPy that no longer support Python 3.7.</li> <li>Users can still run SysIdentPy on Python 3.7, but some features, including some parameter estimation functionalities, will be unavailable.</li> </ul>"},{"location":"changelog/changelog/#impact","title":"IMPACT","text":"<ul> <li>These changes improve the modularity, readability, and maintainability of the codebase. The introduction of the <code>OFRBase</code> class simplifies the implementation of ERR-based algorithms, facilitating future extensions. Comprehensive testing ensures the reliability of both new and existing functionalities.</li> </ul>"},{"location":"changelog/changelog/#testing","title":"TESTING","text":"<ul> <li>All new and existing tests were executed, achieving 92% test coverage.</li> </ul>"},{"location":"changelog/changelog/#v053","title":"v0.5.3","text":""},{"location":"changelog/changelog/#contributors_1","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_1","title":"CHANGES","text":"<p>IMPORTANT! This update addresses a bug related to the Bilinear basis function for models with more the 2 inputs. This release keep providing crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#api-changes_1","title":"API Changes","text":"<ul> <li>Fix Bilinear basis function issue for models with more than 2 inputs. This fix the <code>get_max_xlag</code> method in <code>basis_function_base</code> and also fix how <code>combination_xlag</code> is created.</li> </ul>"},{"location":"changelog/changelog/#v052","title":"v0.5.2","text":""},{"location":"changelog/changelog/#contributors_2","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_2","title":"CHANGES","text":"<p>IMPORTANT! This update addresses a critical bug related to the Polynomial and Bilinear basis function for models with more the 3 inputs. The issue raised due to the changes in basis function for v0.5.0, but has now been resolved. This release keep providing crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#api-changes_2","title":"API Changes","text":"<ul> <li>Fix Polynomial and Bilinear basis function issue for models with more than 3 inputs.</li> </ul>"},{"location":"changelog/changelog/#v051","title":"v0.5.1","text":""},{"location":"changelog/changelog/#contributors_3","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_3","title":"CHANGES","text":"<p>This update addresses a critical bug related to the unbiased estimator. The issue previously impacted all basis functions but has now been resolved. This release keep providing crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#documentation","title":"Documentation","text":"<ul> <li>Remove unnecessary code when importing basis functions in many examples.</li> </ul>"},{"location":"changelog/changelog/#api-changes_3","title":"API Changes","text":"<ul> <li>Fix unbiased estimator for every basis function.</li> </ul>"},{"location":"changelog/changelog/#v050","title":"v0.5.0","text":""},{"location":"changelog/changelog/#contributors_4","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>nataliakeles</li> <li>LeWerner42</li> <li>Suyash Gaikwad</li> </ul>"},{"location":"changelog/changelog/#changes_4","title":"CHANGES","text":"<p>This update introduces major new features and important bug fixes. This release provides crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#new-features_1","title":"New Features","text":"<ul> <li>MAJOR: Add Bilinear Basis Function (thanks nataliakeles). Now the user can use Bilinear NARX models for forecasting.</li> <li>MAJOR: Add Legendre polynomial basis function. Now the user can use Legendre NARX models for forecasting.</li> <li>MAJOR: Add Hermite polynomial basis function. Now the user can use Hermite NARX models for forecasting. MAJOR: Add Hermite Normalized polynomial basis function. Now the user can use Hermite Normalized NARX models for forecasting. MAJOR: Add Laguerre polynomial basis function. Now the user can use Laguerre NARX models for forecasting.</li> </ul>"},{"location":"changelog/changelog/#documentation_1","title":"Documentation","text":"<ul> <li>Add basis function overview.</li> <li>Files related to v.3.* doc removed.</li> <li>Improved formatting in mathematical equations.</li> <li>Fixed typos and grammatical errors in README.md (thanks Suyash Gaikwad and LeWerner42)</li> <li>Minor additions and grammar fixes.</li> <li>Remove book assets from main repository. The assets were moved to sysidentpy-data repository to keep main repository cleaner and lighter.</li> <li>Fixed link in the book cover to ensure it correctly redirects to the book details. Also change x2_val to x_valid in examples of how to use in readme.</li> <li>Add Pix method as an alternative for brazilian sponsors.</li> <li>Fix code documentation for basis function (it was not showing up in the docs before).</li> <li>Remove <code>pip install</code> from the list of the dependencies needed in the chapter.</li> </ul>"},{"location":"changelog/changelog/#datasets","title":"Datasets","text":"<ul> <li>Datasets are now available in a separate repository.</li> </ul>"},{"location":"changelog/changelog/#api-changes_4","title":"API Changes","text":"<ul> <li>add deprecated messages for bias and n in Bersntein basis function. Both parameters will be removed in v0.6.0. Use <code>include_bias</code> and <code>degree</code>, respectively, instead.</li> <li>Deploy-docs.yml: Change option to make a clean build of the documentation.</li> <li>Deploy-docs.yml: Change python version to deploy docs.</li> <li>Support for Python 3.13 depending on the release of the Pytorch 2.6. Every method in sysidentpy works in python 3.13 excluding neural narx.</li> <li>Update mkdocstrings dependency version</li> <li>Change Polynomial check from class name to isinstance method in every class.</li> <li>Remove support for torch==2.4.0 due to pip error in pytorch side. I'll check if it was solved before allow newer versions of pytorch.</li> <li>Make \"main\" the new default branch. Master branch removed.</li> <li>Change actions from master to main branch.</li> <li>Split basis function classes into multiples files (one for each basis).</li> <li>Fix redundant bias check on bersntein basis.</li> <li>Fix docstring math notation in basis functions docstring.</li> <li>Remove requirements.txt file.</li> <li>Extensive code refactoring, including type hint improvements, docstring enhancements, removal of unused code, and other behind-the-scenes changes to support new features.</li> <li>Add model_type in basis function base fit and predict method.</li> <li>Change variable name from <code>combinations</code> to <code>combination_list</code> to avoid any issue with itertools <code>combination</code> method in case I want to use it in the future.</li> <li>Remove requirements.txt file.</li> </ul>"},{"location":"changelog/changelog/#v040","title":"v0.4.0","text":""},{"location":"changelog/changelog/#contributors_5","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_5","title":"CHANGES","text":"<p>This update introduces several major features and changes, including some breaking changes. There is a guide to help you update your code to the new version. Depending on your model definition, you might not need to change anything. I decided to go directly to version v0.4.0 instead of providing incremental updates (0.3.5, 0.3.6, etc.) because the breaking changes are easy to fix and the new features are highly beneficial. This release provides crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#new-features_2","title":"New Features","text":"<ul> <li>MAJOR: NonNegative Least Squares algorithm for parameter estimation.</li> <li>MAJOR: Bounded Variables Least Squares algorithm for parameter estimation.</li> <li>MAJOR: Least Squares Minimal Residual algorithm for parameter estimation.</li> <li>MAJOR: Error Reduction Ratio algorithm enhancement for FROLS model structure selection. Users can now set an <code>err_tol</code> value to stop the algorithm when the sum of the ERR values reaches this threshold, offering a faster alternative to Information Criteria algorithms. A new example is available in the documentation.</li> <li>MAJOR: New Bernstein basis function available, allowing users to choose between Polynomial, Fourier, and Bernstein.</li> <li>MAJOR: v0.1 of the companion book \"Nonlinear System Identification: Theory and Practice With SysIdentPy.\" This open-source book serves as robust documentation for the SysIdentPy package and a friendly introduction to Nonlinear System Identification and Timeseries Forecasting. There are case studies in the book that were not included in the documentation at the time of the update release. The book will always feature more in-depth studies and will be updated regularly with additional case studies.</li> </ul>"},{"location":"changelog/changelog/#documentation_2","title":"Documentation","text":"<ul> <li>All examples updated to reflect changes in v0.4.0.</li> <li>Added guide on defining a custom parameter estimation method and integrating it with SysIdentPy.</li> <li>Documentation moved to the <code>gh-pages</code> branch.</li> <li>Defined a GitHub Action to automatically build the docs when changes are pushed to the main branch.</li> <li>Removal of unused code in general</li> </ul>"},{"location":"changelog/changelog/#datasets_1","title":"Datasets","text":"<ul> <li>Datasets are now available in a separate repository.</li> </ul>"},{"location":"changelog/changelog/#api-changes_5","title":"API Changes","text":"<ul> <li>BREAKING CHANGE: Parameter estimation method must now be imported and passed to the model definition, replacing the previous string method. For example, use <code>from sysidentpy.parameter_estimation import LeastSquares</code> instead of <code>\"least_squares\"</code>. This change enhances code flexibility, organization, readability, and facilitates easier integration of custom methods. A specific doc page is available to guide migration from v0.3.4 to v0.4.0.</li> <li>BREAKING CHANGE: The <code>fit</code> method in MetaMSS now requires only <code>X</code> and <code>y</code> values, omitting the need to pass <code>fit(X=, y=, X_test=, y_test=)</code>.</li> <li>Added support for Python 3.12.</li> <li>Introduced <code>test_size</code> hyperparameter to set the proportion of training data used in the fitting process.</li> <li>Extensive code refactoring, including type hint improvements, docstring enhancements, removal of unused code, and other behind-the-scenes changes to support new features.</li> </ul>"},{"location":"changelog/changelog/#v034","title":"v0.3.4","text":""},{"location":"changelog/changelog/#contributors_6","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>dj-gauthier</li> <li>mtsousa</li> </ul>"},{"location":"changelog/changelog/#changes_6","title":"CHANGES","text":""},{"location":"changelog/changelog/#new-features_3","title":"New Features","text":"<ul> <li>MAJOR: Ridge Regression Parameter Estimation:</li> <li>Introducing Ridge algorithm for model parameter estimation (Issue #104). Set <code>estimator=\"ridge_regression\"</code> and control regularization with the <code>alpha</code> parameter. Special thanks to @dj-gauthier and @mtsousa for their contribution. Users are encouraged to visit https://www.researchgate.net/publication/380429918_Controlling_chaos_using_edge_computing_hardware to explore how @dj-gauthier used SysIdentPy in his research.</li> </ul>"},{"location":"changelog/changelog/#api-changes_6","title":"API Changes","text":"<ul> <li>Improved <code>plotting.py</code> code with type hints and new options for plotting results.</li> <li>Refactored methods to resolve future warnings from numpy.</li> <li>Code refactoring following PEP8 guidelines.</li> <li>Set \"default\" as the default style for plotting to avoid errors in new versions of matplotlib.</li> </ul>"},{"location":"changelog/changelog/#datasets_2","title":"Datasets","text":"<ul> <li>Added <code>buck_id.csv</code> and <code>buck_valid.csv</code> datasets to the SysIdentPy repository.</li> </ul>"},{"location":"changelog/changelog/#documentation_3","title":"Documentation","text":"<ul> <li>Add NFIR example (Issue #103). The notebook show how to build models without past output regressors (using only input regressors).</li> <li>Enhanced usage example for MetaMSS.</li> <li>Continued adding type hints to methods.</li> <li>Improved docstrings throughout the codebase.</li> <li>Minor additions and grammar fixes in documentation.</li> <li>@dj-gauthier provided valuable suggestions for enhancing the documentation, which are currently undergoing refinement and will soon be accessible.</li> </ul>"},{"location":"changelog/changelog/#development-tools","title":"Development Tools","text":"<ul> <li>Added pre-commit hooks to the repository.</li> <li>Enhanced <code>pyproject.toml</code> to assist contributors in setting up their own environment.</li> </ul>"},{"location":"changelog/changelog/#v033","title":"v0.3.3","text":""},{"location":"changelog/changelog/#contributors_7","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>GabrielBuenoLeandro</li> <li>samirmartins</li> </ul>"},{"location":"changelog/changelog/#changes_7","title":"CHANGES","text":"<ul> <li>The update v0.3.3  has been released with additional features, API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_7","title":"API Changes","text":"<ul> <li>MAJOR: Multiobjective Framework: Affine Information Least Squares Algorithm (AILS)<ul> <li>Now you can use AILS to estimate parameters of NARMAX models (and variants) using a multiobjective approach.</li> <li>AILS can be accessed using <code>from sysidentpy.multiobjective_parameter_estimation import AILS</code></li> <li>See the docs for a more in depth explanation of how to use AILS.</li> <li>This feature is related to Issue #101. This work is the result of an undergraduate research conducted by Gabriel Bueno Leandro under the supervision of Samir Milani Martins and Wilson Rocha Lacerda Junior.</li> <li>Several new methods were implemented to get the new feature and you can check all of it in sysidentpy -&gt; multiobjective_parameter_estimation.</li> </ul> </li> <li>API Change: <code>regressor_code</code> variable was renamed as <code>enconding</code> to avoid using the same name as the method in <code>narmax_tool</code> <code>regressor_code</code> method.</li> </ul>"},{"location":"changelog/changelog/#datasets_3","title":"Datasets","text":"<ul> <li>DATASET: Added buck_id.csv and buck_valid.csv dataset to SysIdentPy repository.</li> </ul>"},{"location":"changelog/changelog/#documentation_4","title":"Documentation","text":"<ul> <li>DOC: Add a Multiobjetive Parameter Optimization Notebook showing how to use the new AILS method</li> <li>DOC: Minor additions and grammar fixes.</li> </ul>"},{"location":"changelog/changelog/#v032","title":"v0.3.2","text":""},{"location":"changelog/changelog/#contributors_8","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_8","title":"CHANGES","text":"<ul> <li>The update v0.3.2  has been released with API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_8","title":"API Changes","text":"<ul> <li> <p>Major:</p> <ul> <li>Added Akaike Information Criteria corrected in FROLS. Now the user can use aicc as the information criteria to select the model order when using FROLS algorithm.</li> </ul> </li> <li> <p>FIX: Issue #114. Replace yhat with y in root relative squared error. Thanks @miroder</p> </li> <li>TESTS: Minor changes in tests by removing unnecessary data load.</li> <li>Remove unused code and comments.</li> </ul>"},{"location":"changelog/changelog/#documentation_5","title":"Documentation","text":"<ul> <li>Docs: Minor changes in notebooks. Added AICc method in the information criteria example.</li> </ul>"},{"location":"changelog/changelog/#v031","title":"v0.3.1","text":""},{"location":"changelog/changelog/#contributors_9","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_9","title":"CHANGES","text":"<ul> <li>The update v0.3.1  has been released with API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_9","title":"API Changes","text":"<ul> <li>MetaMSS was returning the max lag of the final model instead of the maximum lag related to the xlag and ylag. This is not wrong (it is related to the issue #55), but this change will be made for all methods at the same time. In this respect, I'm reverted this to return the maximum lag of the xlag and ylag.</li> <li>API Change: Added build_matrix method in BaseMSS. This change improved overall code readability by rewriting if/elif/else clauses in every model structure selection algorithm.</li> <li>API Change: Added bic, aic, fpe, and lilc methods in FROLS. Now the method is selected by using a predefined dictionary with the available options. This change improved overall code readability by rewriting if/elif/else clauses in the FROLS algorithm.</li> <li>TESTS: Added tests for Neural NARX class. The issue with pytorch was fixed and now we have the tests for every model class.</li> <li>Remove unused code and comments.</li> </ul>"},{"location":"changelog/changelog/#v030","title":"v0.3.0","text":""},{"location":"changelog/changelog/#contributors_10","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>gamcorn</li> <li>Gabo-Tor</li> </ul>"},{"location":"changelog/changelog/#changes_10","title":"CHANGES","text":"<ul> <li>The update v0.3.0  has been released with additional features, API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_10","title":"API Changes","text":"<ul> <li> <p>MAJOR: Estimators support in AOLS</p> <ul> <li>Now you can use any SysIdentPy estimator in AOLS model structure selection.</li> </ul> </li> <li> <p>Refactored base class for model structure selection. A refactored base class for model structure selection has been introduced in SysIdentPy. This update aims to enhance the system identification process by preparing the package for new features that are currently in development, like multiobjective parameter estimation, new basis functions and more.</p> </li> </ul> <p>Several methods within the base class have undergone significant restructuring to improve their functionality and optimize their performance. This reorganization will facilitate the incorporation of advanced model selection techniques in the future, which will enable users to obtain dynamic models with robust dynamic and static performance.   - Avoid unnecessary inheritance in every MSS method and improve the readability with better structured classes.   - Rewritten methods to avoid code duplication.   - Improve overall code readability by rewriting if/elif/else clauses.</p> <ul> <li> <p>Breaking Change: <code>X_train</code> and <code>y_train</code> were replaced respectively by <code>X</code> and <code>y</code> in <code>fit</code> method in MetaMSS model structure selection algorithm.  <code>X_test</code> and <code>y_test</code> were replaced by <code>X</code> and <code>y</code> in <code>predict</code> method in MetaMSS.</p> </li> <li> <p>API Change: Added BaseBasisFunction class, an abstract base class for implementing basis functions.</p> </li> <li>Enhancement: Added support for python 3.11.</li> <li>Future Deprecation Warning: The user will have to define the estimator and pass it to every model structure selection algorithm instead of using a string to define the Estimator. Currently the estimator is defined like \"estimator='least_squares'\". In version 0.4.0 the definition will be like \"estimator=LeastSquares()\"</li> <li>FIX: Issue #96. Fix issue with numpy 1.24.* version. Thanks for the contribution @gamcorn.</li> <li>FIX: Issue #91. Fix r2_score metric issue with 2 dimensional arrays.</li> <li>FIX: Issue #90.</li> <li>FIX: Issue #88 .Fix one step ahead prediction error in SimulateNARMAX class (thanks for pointing out, Lalith).</li> <li>FIX: Fix error in selecting the correct regressors in AOLS.</li> <li>Fix: Fix n step ahead prediction method not returning all values of the defined steps-ahead value when passing only the initial condition.</li> <li>FIX: Fix Visible Deprecation Warning raised in get_max_lag method.</li> <li>FIX: Fix deprecation warning in Extended Least Squares Example</li> </ul>"},{"location":"changelog/changelog/#datasets_4","title":"Datasets","text":"<ul> <li>DATASET: Added air passengers dataset to SysIdentPy repository.</li> <li>DATASET: Added San Francisco Hospital Load dataset to SysIdentPy repository.</li> <li>DATASET: Added San Francisco PV GHI dataset to SysIdentPy repository.</li> </ul>"},{"location":"changelog/changelog/#documentation_6","title":"Documentation","text":"<ul> <li>DOC: Improved documentation in Setting Specif Lags page. Now we bring an example of how to set specific lags for MISO models.</li> <li>DOC: Minor additions and grammar fixes.</li> <li>DOC: Improve image visualization using mkdocs-glightbox.</li> <li>Update dev packages versions</li> </ul>"},{"location":"changelog/changelog/#v021","title":"v0.2.1","text":""},{"location":"changelog/changelog/#contributors_11","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_11","title":"CHANGES","text":"<ul> <li>The update v0.2.1  has been released with additional feature, minor API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_11","title":"API Changes","text":"<ul> <li>MAJOR: Neural NARX now support CUDA<ul> <li>Now the user can build Neural NARX models with CUDA support. Just add <code>device='cuda'</code> to use the GPU benefits.</li> <li>Updated docs to show how to use the new feature.</li> </ul> </li> <li>Tests:<ul> <li>Now there are test for almost every function.</li> <li>Neural NARX tests are raising numpy issues. It'll be fixed til next update.</li> </ul> </li> <li>FIX: NFIR models in General Estimators<ul> <li>Fix support for NFIR models using sklearn estimators.</li> </ul> </li> <li>The setup is now handled by the pyproject.toml file.</li> <li>Remove unused code.</li> </ul>"},{"location":"changelog/changelog/#documentation_7","title":"Documentation","text":"<ul> <li> <p>MAJOR: New documentation website</p> <ul> <li>The documentation is now entirely based on Markdown (no rst anymore).</li> <li>We use MkDocs and Material for MkDocs theme now.</li> <li>Dark theme option.</li> <li>The Contribute page have more details to help those who wants to contribute with SysIdentPy.</li> <li>New sections (e.g., Blog, Sponsors, etc.)</li> <li>Many improvements under the hood.</li> </ul> </li> <li> <p>MAJOR: Github Sponsor</p> <ul> <li>Now you can support SysIdentPy by becoming a Sponsor! Details: https://github.com/sponsors/wilsonrljr</li> </ul> </li> <li> <p>Fix docstring variables.</p> </li> <li>Fix code format issues.</li> <li>Fix minor grammatical and spelling mistakes.</li> <li>Fix issues related to html on Jupyter notebooks examples on documentation.</li> <li>Updated Readme.</li> </ul>"},{"location":"changelog/changelog/#v020","title":"v0.2.0","text":""},{"location":"changelog/changelog/#contributors_12","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_12","title":"CHANGES","text":"<ul> <li>The update v0.2.0  has been released with additional feature, minor API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_12","title":"API Changes","text":"<ul> <li> <p>MAJOR: Many new features for General Estimators</p> <ul> <li>Now the user can build General NARX models with Fourier basis function.</li> <li>The user can choose which basis they want by importing it from sysidentpy.basis_function. Check the notebooks with examples of how to use it.</li> <li>Now it is possible to build General NAR models. The user just need to pass model_type=\"NAR\" to build NAR models.</li> <li>Now it is possible to build General NFIR models. The user just need to pass model_type=\"NFIR\" to build NAR models.</li> <li>Now it is possible to run n-steps ahead prediction using General Estimators. Until now only infinity-steps ahead were allowed. Now the users can set any steps they want.</li> <li>Polynomial and Fourier are supported for now. New basis functions will be added in next releases.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>MAJOR: Many new features for NARX Neural Network</p> <ul> <li>Now the user can build Neural NARX models with Fourier basis function.</li> <li>The user can choose which basis they want by importing it from sysidentpy.basis_function. Check the notebooks with examples of how to use it.</li> <li>Now it is possible to build Neural NAR models. The user just need to pass model_type=\"NAR\" to build NAR models.</li> <li>Now it is possible to build Neural NFIR models. The user just need to pass model_type=\"NFIR\" to build NAR models.</li> <li>Now it is possible to run n-steps ahead prediction using Neural NARX. Until now only infinity-steps ahead were allowed. Now the users can set any steps they want.</li> <li>Polynomial and Fourier are supported for now. New basis functions will be added in next releases.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>Major: Support for old methods removed.</p> <ul> <li>Now the old sysidentpy.PolynomialNarmax is not available anymore. All the old features are included in the new API with a lot of new features and performance improvements.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.general_estimators.ModelPrediction</p> <ul> <li>ModelPrediction class was adapted to support General Estimators as a stand-alone class.</li> <li>predict: base method for prediction. Support infinity_steps ahead, one-step ahead and n-steps ahead prediction and any basis function.</li> <li>_one_step_ahead_prediction: Perform the 1-step-ahead prediction for any basis function.</li> <li>_n_step_ahead_prediction: Perform the n-step-ahead prediction for polynomial basis.</li> <li>_model_prediction: Perform the infinity-step-ahead prediction for polynomial basis.</li> <li>_narmax_predict: wrapper for NARMAX and NAR models.</li> <li>_nfir_predict: wrapper for NFIR models.</li> <li>_basis_function_predict: Perform the infinity-step-ahead prediction for basis functions other than polynomial.</li> <li>basis_function_n_step_prediction: Perform the n-step-ahead prediction for basis functions other than polynomial.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.neural_network.ModelPrediction</p> <ul> <li>ModelPrediction class was adapted to support Neural NARX as a stand-alone class.</li> <li>predict: base method for prediction. Support infinity_steps ahead, one-step ahead and n-steps ahead prediction and any basis function.</li> <li>_one_step_ahead_prediction: Perform the 1-step-ahead prediction for any basis function.</li> <li>_n_step_ahead_prediction: Perform the n-step-ahead prediction for polynomial basis.</li> <li>_model_prediction: Perform the infinity-step-ahead prediction for polynomial basis.</li> <li>_narmax_predict: wrapper for NARMAX and NAR models.</li> <li>_nfir_predict: wrapper for NFIR models.</li> <li>_basis_function_predict: Perform the infinity-step-ahead prediction for basis functions other than polynomial.</li> <li>basis_function_n_step_prediction: Perform the n-step-ahead prediction for basis functions other than polynomial.</li> </ul> </li> <li> <p>API Change: Fit method for Neural NARX revamped.</p> <ul> <li>No need to convert the data to tensor before calling Fit method anymore.</li> </ul> </li> </ul> <p>API Change: Keyword and positional arguments     - Now users have to provide parameters with their names, as keyword arguments, instead of positional arguments. This is valid for every model class now.</p> <ul> <li>API Change (new): sysidentpy.utils.narmax_tools<ul> <li>New functions to help user getting useful information to build model. Now we have the regressor_code helper function to help to build neural NARX models.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#documentation_8","title":"Documentation","text":"<ul> <li>DOC: Improved Basic Steps notebook with new details about the prediction function.</li> <li>DOC: NARX Neural Network notebook was updated following the new api and showing new features.</li> <li>DOC: General Estimators notebook was updated following the new api and showing new features.</li> <li>DOC: Fixed minor grammatical and spelling mistakes, including Issues #77 and #78.</li> <li>DOC: Fix issues related to html on Jupyter notebooks examples on documentation.</li> </ul>"},{"location":"changelog/changelog/#v019","title":"v0.1.9","text":""},{"location":"changelog/changelog/#contributors_13","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>samirmartins</li> </ul>"},{"location":"changelog/changelog/#changes_13","title":"CHANGES","text":"<ul> <li>The update v0.1.9  has been released with additional feature, minor API changes and fixes of the new features added in v0.1.7.</li> </ul>"},{"location":"changelog/changelog/#api-changes_13","title":"API Changes","text":"<ul> <li>MAJOR: Entropic Regression Algorithm<ul> <li>Added the new class ER to build NARX models using the Entropic Regression algorithm.</li> <li>Only the Mutual Information KNN is implemented in this version and it may take too long to run on a high number of regressor, so the user should be careful regarding the number of candidates to put in the model.</li> </ul> </li> <li>API: save_load<ul> <li>Added a function to save and load models from file.</li> </ul> </li> <li>API: Added tests for python 3.9</li> <li>Fix : Change condition for n_info_values in FROLS. Now the value defined by the user is compared against X matrix shape instead of regressor space shape. This fix the Fourier basis function usage with more the 15 regressors in FROLS.</li> </ul>"},{"location":"changelog/changelog/#documentation_9","title":"Documentation","text":"<ul> <li>DOC: Save and Load models<ul> <li>Added a notebook showing how to use the save_load method.</li> </ul> </li> <li>DOC: Entropic Regression example<ul> <li>Added notebook with a simple example of how to use AOLS</li> </ul> </li> <li>DOC: Fourier Basis Function Example<ul> <li>Added notebook with a simple example of how to use Fourier Basis Function</li> </ul> </li> <li>DOC: PV forecasting benchmark<ul> <li>FIX AOLS prediction. The example was using the meta_mss model in prediction, so the results for AOLS were wrong.</li> </ul> </li> <li>DOC: Fixed minor grammatical and spelling mistakes.</li> <li>DOC: Fix issues related to html on Jupyter notebooks examples on documentation.</li> </ul>"},{"location":"changelog/changelog/#v018","title":"v0.1.8","text":""},{"location":"changelog/changelog/#contributors_14","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_14","title":"CHANGES","text":"<ul> <li>The update v0.1.8  has been released with additional feature, minor API changes and fixes of the new features added in v0.1.7.</li> </ul>"},{"location":"changelog/changelog/#api-changes_14","title":"API Changes","text":"<ul> <li>MAJOR: Ensemble Basis Functions<ul> <li>Now you can use different basis function together. For now we allow to use Fourier combined with Polynomial of different degrees.</li> </ul> </li> <li>API change: Add \"ensemble\" parameter in basis function to combine the features of different basis function.</li> <li>Fix: N-steps ahead prediction for model_type=\"NAR\" is working properly now with different forecast horizon.</li> </ul>"},{"location":"changelog/changelog/#documentation_10","title":"Documentation","text":"<ul> <li>DOC: Air passenger benchmark<ul> <li>Remove unused code.</li> <li>Use default hyperparameter in SysIdentPy models.</li> </ul> </li> <li>DOC: Load forecasting benchmark<ul> <li>Remove unused code.</li> <li>Use default hyperparameter in SysIdentPy models.</li> </ul> </li> <li>DOC: PV forecasting benchmark<ul> <li>Remove unused code.</li> <li>Use default hyperparameter in SysIdentPy models.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#v017","title":"v0.1.7","text":""},{"location":"changelog/changelog/#contributors_15","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_15","title":"CHANGES","text":"<ul> <li>The update v0.1.7  has been released with major changes and additional features. There are several API modifications, and you will need to change your code to have the new (and upcoming) features. All modifications are meant to make future expansion easier.</li> <li>On the user's side, the changes are not that disruptive, but in the background there are many changes that allowed the inclusion of new features and bug fixes that would be complex to solve without the changes. Check the <code>documentation page &lt;http://sysidentpy.org/notebooks.html&gt;</code>__</li> <li>Many classes were basically rebuild it from scratch, so I suggest to look at the new examples of how to use the new version.</li> <li>I will present the main updates below in order to highlight features and usability and then all API changes will be reported.</li> </ul>"},{"location":"changelog/changelog/#api-changes_15","title":"API Changes","text":"<ul> <li>MAJOR: NARX models with Fourier basis function <code>Issue63 &lt;https://github.com/wilsonrljr/sysidentpy/issues/63&gt;</code>, <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code><ul> <li>The user can choose which basis they want by importing it from sysidentpy.basis_function. Check the notebooks with examples of how to use it.</li> <li>Polynomial and Fourier are supported for now. New basis functions will be added in next releases.</li> </ul> </li> <li>MAJOR: NAR models <code>Issue58 &lt;https://github.com/wilsonrljr/sysidentpy/issues/58&gt;</code>__<ul> <li>It was already possible to build Polynomial NAR models, but with some hacks. Now the user just need to pass model_type=\"NAR\" to build NAR models.</li> <li>The user doesn't need to pass a vector of zeros as input anymore.</li> <li>Works for any model structure selection algorithm (FROLS, AOLS, MetaMSS)</li> </ul> </li> <li>Major: NFIR models <code>Issue59 &lt;https://github.com/wilsonrljr/sysidentpy/issues/59&gt;</code>__<ul> <li>NFIR models are models where the output depends only on past inputs. It was already possible to build Polynomial NFIR models, but with a lot of code on the user's side (much more than NAR, btw). Now the user just need to pass model_type=\"NFIR\" to build NFIR models.</li> <li>Works for any model structure selection algorithm (FROLS, AOLS, MetaMSS)</li> </ul> </li> <li>Major: Select the order for the residues lags to use in Extended Least Squares - elag<ul> <li>The user can select the maximum lag of the residues to be used in the Extended Least Squares algorithm. In previous versions sysidentpy used a predefined subset of residual lags.</li> <li>The degree of the lags follows the degree of the basis function</li> </ul> </li> <li>Major: Residual analysis methods <code>Issue60 &lt;https://github.com/wilsonrljr/sysidentpy/issues/60&gt;</code>__<ul> <li>There are now specific functions to calculate the autocorrelation of the residuals and cross-correlation for the analysis of the residuals. In previous versions the calculation was limited to just two inputs, for example, limiting user usability.</li> </ul> </li> <li>Major: Plotting methods <code>Issue61 &lt;https://github.com/wilsonrljr/sysidentpy/issues/61&gt;</code>__<ul> <li>The plotting functions are now separated from the models objects, so there are more flexibility regarding what to plot.</li> <li>Residual plots were separated from the forecast plot</li> </ul> </li> <li>API Change: sysidentpy.polynomial_basis.PolynomialNarmax is deprecated. Use sysidentpy.model_structure_selection.FROLS instead. <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/62&gt;</code>__<ul> <li>Now the user doesn't need to pass the number of inputs as a parameter.</li> <li>Added the elag parameter for unbiased_estimator. Now the user can define the number of lags of the residues for parameter estimation using the Extended Least Squares algorithm.</li> <li>model_type parameter: now the user can select the model type to be built. The options are \"NARMAX\", \"NAR\" and \"NFIR\". \"NARMAX\" is the default. If you want to build a NAR model without any \"hack\", just set model_type=\"NAR\". The same for \"NFIR\" models.</li> </ul> </li> <li>API Change: sysidentpy.polynomial_basis.MetaMSS is deprecated. Use sysidentpy.model_structure_selection.MetaMSS instead. <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__<ul> <li>Now the user doesn't need to pass the number of inputs as a parameter.</li> <li>Added the elag parameter for unbiased_estimator. Now the user can define the number of lags of the residues for parameter estimation using the Extended Least Squares algorithm.</li> </ul> </li> <li>API Change: sysidentpy.polynomial_basis.AOLS is deprecated. Use sysidentpy.model_structure_selection.AOLS instead. <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__</li> <li>API Change: sysidentpy.polynomial_basis.SimulatePolynomialNarmax is deprecated. Use sysidentpy.simulation.SimulateNARMAX instead.</li> <li>API Change: Introducing sysidentpy.basis_function. Because NARMAX models can be built on different basis function, a new module is added to make easier to implement new basis functions in future updates <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__.<ul> <li>Each basis function class must have a fit and predict method to be used in training and prediction respectively.</li> </ul> </li> <li>API Change: unbiased_estimator method moved to Estimators class.<ul> <li>added elag option</li> <li>change the build_information_matrix method to build_output_matrix</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base<ul> <li>This is the new base for building NARMAX models. The classes have been rewritten to make it easier to expand functionality.</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base.GenerateRegressors<ul> <li>create_narmax_code: Creates the base coding that allows representation for the NARMAX, NAR, and NFIR models.</li> <li>regressor_space: Creates the encoding representation for the NARMAX, NAR, and NFIR models.</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base.ModelInformation<ul> <li>_get_index_from_regressor_code: Get the index of the model code representation in regressor space.</li> <li>_list_output_regressor_code: Create a flattened array of output regressors.</li> <li>_list_input_regressor_code: Create a flattened array of input regressors.</li> <li>_get_lag_from_regressor_code: Get the maximum lag from array of regressors.</li> <li>_get_max_lag_from_model_code: the name says it all.</li> <li>_get_max_lag: Get the maximum lag from ylag and xlag.</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base.InformationMatrix<ul> <li>_create_lagged_X: Create a lagged matrix of inputs without combinations.</li> <li>_create_lagged_y: Create a lagged matrix of the output without combinations.</li> <li>build_output_matrix: Build the information matrix of output values.</li> <li>build_input_matrix: Build the information matrix of input values.</li> <li>build_input_output_matrix: Build the information matrix of input and output values.</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base.ModelPrediction<ul> <li>predict: base method for prediction. Support infinity_steps ahead, one-step ahead and n-steps ahead prediction and any basis function.</li> <li>_one_step_ahead_prediction: Perform the 1-step-ahead prediction for any basis function.</li> <li>_n_step_ahead_prediction: Perform the n-step-ahead prediction for polynomial basis.</li> <li>_model_prediction: Perform the infinity-step-ahead prediction for polynomial basis.</li> <li>_narmax_predict: wrapper for NARMAX and NAR models.</li> <li>_nfir_predict: wrapper for NFIR models.</li> <li>_basis_function_predict: Perform the infinity-step-ahead prediction for basis functions other than polynomial.</li> <li>basis_function_n_step_prediction: Perform the n-step-ahead prediction for basis functions other than polynomial.</li> </ul> </li> <li>API Change (new): sysidentpy.model_structure_selection.FROLS <code>Issue62 &lt;https://github.com/wilsonrljr/sysidentpy/issues/62&gt;</code>, <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code><ul> <li>Based on the old sysidentpy.polynomial_basis.PolynomialNARMAX. The class has been rebuilt with new functions and optimized code.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>Add support for new basis functions.</li> <li>The user can choose the residual lags.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>many under the hood changes.</li> </ul> </li> <li>API Change (new): sysidentpy.model_structure_selection.MetaMSS <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__<ul> <li>Based on the old sysidentpy.polynomial_basis.MetaMSS. The class has been rebuilt with new functions and optimized code.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>The user can choose the residual lags.</li> <li>Extended Least Squares support.</li> <li>Add support for new basis functions.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>many under the hood changes.</li> </ul> </li> <li>API Change (new): sysidentpy.model_structure_selection.AOLS <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__<ul> <li>Based on the old sysidentpy.polynomial_basis.AOLS. The class has been rebuilt with new functions and optimized code.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>Add support for new basis functions.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Change \"l\" parameter to \"L\".</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>many under the hood changes.</li> </ul> </li> <li>API Change (new): sysidentpy.simulation.SimulateNARMAX<ul> <li>Based on the old sysidentpy.polynomial_basis.SimulatePolynomialNarmax. The class has been rebuilt with new functions and optimized code.</li> <li>Fix the Extended Least Squares support.</li> <li>Fix n-steps ahead prediction and 1-step ahead prediction.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>The user can choose the residual lags.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>Do not inherit from the structure selection algorithm anymore, only from narmax_base. Avoid circular import and other issues.</li> <li>many under the hood changes.</li> </ul> </li> <li>API Change (new): sysidentpy.residues<ul> <li>compute_residues_autocorrelation: the name says it all.</li> <li>calculate_residues: get the residues from y and yhat.</li> <li>get_unnormalized_e_acf: compute the unnormalized autocorrelation of the residues.</li> <li>compute_cross_correlation: compute cross correlation between two signals.</li> <li>_input_ccf</li> <li>_normalized_correlation: compute the normalized correlation between two signals.</li> </ul> </li> <li>API Change (new): sysidentpy.utils.plotting<ul> <li>plot_results: plot the forecast</li> <li>plot_residues_correlation: the name says it all.</li> </ul> </li> <li>API Change (new): sysidentpy.utils.display_results<ul> <li>results: return the model regressors, estimated parameter and ERR index of the fitted model in a table.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#documentation_11","title":"Documentation","text":"<ul> <li>DOC: Air passenger benchmark <code>Issue65 &lt;https://github.com/wilsonrljr/sysidentpy/issues/65&gt;</code>__<ul> <li>Added notebook with Air passenger forecasting benchmark.</li> <li>We compare SysIdentPy against prophet, neuralprophet, autoarima, tbats and many more.</li> </ul> </li> <li>DOC: Load forecasting benchmark <code>Issue65 &lt;https://github.com/wilsonrljr/sysidentpy/issues/65&gt;</code>__<ul> <li>Added notebook with load forecasting benchmark.</li> </ul> </li> <li>DOC: PV forecasting benchmark <code>Issue65 &lt;https://github.com/wilsonrljr/sysidentpy/issues/65&gt;</code>__<ul> <li>Added notebook with PV forecasting benchmark.</li> </ul> </li> <li>DOC: Presenting main functionality<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Multiple Inputs usage<ul> <li>Example rewritten following the new api</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Information Criteria - Examples<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Important notes and examples of how to use Extended Least Squares<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Setting specific lags<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Parameter Estimation<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Using the Meta-Model Structure Selection (MetaMSS) algorithm for building Polynomial NARX models<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Using the Accelerated Orthogonal Least-Squares algorithm for building Polynomial NARX models<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Example: F-16 Ground Vibration Test benchmark<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Building NARX Neural Network using Sysidentpy<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Building NARX models using general estimators<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Simulate a Predefined Model<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: System Identification Using Adaptive Filters<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Identification of an electromechanical system<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Example: N-steps-ahead prediction - F-16 Ground Vibration Test benchmark<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Introduction to NARMAX models<ul> <li>Fixed grammatical and spelling mistakes.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#v016","title":"v0.1.6","text":""},{"location":"changelog/changelog/#contributors_16","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_16","title":"CHANGES","text":""},{"location":"changelog/changelog/#api-changes_16","title":"API Changes","text":"<ul> <li>MAJOR: Meta-Model Structure Selection Algorithm (Meta-MSS).<ul> <li>A new method for build NARMAX models based on metaheuristics. The algorithm uses a Binary hybrid Particle Swarm Optimization and Gravitational Search Algorithm with a new cost function to build parsimonious models.</li> <li>New class for the BPSOGSA algorithm. New algorithms can be adapted in the Meta-MSS framework.</li> <li>Future updates will add NARX models for classification and multiobjective model structure selection.</li> </ul> </li> <li>MAJOR: Accelerated Orthogonal Least-Squares algorithm.<ul> <li>Added the new class AOLS to build NARX models using the Accelerated Orthogonal Least-Squares algorithm.</li> <li>At the best of my knowledge, this is the first time this algorithm is used in the NARMAX framework. The tests I've made are promising, but use it with caution until the results are formalized into a research paper.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#documentation_12","title":"Documentation","text":"<ul> <li>Added notebook with a simple example of how to use MetaMSS and a simple model comparison of the Electromechanical system.</li> <li>Added notebook with a simple example of how to use AOLS</li> <li>Added ModelInformation class. This class have methods to return model information such as max_lag of a model code.<ul> <li>added _list_output_regressor_code</li> <li>added _list_input_regressor_code</li> <li>added _get_lag_from_regressor_code</li> <li>added _get_max_lag_from_model_code</li> </ul> </li> <li>Minor performance improvement: added the argument \"predefined_regressors\" in build_information_matrix function on base.py to improve the performance of the Simulation method.</li> <li>Pytorch is now an optional dependency. Use pip install sysidentpy['full']</li> <li>Fix code format issues.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>Fix issues related to html on Jupyter notebooks examples on documentation.</li> <li>Updated Readme with examples of how to use.</li> <li>Improved descriptions and comments in methods.</li> <li>metaheuristics.bpsogsa (detailed description on code docstring)<ul> <li>added evaluate_objective_function</li> <li>added optimize</li> <li>added generate_random_population</li> <li>added mass_calculation</li> <li>added calculate_gravitational_constant</li> <li>added calculate_acceleration</li> <li>added update_velocity_position</li> </ul> </li> <li>FIX issue #52</li> </ul>"},{"location":"changelog/changelog/#v015","title":"v0.1.5","text":""},{"location":"changelog/changelog/#contributors_17","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_17","title":"CHANGES","text":""},{"location":"changelog/changelog/#api-changes_17","title":"API Changes","text":"<ul> <li>MAJOR: n-steps-ahead prediction.<ul> <li>Now you can define the numbers of steps ahead in the predict function.</li> <li>Only for Polynomial models for now. Next update will bring this functionality to Neural NARX and General Estimators.</li> </ul> </li> <li>MAJOR: Simulating predefined models.<ul> <li>Added the new class SimulatePolynomialNarmax to handle the simulation of known model structures.</li> <li>Now you can simulate predefined models by just passing the model structure codification. Check the notebook examples.</li> </ul> </li> <li>Fix code format issues.</li> <li>Added new tests for SimulatePolynomialNarmax and generate_data.</li> <li>Started changes related to numpy 1.19.4 update. There are still some Deprecation warnings that will be fixed in next update.</li> </ul>"},{"location":"changelog/changelog/#documentation_13","title":"Documentation","text":"<ul> <li>Added 4 new notebooks in the example section.</li> <li>Added iterative notebooks. Now you can run the notebooks in Jupyter notebook section of the documentation in Colab.</li> <li>Fix issues related to html on Jupyter notebooks examples on documentation.</li> <li>Updated Readme with examples of how to use.</li> </ul>"},{"location":"changelog/changelog/#v014","title":"v0.1.4","text":""},{"location":"changelog/changelog/#contributors_18","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_18","title":"CHANGES","text":""},{"location":"changelog/changelog/#api-changes_18","title":"API Changes","text":"<ul> <li>MAJOR: Introducing NARX Neural Network in SysIdentPy.<ul> <li>Now you can build NARX Neural Network on SysIdentPy.</li> <li>This feature is built on top of Pytorch. See the docs for more details and examples of how to use.</li> </ul> </li> <li>MAJOR: Introducing general estimators in SysIdentPy.<ul> <li>Now you are able to use any estimator that have Fit/Predict methods (estimators from Sklearn and Catboost, for example) and build NARX models based on those estimators.</li> <li>We use the core functions of SysIdentPy and keep the Fit/Predict approach from those estimators to keep the process easy to use.</li> <li>More estimators are coming soon like XGboost.</li> </ul> </li> <li>Changed the default parameters of the plot_results function.</li> </ul>"},{"location":"changelog/changelog/#documentation_14","title":"Documentation","text":"<ul> <li>Added notebooks to show how to build NARX neural Network.</li> <li>Added notebooks to show how to build NARX models using general estimators.</li> <li>New template for the documentation site.</li> <li>Fix issues related to html on Jupyter notebooks examples on documentation.</li> <li> <p>Updated Readme with examples of how to use.</p> </li> <li> <p>NOTE: We will keep improving the Polynomial NARX models (new model structure selection algorithms and multiobjective identification is on our roadmap). These recent modifications will allow us to introduce new NARX models like PWARX models very soon.</p> </li> </ul>"},{"location":"changelog/changelog/#v013","title":"v0.1.3","text":""},{"location":"changelog/changelog/#contributors_19","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>renard162</li> </ul>"},{"location":"changelog/changelog/#changes_19","title":"CHANGES","text":""},{"location":"changelog/changelog/#api-changes_19","title":"API Changes","text":"<ul> <li>Fixed a bug concerning the xlag and ylag in multiple input scenarios.</li> <li>Refactored predict function. Improved performance up to 87% depending on the number of regressors.</li> <li>You can set lags with different size for each input.</li> <li>Added a new function to get the max value of xlag and ylag. Work with int, list, nested lists.</li> <li>Fixed tests for information criteria.</li> <li>Refactored code of all classes following PEP 8 guidelines to improve readability.</li> <li>Changes on information Criteria tests.</li> <li>Added workflow to run the tests when merge branch into master.</li> </ul>"},{"location":"changelog/changelog/#documentation_15","title":"Documentation","text":"<ul> <li>Added SysIdentPy logo.</li> <li>Added Citation information on Readme.</li> <li>Added new site domain.</li> <li>Updated docs.</li> </ul>"},{"location":"community-support/community-overview/","title":"Contribute","text":"\ud83d\udcac Get Help <p>Need assistance? Explore various options for getting help, including discussion forums, GitHub issues, and community support channels.</p> \ud83e\udd1d Meetups <p>Connect with other SysIdentPy users through meetups, online events, and community discussions. Stay updated on upcoming gatherings and share your experience.</p>"},{"location":"community-support/get-help/","title":"Get Help","text":"<p>Before asking others for help, it\u2019s generally a good idea for you to try to help yourself. SysIdentPy includes several examples in the documentation with tips and notes about the package that might help you. However, if you have any issues and you can't find the answer, reach out using any method described below.</p>"},{"location":"community-support/get-help/#connect-with-the-author","title":"Connect with the author","text":"<p>You can:</p> <ul> <li>Follow me on GitHub.</li> <li>Follow me on <li>Connect with me on Linkedin.<ul> <li>I'll start to use Twitter more often \ud83e\udd37\u200d\u2642 (probably).</li> </ul> </li> <li>Read what I write (or follow me) on Medium.</li>"},{"location":"community-support/get-help/#create-issues","title":"Create issues","text":"<p>You can create a new issue in the GitHub repository, for example to:</p> <ul> <li>Ask a question or ask about a problem.</li> <li>Suggest a new feature.</li> </ul>"},{"location":"community-support/get-help/#join-the-chat","title":"Join the chat","text":"<p>Join the \ud83d\udc65 Discord chat server \ud83d\udc65 and hang out with others in the SysIdentPy community.</p> <p>You can use the chat for anything</p> <p>Have in mind that you can use the chat to talk about anything related to SysIdentPy. Conversations about system identification, dynamical systems, new papers, issues, new features are allowed, but have in mind that if some of the questions could help other users, I'll kindly ask you to open an discussion or an issue on Github as well.</p> <p>I can make sure I always answer everything, even if it takes some time.</p>"},{"location":"community-support/meetups/ai-networks-meetup/","title":"AI Networks Meetup","text":""},{"location":"community-support/meetups/ai-networks-meetup/#about-ai-networks","title":"About AI Networks","text":"<p>Founded on August 5, 2019, AI Networks has quickly established itself as one of the most engaged and focused communities in the realm of artificial intelligence. Our community thrives on a shared passion for knowledge and a commitment to exploring the latest advancements in AI and machine learning.</p> <p>As we celebrate our 5<sup>th</sup> anniversary in August 2024, we reflect on five years of enriching knowledge exchange and vibrant discussions.</p> <p>Our community currently hosts four specialized groups:</p> <ul> <li>AI/ML Brasil - Principal 01 &amp; 02: Our main groups dedicated to a wide range of AI and machine learning topics.</li> <li>AI/ML Brasil - Prompt Engineering: Focused on the art and science of designing effective prompts for AI systems.</li> <li>AI/ML Brasil - Neuroci\u00eancia: Exploring the intersection of artificial intelligence and neuroscience.</li> <li>AI/ML Brasil - Divulga\u00e7\u00e3o Servi\u00e7os IA: Dedicated to the dissemination and promotion of AI services.</li> </ul> <p>Join us to connect with like-minded individuals, engage in thought-provoking discussions, and stay at the forefront of AI innovation.</p> <p>In Portuguese</p> <p>Talk: Modelando Sistemas Din\u00e2micos | AI Networks Meetup</p> <p>Just click in the link above to watch the video.</p>"},{"location":"community-support/meetups/estatidados/","title":"Estatidados Meetup","text":"<p>Estatidados is a big statistic and data science community in Brazil. They host an online meetup that brings together leading researchers and developers from the Statistics and Data science community to join a multiple set of talks covering current trends in the Data Science field.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, gave a meetup talk about SysIdentPy and the video is available bellow:</p> <p>In Portuguese</p> <p>Talk: SysIdentPy: Uma Biblioteca Para Cria\u00e7\u00e3o de Modelos N\u00e3o Lineares para S\u00e9ries Temporais</p> <p>Just click in the link above to watch the video.</p>"},{"location":"community-support/meetups/gcom-meetup/","title":"GCoM Meetup","text":"<p>GCoM (Control and Modelling Group) is a research group of the Department of Electrical Engineering Federal University of S\u00e3o Jo\u00e3o del-Rei. GCoM works on two main areas: Analysis and Modelling Systems and Control Systems. They are devoted to undertake knowledge from Electrical Engineering, Mathematics, Computer Science and Physics to build and analyze models that mimics and control real dynamical systems. Some of applications are on robust control of uncertainty systems, Assistive Technology, Hysteresis system identification, Chaotic Dynamics. Recently, a great effort has been undertaken to better understand the play that numerical computation plays at modelling and control of nonlinear dynamical systems.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, gave a meetup talk about SysIdentPy and the video is available bellow:</p> <p>In Portuguese</p> <p>Talk: Aplica\u00e7\u00f5es em Data Science e Identifica\u00e7\u00e3o de Sistemas com o SysIdentPy</p> <p>Just click in the link above to watch the video.</p>"},{"location":"community-support/meetups/nubank-meetup-open-source/","title":"Nubank Meetup","text":"<p>Nubank is a leading financial technology company in Latin America with more than 54 million clients in Brazil, Mexico and Colombia. They host an online meetup that brings together leading researchers and developers from the Machine Learning (ML) community to join a multiple set of talks covering current trends in ML development.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, joined Bruno Rocha (software engineer at Red Hat) and Tatyana Zabanova (Data Scientist at Nubank )in a talk about the experience of making a open source package. The video is available bellow:</p> <p>In Portuguese</p> <p>Talk: Painel sobre desenvolvimento de bibliotecas em Data Science | Nubank DS &amp; ML Meetup</p> <p>Just click in the link above to watch the video.</p>"},{"location":"community-support/meetups/nubank-meetup/","title":"Nubank Meetup","text":"<p>Nubank is a leading financial technology company in Latin America with more than 54 million clients in Brazil, Mexico and Colombia. They host an online meetup that brings together leading researchers and developers from the Machine Learning (ML) community to join a multiple set of talks covering current trends in ML development.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, gave a meetup talk about SysIdentPy and the video is available bellow:</p> <p>In Portuguese</p> <p>Talk: SysIdentPy: Uma Biblioteca Para Cria\u00e7\u00e3o de Modelos N\u00e3o Lineares para S\u00e9ries Temporais</p> <p>Just click in the link above to watch the video.</p>"},{"location":"developer-guide/contribute/","title":"Contributing","text":"<p>SysIdentPy is intended to be a community project, hence all contributions are welcome! There exist many possible use cases in System Identification field and we can not test all scenarios without your help! If you find any bugs or have suggestions, please report them on issue tracker on GitHub.</p> <p>We welcome new contributors of all experience levels. The SysIdentPy community goals are to be helpful, welcoming, and effective.</p>"},{"location":"developer-guide/contribute/#help-others-with-issues-in-github","title":"Help others with issues in GitHub","text":"<p>You can see existing issues and try and help others, most of the times they are questions that you might already know the answer for.</p>"},{"location":"developer-guide/contribute/#watch-the-github-repository","title":"Watch the GitHub repository","text":"<p>You can watch SysIdentPy in GitHub (clicking the \"watch\" button at the top right):</p> <p>If you select \"Watching\" instead of \"Releases only\" you will receive notifications when someone creates a new issue.</p> <p>Then you can try and help them solve those issues.</p>"},{"location":"developer-guide/contribute/#documentation","title":"Documentation","text":"<p>Documentation is as important as the library itself. English is not the primary language of the main authors, so if you find any typo or anything wrong do not hesitate to point out to us.</p>"},{"location":"developer-guide/contribute/#create-a-pull-request","title":"Create a Pull Request","text":"<p>You can contribute to the source code with Pull Requests, for example:</p> <ul> <li>To fix a typo you found on the documentation.</li> <li>To share an article, video, or podcast you created or found about SysIdentPy.</li> <li>To propose new documentation sections.</li> <li>To fix an existing issue/bug.</li> <li>To add a new feature.</li> </ul>"},{"location":"developer-guide/contribute/#development-environment","title":"Development environment","text":"<p>These are some basic steps to help us with code:</p> <ul> <li> Install and Setup Git on your computer.</li> <li> Fork SysIdentPy.</li> <li> Clone the fork on your local machine.</li> <li> Create a new branch.</li> <li> Make changes following the coding style of the project (or suggesting improvements).</li> <li> Run the tests.</li> <li> Write and/or adapt existing test if needed.</li> <li> Add documentation if needed.</li> <li> Commit.</li> <li> Push to your fork.</li> <li> Open a pull_request.</li> </ul>"},{"location":"developer-guide/contribute/#environment","title":"Environment","text":"<p>Clone the repository using</p> <pre><code>git clone https://github.com/wilsonrljr/sysidentpy.git\n</code></pre> <p>If you already cloned the repository and you know that you need to deep dive in the code, here are some guidelines to set up your environment.</p>"},{"location":"developer-guide/contribute/#virtual-environment-with-venv","title":"Virtual environment with <code>venv</code>","text":"<p>You can create a virtual environment in a directory using Python's <code>venv</code> module or Conda:</p> venvconda <pre><code>$ python -m venv env\n</code></pre> <pre><code>conda create -n env\n</code></pre> <p>That will create a directory <code>./env/</code> with the Python binaries and then you will be able to install packages for that isolated environment.</p>"},{"location":"developer-guide/contribute/#activate-the-environment","title":"Activate the environment","text":"<p>If you created the environment using Python's <code>venv</code> module, activate it with:</p> Linux, macOSWindows PowerShellWindows Bash <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\Activate.ps1\n</code></pre> <p>Or if you use Bash for Windows (e.g. Git Bash):</p> <pre><code>source ./env/Scripts/activate\n</code></pre> <p>If you created the environment using Conda, activate it with:</p> Conda Bash <p>Or if you use Bash for Windows (e.g. Git Bash):</p> <pre><code>conda activate env\n</code></pre> <p>To check it worked, use:</p> Linux, macOS, Windows BashWindows PowerShellWindows Bash <pre><code>$ which pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <pre><code>$ Get-Command pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <pre><code>$ where pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <p>If it shows the <code>pip</code> binary at <code>env/bin/pip</code> then it worked.</p> <p>Tip</p> <p>Every time you install a new package with <code>pip</code> under that environment, activate the environment again.</p> <p>Note</p> <p>We use the <code>pytest</code> package for testing. The test functions are located in tests subdirectories at each folder inside SysIdentPy, which check the validity of the algorithms.</p>"},{"location":"developer-guide/contribute/#dependencies","title":"Dependencies","text":"<p>Install SysIdentPy with the <code>dev</code> and <code>docs</code> option to get all the necessary dependencies to run the tests</p> Dev and Docs dependencies <pre><code>pip install \"sysidentpy[dev, docs]\"\n</code></pre>"},{"location":"developer-guide/contribute/#docs","title":"Docs","text":"<p>First, make sure you set up your environment as described above, that will install all the requirements.</p> <p>The documentation uses MkDocs and Material for MKDocs.</p> <p>All the documentation is in Markdown format in the directory <code>./docs/</code>.</p>"},{"location":"developer-guide/contribute/#check-the-changes","title":"Check the changes","text":"<p>During local development, you can serve the website locally and checks for any changes. This helps making sure that:</p> <ul> <li>All of your modifications were applied.</li> <li>The unmodified files are displaying as expected.</li> </ul> <pre><code>$ mkdocs serve\n\nINFO     -  [13:25:00] Browser connected: http://127.0.0.1:8000\n</code></pre> <p>It will serve the documentation on <code>http://127.0.0.1:8008</code>.</p> <p>That way, you can keep editing the source files and see the changes live.</p> <p>Warning</p> <p>If any modification break the build, you have to serve the website again. Always check your <code>console</code> to make sure you are serving the website.</p>"},{"location":"developer-guide/contribute/#run-tests-locally","title":"Run tests locally","text":"<p>Its always good to check if your implementations/modifications does not break any other part of the package. You can run the SysIdentPy tests locally using <code>pytest</code> in the respective folder to perform all the tests of the corresponding sub-packages.</p>"},{"location":"developer-guide/contribute/#example-of-how-to-run-the-tests","title":"Example of how to run the tests:","text":"<p>Open a terminal emulator of your choice and go to the main directory, e.g,</p> <pre><code>\\sysidentpy\\\n</code></pre> <p>Just type <code>pytest</code> in the terminal emulator</p> <pre><code>pytest\n</code></pre> <p>and you get a result like:</p> <pre><code>========== test session starts ==========\n\nplatform linux -- Python 3.7.6, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\n\nrootdir: ~/sysidentpy\n\nplugins: cov-2.8.1\n\ncollected 12 items\n\ntests/test_regression.py ............ [100%]\n\n========== 12 passed in 2.45s ==================\n</code></pre>"},{"location":"developer-guide/documentation-overview/","title":"SysIdentPy Documentation Restructuring Proposal","text":"<p>This document outlines a reorganization of SysIdentPy\u2019s documentation to improve discoverability, reduce friction for beginners, and align with modern documentation standards. The structure will follow four key categories: Tutorials, How-Tos, Explanations, and Reference Guide, with additional sections for contributors and real-world examples.</p> <p>Acknowledgments: This documentation restructuring draws inspiration from NumPy\u2019s NEP 44, adapting its principles of clarity and logical organization to SysIdentPy\u2019s domain-specific needs in system identification and time series forecasting, while emphasizing tutorials and reproducibility.</p>"},{"location":"developer-guide/documentation-overview/#motivation-and-scope","title":"Motivation and Scope","text":"<p>SysIdentPy\u2019s current documentation (like many scientific Python packages) mixes conceptual explanations, code examples, and API references, which can overwhelm new users. By adopting a user-centric structure inspired by Di\u00e1taxis, we aim to:</p> <ul> <li>Separate learning paths for beginners (Tutorials) and practitioners (How-Tos).</li> <li>Improve material for conceptual understanding (Explanations).</li> <li>Maintain a clean, searchable Reference Guide.</li> <li>Highlight SysIdentPy\u2019s features.</li> </ul> <p>A well-organized documentation structure can significantly improve the experience of our community by providing specific resources for different user groups:</p> <ul> <li> <p>For Beginners: A clear, guided pathway with tutorials and step-by-step instructions helps new users overcome the learning curve.</p> </li> <li> <p>For Researchers: Features such as custom basis functions and model configurations can be easily discovered and understood. With clearly defined sections, researchers can quickly locate the information they need to experiment with new methods.</p> </li> <li> <p>For Industry/Corporate Users: Benchmarking guides and model comparison examples are readily accessible, making it easier for industry and corporate professionals to evaluate and choose the right tools for their specific needs.</p> </li> </ul> <p>The goal is to structure the documentation to meet the specific needs of these diverse user groups, making the learning process faster and more efficient for everyone in the community.</p>"},{"location":"developer-guide/documentation-overview/#proposed-structure","title":"Proposed Structure","text":"<p>Here\u2019s an overview of the main sections in the documentation, outlining the purpose and proposed content for each:</p> <ul> <li>Getting Started</li> <li>User Guide</li> <li>Developer Guide</li> <li>Community &amp; Support</li> <li>About</li> </ul>"},{"location":"developer-guide/documentation-overview/#user-guide","title":"User Guide","text":"<p>The User Guide section is designed to provide a comprehensive understanding of SysIdentPy, covering essential concepts, practical examples, and advanced features. The proposed structure includes:</p>"},{"location":"developer-guide/documentation-overview/#tutorials","title":"Tutorials","text":"<p>Audience: New users with minimal system identification experience.</p> <p>Suggested Content:</p> <ul> <li> Absolute Beginner\u2019s Guide   Start from scratch with easy-to-follow guides designed for those new to SysIdentPy and NARMAX models.</li> <li> Domain-Specific Tutorials   Examples and use cases for fields like engineering, healthcare, finance and so on.</li> </ul> <p>Format: Jupyter Notebooks with narrative explanations and code.</p>"},{"location":"developer-guide/documentation-overview/#how-tos","title":"How-Tos","text":"<p>Audience: Practitioners solving specific problems.</p> <p>Suggested Content:</p> <ul> <li> Model Optimization</li> <li> Advanced Customizations</li> <li> Error Analysis</li> <li> Reproducibility</li> </ul> <p>Format: Short, task-focused markdown files with code snippets.</p>"},{"location":"developer-guide/documentation-overview/#explanations","title":"Explanations","text":"<p>Audience: Users seeking rigorous mathematical foundations.</p> <ul> <li> Companion Book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy. Offer theoretical context for SysIdentPy\u2019s algorithms through a companion book.</li> </ul>"},{"location":"developer-guide/documentation-overview/#reference-guide","title":"Reference Guide","text":"<p>Audience: Advanced users needing API details.</p> <ul> <li> API Reference   Access the complete SysIdentPy source code with well-documented modules and methods.</li> </ul> <p>Format: Auto-generated API docs with cross-linked \"See Also\" sections.</p>"},{"location":"developer-guide/documentation-overview/#developer-guide","title":"Developer Guide","text":"<p>The Developer Guide section aims to provide clear information on the internal structure of SysIdentPy, focusing on implementation details, code examples, and options for customization. The proposed structure includes:</p>"},{"location":"developer-guide/documentation-overview/#how-to-contribute","title":"How To Contribute","text":"<p>Audience: Maintainers and open-source contributors.</p> <ul> <li> Contributor Guide</li> </ul>"},{"location":"developer-guide/documentation-overview/#documentation-guide","title":"Documentation Guide","text":"<p>Audience: Maintainers and open-source contributors.</p> <ul> <li> Writing a tutorial</li> <li> Creating a how-to guide</li> <li> Creating content for the book</li> </ul>"},{"location":"developer-guide/documentation-overview/#community-support","title":"Community &amp; Support","text":"<p>Audience: Individuals of all experience levels, from beginners to experts, with an interest in Python and SysIdentPy.</p> <ul> <li> Get Help</li> <li> Workshops</li> <li> Reading Suggestions</li> <li> Community Discussions</li> </ul>"},{"location":"developer-guide/how-to-add-a-translation/","title":"How to Add a Translation","text":"<p>This page explains how to add or improve a translation of the docs for any language.</p> <p>The site uses MkDocs + Material + <code>mkdocs-static-i18n</code>. English is the reference (fallback). Any other language mirrors its folder structure. If a page is missing in a target language, the English version is shown automatically.</p>"},{"location":"developer-guide/how-to-add-a-translation/#1-overview","title":"1. Overview","text":"<p>There are three typical scenarios:</p> <ol> <li>Improve an existing translated page.</li> <li>Translate an English page that currently has no localized version.</li> <li>Add an entirely new language to the project.</li> </ol> <p>Everything below covers all three.</p>"},{"location":"developer-guide/how-to-add-a-translation/#2-folder-layout","title":"2. Folder layout","text":"<pre><code>/docs\n  en/\n    developer-guide/\n    getting-started/\n    user-guide/\n  &lt;locale&gt;/\n    (mirrors the structure you translate)\n</code></pre> <p>Examples of <code>&lt;locale&gt;</code>: <code>pt</code>, <code>es</code>, <code>fr</code>, <code>de</code>, <code>it</code>, <code>ja</code>, <code>zh</code>, <code>ru</code>. Use short IETF / BCP\u201147 language tags (no country code unless needed, e.g. <code>pt</code> vs <code>pt-BR</code> only if a regional variant is required). Keep it consistent with <code>mkdocs.yml</code>.</p> <p>Relative paths must match. Example:</p> <pre><code>English: docs/en/developer-guide/how-to-add-a-translation.md\nSpanish: docs/es/developer-guide/how-to-add-a-translation.md\nFrench:  docs/fr/developer-guide/how-to-add-a-translation.md\n</code></pre>"},{"location":"developer-guide/how-to-add-a-translation/#3-quick-start-improving-or-adding-a-translation","title":"3. Quick start (improving or adding a translation)","text":"<ol> <li>Fork and clone your fork.</li> <li>Create / activate a virtual environment.</li> <li>Install docs extras:     <pre><code>pip install -e \".[docs]\"\n</code></pre></li> <li>Run the local server:     <pre><code>mkdocs serve\n</code></pre></li> <li>Open the local URL and switch language with the selector.</li> </ol> <p>Restart the server if new files do not show up (MkDocs sometimes needs a restart after new file additions).</p>"},{"location":"developer-guide/how-to-add-a-translation/#4-adding-a-new-language-one-time-setup","title":"4. Adding a new language (one-time setup)","text":"<p>If the language already exists (e.g. <code>pt/</code>), skip this section.</p> <ol> <li>Pick a locale code (e.g. <code>es</code>).</li> <li>Create the folder: <code>docs/es/</code>.</li> <li>Copy the English <code>index.md</code> to <code>docs/es/index.md</code> and translate its visible text.</li> <li>(Optional) Start by copying only a minimal subset (index + key getting-started pages) to keep the first PR reviewable.</li> <li>Update <code>mkdocs.yml</code> <code>i18n.languages</code> section:     <pre><code>- locale: es\n  name: Espa\u00f1ol\n  build: true\n  site_description: &lt;translated site tagline&gt;\n  theme:\n    docs_dir: docs/es/\n    custom_dir: docs/es/\n    site_dir: site/es/\n    logo: overrides/assets/img/logotype-sysidentpy.svg\n</code></pre></li> <li>Do not duplicate nav entries for the new language: the plugin maps them automatically based on folder structure.</li> <li>Run <code>mkdocs serve</code> and confirm the new language appears in the selector.</li> </ol> <p>If you need a regional variant (e.g. <code>pt-BR</code>), use that code consistently in both folder name and <code>mkdocs.yml</code>.</p>"},{"location":"developer-guide/how-to-add-a-translation/#5-adding-a-new-english-page-source-language","title":"5. Adding a new English page (source language)","text":"<ol> <li>Create the file under <code>docs/en/...</code> in the right section.</li> <li>Add front matter:     <pre><code>---\ntemplate: overrides/main.html\ntitle: My Page Title\n---\n</code></pre></li> <li>Add its path to the <code>nav</code> tree in <code>mkdocs.yml</code> (English only).</li> <li>Verify the site builds.</li> <li>(Optional) Add a translator note (HTML comment) for tricky concepts:     <pre><code>&lt;!-- Translator note: keep the term \"NARMAX\" in English. --&gt;\n</code></pre></li> </ol>"},{"location":"developer-guide/how-to-add-a-translation/#6-creating-the-translated-file","title":"6. Creating the translated file","text":"<ol> <li> <p>Mirror the folder path: <code>docs/&lt;locale&gt;/&lt;same relative path&gt;.md</code>.</p> </li> <li> <p>Copy the English file.</p> </li> <li> <p>Translate only human-readable prose. Keep intact:</p> <ul> <li>Code blocks (except inline comments if clarity improves)</li> <li>Identifiers: function/class names, parameters, imports</li> <li>File paths, config keys, URLs</li> <li>Markdown link targets (unless they refer to a language-specific external resource)</li> </ul> </li> <li> <p>Preserve heading hierarchy (<code>#</code>, <code>##</code>, etc.) but translate heading text itself.</p> </li> <li> <p>Keep admonition types (<code>!!! note</code>, <code>!!! warning</code>, etc.). You may translate the title string after the admonition type.</p> </li> <li> <p>For unfinished parts, you can temporarily keep English or add:    <pre><code>!!! note \"Pending translation\"\n    This paragraph still needs translation.\n</code></pre></p> </li> <li> <p>Remove all \"Pending\" notes before final review if completed.</p> </li> </ol>"},{"location":"developer-guide/how-to-add-a-translation/#7-internal-links","title":"7. Internal links","text":"<p>Use relative links without language prefixes: <pre><code>See the [Contribute guide](contribute.md).\n</code></pre> The i18n plugin rewrites them per language. Avoid hardcoding <code>/en/</code> or another locale path unless you intentionally want a fallback link.</p> <p>Anchor links: if you translate a heading, Material generates a localized slug. Keep anchor usage consistent or adjust any <code>(#anchor)</code> references accordingly.</p>"},{"location":"developer-guide/how-to-add-a-translation/#8-images-and-media","title":"8. Images and media","text":"<p>If screenshots contain embedded language text:</p> <ul> <li>Option A: replicate and localize the image inside <code>docs/&lt;locale&gt;/assets/</code> and keep the same filename (per-locale path isolates it).</li> <li>Option B: reuse the English image if text is minimal or language-agnostic.</li> </ul> <p>SVG diagrams: prefer keeping labels English if they are code or model symbols; translate UI captions where helpful.</p>"},{"location":"developer-guide/how-to-add-a-translation/#9-formatting-style-guidelines","title":"9. Formatting &amp; style guidelines","text":"Aspect Guideline Numbers Keep numeric precision; adapt decimal separators only if standard for the target audience (optional). Units Do not translate SI units (e.g. <code>ms</code>, <code>Hz</code>). API names Never translate identifiers. Quotes Follow local typographic conventions only if they do not break Markdown. Capitalization Mirror English capitalization only where it's a proper noun or API name. Tone Neutral, concise, instructional. <p>Avoid machine\u2011translated chunks without revision. Prefer shorter, unambiguous sentences.</p>"},{"location":"developer-guide/how-to-add-a-translation/#10-translation-glossary","title":"10. Translation glossary","text":"<p>Keep these terms consistent across languages. Add target language equivalents as needed:</p> English Term Portuguese (pt) Core concepts model structure estrutura do modelo parameter estimation estima\u00e7\u00e3o de par\u00e2metros residual analysis an\u00e1lise dos res\u00edduos time series s\u00e9rie temporal identification identifica\u00e7\u00e3o Technical terms basis function fun\u00e7\u00e3o de base regression regress\u00e3o algorithm algoritmo validation valida\u00e7\u00e3o simulation simula\u00e7\u00e3o Development terms feature funcionalidade pull request (PR) pull request (PR) branch branch commit commit documentation documenta\u00e7\u00e3o <p>For other languages, follow similar patterns. Prefer clarity over literal translation.</p>"},{"location":"developer-guide/how-to-add-a-translation/#11-review-checklist-per-translated-file","title":"11. Review checklist (per translated file)","text":"<ul> <li> Builds locally (<code>mkdocs serve</code>).</li> <li> File path mirrors English.</li> <li> Relative links work; no <code>/en/</code> hardcoding.</li> <li> Code blocks untouched (except explanatory comments if needed).</li> <li> Terminology consistent with existing pages.</li> <li> No leftover placeholder notes (unless intentionally partial PR).</li> <li> Front matter present and <code>title:</code> localized.</li> </ul>"},{"location":"developer-guide/how-to-add-a-translation/#12-commit-pull-request","title":"12. Commit &amp; Pull Request","text":"<p>Add both English and translated files if you introduce a page + its translation; otherwise just the translated file.</p> <p>Example: <pre><code>git add docs/en/developer-guide/new-topic.md docs/es/developer-guide/new-topic.md\ngit commit -m \"docs: add Spanish translation for new-topic\"\n</code></pre></p> <p>PR description template:</p> <ul> <li>Language: <code>&lt;locale&gt;</code> (e.g. <code>es</code>)</li> <li>Pages added/updated: list paths</li> <li>Sections intentionally left untranslated: (if any)</li> <li>Glossary additions: (if any)</li> <li>Notes for reviewer: context, tricky terms</li> </ul> <p>Partial translations are acceptable\u2014label them clearly so others can help.</p>"},{"location":"developer-guide/how-to-add-a-translation/#13-updating-existing-translations","title":"13. Updating existing translations","text":"<p>When an English page changes:</p> <ol> <li>Open the diff to see what changed.</li> <li>Apply equivalent edits to the translated file.</li> <li>If you cannot translate immediately, keep the English change and add a temporary \"Pending translation\" note.</li> <li>Remove the note once updated.</li> </ol> <p>Aim for incremental PRs rather than large rewrites.</p>"},{"location":"developer-guide/how-to-add-a-translation/#14-common-issues","title":"14. Common issues","text":"Symptom Cause Fix Page shows in English only Missing localized file or wrong path Mirror path under new locale folder Build error after adding language Misconfigured <code>i18n.languages</code> entry Re-check <code>locale</code> key + indentation 404 on internal link Link path differs from English Match relative path to English version Broken anchor Heading translated; old anchor used Update anchor or keep similar heading slug Language not in selector Forgot to add language in <code>mkdocs.yml</code> Add <code>locale</code> entry and restart server"},{"location":"developer-guide/how-to-add-a-translation/#15-automation-optional","title":"15. Automation (optional)","text":"<p>You may use external CAT / translation tools, but always manually review technical terms. Avoid committing raw machine output.</p> <p>If you script copying English files to a new locale, exclude already translated ones to prevent overwriting.</p>"},{"location":"developer-guide/how-to-add-a-translation/#16-questions-support","title":"16. Questions &amp; support","text":"<p>If you are unsure about terminology, open an Issue or Discussion before translating large sections. Early feedback prevents rework.</p> <p>Thanks for helping make the documentation accessible to more people.</p>"},{"location":"developer-guide/how-to-write-a-how-to-guide/","title":"How To Write a How-to Guide","text":"<p>Coming soon</p>"},{"location":"developer-guide/how-to-write-a-tutorial/","title":"How To Write a Tutorial","text":"<p>Coming soon</p>"},{"location":"getting-started/getting-started/","title":"Getting Started","text":"<p>Welcome to SysIdentPy's documentation! Learn how to get started with SysIdentPy in your project. Then, explore SysIdentPy's main concepts and discover additional resources to help you model dynamic systems and time series.</p>      \ud83d\udcda Looking for more details on NARMAX models? \u25bc <p>       For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book:     </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>       This book provides in-depth guidance to support your work with SysIdentPy.     </p> <p>       \ud83d\udee0\ufe0f You can also explore the tutorials in the documentation for practical, hands-on examples.     </p>"},{"location":"getting-started/getting-started/#what-is-sysidentpy","title":"What is SysIdentPy","text":"<p>SysIdentPy is an open-source Python module for System Identification using NARMAX models built on top of numpy and is distributed under the 3-Clause BSD license. SysIdentPy provides an easy-to-use and  flexible framework for building Dynamical Nonlinear Models for time series and dynamic systems.</p> <p>With SysIdentPy, you can:</p> <ul> <li>Build and customize nonlinear forecasting models.</li> <li>Utilize state-of-the-art techniques for model structure selection and parameter estimation.</li> <li>Experiment with neural NARX models and other advanced algorithms.</li> </ul>"},{"location":"getting-started/getting-started/#installation","title":"Installation","text":"<p>SysIdentPy is published as a Python package and can be installed with <code>pip</code>, ideally by using a virtual environment. If not, scroll down and expand the help box. Install with:</p> Latest <pre><code>pip install sysidentpy</code></pre> Neural NARX Support <pre><code>pip install sysidentpy[\"all\"]</code></pre> Specific Version <pre><code>pip install sysidentpy==\"0.5.3\"</code></pre> From Git <pre><code>pip install git+https://github.com/wilsonrljr/sysidentpy.git</code></pre>      \u2753 How to manage my projects dependencies? \u25bc <p>       If you don't have prior experience with Python, we recommend reading                Using Python's pip to Manage Your Projects' Dependencies       , which is a really good introduction on the mechanics of Python package management and helps you troubleshoot if you run into errors.     </p>"},{"location":"getting-started/getting-started/#what-are-the-main-features-of-sysidentpy","title":"What are the main features of SysIdentPy?","text":"\ud83e\udde9 NARMAX Philosophy <p>Build variations like NARX, NAR, ARMA, NFIR, and more.</p> \ud83d\udcdd Model Structure Selection <p>Use methods like FROLS, MetaMSS, and combinations with parameter estimation techniques.</p> \ud83d\udd17 Basis Function <p>Choose from 8+ basis functions, combining linear and nonlinear types for custom NARMAX models.</p> \ud83c\udfaf Parameter Estimation <p>Over 15 parameter estimation methods for exploring various structure selection scenarios.</p> \u2696\ufe0f Multiobjective Estimation <p>Minimize different objective functions using affine information for parameter estimation.</p> \ud83d\udd04 Model Simulation <p>Reproduce paper results easily with SimulateNARMAX. Test and compare published models effortlessly.</p> \ud83e\udd16 Neural NARX (PyTorch) <p>Integrate with PyTorch for custom neural NARX architectures using all PyTorch optimizers and loss functions.</p> \ud83d\udee0\ufe0f General Estimators <p>Compatible with scikit-learn, Catboost, and more for creating NARMAX models.</p>"},{"location":"getting-started/getting-started/#additional-resources","title":"Additional resources","text":"<ul> <li> \ud83e\udd1d Contribute to SysIdentPy </li> <li> \ud83d\udcdc License Information </li> <li> \ud83c\udd98 Get Help &amp; Support </li> <li> \ud83d\udcc5 Meetups </li> <li> \ud83d\udc96 Become a Sponsor </li> <li> \ud83e\udde9 Explore NARMAX Base Code </li> </ul>"},{"location":"getting-started/getting-started/#do-you-like-sysidentpy","title":"Do you like SysIdentPy?","text":"<p>Would you like to help SysIdentPy, other users, and the author? You can \"star\" SysIdentPy in GitHub by clicking in the star button at the top right of the page: https://github.com/wilsonrljr/sysidentpy. \u2b50\ufe0f</p> <p>Starring a repository makes it easy to find it later and help you to find similar projects on GitHub based on Github recommendation contents. Besides, by starring a repository also shows appreciation to the SysIdentPy maintainer for their work.</p> <p> \u00a0 Join our  \"Sponsor\" in github</p>"},{"location":"getting-started/license/","title":"License","text":"<p>BSD 3-Clause License</p> <p>Copyright \u00a9 2019, Wilson Rocha; Luan Pascoal; Samuel Oliveira; Samir Martins All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ul> <li> <p>Redistributions of source code must retain the above copyright notice, this   list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,   this list of conditions and the following disclaimer in the documentation   and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its   contributors may be used to endorse or promote products derived from   this software without specific prior written permission.</p> </li> </ul> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"getting-started/narmax-intro/","title":"Introduction","text":"<p>Author: Wilson Rocha Lacerda Junior</p> <p>This is the first in a series of publications explaining a little bit about NARMAX<sup>1</sup> models. I hope the content of these publications will help those who use or would like to use the SysIdentPy library.</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>You can also explore the tutorials in the documentation for practical, hands-on examples.</p>"},{"location":"getting-started/narmax-intro/#system-identification","title":"System Identification","text":"<p>As I will use the term Systems Identification here and there, let me make a brief definition regarding these terms.</p> <p> Systems identification is one of the major areas that deals with the modeling of data-based processes. In this context, the term \"system\" can be interpreted as any set of operations that process one or more inputs and return one or more outputs. Examples include electrical systems, mechanical systems, biological systems, financial systems, chemical systems \u2026 literally anything you can relate to input and output data. The electricity demand is part of a system whose inputs can be, for example, quantity of the population, quantity of water in the reservoirs, season, events. The price of a property is the output of a system whose entries can be the city, per capita income, neighborhood, number of rooms, how old the house is, and many others. You got the idea.</p> <p> Although there are many things related with Machine Learning, Statistical Learning and other fields,  each field has its particularities.</p>"},{"location":"getting-started/narmax-intro/#so-what-is-a-narmax-model","title":"So, what is a NARMAX model?","text":"<p>You may have noticed the similarity between the acronym NARMAX with the well-known models ARX, ARMAX, etc., which are widely used for forecasting time series. And this resemblance is not by chance. The Autoregressive models with Moving Average and Exogenous Input (ARMAX) and their variations AR, ARX, ARMA (to name just a few) are one of the most used mathematical representations for identifying linear systems.</p> <p> Let's go back to the model. I said that the ARX family of models is commonly used to model linear systems. Linear is the key word here. For nonlinear scenarios we have the NARMAX class. As reported by Billings (one of the creators of NARMAX model) in the book [Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains],  NARMAX started out as a model name, but soon became a philosophy when it comes to identifying nonlinear systems. Obtaining NARMAX models consists of performing the following steps:</p> <p>Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains</p> <ul> <li>Dynamical tests and collecting data;</li> <li>Choice of mathematical representation;</li> <li>Detection of the model structure;</li> <li>Estimation of parameters;</li> <li>Validation;</li> <li>Analysis of the model.</li> </ul> <p>We will cover each of these steps in further publications. The idea of this text is to present an overview of NARMAX models.</p> <p> NARMAX models are not, however, a simple extension of ARMAX models. NARMAX models are able to represent the most different and complex nonlinear systems. Introduced in 1981 by the Electrical Engineer Stephen A. Billings, NARMAX models can be described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) , are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>If we do not include noise terms, \\(e_{k-n_e}\\), we have NARX models. If we set \\(\\ell = 1\\) then we deal with ARMAX models; if \\(\\ell = 1\\) and we do not include input and noise terms, it turns to AR model (ARX if we include inputs, ARMA if we include noise terms instead); if \\(\\ell&gt;1\\) and there is no input terms, we have the NARMA. If there is no input or noise terms, we have NAR. There are several variants, but that is sufficient for now.</p>"},{"location":"getting-started/narmax-intro/#narmax-representation","title":"NARMAX Representation","text":"<p>There are several nonlinear functions representations to approximate the unknown mapping \\(\\mathrm{f}[\\cdot]\\) in the NARMAX methods, e.g.,</p> <ul> <li>neural networks;</li> <li>fuzzy logic-based models;</li> <li>radial basis functions;</li> <li>wavelet basis;</li> <li>polynomial basis;</li> <li>generalized additive models;</li> </ul> <p>The remainder of this post contemplates methods related to the power-form polynomial models, which is the most common used representation. Polynomial NARMAX is a mathematical model based on difference equations and relates the current output as a function of past inputs and outputs.</p>"},{"location":"getting-started/narmax-intro/#polynomial-narmax","title":"Polynomial NARMAX","text":"<p>The polynomial NARMAX model with asymptotically stable equilibrium points can be described as:</p> \\[\\begin{align}     y_k =&amp; \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m}\\\\     &amp;+ \\sum_{i=1}^{p}\\sum_{j=1}^{q}\\Theta_{ye}^{ij}y_{k-i} e_{k-j} + \\sum_{i=1}^{p}\\sum_{m=1}^{r}\\Theta_{yx}^{im}y_{k-i} x_{k-m} \\\\     &amp;+ \\sum_{j=1}^{q}\\sum_{m=1}^{r}\\Theta_{e x}^{jm}e_{k-j} x_{k-m} \\\\     &amp;+ \\sum_{i=1}^{p}\\sum_{j=1}^{q}\\sum_{m=1}^{r}\\Theta_{y e x}^{ijm}y_{k-i} e_{k-j} x_{k-m} \\\\     &amp;+ \\sum_{m_1=1}^{r} \\sum_{m_2=m_1}^{r}\\Theta_{x^2}^{m_1 m_2} x_{k-m_1} x_{k-m_2} \\dotsc \\\\     &amp;+ \\sum_{m_1=1}^{r} \\dotsc \\sum_{m_l=m_{l-1}}^{r} \\Theta_{x^l}^{m_1, \\dotsc, m_2} x_{k-m_1} x_{k-m_l} \\end{align}\\] <p>where \\(\\sum\\nolimits_{0}\\), \\(c_{y}^{i}\\), \\(c_{e}^{j}\\), \\(c_{x}^{m}\\), \\(c_{y\\e}^{ij}\\), \\(c_{yx}^{im}\\), \\(c_{e x}^{jm}\\), \\(c_{y e x}^{ijm}\\), \\(c_{x^2}^{m_1 m_2} \\dotsc c_{x^l}^{m_1, \\dotsc, ml}\\) are constant parameters.</p> <p> Let's take a look at an example of a NARMAX model for an easy understanding. The following is a NARMAX model of degree~\\(2\\), identified from experimental data of a DC motor/generator with no prior knowledge of the model form. If you want more information about the identification process, I wrote a paper comparing a polynomial NARMAX with a neural NARX model using that data (IN PORTUGUESE: Identifica\u00e7\u00e3o de um motor/gerador CC por meio de modelos polinomiais autorregressivos e redes neurais artificiais)</p> \\[\\begin{align}     y_k =&amp; 1.7813y_{k-1}-0,7962y_{k-2}+0,0339x_{k-1} -0,1597x_{k-1} y_{k-1} +0,0338x_{k-2} \\\\     &amp; + 0,1297x_{k-1}y_{k-2} - 0,1396x_{k-2}y_{k-1}+ 0,1086x_{k-2}y_{k-2}+0,0085y_{k-2}^2 + 0.1938e_{k-1}e_{k-2} \\end{align}\\] <p>But how those terms were selected? How the parameters were estimated? These questions will lead us to model structure selection and parameter estimation topics, but, for now,  let us discuss about those topics in a more simple manner.</p> <p> First, the \"structure\" of a model is the set of terms (also called regressors) included in the final model. The parameters are the values multiplying each of theses terms. And looking at the example above we can notice an really important thing regarding polynomial NARMAX models dealt in this text: they have a non-linear structure, but they are linear-in-the-parameters. You will see how this note is important in the post about parameter estimation.</p> <p> In this respect, consider the case where we have the input and output data of some system. For the sake of simplicity, suppose one input and one output. We have the data, but we do not know which lags to choose for the input or the output. Also, we know nothing about the system non-linearity. So, we have to define some values for maximum lags of the input, output and the noise terms, besides the choice of the \\(\\ell\\) value. It's worth to notice that many assumptions taken for linear cases are not valid in the nonlinear scenario and therefore select the maximum lags is not straightforward. So, how those values can make the modeling harder?</p> <p> So we have one input and one output (disregard the noise terms for now). What if we choose the \\(n_y = n_x = \\ell = 2\\)? With these values, we have the following possibilities for compose the final model:</p> \\[\\begin{align}     &amp; constant, y_{k-1}, y_{k-2}, y_{k-1}^2, y_{k-2}^2, x_{k-1}, x_{k-2}, x_{k-1}^2, x_{k-2}^2,y_{k-1}y_{k-2},\\\\     &amp; y_{k-1}x_{k-1}, y_{k-1}x_{k-2}, y_{k-2}x_{k-1}, y_{k-2}x_{k-2}, x_{k-1}x_{k-2} . \\end{align}\\] <p>So we have \\(15\\) candidate terms to compose the final model.</p> <p> Again, we do not know how of those terms are significant to compose the model. One should decide to use all the terms because there are only \\(15\\). This, even in a simple scenario like this, can lead to a very wrong representation of the system that you are trying to modeling. Ok, what if we run a brute force algorithm to test the candidate regressors so we can select only the significant ones? In this case, we have \\(2^{15} = 32768\\) possible model structures to be tested.</p> <p> You can think that it is ok, we have computer power for that. But this case is very simple and the system might have lags equal to \\(10\\) for input and output. If we define \\(n_y = n_x = 10\\) and \\(\\ell=2\\), the number of possible models to be tested increases to \\(2^{231}=3.4508732\\times10^{69}\\). If the non-linearity is set to \\(3\\) then we have \\(2^{1771} = 1.3308291989700907535925992... \\times 10^{533}\\) candidate models.</p> <p> Now, think about the case when we have not 1, but 5, 10 or more inputs... and have to include terms for the noise, and maximum lags are higher than 10... and nonlinearity is higher than 3...</p> <p> And the problem is not solved by only identifying the most significant terms. How do you choose the number of terms to include in the final model. It is not just about check the relevance of each regressor, we have to think about the impact of including \\(5\\), \\(10\\) or \\(50\\) regressors in the model. And do not forget: after selecting the terms, we have to estimate its parameters.</p> <p> As you can see, to select the most significant terms from a huge dictionary of possible terms is not an easy task. And it is hard not only because the complex combinatorial problem and the uncertainty concerning the model order. Identifying the most significant terms in a nonlinear scenario is very difficult because depends on the type of the non-linearity (sparse singularity or near-singular behavior, memory or dumping effects and many others), dynamical response (spatial-temporal systems, time-dependent), the steady-state response,  frequency of the data, the noise...</p> <p> Despite all this complexity, NARMAX models are widely used because it is able to represent complex system with simple and transparent models, which terms are selected using robust algorithms for model structure selection. Model structure selection is the core of NARMAX methods and the scientific community is very active on improving classical methods and developing new ones. As I said, I will introduce some of those methods in another post.</p> <p> I hope this publication served as a brief introduction to NARMAX models. Furthermore, I hope I have sparked your interest in this model class. The link to the other texts will be made available soon, but feel free to contact us if you are interested in collaborating with the SysIdentPy library or if you want to address any questions.</p> <ol> <li> <p>Non-linear Autoregressive Models with Moving Average and Exogenous Input.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting-started/quickstart-guide/","title":"Basic Usage","text":""},{"location":"getting-started/quickstart-guide/#1-prerequisites","title":"1. Prerequisites","text":"<p>You\u2019ll need to know a bit of Python.</p> <p>To work the examples, you\u2019ll need <code>pandas</code> installed in addition to NumPy.</p> <pre><code>pip install sysidentpy pandas\n# Optional: For neural networks and advanced features\npip install sysidentpy[\"all\"]\n</code></pre>"},{"location":"getting-started/quickstart-guide/#2-key-features","title":"2. Key Features","text":"<p>SysIdentPy provides a flexible framework for building, predicting, validating, and visualizing nonlinear time series models. The modeling process involves several key decisions: defining the mathematical representation of the model, choosing the parameter estimation algorithm, selecting the appropriate model structure, and determining the prediction approach.</p> <p>The following features are available in SysIdentPy:</p>"},{"location":"getting-started/quickstart-guide/#model-classes","title":"Model Classes","text":"<ul> <li>NARMAX, NARX, NARMA, NAR, NFIR, ARMAX, ARX, AR, and their variants.</li> </ul>"},{"location":"getting-started/quickstart-guide/#mathematical-representations","title":"Mathematical Representations","text":"<ul> <li>Polynomial</li> <li>Neural</li> <li>Fourier</li> <li>Laguerre</li> <li>Bernstein</li> <li>Bilinear</li> <li>Legendre</li> <li>Hermite</li> <li>HermiteNormalized</li> </ul> <p>You can also define advanced NARX models such as Bayesian and Gradient Boosting models using the GeneralNARX class, which provides seamless integration with various machine learning algorithms.</p>"},{"location":"getting-started/quickstart-guide/#model-structure-selection-algorithms","title":"Model Structure Selection Algorithms","text":"<ul> <li>Forward Regression Orthogonal Least Squares (FROLS)</li> <li>Meta-model Structure Selection (MeMoSS)</li> <li>Accelerated Orthogonal Least Squares (AOLS)</li> <li>Entropic Regression</li> <li>Ultra Orthogonal Least Squares (UOLS)</li> </ul>"},{"location":"getting-started/quickstart-guide/#parameter-estimation-methods","title":"Parameter Estimation Methods","text":"<ul> <li>Least Squares (LS)</li> <li>Total Least Squares (TLS)</li> <li>Recursive Least Squares (RLS)</li> <li>Ridge Regression</li> <li>Non-Negative Least Squares (NNLS)</li> <li>Least Squares Minimal Residues (LSMR)</li> <li>Bounded Variable Least Squares (BVLS)</li> <li>Least Mean Squares (LMS) and its variants:</li> <li>Affine LMS</li> <li>LMS with Sign Error</li> <li>Normalized LMS</li> <li>LMS with Normalized Sign Error</li> <li>LMS with Sign Regressor</li> <li>Normalized LMS with Sign Sign</li> <li>Leaky LMS</li> <li>Fourth-Order LMS</li> <li>Mixed Norm LMS</li> </ul>"},{"location":"getting-started/quickstart-guide/#order-selection-criteria","title":"Order Selection Criteria","text":"<ul> <li>Akaike Information Criterion (AIC)</li> <li>Corrected Akaike Information Criterion (AICc)</li> <li>Bayesian Information Criterion (BIC)</li> <li>Final Prediction Error (FPE)</li> <li>Khundrin's Law of Iterated Logarithm Criterion</li> </ul>"},{"location":"getting-started/quickstart-guide/#prediction-methods","title":"Prediction Methods","text":"<ul> <li>One-step ahead</li> <li>n-steps ahead</li> <li>Infinity-steps ahead</li> </ul>"},{"location":"getting-started/quickstart-guide/#visualization-tools","title":"Visualization Tools","text":"<ul> <li>Prediction plots</li> <li>Residual analysis</li> <li>Model structure visualization</li> <li>Parameter visualization</li> </ul> <p>As you can see, SysIdentPy supports numerous model combinations, each tailored to different use cases. But don\u2019t worry about picking the perfect combination right away. Let\u2019s start with the default settings to get you up and running quickly.</p>      \ud83d\udcda Looking for more details on NARMAX models? \u25bc <p>       For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book:     </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>       This book provides in-depth guidance to support your work with SysIdentPy.     </p> <p>       \ud83d\udee0\ufe0f You can also explore the tutorials in the documentation for practical, hands-on examples.     </p>"},{"location":"getting-started/quickstart-guide/#3-quickstart","title":"3. Quickstart","text":"<p>To keep things simple, let's load some simulated data for the examples.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\n\n# Generate a dataset from a simulated dynamic system.\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=300,\n    colored_noise=False,\n    sigma=0.0001,\n    train_percentage=80\n)\n</code></pre>"},{"location":"getting-started/quickstart-guide/#build-your-first-narx-model","title":"Build your first NARX model","text":"<p>With the loaded dataset, let's build a Polynomial NARX model. Using SysIdentPy's default options, you need to define at least the model structure selection method and the mathematical representation of the model (specified here by the basis function).</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n</code></pre> <p>The model structure selection (MSS) method enables the model's fit and predict operations.</p> <p>While different MSS algorithms come with various hyperparameters, they are not the focus here. In this guide, we will show how to modify these hyperparameters but will not discuss the best configurations.</p> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>To evaluate the model's performance, you can use any of the native metric functions available in SysIdentPy. For example, the Root Relative Squared Error (RRSE) metric can be used as follows:</p> <pre><code>from sysidentpy.metrics import root_relative_squared_error\n\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00014\n</code></pre> <p>To view the final mathematical equation of the Polynomial NARX model, use the results function. This function requires:</p> <ul> <li><code>final_model</code>: The selected regressors after fitting</li> <li><code>theta</code>: The estimated parameters</li> <li><code>err</code>: The error reduction ratio (ERR)</li> </ul> <p>Here\u2019s how to display the results:</p> <p><pre><code>from sysidentpy.utils.display_results import results\n\nr = pd.DataFrame(\n    results(\n        model.final_model, model.theta, model.err,\n        model.n_terms, err_precision=8, dtype='sci'\n        ),\n    columns=['Regressors', 'Parameters', 'ERR'])\nprint(r)\n</code></pre> This output shows the selected regressors, their corresponding estimated parameters, and the contribution of each regressor to the model\u2019s performance (ERR).</p> <pre><code>Regressors     Parameters        ERR\n0        x1(k-2)     0.9000  0.95556574\n1         y(k-1)     0.1999  0.04107943\n2  x1(k-1)y(k-1)     0.1000  0.00335113\n</code></pre> <p>To visualize the model's performance, you can use the <code>plot_results</code> function. This method plots the predicted values against the actual data, allowing you to see how well the model fits the dataset.</p> <p><pre><code>from sysidentpy.utils.plotting import plot_results\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> </p> <p>Residual analysis is essential to check if the model has captured all the relevant dynamics of the system. You can analyze the residuals by computing their autocorrelation and the cross-correlation between the residuals and one of the model inputs.</p> <pre><code>from sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n\n# Compute and plot autocorrelation of the residuals\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\n\n# Compute and plot cross-correlation between residuals and an input\nx1e = compute_cross_correlation(y_valid, yhat, x2_val)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>Here\u2019s the full code example for your reference:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n\n# Generate a dataset from a simulated dynamic system.\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=300,\n    colored_noise=False,\n    sigma=0.0001,\n    train_percentage=80\n)\n\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model, model.theta, model.err,\n        model.n_terms, err_precision=8, dtype='sci'\n        ),\n    columns=['Regressors', 'Parameters', 'ERR'])\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000, figsize=(15, 4))\n# Compute and plot autocorrelation of the residuals\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\n# Compute and plot cross-correlation between residuals and an input\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre>"},{"location":"getting-started/quickstart-guide/#customizing-your-model-configuration","title":"Customizing your model configuration","text":"<p>In the previous section, we showed how easy it is to fit a Polynomial NARX model with SysIdentPy using the default configuration. But what if you want to experiment with different combinations of algorithms for model structure selection, parameter estimation, and other settings?</p>"},{"location":"getting-started/quickstart-guide/#model-structure-selection","title":"Model Structure Selection","text":"<p>SysIdentPy makes this process simple. For instance, if you want to use the Accelerated Orthogonal Least Squares (AOLS) algorithm instead of the default <code>FROLS</code>, you only need to import and use it when defining your model.</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = AOLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>The evaluation, residual analysis, and plots remain the same as before, so they are not shown here.</p> <p>With just a small change in the import statement, you can explore different algorithms. Similarly, you can customize the parameter estimation methods, prediction strategies, and mathematical representations to suit your specific needs.</p> <p>Similarly, you can use the Meta-model Structure Selection</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = MetaMSS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>and the Entropic Regression</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"getting-started/quickstart-guide/#parameter-estimation","title":"Parameter Estimation","text":"<p>Changing the parameter estimation algorithm in SysIdentPy is also straightforward. You can check the list of available algorithms with the following code:</p> <pre><code>from sysidentpy import parameter_estimation\n\nprint(\"Parameter Estimation Algorithms available:\", parameter_estimation.__all__)\n</code></pre> <pre><code>Parameter Estimation Algorithms available: ['LeastSquares', 'RidgeRegression', 'RecursiveLeastSquares', 'TotalLeastSquares', 'LeastMeanSquareMixedNorm', 'LeastMeanSquares', 'LeastMeanSquaresFourth', 'LeastMeanSquaresLeaky', 'LeastMeanSquaresNormalizedLeaky', 'LeastMeanSquaresNormalizedSignRegressor', 'LeastMeanSquaresNormalizedSignSign', 'LeastMeanSquaresSignError', 'LeastMeanSquaresSignSign', 'AffineLeastMeanSquares', 'NormalizedLeastMeanSquares', 'NormalizedLeastMeanSquaresSignError', 'LeastMeanSquaresSignRegressor', 'NonNegativeLeastSquares', 'LeastSquaresMinimalResidual', 'BoundedVariableLeastSquares']\n</code></pre> <p>Although the default estimator may change depending on the model structure selection method, it is usually <code>LeastSquares</code> or <code>RecursiveLeastSquares</code>. To define a specific estimator, simply import the desired method and set it using the estimator hyperparameter:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquaresMinimalResidual\n\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    estimator=LeastSquaresMinimalResidual(),\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>You can apply this approach to any model structure selection algorithm. It\u2019s that simple to change the parameter estimation method.</p>"},{"location":"getting-started/quickstart-guide/#customizing-the-mathematical-representation-basis-function","title":"Customizing the Mathematical Representation - Basis Function","text":"<p>Changing the mathematical representation (basis function) in SysIdentPy is as simple as customizing the parameter estimation method. For example, to build a Fourier NARX model, you just need to import the desired basis function and set it in the model structure selection algorithm.</p> <p>To check all available basis functions, use:</p> <pre><code>from sysidentpy import basis_function\n\nprint(\"Basis Function Available:\", basis_function.__all__)\n</code></pre> <pre><code>Basis Function Available: ['Bersntein', 'Bilinear', 'Fourier', 'Legendre', 'Laguerre', 'Hermite', 'HermiteNormalized', 'Polynomial']\n</code></pre> <p>After choosing the basis function, you can define it in your model as shown below:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Fourier\nfrom sysidentpy.parameter_estimation import LeastSquaresMinimalResidual\n\n\nbasis_function = Fourier(degree=2)\nmodel = AOLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    estimator=LeastSquaresMinimalResidual(),\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>With this approach, you can easily explore different mathematical representations by simply switching the basis function. No complex changes required.</p> <p>Note</p> <p>The <code>results</code> method, which returns the mathematical equation of the fitted model, currently supports only the Polynomial basis function. Support for all basis functions is planned for version 1.0.</p>"},{"location":"getting-started/quickstart-guide/#customizing-the-model-type","title":"Customizing the Model Type","text":"<p>The key difference between a NARX and an ARX model lies in the presence of nonlinear relationships between the regressors. For instance, if you set the degree of the basis function to 2 for Polynomial basis function, as shown in previous examples, you'll have a NARX model. If the degree is set to 1, it results in an ARX model.</p> <p>However, the distinction isn't purely based on the degree of the basis function. It ultimately depends on the final model equation. Even with a degree set to 2, the fitted model might be linear if the model structure selection algorithm removes the nonlinear terms. This means that while setting the degree to 2 gives the algorithm an opportunity to explore nonlinear relationships, the final model might still be linear.</p> <p>Always check the final model to confirm whether it is linear or nonlinear, regardless of the degree you set for the basis function.</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\n# basis_function = Polynomial(degree=2) for NARX (and maybe ARX) or\nbasis_function = Polynomial(degree=1)  # ARX model\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>The difference between NARX, NAR, and NFIR models lies in the type of regressors used. NARX models involve both input and output regressors, while NAR models use only output regressors, and NFIR models use only input regressors.</p> <p>To create a NAR model, you simply need to specify the <code>model_type</code> argument as <code>\"NAR\"</code>. In this case, you don't need to define the lags of the inputs since you're working with output-only regressors. You also don't need to pass input data in the <code>fit</code> and <code>predict</code> methods. Only the output data is required.</p> <p>Because NAR models do not include input variables to define the forecasting horizon, you must set the <code>forecast_horizon</code> parameter to specify how many periods ahead you want to predict.</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\n# basis_function = Polynomial(degree=2) for NARX (and maybe ARX) or\nbasis_function = Polynomial(degree=1)  # ARX model\nmodel = FROLS(\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\n\nmodel.fit(y=y_train)\nyhat = model.predict(y=y_valid, forecast_horizon=23)\n</code></pre> <p>For the NFIR model, however, you still need to pass the output array because autoregressive models require initial conditions to operate.</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\n# basis_function = Polynomial(degree=2) for NARX (and maybe ARX) or\nbasis_function = Polynomial(degree=1)  # ARX model\nmodel = FROLS(\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NFIR\",  # Specify NFIR model type\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>      \ud83d\udcda Looking for more details on what are initial conditions? \u25bc <p>       Check chapter 9 of our companion book for more information on why autoregressive models need initial conditions to operate:     </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy"},{"location":"getting-started/quickstart-guide/#prediction-and-forecasting-horizon","title":"Prediction and Forecasting Horizon","text":"<p>By default, when you call <code>model.predict(X=x_valid, y=y_valid)</code>, it performs an infinity-steps ahead prediction, also known as a free run simulation. However, if you need to make a specific number of steps ahead prediction, such as a one-step ahead or n-steps ahead forecast, you can simply pass the <code>steps_ahead</code> hyperparameter in the <code>predict</code> method:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\n# basis_function = Polynomial(degree=2) for NARX (and maybe ARX) or\nbasis_function = Polynomial(degree=2)  # ARX model\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\n# one-step ahead\nyhat = model.predict(X=x_valid, y=y_valid, steps_ahead=1)\n\n# 4-steps ahead\nyhat_4_steps = model.predict(X=x_valid, y=y_valid, steps_ahead=4)\n</code></pre>      \ud83d\udcda Looking for more details about how steps-ahead prediction works? \u25bc <p>       Check chapter 9 of our companion book for more information on how infinity-steps, n-steps and one-step ahead prediction works:     </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy"},{"location":"getting-started/quickstart-guide/#order-selection","title":"Order Selection","text":"<p>Order selection is a classical approach to automatically determine the optimal model order when using the FROLS algorithm. It helps in identifying the best combination of lags and regressors by evaluating different models based on an information criterion.</p> <p>Important</p> <p>Information criteria are only applicable when using the FROLS algorithm. Other algorithms employ alternative methods for model order selection, each developed to their specific approach.</p> <p>To enable order selection, simply: 1. Set <code>order_selection=True</code>. 2. Specify the desired <code>info_criteria</code> (e.g., <code>\"aic\"</code>, <code>\"aicc\"</code>, <code>\"bic\"</code>, <code>\"fpe\"</code>, or <code>\"lilc\"</code>).</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    order_selection=True,\n    info_criteria=\"bic\"\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>You can control how many regressors are tested during order selection using the <code>n_info_values</code> hyperparameter. The default is <code>15</code>, but you might want to increase it when working with high lag orders or multiple input variables.</p> <pre><code>model = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    order_selection=True,\n    info_criteria=\"bic\",\n    n_info_values=50\n)\n</code></pre> <p>Important</p> <p>Increasing <code>n_info_values</code> can improve accuracy but will also increase computational time.</p>"},{"location":"getting-started/quickstart-guide/#narx-neural-network","title":"NARX Neural Network","text":"<p>You can create a Neural NARX Network with SysIdentPy, thanks to its seamless integration with PyTorch. This flexibility allows you to design various Neural NARX architectures by customizing not only the hidden layer configurations and other neural network parameters but also by selecting any available basis function, just like in other NARX representations.</p> <pre><code>from torch import nn\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\n\n\nbasis_function=Polynomial(degree=1)\n\nclass NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(4, 10)\n        self.lin2 = nn.Linear(10, 10)\n        self.lin3 = nn.Linear(10, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n\n\nnarx_net = NARXNN(\n    net=NARX(),\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func='mse_loss',\n    optimizer='Adam',\n    epochs=200,\n    verbose=False,\n    optim_params={'betas': (0.9, 0.999), 'eps': 1e-05} # optional parameters of the optimizer\n)\n\nnarx_net.fit(X=x_train, y=y_train)\nyhat = narx_net.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=1000, figsize=(15, 4))\n</code></pre> <p></p>"},{"location":"getting-started/quickstart-guide/#general-estimators","title":"General Estimators","text":"<p>SysIdentPy also offers the flexibility to integrate any regression method from popular packages like <code>scikit-learn</code>, <code>xgboost</code>, <code>catboost</code>, and many others. To make it work, the estimator simply needs to follow the standard <code>fit</code> and <code>predict</code> API.</p> <p>This significantly expands the range of possible NARX model representations, enabling diverse analyses to help you build the best model for your specific use case.</p> <p>The following example demonstrates how to use a <code>catboost</code> model. Ensure you have <code>catboost</code> installed before running the example.</p> <pre><code>from sysidentpy.general_estimators import NARX\nfrom catboost import CatBoostRegressor\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\n\ncatboost_narx = NARX(\n    base_estimator=CatBoostRegressor(\n        iterations=300,\n        learning_rate=0.1,\n        depth=6),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    fit_params={'verbose': False}\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=200)\n</code></pre> <p></p> <p>To highlight the importance of transforming <code>catboost</code> into a NARX model, the following example shows the performance of <code>catboost</code> without the NARX configuration.</p> <pre><code>catboost = CatBoostRegressor(\n    iterations=300,\n    learning_rate=0.1,\n    depth=6\n)\n\ncatboost.fit(x_train, y_train, verbose=False)\nplot_results(y=y_valid, yhat=catboost.predict(x_valid), figsize=(15, 4))\n</code></pre> <p></p> <p>Note that you can still explore various combinations to better fit your use case. For example, you can create a CatBoost NARX model using a Fourier basis function and perform an n-steps ahead prediction. This flexibility allows you to capture complex seasonality patterns while leveraging CatBoost's powerful gradient boosting capabilities. The same approach applies to any other regression model you choose, enabling you to experiment with different basis functions, prediction horizons, and estimators to find the best configuration for your specific problem.</p> <p>This is just a quickstart guide to SysIdentPy. For more comprehensive tutorials, step-by-step guides, detailed explanations, and advanced use cases, be sure to check out our full documentation and companion book. They provide in-depth content to help you get the most out of SysIdentPy for your system identification and forecasting tasks.</p>"},{"location":"landing-page/about-us/","title":"Project History","text":"<p>The project was started by Wilson R. L. Junior, Luan Pascoal and Samir A. M. Martins as a project for System Identification discipline. We have been working with System Identification for several years (Nonlinear Systems, Machine Learning, Chaotic Systems, Hysteretic models, etc) for several years.</p> <p> Every work we did was using a great tool, but a paid one: Matlab. We started looking for some free alternatives to build NARMAX and its variants (AR, ARX, ARMAX, NAR, NARX, NFIR, Neural NARX, etc.) models using the methods known in System Identification community, but we didn't find any package written in Python with the features we needed to keep doing our research.  Besides, it was always too difficult to find source code of the papers working with NARMAX models and reproduce results was a really hard thing to do.</p> <p> In that context, SysIdentPy was idealized with the following goal: be a free and open source package to help the community design NARMAX models. More than that, be a free and robust alternative to one of the most used tools to build NARMAX models, which is the Matlab's System Identification Toolbox.</p> <p> Samuel joined early in 2019 to help us achieve our goal.</p> <p></p>"},{"location":"landing-page/about-us/#active-maintainers","title":"Active Maintainers","text":"<p>The project is actively maintained by Wilson Rocha Lacerda Junior and looking for contributors.</p>"},{"location":"landing-page/about-us/#citation","title":"Citation","text":"<p>If you use SysIdentPy on your project, please drop me a line.</p> <p>Send email </p> <p>If you use SysIdentPy on your scientific publication, we would appreciate citations to the following paper:</p> <p>Lacerda et al., (2020). SysIdentPy: A Python package for System Identification using NARMAX models. Journal of Open Source Software, 5(54), 2384, https://doi.org/10.21105/joss.02384 <pre><code>    @article{Lacerda2020,\n      doi = {10.21105/joss.02384},\n      url = {https://doi.org/10.21105/joss.02384},\n      year = {2020},\n      publisher = {The Open Journal},\n      volume = {5},\n      number = {54},\n      pages = {2384},\n      author = {Wilson Rocha Lacerda Junior and Luan Pascoal Costa da Andrade and Samuel Carlos Pessoa Oliveira and Samir Angelo Milani Martins},\n      title = {SysIdentPy: A Python package for System Identification using NARMAX models},\n      journal = {Journal of Open Source Software}\n    }\n</code></pre></p>"},{"location":"landing-page/about-us/#inspiration","title":"Inspiration","text":"<p>The documentation and structure (even this section) is openly inspired by sklearn, einsteinpy, and many others as we used (and keep using) them to learn.</p>"},{"location":"landing-page/about-us/#future","title":"Future","text":"<p>SysIdentPy is already useful for many researchers and companies to build NARX models for dynamical systems. But still, there are many improvements and features to come. SysIdentPy has a great future ahead, and your help is greatly appreciated.</p>"},{"location":"landing-page/about-us/#contributors","title":"Contributors","text":""},{"location":"landing-page/sponsor/","title":"Sponsors","text":"<p>As a free and open source project, SysIdentPy relies on the support of the community for its development. If you work for an organization that uses and benefits from SysIdentPy, please consider supporting us.</p> <p>SysIdentPy does not follow the sponsorware release strategy, which means that all features are released to the public at the same time. SysIdentPy is a community driven project, however sponsorships will help to assure its sustainability.</p> <p>The main goal of sponsorships it to make this project sustainable. Your donation goes to support a variety of services and development as they buy the maintainers of this project time to work on the development of new features, bug fixing, stability improvement, issue triage and general support.</p> <p>Read on to learn how to become a sponsor!</p>"},{"location":"landing-page/sponsor/#sponsorships","title":"Sponsorships","text":"<p>Every donation counts and would be greatly appreciated!</p> <p>Sponsorships start as low as $1 a month.<sup>1</sup></p>"},{"location":"landing-page/sponsor/#how-to-become-a-sponsor","title":"How to become a sponsor","text":"<p>Thanks for your interest in sponsoring! In order to become an eligible sponsor with your GitHub account, visit wilsonrljr's sponsor profile, and complete a sponsorship of $1 a month or more. You can use your individual or organization GitHub account for sponsoring.</p> <p> \u00a0 Join our  awesome sponsors</p> <p>If you're in Brazil, you can support me by making a donation via Pix. Just scan the QR code below.</p> <p> </p> <p>Special thanks to our sponsors:</p> <p> Monthly Sponsors</p> <p> </p> <p> Individual Sponsors</p> <p> </p>"},{"location":"landing-page/sponsor/#powered-by","title":"Powered by","text":""},{"location":"landing-page/sponsor/#goals","title":"Goals","text":"<p>The following section lists all funding goals. Each goal contains a list of features prefixed with a checkmark symbol, denoting whether a feature is  already available or  planned, but not yet implemented. When the funding goal is hit, the features are released for general availability.</p>"},{"location":"landing-page/sponsor/#frequently-asked-questions","title":"Frequently asked questions","text":"I don't want to sponsor anymore. Can I cancel my sponsorship? <p>Yes, you can cancel your sponsorship anytime! If you no longer want to sponsor SysIdentPy in GitHub Sponsors, you can request a cancellation which will become effective at the end of the billing cycle. Just remember: sponsorships are non-refundable!</p> <p>If you have any problems or further questions, please reach out to wilsonrljr@outlook.com.</p> We don't want to pay for sponsorship every month. Are there any other options? <p>Yes. You can sponsor on a yearly basis by switching your GitHub account to a yearly billing cycle or just choose an one time donation.</p> <p>If you have any problems or further questions, please reach out to wilsonrljr@outlook.com.</p> <ol> <li> <p>Note that $1 a month is the minimum amount to become a sponsor in Github Sponsor Program.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/overview/","title":"Overview","text":"\ud83e\udde9 Tutorials <p>Follow practical tutorials covering how to build models such as NARX, Neural NARX, NFIR and variants. Learn essential steps, from data preparation to model evaluation, using SysIdentPy\u2019s main features.</p> \ud83d\udcdd Companion Book <p>Looking for more details on NARMAX models? Our book, Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy, covers the theory behind models and methods, while showing how to implement them using SysIdentPy through a wide range of examples and benchmarks.</p> \ud83d\udd17 How To <p>Find practical \"how-to\" guides on a variety of topics, including selecting basis functions, configuring model parameters, performing predictions, and evaluating model performance. Get straightforward solutions for common modeling tasks.</p> \ud83c\udfaf API <p>Explore the complete API reference with detailed documentation of SysIdentPy\u2019s source code. Understand class structures, methods, and parameters to extend and customize functionalities for your projects.</p>"},{"location":"user-guide/API/aols/","title":"Documentation for <code>AOLS</code>","text":"<p>NARMAX Models using the Accelerated Orthogonal Least-Squares algorithm.</p>"},{"location":"user-guide/API/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS","title":"<code>AOLS</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Accelerated Orthogonal Least Squares Algorithm.</p> <p>Build Polynomial NARMAX model using the Accelerated Orthogonal Least-Squares ([1]_). This algorithm is based on the Matlab code available on: https://github.com/realabolfazl/AOLS/</p> <p>The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},     \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>k</code> <code>int</code> <p>The sparsity level.</p> <code>1</code> <code>L</code> <code>int</code> <p>Number of selected indices per iteration.</p> <code>1</code> <code>threshold</code> <code>float</code> <p>The desired accuracy used to stop the iterative selection.</p> <code>1e-9</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import AOLS\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n...                                                    colored_noise=True,\n...                                                    sigma=0.2,\n...                                                    train_percentage=90)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = AOLS(basis_function=basis_function,\n...              ylag=2, xlag=2\n...              )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale    Sparse Reconstruction    https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311</li> <li>Code:    https://github.com/realabolfazl/AOLS/</li> </ul> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>class AOLS(BaseMSS):\n    r\"\"\"Accelerated Orthogonal Least Squares Algorithm.\n\n    Build Polynomial NARMAX model using the Accelerated Orthogonal Least-Squares ([1]_).\n    This algorithm is based on the Matlab code available on:\n    https://github.com/realabolfazl/AOLS/\n\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},\n        \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    k : int, default=1\n        The sparsity level.\n    L : int, default=1\n        Number of selected indices per iteration.\n    threshold : float, default=1e-9\n        The desired accuracy used to stop the iterative selection.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import AOLS\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = AOLS(basis_function=basis_function,\n    ...              ylag=2, xlag=2\n    ...              )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale\n       Sparse Reconstruction\n       https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311\n    - Code:\n       https://github.com/realabolfazl/AOLS/\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        k: int = 1,\n        L: int = 1,\n        threshold: float = 10e-10,\n        model_type: str = \"NARMAX\",\n        estimator: Estimators = LeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n    ):\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.xlag = xlag\n        self.ylag = ylag\n        self.max_lag = self._get_max_lag()\n        self.k = k\n        self.L = L\n        self.estimator = estimator\n        self.threshold = threshold\n        self.res = None\n        self.n_inputs = None\n        self.theta = None\n        self.regressor_code = None\n        self.pivv = None\n        self.final_model = None\n        self.n_terms = None\n        self.err = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if not isinstance(self.k, int) or self.k &lt; 1:\n            raise ValueError(f\"k must be integer and &gt; zero. Got {self.k}\")\n\n        if not isinstance(self.L, int) or self.L &lt; 1:\n            raise ValueError(f\"L must be integer and &gt; zero. Got {self.L}\")\n\n        if not isinstance(self.threshold, (int, float)) or self.threshold &lt; 0:\n            raise ValueError(\n                f\"threshold must be integer and &gt; zero. Got {self.threshold}\"\n            )\n\n    def aols(\n        self, psi: np.ndarray, y: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the Accelerated Orthogonal Least-Squares algorithm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = n_samples\n            The target data used in the identification process.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated coefficients for the selected regressors.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        residual_norm : float\n            The final residual norm.\n\n        References\n        ----------\n        - Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale\n           Sparse Reconstruction\n           https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311\n\n        \"\"\"\n        n, m = psi.shape\n        theta = np.zeros([m, 1])\n        r = y[self.max_lag :].reshape(-1, 1).copy()\n        it = 0\n        max_iter = int(min(self.k, np.floor(n / self.L)))\n        selected_indices = np.full(max_iter * self.L, -1, dtype=np.int64)\n        basis_matrix = np.zeros([n, max_iter * self.L])\n        transformed_psi = psi.copy()\n        eps = np.finfo(float).eps\n        numerator = (r.T @ psi).ravel()\n        denominator = np.sum(psi * transformed_psi, axis=0)\n        q = np.zeros_like(numerator)\n        np.divide(numerator, denominator, out=q, where=np.abs(denominator) &gt; eps)\n        while norm(r) &gt; self.threshold and it &lt; max_iter:\n            it = it + 1\n            offset = (it - 1) * self.L\n            if it &gt; 1:\n                basis_vec = basis_matrix[:, offset].reshape(-1, 1)\n                np.subtract(\n                    transformed_psi,\n                    basis_vec @ (basis_vec.T @ psi),\n                    out=transformed_psi,\n                )\n\n            contribution = np.einsum(\"ij,ij-&gt;j\", transformed_psi, transformed_psi) * (\n                q**2\n            )\n            previous = selected_indices[:offset]\n            sub_ind = list(previous[previous &gt;= 0].astype(int))\n            contribution[sub_ind] = 0\n            contribution[~np.isfinite(contribution)] = 0\n            block_size = min(self.L, len(contribution))\n            if block_size == 0:\n                break\n            top_candidates = np.argpartition(contribution, -block_size)[-block_size:]\n            current_indices = top_candidates[\n                contribution[top_candidates].argsort()[::-1]\n            ]\n            selected_indices[offset : offset + block_size] = current_indices\n            for i, idx in enumerate(current_indices):\n                temp = transformed_psi[:, idx].reshape(-1, 1) * q[idx]\n                temp_norm = norm(temp)\n                if temp_norm &lt;= eps:\n                    continue\n                basis_matrix[:, offset + i] = (temp / temp_norm).ravel()\n                np.subtract(r, temp, out=r)\n\n                basis_vec = basis_matrix[:, offset + i].reshape(-1, 1)\n                np.subtract(\n                    transformed_psi,\n                    basis_vec @ (basis_vec.T @ psi),\n                    out=transformed_psi,\n                )\n\n                numerator = (r.T @ psi).ravel()\n                denominator = np.sum(psi * transformed_psi, axis=0)\n                np.divide(\n                    numerator,\n                    denominator,\n                    out=q,\n                    where=np.abs(denominator) &gt; eps,\n                )\n\n        selected_indices = selected_indices[selected_indices &gt;= 0].ravel().astype(int)\n        residual_norm = norm(r)\n        theta[selected_indices] = self.estimator.optimize(\n            psi[:, selected_indices], y[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if self.L &gt; 1 and len(selected_indices) &gt; self.k:\n            sorted_local = np.argsort(np.abs(theta[selected_indices]).ravel())[::-1][\n                : self.k\n            ]\n            top_indices = selected_indices[sorted_local]\n            theta_filtered = np.zeros_like(theta)\n            theta_filtered[top_indices] = self.estimator.optimize(\n                psi[:, top_indices], y[self.max_lag :, 0].reshape(-1, 1)\n            )\n            theta = theta_filtered\n            selected_indices = top_indices\n            residual_norm = norm(\n                y[self.max_lag :].reshape(-1, 1)\n                - psi[:, selected_indices] @ theta[selected_indices]\n            )\n\n        pivv = np.argwhere(theta.ravel() != 0).ravel()\n        theta = theta[theta != 0]\n        return theta.reshape(-1, 1), pivv, residual_norm\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: Optional[np.ndarray] = None):\n        \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n        The 'fit' function allows a friendly usage by the user.\n        Given two arguments, x and y, fit training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n        if X is not None:\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        (self.theta, self.pivv, self.res) = self.aols(reg_matrix, y)\n        repetition = len(reg_matrix)\n        if isinstance(self.basis_function, Polynomial):\n            self.final_model = self.regressor_code[self.pivv, :].copy()\n        else:\n            self.regressor_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.final_model = self.regressor_code[self.pivv, :].copy()\n\n        self.n_terms = len(\n            self.theta\n        )  # the number of terms we selected (necessary in the 'results' methods)\n        self.err = self.n_terms * [\n            0\n        ]  # just to use the `results` method. Will be changed in future updates.\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: Optional[np.ndarray] = None,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(\n        self, x_base: Optional[np.ndarray], y: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(\n            x_base, y, self.xlag, self.ylag, self.model_type\n        )\n        x_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(\n        self, x: Optional[np.ndarray], y_initial: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = super()._basis_function_predict(x, y_initial, forecast_horizon)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = super()._basis_function_n_step_prediction(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        yhat = super()._basis_function_n_steps_horizon(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.aols","title":"<code>aols(psi, y)</code>","text":"<p>Perform the Accelerated Orthogonal Least-Squares algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape = n_samples</code> <p>The target data used in the identification process.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated coefficients for the selected regressors.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>residual_norm</code> <code>float</code> <p>The final residual norm.</p> References <ul> <li>Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale    Sparse Reconstruction    https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311</li> </ul> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>def aols(\n    self, psi: np.ndarray, y: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the Accelerated Orthogonal Least-Squares algorithm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = n_samples\n        The target data used in the identification process.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated coefficients for the selected regressors.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    residual_norm : float\n        The final residual norm.\n\n    References\n    ----------\n    - Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale\n       Sparse Reconstruction\n       https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311\n\n    \"\"\"\n    n, m = psi.shape\n    theta = np.zeros([m, 1])\n    r = y[self.max_lag :].reshape(-1, 1).copy()\n    it = 0\n    max_iter = int(min(self.k, np.floor(n / self.L)))\n    selected_indices = np.full(max_iter * self.L, -1, dtype=np.int64)\n    basis_matrix = np.zeros([n, max_iter * self.L])\n    transformed_psi = psi.copy()\n    eps = np.finfo(float).eps\n    numerator = (r.T @ psi).ravel()\n    denominator = np.sum(psi * transformed_psi, axis=0)\n    q = np.zeros_like(numerator)\n    np.divide(numerator, denominator, out=q, where=np.abs(denominator) &gt; eps)\n    while norm(r) &gt; self.threshold and it &lt; max_iter:\n        it = it + 1\n        offset = (it - 1) * self.L\n        if it &gt; 1:\n            basis_vec = basis_matrix[:, offset].reshape(-1, 1)\n            np.subtract(\n                transformed_psi,\n                basis_vec @ (basis_vec.T @ psi),\n                out=transformed_psi,\n            )\n\n        contribution = np.einsum(\"ij,ij-&gt;j\", transformed_psi, transformed_psi) * (\n            q**2\n        )\n        previous = selected_indices[:offset]\n        sub_ind = list(previous[previous &gt;= 0].astype(int))\n        contribution[sub_ind] = 0\n        contribution[~np.isfinite(contribution)] = 0\n        block_size = min(self.L, len(contribution))\n        if block_size == 0:\n            break\n        top_candidates = np.argpartition(contribution, -block_size)[-block_size:]\n        current_indices = top_candidates[\n            contribution[top_candidates].argsort()[::-1]\n        ]\n        selected_indices[offset : offset + block_size] = current_indices\n        for i, idx in enumerate(current_indices):\n            temp = transformed_psi[:, idx].reshape(-1, 1) * q[idx]\n            temp_norm = norm(temp)\n            if temp_norm &lt;= eps:\n                continue\n            basis_matrix[:, offset + i] = (temp / temp_norm).ravel()\n            np.subtract(r, temp, out=r)\n\n            basis_vec = basis_matrix[:, offset + i].reshape(-1, 1)\n            np.subtract(\n                transformed_psi,\n                basis_vec @ (basis_vec.T @ psi),\n                out=transformed_psi,\n            )\n\n            numerator = (r.T @ psi).ravel()\n            denominator = np.sum(psi * transformed_psi, axis=0)\n            np.divide(\n                numerator,\n                denominator,\n                out=q,\n                where=np.abs(denominator) &gt; eps,\n            )\n\n    selected_indices = selected_indices[selected_indices &gt;= 0].ravel().astype(int)\n    residual_norm = norm(r)\n    theta[selected_indices] = self.estimator.optimize(\n        psi[:, selected_indices], y[self.max_lag :, 0].reshape(-1, 1)\n    )\n    if self.L &gt; 1 and len(selected_indices) &gt; self.k:\n        sorted_local = np.argsort(np.abs(theta[selected_indices]).ravel())[::-1][\n            : self.k\n        ]\n        top_indices = selected_indices[sorted_local]\n        theta_filtered = np.zeros_like(theta)\n        theta_filtered[top_indices] = self.estimator.optimize(\n            psi[:, top_indices], y[self.max_lag :, 0].reshape(-1, 1)\n        )\n        theta = theta_filtered\n        selected_indices = top_indices\n        residual_norm = norm(\n            y[self.max_lag :].reshape(-1, 1)\n            - psi[:, selected_indices] @ theta[selected_indices]\n        )\n\n    pivv = np.argwhere(theta.ravel() != 0).ravel()\n    theta = theta[theta != 0]\n    return theta.reshape(-1, 1), pivv, residual_norm\n</code></pre>"},{"location":"user-guide/API/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Fit polynomial NARMAX model using AOLS algorithm.</p> <p>The 'fit' function allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>info_values</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: Optional[np.ndarray] = None):\n    \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n    The 'fit' function allows a friendly usage by the user.\n    Given two arguments, x and y, fit training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n    reg_matrix = self.basis_function.fit(\n        lagged_data,\n        self.max_lag,\n        self.ylag,\n        self.xlag,\n        self.model_type,\n        predefined_regressors=None,\n    )\n\n    if X is not None:\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    (self.theta, self.pivv, self.res) = self.aols(reg_matrix, y)\n    repetition = len(reg_matrix)\n    if isinstance(self.basis_function, Polynomial):\n        self.final_model = self.regressor_code[self.pivv, :].copy()\n    else:\n        self.regressor_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.final_model = self.regressor_code[self.pivv, :].copy()\n\n    self.n_terms = len(\n        self.theta\n    )  # the number of terms we selected (necessary in the 'results' methods)\n    self.err = self.n_terms * [\n        0\n    ]  # just to use the `results` method. Will be changed in future updates.\n    return self\n</code></pre>"},{"location":"user-guide/API/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=0)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: Optional[np.ndarray] = None,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/basis-function/","title":"Documentation for <code>Basis Functions</code>","text":"<p>Bersntein Basis Function for NARMAX models.</p> <p>Bilinear Basis Function for NARMAX models.</p> <p>Fourier Basis Function for NARMAX models.</p> <p>Legendre Basis Function for NARMAX models.</p> <p>Polynomial Basis Function for NARMAX models.</p> <p>Hermite Basis Function for NARMAX models.</p> <p>Hermite Basis Function for NARMAX models.</p> <p>Laguerre Basis Function for NARMAX models.</p>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bernstein.Bernstein","title":"<code>Bernstein</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Bersntein basis function.</p> <p>Generate Bernstein basis functions.</p> <p>This class constructs a new feature matrix consisting of Bernstein basis functions for a given degree. Bernstein polynomials are useful in numerical analysis, curve fitting, and approximation theory due to their smoothness and the ability to approximate any continuous function on a closed interval.</p> <p>The Bernstein polynomial of degree \\(n\\) for a variable \\(x\\) is defined as:</p> \\[     B_{i,n}(x) = \\binom{n}{i} x^i (1 - x)^{n - i} \\quad \\text{for} \\quad i = 0, 1,     \\ldots, n \\] <p>where \\(\\binom{n}{i}\\) is the binomial coefficient, given by:</p> \\[     \\binom{n}{i} = \\frac{n!}{i! (n - i)!} \\] <p>Bernstein polynomials form a basis for the space of polynomials of degree at most \\(n\\). They are particularly useful in approximation theory because they can approximate any continuous function on the interval \\([0, 1]\\) as \\(n\\) increases.</p> <p>Be aware that the number of features in the output array scales significantly with the number of inputs, the maximum lag of the input, and the polynomial degree.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int(max_degree)</code> <p>The maximum degree of the polynomial features.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix. deprecated in v.0.5.0    <code>bias</code> is deprecated in 0.5.0 and will be removed in 0.6.0.    Use <code>include_bias</code> instead.</p> <code>True</code> <code>n</code> <code>int</code> <p>The maximum degree of the bersntein polynomial features. deprecated in v.0.5.0    <code>n</code> is deprecated in 0.5.0 and will be removed in 0.6.0.    Use <code>degree</code> instead.</p> <code>1</code> Notes <p>Be aware that the number of features in the output array scales significantly as the number of inputs, the max lag of the input and output.</p> References <ul> <li>Blog: this method is based on the content provided by Alex Shtoff in his blog.     The content is easy to follow and every user is referred to is blog to check     not only the Bersntein method, but also different topics that Alex discuss     there!     https://alexshtf.github.io/2024/01/21/Bernstein.html</li> <li>Wikipedia: Bernstein polynomial     https://en.wikipedia.org/wiki/Bernstein_polynomial</li> </ul> Source code in <code>sysidentpy/basis_function/_bernstein.py</code> <pre><code>@deprecated(\n    version=\"v0.5.0\",\n    future_version=\"v1.0.0\",\n    message=(\n        \" `bias` and `n` are deprecated in 0.5.0 and will be removed in 1.0.0.\"\n        \" Use `include_bias` and `degree`, respectively, instead.\"\n    ),\n)\nclass Bernstein(BaseBasisFunction):\n    r\"\"\"Build Bersntein basis function.\n\n    Generate Bernstein basis functions.\n\n    This class constructs a new feature matrix consisting of Bernstein basis functions\n    for a given degree. Bernstein polynomials are useful in numerical analysis, curve\n    fitting, and approximation theory due to their smoothness and the ability to\n    approximate any continuous function on a closed interval.\n\n    The Bernstein polynomial of degree \\(n\\) for a variable \\(x\\) is defined as:\n\n    $$\n        B_{i,n}(x) = \\binom{n}{i} x^i (1 - x)^{n - i} \\quad \\text{for} \\quad i = 0, 1,\n        \\ldots, n\n    $$\n\n    where \\(\\binom{n}{i}\\) is the binomial coefficient, given by:\n\n    $$\n        \\binom{n}{i} = \\frac{n!}{i! (n - i)!}\n    $$\n\n    Bernstein polynomials form a basis for the space of polynomials of degree at most\n    \\(n\\). They are particularly useful in approximation theory because they can\n    approximate any continuous function on the interval \\([0, 1]\\) as \\(n\\) increases.\n\n    Be aware that the number of features in the output array scales significantly with\n    the number of inputs, the maximum lag of the input, and the polynomial degree.\n\n    Parameters\n    ----------\n    degree : int (max_degree), default=1\n        The maximum degree of the polynomial features.\n    bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n        deprecated in v.0.5.0\n           `bias` is deprecated in 0.5.0 and will be removed in 0.6.0.\n           Use `include_bias` instead.\n    n : int, default=1\n        The maximum degree of the bersntein polynomial features.\n        deprecated in v.0.5.0\n           `n` is deprecated in 0.5.0 and will be removed in 0.6.0.\n           Use `degree` instead.\n\n    Notes\n    -----\n    Be aware that the number of features in the output array scales\n    significantly as the number of inputs, the max lag of the input and output.\n\n    References\n    ----------\n    - Blog: this method is based on the content provided by Alex Shtoff in his blog.\n        The content is easy to follow and every user is referred to is blog to check\n        not only the Bersntein method, but also different topics that Alex discuss\n        there!\n        https://alexshtf.github.io/2024/01/21/Bernstein.html\n    - Wikipedia: Bernstein polynomial\n        https://en.wikipedia.org/wiki/Bernstein_polynomial\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        n: Optional[int] = None,\n        bias: Optional[bool] = None,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        if n is not None:\n            self.degree = n\n        else:\n            self.degree = degree\n\n        if bias is not None:\n            self.include_bias = bias\n        else:\n            self.include_bias = include_bias\n\n        self.ensemble = ensemble\n\n    def _bernstein_expansion(self, data: np.ndarray):\n        k = np.arange(1 + self.degree)\n        base = binom.pmf(k, self.degree, data[:, None])\n        return base\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because the data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._bernstein_expansion(data[:, col]) for col in range(n_features)]\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Bernstein Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bernstein.Bernstein.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Bernstein Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_bernstein.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Bernstein Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bilinear.Bilinear","title":"<code>Bilinear</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Bilinear basis function.</p> <p>A general bilinear input-output model takes the form</p> \\[     y(k) = a_0 + \\sum_{i=1}^{n_y} a_i y(k-i) + \\sum_{i=1}^{n_u} b_i u(k-i) +     \\sum_{i=1}^{n_y} \\sum_{j=1}^{n_u} c_{ij} y(k-i) u(k-j) \\] <p>This is a special case of the Polynomial NARMAX model.</p> <p>Bilinear system theory has been widely studied and it plays an important role in the context of continuous-time systems.  This is because, roughly speaking, the set of bilinear systems is dense in the space of continuous-time systems and any continuous causal functional can be arbitrarily well approximated by bilinear systems within any bounded time interval (see for example Fliess and Normand-Cyrot 1982). Moreover, many real continuous-time processes are naturally in bilinear form. A few examples are distillation columns (Espa\u00f1a and Landau 1978), nuclear and thermal control processes (Mohler 1973).</p> <p>Sampling the continuous-time bilinear system, however, produces a NARMAX model which is more complex than a discrete-time bilinear model.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int(max_degree)</code> <p>The maximum degree of the polynomial features.</p> <code>2</code> Notes <p>Be aware that the number of features in the output array scales significantly as the number of inputs, the max lag of the input and output, and degree increases. High degrees can cause overfitting.</p> Source code in <code>sysidentpy/basis_function/_bilinear.py</code> <pre><code>class Bilinear(BaseBasisFunction):\n    r\"\"\"Build Bilinear basis function.\n\n    A general bilinear input-output model takes the form\n\n    $$\n        y(k) = a_0 + \\sum_{i=1}^{n_y} a_i y(k-i) + \\sum_{i=1}^{n_u} b_i u(k-i) +\n        \\sum_{i=1}^{n_y} \\sum_{j=1}^{n_u} c_{ij} y(k-i) u(k-j)\n    $$\n\n    This is a special case of the Polynomial NARMAX model.\n\n    Bilinear system theory has been widely studied and it plays an important role in the\n    context of continuous-time systems.  This is because, roughly speaking, the set of\n    bilinear systems is dense in the space of continuous-time systems and any continuous\n    causal functional can be arbitrarily well approximated by bilinear systems within\n    any bounded time interval (see for example Fliess and Normand-Cyrot 1982). Moreover,\n    many real continuous-time processes are naturally in bilinear form. A few examples\n    are distillation columns (Espa\u00f1a and Landau 1978), nuclear and thermal control\n    processes (Mohler 1973).\n\n    Sampling the continuous-time bilinear system, however, produces a NARMAX model\n    which is more complex than a discrete-time bilinear model.\n\n    Parameters\n    ----------\n    degree : int (max_degree), default=2\n        The maximum degree of the polynomial features.\n\n    Notes\n    -----\n    Be aware that the number of features in the output array scales\n    significantly as the number of inputs, the max lag of the input and output, and\n    degree increases. High degrees can cause overfitting.\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 2,\n    ):\n        self.degree = degree\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build the Bilinear information matrix.\n\n        Each column of the information matrix represents a candidate\n        regressor. The set of candidate regressors are based on xlag,\n        ylag, and degree defined by the user.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Target data used on training phase.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors : ndarray of int\n            The index of the selected regressors by the Model Structure\n            Selection algorithm.\n\n        Returns\n        -------\n        psi = ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        \"\"\"\n        # Create combinations of all columns based on its index\n        iterable_list = range(data.shape[1])\n        combination_list = list(\n            combinations_with_replacement(iterable_list, self.degree)\n        )\n        if self.degree == 1:\n            warnings.warn(\n                \"You choose a bilinear basis function and nonlinear degree = 1.\"\n                \"In this case, you have a linear polynomial model.\",\n                stacklevel=2,\n            )\n\n        ny = get_max_ylag(ylag)\n        combination_ylag = list(\n            combinations_with_replacement(list(range(1, ny + 1)), self.degree)\n        )\n        if isinstance(xlag, int):\n            xlag = [xlag]\n\n        combination_xlag = []\n        ni = 0\n        for lag in xlag:\n            nx = get_max_xlag(lag)\n            combination_lag = list(\n                combinations_with_replacement(\n                    list(range(ny + 1 + ni, nx + ny + 1 + ni)), self.degree\n                )\n            )\n            combination_xlag.append(combination_lag)\n            ni += nx\n\n        combination_xlag = list(chain.from_iterable(combination_xlag))\n        combinations_xy = combination_xlag + combination_ylag\n        combination_list = list(set(combination_list) - set(combinations_xy))\n\n        if predefined_regressors is not None:\n            combination_list = [\n                combination_list[index] for index in predefined_regressors\n            ]\n\n        psi = np.column_stack(\n            [\n                np.prod(data[:, combination_list[i]], axis=1)\n                for i in range(len(combination_list))\n            ]\n        )\n        psi = psi[max_lag:, :]\n        return psi\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Polynomial Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bilinear.Bilinear.fit","title":"<code>fit(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build the Bilinear information matrix.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree defined by the user.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Target data used on training phase.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>ndarray of int</code> <p>The index of the selected regressors by the Model Structure Selection algorithm.</p> <code>None</code> <p>Returns:</p> Type Description <code>psi = ndarray of floats</code> <p>The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/basis_function/_bilinear.py</code> <pre><code>def fit(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build the Bilinear information matrix.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree defined by the user.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Target data used on training phase.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors : ndarray of int\n        The index of the selected regressors by the Model Structure\n        Selection algorithm.\n\n    Returns\n    -------\n    psi = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # Create combinations of all columns based on its index\n    iterable_list = range(data.shape[1])\n    combination_list = list(\n        combinations_with_replacement(iterable_list, self.degree)\n    )\n    if self.degree == 1:\n        warnings.warn(\n            \"You choose a bilinear basis function and nonlinear degree = 1.\"\n            \"In this case, you have a linear polynomial model.\",\n            stacklevel=2,\n        )\n\n    ny = get_max_ylag(ylag)\n    combination_ylag = list(\n        combinations_with_replacement(list(range(1, ny + 1)), self.degree)\n    )\n    if isinstance(xlag, int):\n        xlag = [xlag]\n\n    combination_xlag = []\n    ni = 0\n    for lag in xlag:\n        nx = get_max_xlag(lag)\n        combination_lag = list(\n            combinations_with_replacement(\n                list(range(ny + 1 + ni, nx + ny + 1 + ni)), self.degree\n            )\n        )\n        combination_xlag.append(combination_lag)\n        ni += nx\n\n    combination_xlag = list(chain.from_iterable(combination_xlag))\n    combinations_xy = combination_xlag + combination_ylag\n    combination_list = list(set(combination_list) - set(combinations_xy))\n\n    if predefined_regressors is not None:\n        combination_list = [\n            combination_list[index] for index in predefined_regressors\n        ]\n\n    psi = np.column_stack(\n        [\n            np.prod(data[:, combination_list[i]], axis=1)\n            for i in range(len(combination_list))\n        ]\n    )\n    psi = psi[max_lag:, :]\n    return psi\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bilinear.Bilinear.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Polynomial Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_bilinear.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Polynomial Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._fourier.Fourier","title":"<code>Fourier</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Fourier basis function.</p> <p>Generate a new feature matrix consisting of all Fourier features with respect to the number of harmonics.</p> <p>The Fourier expansion is given by:</p> <p>If you set \\(\\mathcal{F}\\) as the Fourier extension</p> \\[ \\mathcal{F}(x) = [\\cos(\\pi x), \\sin(\\pi x), \\cos(2\\pi x), \\sin(2\\pi x), \\ldots, \\cos(N\\pi x), \\sin(N\\pi x)] \\] <p>In this case, the Fourier ARX representation will be:</p> \\[\\begin{aligned} y_k = &amp;\\Big[ \\cos(\\pi y_{k-1}), \\sin(\\pi y_{k-1}), \\cos(2\\pi y_{k-1}), \\sin(2\\pi y_{k-1}), \\ldots, \\cos(N\\pi y_{k-1}), \\sin(N\\pi y_{k-1}), \\\\ &amp;\\ \\ \\cos(\\pi y_{k-2}), \\sin(\\pi y_{k-2}), \\ldots, \\cos(N\\pi y_{k-n_y}), \\sin(N\\pi y_{k-n_y}), \\\\ &amp;\\ \\ \\cos(\\pi x_{k-1}), \\sin(\\pi x_{k-1}), \\cos(2\\pi x_{k-1}), \\sin(2\\pi x_{k-1}), \\ldots, \\cos(N\\pi x_{k-n_x}), \\sin(N\\pi x_{k-n_x}) \\Big] \\\\ &amp;\\ \\ + e_k \\end{aligned}\\] <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int(max_degree)</code> <p>The maximum degree of the polynomial features.</p> <code>2</code> Notes <p>Be aware that the number of features in the output array scales significantly as the number of inputs, the max lag of the input and output.</p> Source code in <code>sysidentpy/basis_function/_fourier.py</code> <pre><code>class Fourier(BaseBasisFunction):\n    r\"\"\"Build Fourier basis function.\n\n    Generate a new feature matrix consisting of all Fourier features\n    with respect to the number of harmonics.\n\n    The Fourier expansion is given by:\n\n    If you set $\\mathcal{F}$ as the Fourier extension\n\n    $$\n    \\mathcal{F}(x) = [\\cos(\\pi x), \\sin(\\pi x), \\cos(2\\pi x), \\sin(2\\pi x), \\ldots,\n    \\cos(N\\pi x), \\sin(N\\pi x)]\n    $$\n\n    In this case, the Fourier ARX representation will be:\n\n    \\begin{aligned}\n    y_k = &amp;\\Big[ \\cos(\\pi y_{k-1}), \\sin(\\pi y_{k-1}), \\cos(2\\pi y_{k-1}),\n    \\sin(2\\pi y_{k-1}), \\ldots, \\cos(N\\pi y_{k-1}), \\sin(N\\pi y_{k-1}), \\\\\n    &amp;\\ \\ \\cos(\\pi y_{k-2}), \\sin(\\pi y_{k-2}), \\ldots, \\cos(N\\pi y_{k-n_y}),\n    \\sin(N\\pi y_{k-n_y}), \\\\\n    &amp;\\ \\ \\cos(\\pi x_{k-1}), \\sin(\\pi x_{k-1}), \\cos(2\\pi x_{k-1}), \\sin(2\\pi x_{k-1}),\n    \\ldots, \\cos(N\\pi x_{k-n_x}), \\sin(N\\pi x_{k-n_x}) \\Big] \\\\\n    &amp;\\ \\ + e_k\n    \\end{aligned}\n\n    Parameters\n    ----------\n    degree : int (max_degree), default=2\n        The maximum degree of the polynomial features.\n\n    Notes\n    -----\n    Be aware that the number of features in the output array scales\n    significantly as the number of inputs, the max lag of the input and output.\n\n    \"\"\"\n\n    def __init__(\n        self, n: int = 1, p: float = 2 * np.pi, degree: int = 1, ensemble: bool = True\n    ):\n        self.n = n\n        self.p = p\n        self.degree = degree\n        self.ensemble = ensemble\n\n    def _fourier_expansion(self, data: np.ndarray, n: int):\n        base = np.column_stack(\n            [\n                np.cos(2 * np.pi * data * n / self.p),\n                np.sin(2 * np.pi * data * n / self.p),\n            ]\n        )\n        return base\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build the Polynomial information matrix.\n\n        Each column of the information matrix represents a candidate\n        regressor. The set of candidate regressors are based on xlag,\n        ylag, and degree defined by the user.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Target data used on training phase.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors : ndarray of int\n            The index of the selected regressors by the Model Structure\n            Selection algorithm.\n\n        Returns\n        -------\n        psi = ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        \"\"\"\n        # remove intercept (because the data always have the intercept)\n        if self.degree &gt; 1:\n            data = Polynomial().fit(\n                data, max_lag, ylag, xlag, model_type, predefined_regressors=None\n            )\n            data = data[:, 1:]\n        else:\n            data = data[max_lag:, 1:]\n\n        columns = list(range(data.shape[1]))\n        harmonics = list(range(1, self.n + 1))\n        psi = np.zeros([len(data), 1])\n\n        for col in columns:\n            base_col = np.column_stack(\n                [self._fourier_expansion(data[:, col], h) for h in harmonics]\n            )\n            psi = np.column_stack([psi, base_col])\n\n        if self.ensemble:\n            psi = psi[:, 1:]\n            psi = np.column_stack([data, psi])\n        else:\n            psi = psi[:, 1:]\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Fourier Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._fourier.Fourier.fit","title":"<code>fit(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build the Polynomial information matrix.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree defined by the user.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Target data used on training phase.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>ndarray of int</code> <p>The index of the selected regressors by the Model Structure Selection algorithm.</p> <code>None</code> <p>Returns:</p> Type Description <code>psi = ndarray of floats</code> <p>The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/basis_function/_fourier.py</code> <pre><code>def fit(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build the Polynomial information matrix.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree defined by the user.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Target data used on training phase.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors : ndarray of int\n        The index of the selected regressors by the Model Structure\n        Selection algorithm.\n\n    Returns\n    -------\n    psi = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # remove intercept (because the data always have the intercept)\n    if self.degree &gt; 1:\n        data = Polynomial().fit(\n            data, max_lag, ylag, xlag, model_type, predefined_regressors=None\n        )\n        data = data[:, 1:]\n    else:\n        data = data[max_lag:, 1:]\n\n    columns = list(range(data.shape[1]))\n    harmonics = list(range(1, self.n + 1))\n    psi = np.zeros([len(data), 1])\n\n    for col in columns:\n        base_col = np.column_stack(\n            [self._fourier_expansion(data[:, col], h) for h in harmonics]\n        )\n        psi = np.column_stack([psi, base_col])\n\n    if self.ensemble:\n        psi = psi[:, 1:]\n        psi = np.column_stack([data, psi])\n    else:\n        psi = psi[:, 1:]\n\n    if predefined_regressors is None:\n        return psi\n\n    return psi[:, predefined_regressors]\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._fourier.Fourier.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Fourier Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_fourier.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Fourier Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._legendre.Legendre","title":"<code>Legendre</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Legendre basis function expansion.</p> <p>This class constructs a feature matrix consisting of Legendre polynomial basis functions up to a specified degree. Legendre polynomials, denoted by \\(P_n(x)\\), are orthogonal polynomials over the interval \\([-1, 1]\\) with respect to the uniform weight function \\(w(x) = 1\\). They are widely used in numerical analysis, curve fitting, and approximation theory.</p> <p>The Legendre polynomial \\(P_n(x)\\) of degree \\(n\\) is defined by the following recurrence relation:</p> \\[ P_0(x) = 1 \\] \\[ P_1(x) = x \\] \\[ (n+1) P_{n+1}(x) = (2n + 1)x P_n(x) - n P_{n-1}(x) \\] <p>where \\(P_n(x)\\) represents the Legendre polynomial of degree \\(n\\).</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>The maximum degree of the Legendre polynomial basis functions to be generated.</p> <code>2</code> <code>include_bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix.</p> <code>True</code> <code>ensemble</code> <code>bool</code> <p>If True, the original data is concatenated with the polynomial features.</p> <code>False</code> Notes <p>The number of features in the output matrix increases as the degree of the polynomial increases, which can lead to a high-dimensional feature space. Consider using dimensionality reduction techniques if overfitting becomes an issue.</p> References <ul> <li>Wikipedia: Legendre polynomial     https://en.wikipedia.org/wiki/Legendre_polynomials</li> </ul> Source code in <code>sysidentpy/basis_function/_legendre.py</code> <pre><code>class Legendre(BaseBasisFunction):\n    r\"\"\"Build Legendre basis function expansion.\n\n    This class constructs a feature matrix consisting of Legendre polynomial basis\n    functions up to a specified degree. Legendre polynomials, denoted by $P_n(x)$,\n    are orthogonal polynomials over the interval $[-1, 1]$ with respect to the\n    uniform weight function $w(x) = 1$. They are widely used in numerical analysis,\n    curve fitting, and approximation theory.\n\n    The Legendre polynomial $P_n(x)$ of degree $n$ is defined by the following\n    recurrence relation:\n\n    $$\n    P_0(x) = 1\n    $$\n\n    $$\n    P_1(x) = x\n    $$\n\n    $$\n    (n+1) P_{n+1}(x) = (2n + 1)x P_n(x) - n P_{n-1}(x)\n    $$\n\n    where $P_n(x)$ represents the Legendre polynomial of degree $n$.\n\n    Parameters\n    ----------\n    degree : int, default=2\n        The maximum degree of the Legendre polynomial basis functions to be generated.\n\n    include_bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n\n    ensemble : bool, default=False\n        If True, the original data is concatenated with the polynomial features.\n\n    Notes\n    -----\n    The number of features in the output matrix increases as the degree of the\n    polynomial increases, which can lead to a high-dimensional feature space.\n    Consider using dimensionality reduction techniques if overfitting becomes an issue.\n\n    References\n    ----------\n    - Wikipedia: Legendre polynomial\n        https://en.wikipedia.org/wiki/Legendre_polynomials\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        self.degree = degree\n        self.include_bias = include_bias\n        self.ensemble = ensemble\n\n    def _legendre_expansion(self, data: np.ndarray):\n        num_samples = data.shape[0]\n        basis = np.zeros((num_samples, self.degree + 1))\n        for n in range(self.degree + 1):\n            basis[:, n] = eval_legendre(n, data)\n        return basis\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._legendre_expansion(data[:, col]) for col in range(n_features)]\n        # remove P0(x) = 1 from every column expansion\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Legendre Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._legendre.Legendre.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Legendre Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_legendre.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Legendre Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._polynomial.Polynomial","title":"<code>Polynomial</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build polynomial basis function.</p> <p>Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree.</p> \\[     y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}u_{k-j}^{b_i, j}     \\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\] <p>where \\(p\\) is the number of regressors, \\(\\Theta_i\\) are the model parameters, and \\(a_i, m, b_i, j\\) and \\(d_i, l \\in \\mathbb{N}\\) are the exponents of the output, input and noise terms, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int(max_degree)</code> <p>The maximum degree of the polynomial features.</p> <code>2</code> Notes <p>Be aware that the number of features in the output array scales significantly as the number of inputs, the max lag of the input and output, and degree increases. High degrees can cause overfitting.</p> Source code in <code>sysidentpy/basis_function/_polynomial.py</code> <pre><code>class Polynomial(BaseBasisFunction):\n    r\"\"\"Build polynomial basis function.\n\n    Generate a new feature matrix consisting of all polynomial combinations\n    of the features with degree less than or equal to the specified degree.\n\n    $$\n        y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}u_{k-j}^{b_i, j}\n        \\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m}\n    $$\n\n    where $p$ is the number of regressors, $\\Theta_i$ are the\n    model parameters, and $a_i, m, b_i, j$ and $d_i, l \\in \\mathbb{N}$\n    are the exponents of the output, input and noise terms, respectively.\n\n    Parameters\n    ----------\n    degree : int (max_degree), default=2\n        The maximum degree of the polynomial features.\n\n    Notes\n    -----\n    Be aware that the number of features in the output array scales\n    significantly as the number of inputs, the max lag of the input and output, and\n    degree increases. High degrees can cause overfitting.\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 2,\n    ):\n        self.degree = degree\n        # Cache combination indices per (n_features, degree) to avoid rebuilding\n        self._combination_cache: Dict[Tuple[int, int], np.ndarray] = {}\n\n    def _get_combination_indices(self, n_features: int) -&gt; np.ndarray:\n        \"\"\"Return cached column-index combinations for the current degree.\"\"\"\n        key = (n_features, self.degree)\n        if key not in self._combination_cache:\n            iterable = range(n_features)\n            combos = np.array(\n                list(combinations_with_replacement(iterable, self.degree)),\n                dtype=np.int32,\n            )\n            self._combination_cache[key] = combos\n        return self._combination_cache[key]\n\n    def _evaluate_terms(\n        self,\n        data: np.ndarray,\n        predefined_regressors: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Vectorized polynomial feature construction without Python loops.\"\"\"\n        n_features = data.shape[1]\n        combos = self._get_combination_indices(n_features)\n        if predefined_regressors is not None:\n            combos = combos[np.asarray(predefined_regressors, dtype=int)]\n\n        # Start with ones so we can multiply each degree slice in place\n        n_samples = data.shape[0]\n        n_terms = combos.shape[0]\n        psi = np.ones((n_samples, n_terms), dtype=data.dtype)\n\n        # Multiply column-wise using the cached combination indices\n        for degree_idx in range(self.degree):\n            cols = combos[:, degree_idx]\n            psi *= data[:, cols]\n\n        return psi\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build the Polynomial information matrix.\n\n        Each columns of the information matrix represents a candidate\n        regressor. The set of candidate regressors are based on xlag,\n        ylag, and degree defined by the user.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Target data used on training phase.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors : ndarray of int\n            The index of the selected regressors by the Model Structure\n            Selection algorithm.\n\n        Returns\n        -------\n        psi = ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        \"\"\"\n        # Create combinations of all columns based on its index\n        psi = self._evaluate_terms(data, predefined_regressors)\n        return psi[max_lag:, :]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Polynomial Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._polynomial.Polynomial.fit","title":"<code>fit(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build the Polynomial information matrix.</p> <p>Each columns of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree defined by the user.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Target data used on training phase.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>ndarray of int</code> <p>The index of the selected regressors by the Model Structure Selection algorithm.</p> <code>None</code> <p>Returns:</p> Type Description <code>psi = ndarray of floats</code> <p>The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/basis_function/_polynomial.py</code> <pre><code>def fit(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build the Polynomial information matrix.\n\n    Each columns of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree defined by the user.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Target data used on training phase.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors : ndarray of int\n        The index of the selected regressors by the Model Structure\n        Selection algorithm.\n\n    Returns\n    -------\n    psi = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # Create combinations of all columns based on its index\n    psi = self._evaluate_terms(data, predefined_regressors)\n    return psi[max_lag:, :]\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._polynomial.Polynomial.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Polynomial Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_polynomial.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Polynomial Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._hermite.Hermite","title":"<code>Hermite</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Hermite basis function expansion.</p> <p>This class constructs a feature matrix consisting of Hermite polynomial basis functions up to a specified degree. Hermite polynomials, denoted by \\(H_n(x)\\), are orthogonal polynomials over the interval \\((-\\infty, \\infty)\\) with respect to the weight function \\(w(x) = e^{-x^2}\\). These polynomials are widely used in probability theory, quantum mechanics, and numerical analysis, particularly in solving the quantum harmonic oscillator and in the field of statistics.</p> <p>Physicist's Hermite polynomials \\(H_n(x)\\), often used in physics: $$ H_n(x) = (-1)^n e<sup>{x</sup>2} \\frac{d<sup>n}{dx</sup>n} e<sup>{-x</sup>2} $$</p> <p>The Hermite polynomial \\(H_n(x)\\) of degree \\(n\\) can be also defined by the following recurrence relation:</p> \\[ H_0(x) = 1 \\] \\[ H_1(x) = 2x \\] \\[ H_{n+1}(x) = 2x H_n(x) - 2n H_{n-1}(x) \\] <p>where \\(H_n(x)\\) represents the Hermite polynomial of degree \\(n\\).</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>The maximum degree of the Hermite polynomial basis functions to be generated.</p> <code>2</code> <code>include_bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix.</p> <code>True</code> <code>ensemble</code> <code>bool</code> <p>If True, the original data is concatenated with the polynomial features.</p> <code>False</code> Notes <p>The number of features in the output matrix increases as the degree of the polynomial increases, which can lead to a high-dimensional feature space. Consider using dimensionality reduction techniques if overfitting becomes an issue.</p> References <ul> <li>Wikipedia: Hermite polynomial     https://en.wikipedia.org/wiki/Hermite_polynomials</li> <li>Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_hermite.html</li> </ul> Source code in <code>sysidentpy/basis_function/_hermite.py</code> <pre><code>class Hermite(BaseBasisFunction):\n    r\"\"\"Build Hermite basis function expansion.\n\n    This class constructs a feature matrix consisting of Hermite polynomial basis\n    functions up to a specified degree. Hermite polynomials, denoted by $H_n(x)$,\n    are orthogonal polynomials over the interval $(-\\infty, \\infty)$ with respect\n    to the weight function $w(x) = e^{-x^2}$. These polynomials are widely used in\n    probability theory, quantum mechanics, and numerical analysis, particularly in\n    solving the quantum harmonic oscillator and in the field of statistics.\n\n    **Physicist's Hermite polynomials** $H_n(x)$, often used in physics:\n    $$\n    H_n(x) = (-1)^n e^{x^2} \\frac{d^n}{dx^n} e^{-x^2}\n    $$\n\n    The Hermite polynomial $H_n(x)$ of degree $n$ can be also defined by the following\n    recurrence relation:\n\n    $$\n    H_0(x) = 1\n    $$\n\n    $$\n    H_1(x) = 2x\n    $$\n\n    $$\n    H_{n+1}(x) = 2x H_n(x) - 2n H_{n-1}(x)\n    $$\n\n    where $H_n(x)$ represents the Hermite polynomial of degree $n$.\n\n    Parameters\n    ----------\n    degree : int, default=2\n        The maximum degree of the Hermite polynomial basis functions to be generated.\n\n    include_bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n\n    ensemble : bool, default=False\n        If True, the original data is concatenated with the polynomial features.\n\n    Notes\n    -----\n    The number of features in the output matrix increases as the degree of the\n    polynomial increases, which can lead to a high-dimensional feature space.\n    Consider using dimensionality reduction techniques if overfitting becomes an issue.\n\n    References\n    ----------\n    - Wikipedia: Hermite polynomial\n        https://en.wikipedia.org/wiki/Hermite_polynomials\n    - Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_hermite.html\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        self.degree = degree\n        self.include_bias = include_bias\n        self.ensemble = ensemble\n\n    def _hermite_expansion(self, data: np.ndarray):\n        num_samples = data.shape[0]\n        basis = np.zeros((num_samples, self.degree + 1))\n        for n in range(self.degree + 1):\n            basis[:, n] = eval_hermite(n, data)\n        return basis\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._hermite_expansion(data[:, col]) for col in range(n_features)]\n        # remove P0(x) = 1 from every column expansion\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Hermite Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._hermite.Hermite.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Hermite Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_hermite.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Hermite Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._hermite_normalized.HermiteNormalized","title":"<code>HermiteNormalized</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build probabilist's Normalized Hermite basis function expansion.</p> <p>This class constructs a feature matrix consisting of Hermite Normalized polynomial basis functions up to a specified degree. Hermite (normalized) polynomials, denoted by \\(H_n(x)\\), are orthogonal polynomials over the interval \\((-\\infty, \\infty)\\) with respect to the weight function \\(w(x) = e^{-x^2}\\). These polynomials are widely used in probability theory, quantum mechanics, and numerical analysis, particularly in solving the quantum harmonic oscillator and in the field of statistics.</p> <p>Probabilist's Hermite polynomials \\(He_n(x)\\): $$ He_n(x) = (-1)^n e<sup>{x</sup>2/2} \\frac{d<sup>n}{dx</sup>n} e<sup>{-x</sup>2/2} $$</p> <p>where \\(He_n(x)\\) represents the probabilist's Normalized Hermite polynomial of degree \\(n\\).</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>The maximum degree of the Hermite polynomial basis functions to be generated.</p> <code>2</code> <code>include_bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix.</p> <code>True</code> <code>ensemble</code> <code>bool</code> <p>If True, the original data is concatenated with the polynomial features.</p> <code>False</code> Notes <p>The number of features in the output matrix increases as the degree of the polynomial increases, which can lead to a high-dimensional feature space. Consider using dimensionality reduction techniques if overfitting becomes an issue.</p> References <ul> <li>Wikipedia: Hermite polynomial     https://en.wikipedia.org/wiki/Hermite_polynomials</li> <li>Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_hermitenorm.html</li> </ul> Source code in <code>sysidentpy/basis_function/_hermite_normalized.py</code> <pre><code>class HermiteNormalized(BaseBasisFunction):\n    r\"\"\"Build probabilist's Normalized Hermite basis function expansion.\n\n    This class constructs a feature matrix consisting of Hermite Normalized polynomial\n    basis functions up to a specified degree. Hermite (normalized) polynomials,\n    denoted by $H_n(x)$, are orthogonal polynomials over the interval\n    $(-\\infty, \\infty)$ with respect to the weight function $w(x) = e^{-x^2}$.\n    These polynomials are widely used in probability theory, quantum mechanics,\n    and numerical analysis, particularly in solving the quantum harmonic oscillator\n    and in the field of statistics.\n\n    Probabilist's Hermite polynomials $He_n(x)$:\n    $$\n    He_n(x) = (-1)^n e^{x^2/2} \\frac{d^n}{dx^n} e^{-x^2/2}\n    $$\n\n    where $He_n(x)$ represents the probabilist's Normalized Hermite polynomial\n    of degree $n$.\n\n    Parameters\n    ----------\n    degree : int, default=2\n        The maximum degree of the Hermite polynomial basis functions to be generated.\n\n    include_bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n\n    ensemble : bool, default=False\n        If True, the original data is concatenated with the polynomial features.\n\n    Notes\n    -----\n    The number of features in the output matrix increases as the degree of the\n    polynomial increases, which can lead to a high-dimensional feature space.\n    Consider using dimensionality reduction techniques if overfitting becomes an issue.\n\n    References\n    ----------\n    - Wikipedia: Hermite polynomial\n        https://en.wikipedia.org/wiki/Hermite_polynomials\n    - Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_hermitenorm.html\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        self.degree = degree\n        self.include_bias = include_bias\n        self.ensemble = ensemble\n\n    def _hermitenorm_expansion(self, data: np.ndarray):\n        num_samples = data.shape[0]\n        basis = np.zeros((num_samples, self.degree + 1))\n        for n in range(self.degree + 1):\n            basis[:, n] = eval_hermitenorm(n, data)\n        return basis\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._hermitenorm_expansion(data[:, col]) for col in range(n_features)]\n        # remove P0(x) = 1 from every column expansion\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Hermite Normalized Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._hermite_normalized.HermiteNormalized.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Hermite Normalized Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_hermite_normalized.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Hermite Normalized Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._laguerre.Laguerre","title":"<code>Laguerre</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Laguerre basis function expansion.</p> <p>This class constructs a feature matrix consisting of Laguerre polynomial basis functions up to a specified degree. Laguerre polynomials, denoted by \\(L_n(x)\\), are orthogonal polynomials over the interval \\([0, \\infty)\\) with respect to the weight function \\(w(x) = e^{-x}\\). These polynomials are commonly used in physics, particularly in quantum mechanics, and in numerical analysis.</p> <p>The Laguerre polynomial \\(L_n(x)\\) of degree \\(n\\) is defined by the following recurrence relation:</p> \\[ L_0(x) = 1 \\] \\[ L_1(x) = 1 - x \\] \\[ (n+1) L_{n+1}(x) = (2n + 1 - x) L_n(x) - n L_{n-1}(x) \\] <p>where \\(L_n(x)\\) represents the Laguerre polynomial of degree \\(n\\).</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>The maximum degree of the Laguerre polynomial basis functions to be generated.</p> <code>2</code> <code>include_bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix.</p> <code>True</code> <code>ensemble</code> <code>bool</code> <p>If True, the original data is concatenated with the polynomial features.</p> <code>False</code> Notes <p>The number of features in the output matrix increases as the degree of the polynomial increases, which can lead to a high-dimensional feature space. Consider using dimensionality reduction techniques if overfitting becomes an issue.</p> References <ul> <li>Wikipedia: Laguerre polynomial     https://en.wikipedia.org/wiki/Laguerre_polynomials</li> <li>Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_laguerre.html</li> <li>Milton Abramowitz and Irene A. Stegun, eds. Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover, 1972.</li> </ul> Source code in <code>sysidentpy/basis_function/_laguerre.py</code> <pre><code>class Laguerre(BaseBasisFunction):\n    r\"\"\"Build Laguerre basis function expansion.\n\n    This class constructs a feature matrix consisting of Laguerre polynomial basis\n    functions up to a specified degree. Laguerre polynomials, denoted by $L_n(x)$,\n    are orthogonal polynomials over the interval $[0, \\infty)$ with respect to the\n    weight function $w(x) = e^{-x}$. These polynomials are commonly used in\n    physics, particularly in quantum mechanics, and in numerical analysis.\n\n    The Laguerre polynomial $L_n(x)$ of degree $n$ is defined by the following\n    recurrence relation:\n\n    $$\n    L_0(x) = 1\n    $$\n\n    $$\n    L_1(x) = 1 - x\n    $$\n\n    $$\n    (n+1) L_{n+1}(x) = (2n + 1 - x) L_n(x) - n L_{n-1}(x)\n    $$\n\n    where $L_n(x)$ represents the Laguerre polynomial of degree $n$.\n\n    Parameters\n    ----------\n    degree : int, default=2\n        The maximum degree of the Laguerre polynomial basis functions to be generated.\n\n    include_bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n\n    ensemble : bool, default=False\n        If True, the original data is concatenated with the polynomial features.\n\n    Notes\n    -----\n    The number of features in the output matrix increases as the degree of the\n    polynomial increases, which can lead to a high-dimensional feature space.\n    Consider using dimensionality reduction techniques if overfitting becomes an issue.\n\n    References\n    ----------\n    - Wikipedia: Laguerre polynomial\n        https://en.wikipedia.org/wiki/Laguerre_polynomials\n    - Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_laguerre.html\n    - Milton Abramowitz and Irene A. Stegun, eds. Handbook of Mathematical Functions\n    with Formulas, Graphs, and Mathematical Tables. New York: Dover, 1972.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        self.degree = degree\n        self.include_bias = include_bias\n        self.ensemble = ensemble\n\n    def _laguerre_expansion(self, data: np.ndarray):\n        num_samples = data.shape[0]\n        basis = np.zeros((num_samples, self.degree + 1))\n        for n in range(self.degree + 1):\n            basis[:, n] = eval_laguerre(n, data)\n        return basis\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._laguerre_expansion(data[:, col]) for col in range(n_features)]\n        # remove P0(x) = 1 from every column expansion\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Laguerre Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._laguerre.Laguerre.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Laguerre Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_laguerre.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Laguerre Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/entropic-regression/","title":"Documentation for <code>Entropic Regression</code>","text":"<p>Build Polynomial NARMAX Models using the Entropic Regression algorithm.</p>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER","title":"<code>ER</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Entropic Regression Algorithm.</p> <p>Build Polynomial NARMAX model using the Entropic Regression Algorithm ([1]_). This algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p> <p>The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},     e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>k</code> <code>int</code> <p>The kth nearest neighbor to be used in estimation.</p> <code>2</code> <code>q</code> <code>float</code> <p>Quantile to compute, which must be between 0 and 1 inclusive.</p> <code>0.99</code> <code>p</code> <code>default=inf,</code> <p>Lp Measure of the distance in Knn estimator.</p> <code>inf</code> <code>n_perm</code> <code>int</code> <p>Number of permutation to be used in shuffle test</p> <code>200</code> <code>estimator</code> <code>str</code> <p>The parameter estimation method.</p> <code>\"least_squares\"</code> <code>skip_forward</code> <code>bool</code> <p>To be used for difficult and highly uncertain problems. Skipping the forward selection results in more accurate solution, but comes with higher computational cost.</p> <code>False</code> <code>model_type</code> <code>str</code> <p>The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models</p> <code>'NARMAX'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import ER\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n...                                                    colored_noise=True,\n...                                                    sigma=0.2,\n...                                                    train_percentage=90)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = ER(basis_function=basis_function,\n...              ylag=2, xlag=2\n...              )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>class ER(BaseMSS):\n    r\"\"\"Entropic Regression Algorithm.\n\n    Build Polynomial NARMAX model using the Entropic Regression Algorithm ([1]_).\n    This algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},\n        e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    k : int, default=2\n        The kth nearest neighbor to be used in estimation.\n    q : float, default=0.99\n        Quantile to compute, which must be between 0 and 1 inclusive.\n    p : default=inf,\n        Lp Measure of the distance in Knn estimator.\n    n_perm: int, default=200\n        Number of permutation to be used in shuffle test\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    skip_forward = bool, default=False\n        To be used for difficult and highly uncertain problems.\n        Skipping the forward selection results in more accurate solution,\n        but comes with higher computational cost.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import ER\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = ER(basis_function=basis_function,\n    ...              ylag=2, xlag=2\n    ...              )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 1,\n        xlag: Union[int, list] = 1,\n        q: float = 0.99,\n        estimator: Estimators_Union = LeastSquares(),\n        h: float = 0.01,\n        k: int = 2,\n        mutual_information_estimator: str = \"mutual_information_knn\",\n        n_perm: int = 200,\n        p: float = np.inf,\n        skip_forward: bool = False,\n        model_type: str = \"NARMAX\",\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        random_state: Optional[int] = None,\n    ):\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.xlag = xlag\n        self.ylag = ylag\n        self.non_degree = basis_function.degree\n        self.max_lag = self._get_max_lag()\n        self.k = k\n        self.estimator = estimator\n        self.q = q\n        self.h = h\n        self.mutual_information_estimator = mutual_information_estimator\n        self.n_perm = n_perm\n        self.p = p\n        self.skip_forward = skip_forward\n        self.random_state = random_state\n        self.rng = check_random_state(random_state)\n        self.tol = None\n        self.n_inputs = None\n        self.estimated_tolerance = None\n        self.regressor_code = None\n        self.final_model = None\n        self.theta = None\n        self.n_terms = None\n        self.err = None\n        self.pivv = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if not isinstance(self.k, int) or self.k &lt; 1:\n            raise ValueError(f\"k must be integer and &gt; zero. Got {self.k}\")\n\n        if not isinstance(self.n_perm, int) or self.n_perm &lt; 1:\n            raise ValueError(f\"n_perm must be integer and &gt; zero. Got {self.n_perm}\")\n\n        if not isinstance(self.q, float) or self.q &gt; 1 or self.q &lt;= 0:\n            raise ValueError(\n                f\"q must be float and must be between 0 and 1 inclusive. Got {self.q}\"\n            )\n\n        if not isinstance(self.skip_forward, bool):\n            raise TypeError(\n                f\"skip_forward must be False or True. Got {self.skip_forward}\"\n            )\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def mutual_information_knn(self, y, y_perm):\n        \"\"\"Find the mutual information.\n\n        Finds the mutual information between $x$ and $y$ given $z$.\n\n        This code is based on Matlab Entropic Regression package.\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The source signal.\n        y_perm : ndarray of floats\n            The destination signal.\n\n        Returns\n        -------\n        ksg_estimation : float\n            The conditioned mutual information.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        joint_space = np.concatenate([y, y_perm], axis=1)\n        smallest_distance = np.sort(\n            cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n        )\n        idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n        idx = idx[np.argsort(smallest_distance[-1, idx])]\n        smallest_distance = smallest_distance[:, idx]\n        epsilon = smallest_distance[:, -1].reshape(-1, 1)\n        smallest_distance_y = cdist(y, y, \"minkowski\", p=self.p)\n        less_than_array_nx = np.array((smallest_distance_y &lt; epsilon)).astype(int)\n        nx = (np.sum(less_than_array_nx, axis=1) - 1).reshape(-1, 1)\n        smallest_distance_y_perm = cdist(y_perm, y_perm, \"minkowski\", p=self.p)\n        less_than_array_ny = np.array((smallest_distance_y_perm &lt; epsilon)).astype(int)\n        ny = (np.sum(less_than_array_ny, axis=1) - 1).reshape(-1, 1)\n        arr = psi(nx + 1) + psi(ny + 1)\n        ksg_estimation = (\n            psi(self.k) + psi(y.shape[0]) - np.nanmean(arr[np.isfinite(arr)])\n        )\n        return ksg_estimation\n\n    def entropic_regression_backward(self, reg_matrix, y, piv):\n        \"\"\"Entropic Regression Backward Greedy Feature Elimination.\n\n        This algorithm is based on the Matlab package available on:\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        reg_matrix : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        piv : ndarray of ints\n            The set of indices to investigate\n\n        Returns\n        -------\n        piv : ndarray of ints\n            The set of remaining indices after the\n            Backward Greedy Feature Elimination.\n\n        \"\"\"\n        min_value = -np.inf\n        piv = np.array(piv)\n        ix = []\n        while (min_value &lt;= self.tol) and (len(piv) &gt; 1):\n            initial_array = np.full((1, len(piv)), np.inf)\n            for i in range(initial_array.shape[1]):\n                if piv[i] not in []:  # if you want to keep any regressor\n                    rem = np.setdiff1d(piv, piv[i])\n                    f1 = reg_matrix[:, piv] @ pinv(reg_matrix[:, piv]) @ y\n                    f2 = reg_matrix[:, rem] @ pinv(reg_matrix[:, rem]) @ y\n                    initial_array[0, i] = self.conditional_mutual_information(y, f1, f2)\n\n            ix = np.argmin(initial_array)\n            min_value = initial_array[0, ix]\n            piv = np.delete(piv, ix)\n\n        return piv\n\n    def entropic_regression_forward(self, reg_matrix, y):\n        \"\"\"Entropic Regression Forward Greedy Feature Selection.\n\n        This algorithm is based on the Matlab package available on:\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        reg_matrix : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n\n        Returns\n        -------\n        selected_terms : ndarray of ints\n            The set of selected regressors after the\n            Forward Greedy Feature Selection.\n        success : boolean\n            Indicate if the forward selection succeed.\n            If high degree of uncertainty is detected, and many parameters are\n            selected, the success flag will be set to false. Then, the\n            backward elimination will be applied for all indices.\n\n        \"\"\"\n        success = True\n        ix = []\n        selected_terms = []\n        reg_matrix_columns = np.array(list(range(reg_matrix.shape[1])))\n        self.tol = self.tolerance_estimator(y)\n        ksg_max = getattr(self, self.mutual_information_estimator)(\n            y, reg_matrix @ pinv(reg_matrix) @ y\n        )\n        stop_criteria = False\n        while stop_criteria is False:\n            selected_terms = np.ravel(\n                [*selected_terms, *np.array([reg_matrix_columns[ix]])]\n            )\n            if len(selected_terms) != 0:\n                ksg_local = getattr(self, self.mutual_information_estimator)(\n                    y,\n                    reg_matrix[:, selected_terms]\n                    @ pinv(reg_matrix[:, selected_terms])\n                    @ y,\n                )\n            else:\n                ksg_local = getattr(self, self.mutual_information_estimator)(\n                    y, np.zeros_like(y)\n                )\n\n            initial_vector = np.full((1, reg_matrix.shape[1]), -np.inf)\n            for i in range(reg_matrix.shape[1]):\n                if reg_matrix_columns[i] not in selected_terms:\n                    f1 = (\n                        reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]]\n                        @ pinv(reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]])\n                        @ y\n                    )\n                    if len(selected_terms) != 0:\n                        f2 = (\n                            reg_matrix[:, selected_terms]\n                            @ pinv(reg_matrix[:, selected_terms])\n                            @ y\n                        )\n                    else:\n                        f2 = np.zeros_like(y)\n                    vp_estimation = self.conditional_mutual_information(y, f1, f2)\n                    initial_vector[0, i] = vp_estimation\n                else:\n                    continue\n\n            ix = np.nanargmax(initial_vector)\n            max_value = initial_vector[0, ix]\n\n            if (ksg_max - ksg_local &lt;= self.tol) or (max_value &lt;= self.tol):\n                stop_criteria = True\n            elif len(selected_terms) &gt; np.max([8, reg_matrix.shape[1] / 2]):\n                success = False\n                stop_criteria = True\n\n        return selected_terms, success\n\n    def conditional_mutual_information(self, y, f1, f2):\n        \"\"\"Find the conditional mutual information.\n\n        Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n        This code is based on Matlab Entropic Regression package.\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The source signal.\n        f1 : ndarray of floats\n            The destination signal.\n        f2 : ndarray of floats\n            The condition set.\n\n        Returns\n        -------\n        vp_estimation : float\n            The conditioned mutual information.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        joint_space = np.concatenate([y, f1, f2], axis=1)\n        smallest_distance = np.sort(\n            cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n        )\n        idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n        idx = idx[np.argsort(smallest_distance[-1, idx])]\n        smallest_distance = smallest_distance[:, idx]\n        epsilon = smallest_distance[:, -1].reshape(-1, 1)\n        # Find number of points from (y,f2), (f1,f2), and (f2,f2) that lies withing the\n        # k^{th} nearest neighbor distance from each point of themselves.\n        smallest_distance_y_f2 = cdist(\n            np.concatenate([y, f2], axis=1),\n            np.concatenate([y, f2], axis=1),\n            \"minkowski\",\n            p=self.p,\n        )\n        less_than_array_y_f2 = np.array((smallest_distance_y_f2 &lt; epsilon)).astype(int)\n        y_f2 = (np.sum(less_than_array_y_f2, axis=1) - 1).reshape(-1, 1)\n\n        smallest_distance_f1_f2 = cdist(\n            np.concatenate([f1, f2], axis=1),\n            np.concatenate([f1, f2], axis=1),\n            \"minkowski\",\n            p=self.p,\n        )\n        less_than_array_f1_f2 = np.array((smallest_distance_f1_f2 &lt; epsilon)).astype(\n            int\n        )\n        f1_f2 = (np.sum(less_than_array_f1_f2, axis=1) - 1).reshape(-1, 1)\n\n        smallest_distance_f2 = cdist(f2, f2, \"minkowski\", p=self.p)\n        less_than_array_f2 = np.array((smallest_distance_f2 &lt; epsilon)).astype(int)\n        f2_f2 = (np.sum(less_than_array_f2, axis=1) - 1).reshape(-1, 1)\n        arr = psi(y_f2 + 1) + psi(f1_f2 + 1) - psi(f2_f2 + 1)\n        vp_estimation = psi(self.k) - np.nanmean(arr[np.isfinite(arr)])\n        return vp_estimation\n\n    def tolerance_estimator(self, y):\n        \"\"\"Tolerance Estimation for mutual independence test.\n\n        Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n        This code is based on Matlab Entropic Regression package.\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The source signal.\n\n        Returns\n        -------\n        tol : float\n            The tolerance value given q.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        ksg_estimation = []\n        for _ in range(self.n_perm):\n            mutual_information_output = getattr(\n                self, self.mutual_information_estimator\n            )(y, self.rng.permutation(y))\n\n            ksg_estimation.append(mutual_information_output)\n\n        ksg_estimation = np.array(ksg_estimation)\n        tol = np.quantile(ksg_estimation, self.q)\n        return tol\n\n    def fit(self, *, X=None, y=None):\n        \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n        The 'fit' function allows a friendly usage by the user.\n        Given two arguments, x and y, fit training data.\n\n        The Entropic Regression algorithm is based on the Matlab package available on:\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n        if X is not None:\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n\n        if self.regressor_code.shape[0] &gt; 90:\n            warnings.warn(\n                \"Given the higher number of possible regressors\"\n                f\" ({self.regressor_code.shape[0]}), the Entropic Regression\"\n                \" algorithm may take long time to run. Consider reducing the\"\n                \" number of regressors \",\n                stacklevel=2,\n            )\n\n        y_full = y.copy()\n        y = y[self.max_lag :].reshape(-1, 1)\n        self.tol = 0\n        ksg_estimation = []\n        for _ in range(self.n_perm):\n            mutual_information_output = getattr(\n                self, self.mutual_information_estimator\n            )(y, self.rng.permutation(y))\n            ksg_estimation.append(mutual_information_output)\n\n        ksg_estimation = np.array(ksg_estimation).reshape(-1, 1)\n        self.tol = np.quantile(ksg_estimation, self.q)\n        self.estimated_tolerance = self.tol\n        success = False\n        if not self.skip_forward:\n            selected_terms, success = self.entropic_regression_forward(reg_matrix, y)\n\n        if not success or self.skip_forward:\n            selected_terms = np.array(list(range(reg_matrix.shape[1])))\n\n        selected_terms_backward = self.entropic_regression_backward(\n            reg_matrix[:, selected_terms], y, list(range(len(selected_terms)))\n        )\n\n        final_model = selected_terms[selected_terms_backward]\n        # re-check for the constant term (add it to the estimated indices)\n        if 0 not in final_model:\n            final_model = np.array([0, *final_model])\n\n        repetition = len(reg_matrix)\n        if isinstance(self.basis_function, Polynomial):\n            self.final_model = self.regressor_code[final_model, :].copy()\n        else:\n            self.regressor_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.final_model = self.regressor_code[final_model, :].copy()\n\n        self.theta = self.estimator.optimize(\n            reg_matrix[:, final_model], y_full[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if (np.abs(self.theta[0]) &lt; self.h) and (\n            np.sum((self.theta != 0).astype(int)) &gt; 1\n        ):\n            self.theta = self.theta[1:].reshape(-1, 1)\n            self.final_model = self.final_model[1:, :]\n            final_model = final_model[1:]\n\n        self.n_terms = len(\n            self.theta\n        )  # the number of terms we selected (necessary in the 'results' methods)\n        self.err = self.n_terms * [\n            0\n        ]  # just to use the `results` method. Will be changed in next update.\n        self.pivv = final_model\n        return self\n\n    def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(self, x, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n\n        x_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, x, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        elif self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n        else:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def _narmax_predict(self, x, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(self, x, y_initial):\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = super()._basis_function_predict(x, y_initial, forecast_horizon)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = super()._basis_function_n_step_prediction(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(self, x, y, steps_ahead, forecast_horizon):\n        yhat = super()._basis_function_n_steps_horizon(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.conditional_mutual_information","title":"<code>conditional_mutual_information(y, f1, f2)</code>","text":"<p>Find the conditional mutual information.</p> <p>Finds the conditioned mutual information between \\(y\\) and \\(f1\\) given \\(f2\\).</p> <p>This code is based on Matlab Entropic Regression package. https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The source signal.</p> required <code>f1</code> <code>ndarray of floats</code> <p>The destination signal.</p> required <code>f2</code> <code>ndarray of floats</code> <p>The condition set.</p> required <p>Returns:</p> Name Type Description <code>vp_estimation</code> <code>float</code> <p>The conditioned mutual information.</p> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def conditional_mutual_information(self, y, f1, f2):\n    \"\"\"Find the conditional mutual information.\n\n    Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n    This code is based on Matlab Entropic Regression package.\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The source signal.\n    f1 : ndarray of floats\n        The destination signal.\n    f2 : ndarray of floats\n        The condition set.\n\n    Returns\n    -------\n    vp_estimation : float\n        The conditioned mutual information.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    joint_space = np.concatenate([y, f1, f2], axis=1)\n    smallest_distance = np.sort(\n        cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n    )\n    idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n    idx = idx[np.argsort(smallest_distance[-1, idx])]\n    smallest_distance = smallest_distance[:, idx]\n    epsilon = smallest_distance[:, -1].reshape(-1, 1)\n    # Find number of points from (y,f2), (f1,f2), and (f2,f2) that lies withing the\n    # k^{th} nearest neighbor distance from each point of themselves.\n    smallest_distance_y_f2 = cdist(\n        np.concatenate([y, f2], axis=1),\n        np.concatenate([y, f2], axis=1),\n        \"minkowski\",\n        p=self.p,\n    )\n    less_than_array_y_f2 = np.array((smallest_distance_y_f2 &lt; epsilon)).astype(int)\n    y_f2 = (np.sum(less_than_array_y_f2, axis=1) - 1).reshape(-1, 1)\n\n    smallest_distance_f1_f2 = cdist(\n        np.concatenate([f1, f2], axis=1),\n        np.concatenate([f1, f2], axis=1),\n        \"minkowski\",\n        p=self.p,\n    )\n    less_than_array_f1_f2 = np.array((smallest_distance_f1_f2 &lt; epsilon)).astype(\n        int\n    )\n    f1_f2 = (np.sum(less_than_array_f1_f2, axis=1) - 1).reshape(-1, 1)\n\n    smallest_distance_f2 = cdist(f2, f2, \"minkowski\", p=self.p)\n    less_than_array_f2 = np.array((smallest_distance_f2 &lt; epsilon)).astype(int)\n    f2_f2 = (np.sum(less_than_array_f2, axis=1) - 1).reshape(-1, 1)\n    arr = psi(y_f2 + 1) + psi(f1_f2 + 1) - psi(f2_f2 + 1)\n    vp_estimation = psi(self.k) - np.nanmean(arr[np.isfinite(arr)])\n    return vp_estimation\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_backward","title":"<code>entropic_regression_backward(reg_matrix, y, piv)</code>","text":"<p>Entropic Regression Backward Greedy Feature Elimination.</p> <p>This algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>reg_matrix</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> required <code>piv</code> <code>ndarray of ints</code> <p>The set of indices to investigate</p> required <p>Returns:</p> Name Type Description <code>piv</code> <code>ndarray of ints</code> <p>The set of remaining indices after the Backward Greedy Feature Elimination.</p> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def entropic_regression_backward(self, reg_matrix, y, piv):\n    \"\"\"Entropic Regression Backward Greedy Feature Elimination.\n\n    This algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    reg_matrix : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    piv : ndarray of ints\n        The set of indices to investigate\n\n    Returns\n    -------\n    piv : ndarray of ints\n        The set of remaining indices after the\n        Backward Greedy Feature Elimination.\n\n    \"\"\"\n    min_value = -np.inf\n    piv = np.array(piv)\n    ix = []\n    while (min_value &lt;= self.tol) and (len(piv) &gt; 1):\n        initial_array = np.full((1, len(piv)), np.inf)\n        for i in range(initial_array.shape[1]):\n            if piv[i] not in []:  # if you want to keep any regressor\n                rem = np.setdiff1d(piv, piv[i])\n                f1 = reg_matrix[:, piv] @ pinv(reg_matrix[:, piv]) @ y\n                f2 = reg_matrix[:, rem] @ pinv(reg_matrix[:, rem]) @ y\n                initial_array[0, i] = self.conditional_mutual_information(y, f1, f2)\n\n        ix = np.argmin(initial_array)\n        min_value = initial_array[0, ix]\n        piv = np.delete(piv, ix)\n\n    return piv\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_forward","title":"<code>entropic_regression_forward(reg_matrix, y)</code>","text":"<p>Entropic Regression Forward Greedy Feature Selection.</p> <p>This algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>reg_matrix</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> required <p>Returns:</p> Name Type Description <code>selected_terms</code> <code>ndarray of ints</code> <p>The set of selected regressors after the Forward Greedy Feature Selection.</p> <code>success</code> <code>boolean</code> <p>Indicate if the forward selection succeed. If high degree of uncertainty is detected, and many parameters are selected, the success flag will be set to false. Then, the backward elimination will be applied for all indices.</p> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def entropic_regression_forward(self, reg_matrix, y):\n    \"\"\"Entropic Regression Forward Greedy Feature Selection.\n\n    This algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    reg_matrix : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n\n    Returns\n    -------\n    selected_terms : ndarray of ints\n        The set of selected regressors after the\n        Forward Greedy Feature Selection.\n    success : boolean\n        Indicate if the forward selection succeed.\n        If high degree of uncertainty is detected, and many parameters are\n        selected, the success flag will be set to false. Then, the\n        backward elimination will be applied for all indices.\n\n    \"\"\"\n    success = True\n    ix = []\n    selected_terms = []\n    reg_matrix_columns = np.array(list(range(reg_matrix.shape[1])))\n    self.tol = self.tolerance_estimator(y)\n    ksg_max = getattr(self, self.mutual_information_estimator)(\n        y, reg_matrix @ pinv(reg_matrix) @ y\n    )\n    stop_criteria = False\n    while stop_criteria is False:\n        selected_terms = np.ravel(\n            [*selected_terms, *np.array([reg_matrix_columns[ix]])]\n        )\n        if len(selected_terms) != 0:\n            ksg_local = getattr(self, self.mutual_information_estimator)(\n                y,\n                reg_matrix[:, selected_terms]\n                @ pinv(reg_matrix[:, selected_terms])\n                @ y,\n            )\n        else:\n            ksg_local = getattr(self, self.mutual_information_estimator)(\n                y, np.zeros_like(y)\n            )\n\n        initial_vector = np.full((1, reg_matrix.shape[1]), -np.inf)\n        for i in range(reg_matrix.shape[1]):\n            if reg_matrix_columns[i] not in selected_terms:\n                f1 = (\n                    reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]]\n                    @ pinv(reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]])\n                    @ y\n                )\n                if len(selected_terms) != 0:\n                    f2 = (\n                        reg_matrix[:, selected_terms]\n                        @ pinv(reg_matrix[:, selected_terms])\n                        @ y\n                    )\n                else:\n                    f2 = np.zeros_like(y)\n                vp_estimation = self.conditional_mutual_information(y, f1, f2)\n                initial_vector[0, i] = vp_estimation\n            else:\n                continue\n\n        ix = np.nanargmax(initial_vector)\n        max_value = initial_vector[0, ix]\n\n        if (ksg_max - ksg_local &lt;= self.tol) or (max_value &lt;= self.tol):\n            stop_criteria = True\n        elif len(selected_terms) &gt; np.max([8, reg_matrix.shape[1] / 2]):\n            success = False\n            stop_criteria = True\n\n    return selected_terms, success\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Fit polynomial NARMAX model using AOLS algorithm.</p> <p>The 'fit' function allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>The Entropic Regression algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def fit(self, *, X=None, y=None):\n    \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n    The 'fit' function allows a friendly usage by the user.\n    Given two arguments, x and y, fit training data.\n\n    The Entropic Regression algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n\n    reg_matrix = self.basis_function.fit(\n        lagged_data,\n        self.max_lag,\n        self.ylag,\n        self.xlag,\n        self.model_type,\n        predefined_regressors=None,\n    )\n\n    if X is not None:\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n\n    if self.regressor_code.shape[0] &gt; 90:\n        warnings.warn(\n            \"Given the higher number of possible regressors\"\n            f\" ({self.regressor_code.shape[0]}), the Entropic Regression\"\n            \" algorithm may take long time to run. Consider reducing the\"\n            \" number of regressors \",\n            stacklevel=2,\n        )\n\n    y_full = y.copy()\n    y = y[self.max_lag :].reshape(-1, 1)\n    self.tol = 0\n    ksg_estimation = []\n    for _ in range(self.n_perm):\n        mutual_information_output = getattr(\n            self, self.mutual_information_estimator\n        )(y, self.rng.permutation(y))\n        ksg_estimation.append(mutual_information_output)\n\n    ksg_estimation = np.array(ksg_estimation).reshape(-1, 1)\n    self.tol = np.quantile(ksg_estimation, self.q)\n    self.estimated_tolerance = self.tol\n    success = False\n    if not self.skip_forward:\n        selected_terms, success = self.entropic_regression_forward(reg_matrix, y)\n\n    if not success or self.skip_forward:\n        selected_terms = np.array(list(range(reg_matrix.shape[1])))\n\n    selected_terms_backward = self.entropic_regression_backward(\n        reg_matrix[:, selected_terms], y, list(range(len(selected_terms)))\n    )\n\n    final_model = selected_terms[selected_terms_backward]\n    # re-check for the constant term (add it to the estimated indices)\n    if 0 not in final_model:\n        final_model = np.array([0, *final_model])\n\n    repetition = len(reg_matrix)\n    if isinstance(self.basis_function, Polynomial):\n        self.final_model = self.regressor_code[final_model, :].copy()\n    else:\n        self.regressor_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.final_model = self.regressor_code[final_model, :].copy()\n\n    self.theta = self.estimator.optimize(\n        reg_matrix[:, final_model], y_full[self.max_lag :, 0].reshape(-1, 1)\n    )\n    if (np.abs(self.theta[0]) &lt; self.h) and (\n        np.sum((self.theta != 0).astype(int)) &gt; 1\n    ):\n        self.theta = self.theta[1:].reshape(-1, 1)\n        self.final_model = self.final_model[1:, :]\n        final_model = final_model[1:]\n\n    self.n_terms = len(\n        self.theta\n    )  # the number of terms we selected (necessary in the 'results' methods)\n    self.err = self.n_terms * [\n        0\n    ]  # just to use the `results` method. Will be changed in next update.\n    self.pivv = final_model\n    return self\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.mutual_information_knn","title":"<code>mutual_information_knn(y, y_perm)</code>","text":"<p>Find the mutual information.</p> <p>Finds the mutual information between \\(x\\) and \\(y\\) given \\(z\\).</p> <p>This code is based on Matlab Entropic Regression package.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The source signal.</p> required <code>y_perm</code> <code>ndarray of floats</code> <p>The destination signal.</p> required <p>Returns:</p> Name Type Description <code>ksg_estimation</code> <code>float</code> <p>The conditioned mutual information.</p> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def mutual_information_knn(self, y, y_perm):\n    \"\"\"Find the mutual information.\n\n    Finds the mutual information between $x$ and $y$ given $z$.\n\n    This code is based on Matlab Entropic Regression package.\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The source signal.\n    y_perm : ndarray of floats\n        The destination signal.\n\n    Returns\n    -------\n    ksg_estimation : float\n        The conditioned mutual information.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    joint_space = np.concatenate([y, y_perm], axis=1)\n    smallest_distance = np.sort(\n        cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n    )\n    idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n    idx = idx[np.argsort(smallest_distance[-1, idx])]\n    smallest_distance = smallest_distance[:, idx]\n    epsilon = smallest_distance[:, -1].reshape(-1, 1)\n    smallest_distance_y = cdist(y, y, \"minkowski\", p=self.p)\n    less_than_array_nx = np.array((smallest_distance_y &lt; epsilon)).astype(int)\n    nx = (np.sum(less_than_array_nx, axis=1) - 1).reshape(-1, 1)\n    smallest_distance_y_perm = cdist(y_perm, y_perm, \"minkowski\", p=self.p)\n    less_than_array_ny = np.array((smallest_distance_y_perm &lt; epsilon)).astype(int)\n    ny = (np.sum(less_than_array_ny, axis=1) - 1).reshape(-1, 1)\n    arr = psi(nx + 1) + psi(ny + 1)\n    ksg_estimation = (\n        psi(self.k) + psi(y.shape[0]) - np.nanmean(arr[np.isfinite(arr)])\n    )\n    return ksg_estimation\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.tolerance_estimator","title":"<code>tolerance_estimator(y)</code>","text":"<p>Tolerance Estimation for mutual independence test.</p> <p>Finds the conditioned mutual information between \\(y\\) and \\(f1\\) given \\(f2\\).</p> <p>This code is based on Matlab Entropic Regression package. https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The source signal.</p> required <p>Returns:</p> Name Type Description <code>tol</code> <code>float</code> <p>The tolerance value given q.</p> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def tolerance_estimator(self, y):\n    \"\"\"Tolerance Estimation for mutual independence test.\n\n    Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n    This code is based on Matlab Entropic Regression package.\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The source signal.\n\n    Returns\n    -------\n    tol : float\n        The tolerance value given q.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    ksg_estimation = []\n    for _ in range(self.n_perm):\n        mutual_information_output = getattr(\n            self, self.mutual_information_estimator\n        )(y, self.rng.permutation(y))\n\n        ksg_estimation.append(mutual_information_output)\n\n    ksg_estimation = np.array(ksg_estimation)\n    tol = np.quantile(ksg_estimation, self.q)\n    return tol\n</code></pre>"},{"location":"user-guide/API/frols/","title":"Documentation for <code>FROLS</code>","text":"<p>Build Polynomial NARMAX Models using FROLS algorithm.</p>"},{"location":"user-guide/API/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS","title":"<code>FROLS</code>","text":"<p>               Bases: <code>OFRBase</code></p> <p>Forward Regression Orthogonal Least Squares algorithm.</p> <p>This class uses the FROLS algorithm ([1], [2]) to build NARMAX models. The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},     \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>elag</code> <code>int</code> <p>The maximum lag of the residues regressors.</p> <code>2</code> <code>order_selection</code> <code>bool</code> <p>Whether to use information criteria for order selection.</p> <code>True</code> <code>info_criteria</code> <code>str</code> <p>The information criteria method to be used.</p> <code>\"aic\"</code> <code>n_terms</code> <code>int</code> <p>The number of the model terms to be selected. Note that n_terms overwrite the information criteria values.</p> <code>None</code> <code>n_info_values</code> <code>int</code> <p>The number of iterations of the information criteria method.</p> <code>10</code> <code>estimator</code> <code>str</code> <p>The parameter estimation method.</p> <code>\"least_squares\"</code> <code>model_type</code> <code>str</code> <p>The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models</p> <code>'NARMAX'</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <code>alpha</code> <code>float</code> <p>Regularization parameter used in ridge regression. Ridge regression parameter that regularizes the algorithm to prevent over fitting. If the input is a noisy signal, the ridge parameter is likely to be set close to the noise level, at least as a starting point. Entered through the self data structure.</p> <code>np.finfo(np.float64).eps</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import FROLS\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n...                                                    colored_noise=True,\n...                                                    sigma=0.2,\n...                                                    train_percentage=90)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = FROLS(basis_function=basis_function,\n...               order_selection=True,\n...               n_info_values=10,\n...               extended_least_squares=False,\n...               ylag=2,\n...               xlag=2,\n...               info_criteria='aic',\n...               )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>class FROLS(OFRBase):\n    r\"\"\"Forward Regression Orthogonal Least Squares algorithm.\n\n    This class uses the FROLS algorithm ([1]_, [2]_) to build NARMAX models.\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},\n        \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    elag : int, default=2\n        The maximum lag of the residues regressors.\n    order_selection: bool, default=False\n        Whether to use information criteria for order selection.\n    info_criteria : str, default=\"aic\"\n        The information criteria method to be used.\n    n_terms : int, default=None\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_info_values : int, default=10\n        The number of iterations of the information\n        criteria method.\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    alpha : float, default=np.finfo(np.float64).eps\n        Regularization parameter used in ridge regression.\n        Ridge regression parameter that regularizes the algorithm to prevent over\n        fitting. If the input is a noisy signal, the ridge parameter is likely to be\n        set close to the noise level, at least as a starting point.\n        Entered through the self data structure.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import FROLS\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = FROLS(basis_function=basis_function,\n    ...               order_selection=True,\n    ...               n_info_values=10,\n    ...               extended_least_squares=False,\n    ...               ylag=2,\n    ...               xlag=2,\n    ...               info_criteria='aic',\n    ...               )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        elag: Union[int, list] = 2,\n        order_selection: bool = True,\n        info_criteria: str = \"aic\",\n        n_terms: Union[int, None] = None,\n        n_info_values: int = 15,\n        estimator: Estimators = RecursiveLeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n        eps: np.float64 = np.finfo(np.float64).eps,\n        alpha: float = 0,\n        err_tol: Optional[float] = None,\n    ):\n        self.order_selection = order_selection\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.info_criteria = info_criteria\n        self.info_criteria_function = get_info_criteria(info_criteria)\n        self.n_info_values = n_info_values\n        self.n_terms = n_terms\n        self.estimator = estimator\n        self.elag = elag\n        self.model_type = model_type\n        self.basis_function = basis_function\n        self.eps = eps\n        if isinstance(self.estimator, RidgeRegression):\n            self.alpha = self.estimator.alpha\n        else:\n            self.alpha = alpha\n\n        self.err_tol = err_tol\n        self._validate_params()\n        self.n_inputs = None\n        self.regressor_code = None\n        self.info_values = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n\n    def run_mss_algorithm(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        return self.error_reduction_ratio(psi, y, process_term_number)\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n        \"\"\"Fit polynomial NARMAX model.\n\n        This is an 'alpha' version of the 'fit' function which allows\n        a friendly usage by the user. Given two arguments, x and y, fit\n        training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        super().fit(X=X, y=y)\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        yhat = super().predict(\n            X=X, y=y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        return yhat\n</code></pre>"},{"location":"user-guide/API/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.fit","title":"<code>fit(*, X=None, y)</code>","text":"<p>Fit polynomial NARMAX model.</p> <p>This is an 'alpha' version of the 'fit' function which allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>info_values</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n    \"\"\"Fit polynomial NARMAX model.\n\n    This is an 'alpha' version of the 'fit' function which allows\n    a friendly usage by the user. Given two arguments, x and y, fit\n    training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    super().fit(X=X, y=y)\n    return self\n</code></pre>"},{"location":"user-guide/API/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.predict","title":"<code>predict(*, X=None, y, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> required <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: np.ndarray,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    yhat = super().predict(\n        X=X, y=y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    return yhat\n</code></pre>"},{"location":"user-guide/API/general-estimators/","title":"Documentation for <code>General Estimators</code>","text":"<p>Build NARX Models Using general estimators.</p>"},{"location":"user-guide/API/general-estimators/#sysidentpy.general_estimators.narx.NARX","title":"<code>NARX</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>NARX model build on top of general estimators.</p> <p>The Nonlinear AutoRegressive with eXogenous inputs (NARX) model is mathematically described by:</p> \\[     y(t) = F\\left(\\mathbf{\\phi}(t)\\right) + \\epsilon(t) \\] <p>where \\(\\mathbf{\\phi}(t)\\) is the regression vector composed of lagged inputs and outputs:</p> \\[     \\mathbf{\\phi}(t) = [y(t-1), \\ldots, y(t-n_y), x(t-1), \\ldots, x(t-n_x)] \\] <p>Here, \\(n_y\\) (<code>ylag</code>) and \\(n_x\\) (<code>xlag</code>) are the maximum lags for the output and input, respectively. The function \\(F\\) is approximated by the base estimator. For NARMAX models, the regression vector includes lagged residuals:</p> \\[     \\mathbf{\\phi}(t) = [y(t-1), \\ldots, y(t-n_y), x(t-1), \\ldots, x(t-n_x),     \\epsilon(t-1), \\ldots, \\epsilon(t-n_e)] \\] <p>where \\(n_e\\) is determined by the basis function and <code>model_type</code> parameter.</p> <p>This implementation uses <code>GenerateRegressors</code> and <code>InformationMatrix</code> to construct lagged features and allows infinite-step-ahead prediction via iterative methods.</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag order of the output \\(n_y\\) (number of past output terms used).</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag order of the input \\(n_x\\) (number of past input terms used).</p> <code>2</code> <code>fit_params</code> <code>dict</code> <p>Additional parameters to pass to the <code>fit</code> method of the base estimator.</p> <code>None</code> <code>base_estimator</code> <code>estimator object</code> <p>An sklearn-compatible estimator with <code>fit</code> and <code>predict</code> methods.</p> <code>None</code> <code>basis_function</code> <code>basis function object</code> <p>Nonlinear transformation applied to regressors (e.g., Polynomial, Fourier).</p> <code>Polynomial</code> <code>model_type</code> <code>(NARMAX, NAR, NFIR)</code> <p>Model structure. Use \"NARMAX\" to include lagged residuals in the regression vector.</p> <code>\"NARMAX\"</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sysidentpy.general_estimators import NARX\n&gt;&gt;&gt; from sklearn.linear_model import BayesianRidge\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n&gt;&gt;&gt; # Generate data and fit model\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = NARX(\n...     base_estimator=BayesianRidge(),\n...     xlag=2,\n...     ylag=2,\n...     basis_function=basis_function,\n...     model_type=\"NARMAX\"\n... )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; # Evaluation and plotting code here\n0.000131\n</code></pre> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>class NARX(BaseMSS):\n    r\"\"\"NARX model build on top of general estimators.\n\n    The Nonlinear AutoRegressive with eXogenous inputs (NARX) model is mathematically\n    described by:\n\n    $$\n        y(t) = F\\left(\\mathbf{\\phi}(t)\\right) + \\epsilon(t)\n    $$\n\n    where $\\mathbf{\\phi}(t)$ is the regression vector composed of lagged inputs and\n    outputs:\n\n    $$\n        \\mathbf{\\phi}(t) = [y(t-1), \\ldots, y(t-n_y), x(t-1), \\ldots, x(t-n_x)]\n    $$\n\n    Here, $n_y$ (``ylag``) and $n_x$ (``xlag``) are the maximum lags for the output and\n    input, respectively. The function $F$ is approximated by the base estimator. For\n    NARMAX models, the regression vector includes lagged residuals:\n\n    $$\n        \\mathbf{\\phi}(t) = [y(t-1), \\ldots, y(t-n_y), x(t-1), \\ldots, x(t-n_x),\n        \\epsilon(t-1), \\ldots, \\epsilon(t-n_e)]\n    $$\n\n    where $n_e$ is determined by the basis function and ``model_type`` parameter.\n\n    This implementation uses ``GenerateRegressors`` and ``InformationMatrix`` to\n    construct lagged features and allows infinite-step-ahead prediction via iterative\n    methods.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag order of the output $n_y$ (number of past output terms used).\n    xlag : int, default=2\n        The maximum lag order of the input $n_x$ (number of past input terms used).\n    fit_params : dict, default=None\n        Additional parameters to pass to the ``fit`` method of the base estimator.\n    base_estimator : estimator object, default=None\n        An sklearn-compatible estimator with ``fit`` and ``predict`` methods.\n    basis_function : basis function object, default=Polynomial\n        Nonlinear transformation applied to regressors (e.g., Polynomial, Fourier).\n    model_type : {\"NARMAX\", \"NAR\", \"NFIR\"}, default=\"NARMAX\"\n        Model structure. Use \"NARMAX\" to include lagged residuals in the regression\n        vector.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sysidentpy.general_estimators import NARX\n    &gt;&gt;&gt; from sklearn.linear_model import BayesianRidge\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; # Generate data and fit model\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = NARX(\n    ...     base_estimator=BayesianRidge(),\n    ...     xlag=2,\n    ...     ylag=2,\n    ...     basis_function=basis_function,\n    ...     model_type=\"NARMAX\"\n    ... )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; # Evaluation and plotting code here\n    0.000131\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[List[Any], Any] = 1,\n        xlag: Union[List[Any], Any] = 1,\n        model_type: str = \"NARMAX\",\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        base_estimator=None,\n        fit_params=None,\n    ):\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.non_degree = basis_function.degree\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.base_estimator = base_estimator\n        if fit_params is None:\n            fit_params = {}\n\n        self.fit_params = fit_params\n        self.ensemble = None\n        self.n_inputs = None\n        self.regressor_code = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def fit(self, *, X=None, y=None):\n        \"\"\"Train a NARX Neural Network model.\n\n        This is a training pipeline that allows a friendly usage\n        by the user. All the lagged features are built using the\n        SysIdentPy classes and we use the fit method of the base\n        estimator of the sklearn to fit the model.\n\n        Parameters\n        ----------\n        X : ndarrays of floats\n            The input data to be used in the training process.\n        y : ndarrays of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        base_estimator : sklearn estimator\n            The model fitted.\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n        if X is not None:\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        self.final_model = self.regressor_code\n        y = y[self.max_lag :].ravel()\n\n        self.base_estimator.fit(reg_matrix, y, **self.fit_params)\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[NDArray] = None,\n        y: Optional[NDArray] = None,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = 1,\n    ) -&gt; NDArray:\n        \"\"\"Return the predicted given an input and initial values.\n\n        The predict function allows a friendly usage by the user.\n        Given a trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future).\n\n        Currently, we only support infinity-steps-ahead prediction,\n        but run 1-step-ahead prediction manually is straightforward.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(self, x_base, y=None):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        lagged_data = build_lagged_matrix(\n            x_base, y, self.xlag, self.ylag, self.model_type\n        )\n        x_base = self.basis_function.transform(\n            lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n        )\n\n        yhat = self.base_estimator.predict(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _nar_step_ahead(self, y, steps_ahead):\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        yhat = np.zeros(len(y) + steps_ahead, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    x=None, y_initial=y[step:i], forecast_horizon=steps_ahead\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=None, y_initial=y[steps[-1] : i]\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=None, y_initial=y[0:i], forecast_horizon=steps_ahead\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    def narmax_n_step_ahead(self, x, y, steps_ahead):\n        \"\"\"N steps ahead prediction method for NARMAX model.\"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        x = x.reshape(-1, self.n_inputs)\n        yhat = np.zeros(x.shape[0], dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    x=x[step : i + steps_ahead],\n                    y_initial=y[step:i],\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[steps[-1] : i + steps_ahead],\n                y_initial=y[steps[-1] : i],\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[0 : i + steps_ahead],\n                y_initial=y[0:i],\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, x, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if self.model_type == \"NARMAX\":\n            return self.narmax_n_step_ahead(x, y, steps_ahead)\n\n        if self.model_type == \"NAR\":\n            return self._nar_step_ahead(y, steps_ahead)\n\n    def _model_prediction(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(self, x, y_initial, forecast_horizon=None):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            if forecast_horizon is None:\n                raise ValueError(\n                    \"forecast_horizon cannot be None when x is None for NARX prediction\"\n                )\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = np.zeros(forecast_horizon, dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        model_exponents = np.vstack(\n            [self._code2exponents(code=model) for model in self.final_model]\n        )\n        raw_regressor = np.zeros(model_exponents.shape[1], dtype=float)\n        regressor_powers = np.empty_like(model_exponents)\n        for i in range(self.max_lag, forecast_horizon):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n            np.power(raw_regressor, model_exponents, out=regressor_powers)\n            regressor_value = np.prod(regressor_powers, axis=1)\n            y_output[i] = self.base_estimator.predict(regressor_value.reshape(1, -1))[0]\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    def _nfir_predict(self, x, y_initial):\n        y_output = np.zeros(x.shape[0], dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n        x = x.reshape(-1, self.n_inputs)\n        model_exponents = np.vstack(\n            [self._code2exponents(code=model) for model in self.final_model]\n        )\n        raw_regressor = np.zeros(model_exponents.shape[1], dtype=float)\n        regressor_powers = np.empty_like(model_exponents)\n        for i in range(self.max_lag, x.shape[0]):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n            np.power(raw_regressor, model_exponents, out=regressor_powers)\n            regressor_value = np.prod(regressor_powers, axis=1)\n            y_output[i] = self.base_estimator.predict(regressor_value.reshape(1, -1))[0]\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        analyzed_elements_number = self.max_lag + 1\n\n        for i in range(forecast_horizon - self.max_lag):\n            lagged_data = build_lagged_matrix(\n                x[i : i + analyzed_elements_number],\n                yhat[i : i + analyzed_elements_number].reshape(-1, 1),\n                self.xlag,\n                self.ylag,\n                self.model_type,\n            )\n            x_tmp = self.basis_function.transform(\n                lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n            )\n\n            a = self.base_estimator.predict(x_tmp)\n            yhat[i + self.max_lag] = a[0]\n\n        return yhat[self.max_lag :].reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead],\n                    y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        return yhat[self.max_lag : :].reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(self, x, y, steps_ahead, forecast_horizon):\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead],\n                    y[k : i + steps_ahead],\n                    forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat[self.max_lag : :].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/general-estimators/#sysidentpy.general_estimators.narx.NARX.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Train a NARX Neural Network model.</p> <p>This is a training pipeline that allows a friendly usage by the user. All the lagged features are built using the SysIdentPy classes and we use the fit method of the base estimator of the sklearn to fit the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarrays of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarrays of floats</code> <p>The output data to be used in the training process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>base_estimator</code> <code>sklearn estimator</code> <p>The model fitted.</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>def fit(self, *, X=None, y=None):\n    \"\"\"Train a NARX Neural Network model.\n\n    This is a training pipeline that allows a friendly usage\n    by the user. All the lagged features are built using the\n    SysIdentPy classes and we use the fit method of the base\n    estimator of the sklearn to fit the model.\n\n    Parameters\n    ----------\n    X : ndarrays of floats\n        The input data to be used in the training process.\n    y : ndarrays of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    base_estimator : sklearn estimator\n        The model fitted.\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n    reg_matrix = self.basis_function.fit(\n        lagged_data,\n        self.max_lag,\n        self.ylag,\n        self.xlag,\n        self.model_type,\n        predefined_regressors=None,\n    )\n\n    if X is not None:\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    self.final_model = self.regressor_code\n    y = y[self.max_lag :].ravel()\n\n    self.base_estimator.fit(reg_matrix, y, **self.fit_params)\n    return self\n</code></pre>"},{"location":"user-guide/API/general-estimators/#sysidentpy.general_estimators.narx.NARX.narmax_n_step_ahead","title":"<code>narmax_n_step_ahead(x, y, steps_ahead)</code>","text":"<p>N steps ahead prediction method for NARMAX model.</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>def narmax_n_step_ahead(self, x, y, steps_ahead):\n    \"\"\"N steps ahead prediction method for NARMAX model.\"\"\"\n    if len(y) &lt; self.max_lag:\n        raise ValueError(\n            \"Insufficient initial condition elements! Expected at least\"\n            f\" {self.max_lag} elements.\"\n        )\n\n    to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n    x = x.reshape(-1, self.n_inputs)\n    yhat = np.zeros(x.shape[0], dtype=float)\n    yhat.fill(np.nan)\n    yhat[: self.max_lag] = y[: self.max_lag, 0]\n    i = self.max_lag\n    steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n    if len(steps) &gt; 1:\n        for step in steps[:-1]:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[step : i + steps_ahead],\n                y_initial=y[step:i],\n            )[-steps_ahead:].ravel()\n            i += steps_ahead\n\n        steps_ahead = np.sum(np.isnan(yhat))\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            x=x[steps[-1] : i + steps_ahead],\n            y_initial=y[steps[-1] : i],\n        )[-steps_ahead:].ravel()\n    else:\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            x=x[0 : i + steps_ahead],\n            y_initial=y[0:i],\n        )[-steps_ahead:].ravel()\n\n    yhat = yhat.ravel()[self.max_lag : :]\n    return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/general-estimators/#sysidentpy.general_estimators.narx.NARX.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=1)</code>","text":"<p>Return the predicted given an input and initial values.</p> <p>The predict function allows a friendly usage by the user. Given a trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future).</p> <p>Currently, we only support infinity-steps-ahead prediction, but run 1-step-ahead prediction manually is straightforward.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[NDArray] = None,\n    y: Optional[NDArray] = None,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: Optional[int] = 1,\n) -&gt; NDArray:\n    \"\"\"Return the predicted given an input and initial values.\n\n    The predict function allows a friendly usage by the user.\n    Given a trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future).\n\n    Currently, we only support infinity-steps-ahead prediction,\n    but run 1-step-ahead prediction manually is straightforward.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/metaheuristics/","title":"Documentation for <code>Metaheuristics</code>","text":"<p>Binary Hybrid Particle Swarm Optimization and Gravitational Search Algorithm.</p>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA","title":"<code>BPSOGSA</code>","text":"<p>Binary Hybrid Particle Swarm Optimization and Gravitational Search Algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>maxiter</code> <code>int</code> <p>The maximum number of iterations.</p> <code>30</code> <code>alpha</code> <code>int</code> <p>The descending coefficient of the gravitational constant.</p> <code>23</code> <code>g_zero</code> <code>int</code> <p>The initial value of the gravitational constant.</p> <code>100</code> <code>k_agents_percent</code> <p>Percent of agents applying force to the others in the last iteration.</p> <code>2</code> <code>norm</code> <code>int</code> <p>The information criteria method to be used.</p> <code>-2</code> <code>power</code> <code>int</code> <p>The number of the model terms to be selected. Note that n_terms overwrite the information criteria values.</p> <code>2</code> <code>n_agents</code> <code>int</code> <p>The number of agents to search the optimal solution.</p> <code>10</code> <code>dimension</code> <code>int</code> <p>The dimension of the search space. criteria method.</p> <code>15</code> <code>p_zeros</code> <code>float</code> <p>The probability of getting ones in the construction of the population.</p> <code>0.5</code> <code>p_zeros</code> <code>float</code> <p>The probability of getting zeros in the construction of the population.</p> <code>0.5</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.metaheuristics import BPSOGSA\n&gt;&gt;&gt; opt = BPSOGSA(maxiter=100,\n...               k_agents_percent=2,\n...               n_agents=10,\n...               dimension=20\n...               )\n&gt;&gt;&gt; opt.optimize()\n&gt;&gt;&gt; plt.plot(opt.best_by_iter)\n&gt;&gt;&gt; plt.show()\n&gt;&gt;&gt; print(opt.optimal_fitness_value)\n</code></pre> References <ul> <li>A New Hybrid PSOGSA Algorithm for Function Optimization,    https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa</li> <li>Manuscript: Particle swarm optimization: developments, applications and resources.</li> <li>Manuscript: S-shaped versus V-shaped transfer functions for binary    particle swarm optimization</li> <li>Manuscript: BGSA: Binary Gravitational Search Algorithm.</li> <li>Manuscript: A taxonomy of hybrid metaheuristics</li> </ul> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>class BPSOGSA:\n    \"\"\"Binary Hybrid Particle Swarm Optimization and Gravitational Search Algorithm.\n\n    Parameters\n    ----------\n    maxiter : int, default=30\n        The maximum number of iterations.\n    alpha : int, default=23\n        The descending coefficient of the gravitational constant.\n    g_zero : int, default=100\n        The initial value of the gravitational constant.\n    k_agents_percent: int, default=2\n        Percent of agents applying force to the others in the last iteration.\n    norm : int, default=-2\n        The information criteria method to be used.\n    power : int, default=2\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_agents : int, default=10\n        The number of agents to search the optimal solution.\n    dimension : int, default=15\n        The dimension of the search space.\n        criteria method.\n    p_zeros : float, default=0.5\n        The probability of getting ones in the construction of the population.\n    p_zeros : float, default=0.5\n        The probability of getting zeros in the construction of the population.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.metaheuristics import BPSOGSA\n    &gt;&gt;&gt; opt = BPSOGSA(maxiter=100,\n    ...               k_agents_percent=2,\n    ...               n_agents=10,\n    ...               dimension=20\n    ...               )\n    &gt;&gt;&gt; opt.optimize()\n    &gt;&gt;&gt; plt.plot(opt.best_by_iter)\n    &gt;&gt;&gt; plt.show()\n    &gt;&gt;&gt; print(opt.optimal_fitness_value)\n\n    References\n    ----------\n    - A New Hybrid PSOGSA Algorithm for Function Optimization,\n       https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa\n    - Manuscript: Particle swarm optimization: developments, applications and resources.\n    - Manuscript: S-shaped versus V-shaped transfer functions for binary\n       particle swarm optimization\n    - Manuscript: BGSA: Binary Gravitational Search Algorithm.\n    - Manuscript: A taxonomy of hybrid metaheuristics\n\n    \"\"\"\n\n    def __init__(\n        self,\n        maxiter=30,\n        alpha=23,\n        g_zero=100,\n        k_agents_percent=2,\n        norm=-2,\n        power=2,\n        n_agents=10,\n        dimension=15,\n        p_zeros=0.5,\n        p_ones=0.5,\n    ):\n        self.dimension = dimension\n        self.n_agents = n_agents\n        self.maxiter = maxiter\n        self.g_zero = g_zero\n        self.alpha = alpha\n        self.k_agents_percent = k_agents_percent\n        self.norm = norm\n        self.power = power\n        self.p_zeros = p_zeros\n        self.p_ones = p_ones\n        self.best_by_iter = None\n        self.mean_by_iter = None\n        self.optimal_fitness_value = None\n        self.optimal_model = None\n        super(BPSOGSA, self).__init__()\n\n    def evaluate_objective_function(self, candidate_solution):\n        \"\"\"Define a function to be optimized.\"\"\"\n        total = 0\n        for candidate in candidate_solution:\n            total += candidate**2\n        return total\n\n    def optimize(self):\n        \"\"\"Run the BPSOGSA algorithm.\n\n        This algorithm is based on the Matlab implementation provided by the\n        author of the BPSOGSA algorithm.\n\n        References\n        ----------\n        - A New Hybrid PSOGSA Algorithm for Function Optimization.\n           https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa\n        - Manuscript: Particle swarm optimization: developments, applications and\n            resources.\n        - Manuscript: S-shaped versus V-shaped transfer functions for binary.\n           particle swarm optimization\n        - Manuscript: BGSA: Binary Gravitational Search Algorithm.\n        - Manuscript: A taxonomy of hybrid metaheuristics.\n\n        \"\"\"\n        velocity = np.zeros([self.dimension, self.n_agents])\n        population = self.generate_random_population()\n        self.best_by_iter = []\n        self.mean_by_iter = []\n        self.optimal_fitness_value = np.inf\n        self.optimal_model = None\n\n        for i in range(self.maxiter):\n            fitness = self.evaluate_objective_function(population)\n\n            column_of_best_solution = np.argmin(fitness)\n            current_best_fitness = fitness[column_of_best_solution]\n\n            if current_best_fitness &lt; self.optimal_fitness_value:\n                self.optimal_fitness_value = current_best_fitness\n                self.optimal_model = population[:, column_of_best_solution].copy()\n\n            self.best_by_iter.append(self.optimal_fitness_value)\n            self.mean_by_iter.append(np.mean(fitness))\n            agent_mass = self.mass_calculation(fitness)\n            gravitational_constant = self.calculate_gravitational_constant(i)\n            acceleration = self.calculate_acceleration(\n                population, agent_mass, gravitational_constant, i\n            )\n            velocity, population = self.update_velocity_position(\n                population,\n                acceleration,\n                velocity,\n                i,\n            )\n\n        return self\n\n    def generate_random_population(self, random_state=None):\n        \"\"\"Generate the initial population of agents randomly.\n\n        Returns\n        -------\n        population : ndarray of zeros and ones\n            The initial population of agents.\n\n        \"\"\"\n        rng = check_random_state(random_state)\n        population = rng.choice(\n            [0, 1], size=(self.dimension, self.n_agents), p=[self.p_zeros, self.p_ones]\n        )\n        return population\n\n    def mass_calculation(self, fitness_value):\n        \"\"\"Calculate the inertial masses of the agents.\n\n        Parameters\n        ----------\n        fitness_value : ndarray\n            The fitness value of each agent.\n\n        Returns\n        -------\n        agent_mass : ndarray of floats\n            The mass of each agent.\n\n        \"\"\"\n        highest_fitness_value = np.nanmax(fitness_value)\n        lowest_fitness_value = np.nanmin(fitness_value)\n\n        column_fitness = len(fitness_value)\n        if highest_fitness_value == lowest_fitness_value:\n            agent_mass = np.ones([column_fitness, 1])\n        else:\n            best_fitness_value = lowest_fitness_value\n            worst_fitness_value = highest_fitness_value\n            agent_mass = (fitness_value - 0.99 * worst_fitness_value) / (\n                best_fitness_value - worst_fitness_value\n            )\n\n        agent_mass = (5 * agent_mass) / np.sum(agent_mass)\n        return agent_mass\n\n    def calculate_gravitational_constant(self, iteration):\n        \"\"\"Update the gravitational constant.\n\n        Parameters\n        ----------\n        iteration : int\n            The specific time.\n\n        Returns\n        -------\n        gravitational_constant : float\n            The gravitational_constant at time defined by the iteration.\n\n        \"\"\"\n        gravitational_constant = self.g_zero * np.exp(\n            (-self.alpha * (iteration + 1)) / self.maxiter\n        )\n\n        return gravitational_constant\n\n    def calculate_acceleration(\n        self, population, agent_mass, gravitational_constant, iteration\n    ):\n        \"\"\"Calculate the acceleration of each agent.\n\n        Parameters\n        ----------\n        population : ndarray of zeros and ones\n            The population defined by the agents.\n        agent_mass : ndarray of floats\n            The mass of each agent.\n        gravitational_constant : float\n            The gravitational_constant at time defined by the iteration.\n        iteration : int\n            The current iteration.\n\n        Returns\n        -------\n        acceleration : ndarray of floats\n            The acceleration of each agent.\n\n        \"\"\"\n        k_best_agents = self.k_agents_percent + (1 - iteration / self.maxiter) * (\n            100 - self.k_agents_percent\n        )\n        k_best_agents = round(self.n_agents * k_best_agents / 100)\n\n        maximum_value_index = np.argsort(agent_mass)[::-1].ravel()\n        gravitational_force = np.zeros([self.dimension, self.n_agents])\n        for i in range(self.n_agents):\n            for j in range(k_best_agents):\n                if maximum_value_index[j] != i:\n                    euclidean_distance = np.linalg.norm(\n                        population[:, i] - population[:, maximum_value_index[j]],\n                        self.norm,\n                    )\n                    gravitational_force[:, i] = gravitational_force[\n                        :, i\n                    ] + np.random.rand(self.dimension) * agent_mass[\n                        maximum_value_index[j]\n                    ] * (\n                        population[:, maximum_value_index[j]] - population[:, i]\n                    ) / (\n                        euclidean_distance**self.power + np.finfo(np.float64).eps\n                    )\n\n        acceleration = gravitational_force * gravitational_constant\n        return acceleration\n\n    def update_velocity_position(\n        self,\n        population,\n        acceleration,\n        velocity,\n        iteration,\n    ):\n        \"\"\"Update the velocity and position of each agent.\n\n        Parameters\n        ----------\n        population : ndarray of zeros and ones\n            The population defined by the agents.\n        acceleration : ndarray of floats\n            The acceleration of each agent.\n        velocity : ndarray of floats\n            The velocity of each agent.\n        iteration : int\n            The current iteration.\n\n        Returns\n        -------\n        velocity : ndarray of floats\n            The updated velocity of each agent.\n        population : ndarray of zeros and ones\n            The updated population defined by the agents.\n\n        \"\"\"\n        c_factor_local_best = -2 * ((iteration**3) / (self.maxiter**3)) + 2\n        c_factor_global_best = 2 * ((iteration**3) / (self.maxiter**3)) + 2\n        global_best = np.repeat(self.optimal_model, self.n_agents, axis=0).reshape(\n            self.dimension, self.n_agents\n        )\n\n        velocity = (\n            np.random.rand(self.dimension, self.n_agents) * velocity\n            + c_factor_local_best * acceleration\n            + c_factor_global_best * (global_best - population)\n        )\n        r = np.random.rand(self.dimension, self.n_agents)\n        transform_to_binary = np.absolute(np.tanh(velocity))\n        ind = np.where(r &lt; transform_to_binary)\n        population[ind] = 1 - population[ind]\n        return velocity, population\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_acceleration","title":"<code>calculate_acceleration(population, agent_mass, gravitational_constant, iteration)</code>","text":"<p>Calculate the acceleration of each agent.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>ndarray of zeros and ones</code> <p>The population defined by the agents.</p> required <code>agent_mass</code> <code>ndarray of floats</code> <p>The mass of each agent.</p> required <code>gravitational_constant</code> <code>float</code> <p>The gravitational_constant at time defined by the iteration.</p> required <code>iteration</code> <code>int</code> <p>The current iteration.</p> required <p>Returns:</p> Name Type Description <code>acceleration</code> <code>ndarray of floats</code> <p>The acceleration of each agent.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def calculate_acceleration(\n    self, population, agent_mass, gravitational_constant, iteration\n):\n    \"\"\"Calculate the acceleration of each agent.\n\n    Parameters\n    ----------\n    population : ndarray of zeros and ones\n        The population defined by the agents.\n    agent_mass : ndarray of floats\n        The mass of each agent.\n    gravitational_constant : float\n        The gravitational_constant at time defined by the iteration.\n    iteration : int\n        The current iteration.\n\n    Returns\n    -------\n    acceleration : ndarray of floats\n        The acceleration of each agent.\n\n    \"\"\"\n    k_best_agents = self.k_agents_percent + (1 - iteration / self.maxiter) * (\n        100 - self.k_agents_percent\n    )\n    k_best_agents = round(self.n_agents * k_best_agents / 100)\n\n    maximum_value_index = np.argsort(agent_mass)[::-1].ravel()\n    gravitational_force = np.zeros([self.dimension, self.n_agents])\n    for i in range(self.n_agents):\n        for j in range(k_best_agents):\n            if maximum_value_index[j] != i:\n                euclidean_distance = np.linalg.norm(\n                    population[:, i] - population[:, maximum_value_index[j]],\n                    self.norm,\n                )\n                gravitational_force[:, i] = gravitational_force[\n                    :, i\n                ] + np.random.rand(self.dimension) * agent_mass[\n                    maximum_value_index[j]\n                ] * (\n                    population[:, maximum_value_index[j]] - population[:, i]\n                ) / (\n                    euclidean_distance**self.power + np.finfo(np.float64).eps\n                )\n\n    acceleration = gravitational_force * gravitational_constant\n    return acceleration\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_gravitational_constant","title":"<code>calculate_gravitational_constant(iteration)</code>","text":"<p>Update the gravitational constant.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>The specific time.</p> required <p>Returns:</p> Name Type Description <code>gravitational_constant</code> <code>float</code> <p>The gravitational_constant at time defined by the iteration.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def calculate_gravitational_constant(self, iteration):\n    \"\"\"Update the gravitational constant.\n\n    Parameters\n    ----------\n    iteration : int\n        The specific time.\n\n    Returns\n    -------\n    gravitational_constant : float\n        The gravitational_constant at time defined by the iteration.\n\n    \"\"\"\n    gravitational_constant = self.g_zero * np.exp(\n        (-self.alpha * (iteration + 1)) / self.maxiter\n    )\n\n    return gravitational_constant\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.evaluate_objective_function","title":"<code>evaluate_objective_function(candidate_solution)</code>","text":"<p>Define a function to be optimized.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def evaluate_objective_function(self, candidate_solution):\n    \"\"\"Define a function to be optimized.\"\"\"\n    total = 0\n    for candidate in candidate_solution:\n        total += candidate**2\n    return total\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.generate_random_population","title":"<code>generate_random_population(random_state=None)</code>","text":"<p>Generate the initial population of agents randomly.</p> <p>Returns:</p> Name Type Description <code>population</code> <code>ndarray of zeros and ones</code> <p>The initial population of agents.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def generate_random_population(self, random_state=None):\n    \"\"\"Generate the initial population of agents randomly.\n\n    Returns\n    -------\n    population : ndarray of zeros and ones\n        The initial population of agents.\n\n    \"\"\"\n    rng = check_random_state(random_state)\n    population = rng.choice(\n        [0, 1], size=(self.dimension, self.n_agents), p=[self.p_zeros, self.p_ones]\n    )\n    return population\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.mass_calculation","title":"<code>mass_calculation(fitness_value)</code>","text":"<p>Calculate the inertial masses of the agents.</p> <p>Parameters:</p> Name Type Description Default <code>fitness_value</code> <code>ndarray</code> <p>The fitness value of each agent.</p> required <p>Returns:</p> Name Type Description <code>agent_mass</code> <code>ndarray of floats</code> <p>The mass of each agent.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def mass_calculation(self, fitness_value):\n    \"\"\"Calculate the inertial masses of the agents.\n\n    Parameters\n    ----------\n    fitness_value : ndarray\n        The fitness value of each agent.\n\n    Returns\n    -------\n    agent_mass : ndarray of floats\n        The mass of each agent.\n\n    \"\"\"\n    highest_fitness_value = np.nanmax(fitness_value)\n    lowest_fitness_value = np.nanmin(fitness_value)\n\n    column_fitness = len(fitness_value)\n    if highest_fitness_value == lowest_fitness_value:\n        agent_mass = np.ones([column_fitness, 1])\n    else:\n        best_fitness_value = lowest_fitness_value\n        worst_fitness_value = highest_fitness_value\n        agent_mass = (fitness_value - 0.99 * worst_fitness_value) / (\n            best_fitness_value - worst_fitness_value\n        )\n\n    agent_mass = (5 * agent_mass) / np.sum(agent_mass)\n    return agent_mass\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.optimize","title":"<code>optimize()</code>","text":"<p>Run the BPSOGSA algorithm.</p> <p>This algorithm is based on the Matlab implementation provided by the author of the BPSOGSA algorithm.</p> References <ul> <li>A New Hybrid PSOGSA Algorithm for Function Optimization.    https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa</li> <li>Manuscript: Particle swarm optimization: developments, applications and     resources.</li> <li>Manuscript: S-shaped versus V-shaped transfer functions for binary.    particle swarm optimization</li> <li>Manuscript: BGSA: Binary Gravitational Search Algorithm.</li> <li>Manuscript: A taxonomy of hybrid metaheuristics.</li> </ul> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def optimize(self):\n    \"\"\"Run the BPSOGSA algorithm.\n\n    This algorithm is based on the Matlab implementation provided by the\n    author of the BPSOGSA algorithm.\n\n    References\n    ----------\n    - A New Hybrid PSOGSA Algorithm for Function Optimization.\n       https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa\n    - Manuscript: Particle swarm optimization: developments, applications and\n        resources.\n    - Manuscript: S-shaped versus V-shaped transfer functions for binary.\n       particle swarm optimization\n    - Manuscript: BGSA: Binary Gravitational Search Algorithm.\n    - Manuscript: A taxonomy of hybrid metaheuristics.\n\n    \"\"\"\n    velocity = np.zeros([self.dimension, self.n_agents])\n    population = self.generate_random_population()\n    self.best_by_iter = []\n    self.mean_by_iter = []\n    self.optimal_fitness_value = np.inf\n    self.optimal_model = None\n\n    for i in range(self.maxiter):\n        fitness = self.evaluate_objective_function(population)\n\n        column_of_best_solution = np.argmin(fitness)\n        current_best_fitness = fitness[column_of_best_solution]\n\n        if current_best_fitness &lt; self.optimal_fitness_value:\n            self.optimal_fitness_value = current_best_fitness\n            self.optimal_model = population[:, column_of_best_solution].copy()\n\n        self.best_by_iter.append(self.optimal_fitness_value)\n        self.mean_by_iter.append(np.mean(fitness))\n        agent_mass = self.mass_calculation(fitness)\n        gravitational_constant = self.calculate_gravitational_constant(i)\n        acceleration = self.calculate_acceleration(\n            population, agent_mass, gravitational_constant, i\n        )\n        velocity, population = self.update_velocity_position(\n            population,\n            acceleration,\n            velocity,\n            i,\n        )\n\n    return self\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.update_velocity_position","title":"<code>update_velocity_position(population, acceleration, velocity, iteration)</code>","text":"<p>Update the velocity and position of each agent.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>ndarray of zeros and ones</code> <p>The population defined by the agents.</p> required <code>acceleration</code> <code>ndarray of floats</code> <p>The acceleration of each agent.</p> required <code>velocity</code> <code>ndarray of floats</code> <p>The velocity of each agent.</p> required <code>iteration</code> <code>int</code> <p>The current iteration.</p> required <p>Returns:</p> Name Type Description <code>velocity</code> <code>ndarray of floats</code> <p>The updated velocity of each agent.</p> <code>population</code> <code>ndarray of zeros and ones</code> <p>The updated population defined by the agents.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def update_velocity_position(\n    self,\n    population,\n    acceleration,\n    velocity,\n    iteration,\n):\n    \"\"\"Update the velocity and position of each agent.\n\n    Parameters\n    ----------\n    population : ndarray of zeros and ones\n        The population defined by the agents.\n    acceleration : ndarray of floats\n        The acceleration of each agent.\n    velocity : ndarray of floats\n        The velocity of each agent.\n    iteration : int\n        The current iteration.\n\n    Returns\n    -------\n    velocity : ndarray of floats\n        The updated velocity of each agent.\n    population : ndarray of zeros and ones\n        The updated population defined by the agents.\n\n    \"\"\"\n    c_factor_local_best = -2 * ((iteration**3) / (self.maxiter**3)) + 2\n    c_factor_global_best = 2 * ((iteration**3) / (self.maxiter**3)) + 2\n    global_best = np.repeat(self.optimal_model, self.n_agents, axis=0).reshape(\n        self.dimension, self.n_agents\n    )\n\n    velocity = (\n        np.random.rand(self.dimension, self.n_agents) * velocity\n        + c_factor_local_best * acceleration\n        + c_factor_global_best * (global_best - population)\n    )\n    r = np.random.rand(self.dimension, self.n_agents)\n    transform_to_binary = np.absolute(np.tanh(velocity))\n    ind = np.where(r &lt; transform_to_binary)\n    population[ind] = 1 - population[ind]\n    return velocity, population\n</code></pre>"},{"location":"user-guide/API/metamss/","title":"Documentation for <code>MetaMSS</code>","text":"<p>Meta Model Structure Selection.</p>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS","title":"<code>MetaMSS</code>","text":"<p>               Bases: <code>SimulateNARMAX</code>, <code>BPSOGSA</code></p> <p>Meta-Model Structure Selection: Building Polynomial NARMAX model.</p> <p>This class uses the MetaMSS ([1], [2], [3]_) algorithm to build NARMAX models. The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},     e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>loss_func</code> <code>str</code> <p>The loss function to be minimized.</p> <code>\"metamss_loss\"</code> <code>estimator</code> <code>str</code> <p>The parameter estimation method.</p> <code>\"least_squares\"</code> <code>estimate_parameter</code> <code>bool</code> <p>Whether to estimate the model parameters.</p> <code>True</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>eps</code> <code>maxiter</code> <code>int</code> <p>The maximum number of iterations.</p> <code>30</code> <code>alpha</code> <code>int</code> <p>The descending coefficient of the gravitational constant.</p> <code>23</code> <code>g_zero</code> <code>int</code> <p>The initial value of the gravitational constant.</p> <code>100</code> <code>k_agents_percent</code> <code>int</code> <p>Percent of agents applying force to the others in the last iteration.</p> <code>2</code> <code>norm</code> <code>int</code> <p>The information criteria method to be used.</p> <code>-2</code> <code>power</code> <code>int</code> <p>The number of the model terms to be selected. Note that n_terms overwrite the information criteria values.</p> <code>2</code> <code>n_agents</code> <code>int</code> <p>The number of agents to search the optimal solution.</p> <code>10</code> <code>p_zeros</code> <code>float</code> <p>The probability of getting ones in the construction of the population.</p> <code>0.5</code> <code>p_zeros</code> <code>float</code> <p>The probability of getting zeros in the construction of the population.</p> <code>0.5</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import MetaMSS\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=400,\n...                                                    colored_noise=False,\n...                                                    sigma=0.001,\n...                                                    train_percentage=80)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = MetaMSS(\n...     basis_function=basis_function,\n...     norm=-2,\n...     xlag=7,\n...     ylag=7,\n...     k_agents_percent=2,\n...     estimate_parameter=True,\n...     maxiter=30,\n...     n_agents=10,\n...     p_value=0.05,\n...     loss_func='metamss_loss'\n... )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Manuscript: Meta-Model Structure Selection: Building Polynomial NARX Model    for Regression and Classification    https://arxiv.org/pdf/2109.09917.pdf</li> <li>Manuscript (Portuguese): Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares    Utilizando o Algoritmo H\u00edbrido e Bin\u00e1rio de Otimiza\u00e7\u00e3o por    Enxame de Part\u00edculas e Busca Gravitacional    DOI: 10.17648/sbai-2019-111317</li> <li>Master thesis: Meta model structure selection: an algorithm for    building polynomial NARX models for regression and classification</li> </ul> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>class MetaMSS(SimulateNARMAX, BPSOGSA):\n    r\"\"\"Meta-Model Structure Selection: Building Polynomial NARMAX model.\n\n    This class uses the MetaMSS ([1]_, [2]_, [3]_) algorithm to build NARMAX models.\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},\n        e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    loss_func : str, default=\"metamss_loss\"\n        The loss function to be minimized.\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    estimate_parameter : bool, default=True\n        Whether to estimate the model parameters.\n    eps : float\n        Normalization factor of the normalized filters.\n    maxiter : int, default=30\n        The maximum number of iterations.\n    alpha : int, default=23\n        The descending coefficient of the gravitational constant.\n    g_zero : int, default=100\n        The initial value of the gravitational constant.\n    k_agents_percent: int, default=2\n        Percent of agents applying force to the others in the last iteration.\n    norm : int, default=-2\n        The information criteria method to be used.\n    power : int, default=2\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_agents : int, default=10\n        The number of agents to search the optimal solution.\n    p_zeros : float, default=0.5\n        The probability of getting ones in the construction of the population.\n    p_zeros : float, default=0.5\n        The probability of getting zeros in the construction of the population.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import MetaMSS\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=400,\n    ...                                                    colored_noise=False,\n    ...                                                    sigma=0.001,\n    ...                                                    train_percentage=80)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = MetaMSS(\n    ...     basis_function=basis_function,\n    ...     norm=-2,\n    ...     xlag=7,\n    ...     ylag=7,\n    ...     k_agents_percent=2,\n    ...     estimate_parameter=True,\n    ...     maxiter=30,\n    ...     n_agents=10,\n    ...     p_value=0.05,\n    ...     loss_func='metamss_loss'\n    ... )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Meta-Model Structure Selection: Building Polynomial NARX Model\n       for Regression and Classification\n       https://arxiv.org/pdf/2109.09917.pdf\n    - Manuscript (Portuguese): Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares\n       Utilizando o Algoritmo H\u00edbrido e Bin\u00e1rio de Otimiza\u00e7\u00e3o por\n       Enxame de Part\u00edculas e Busca Gravitacional\n       DOI: 10.17648/sbai-2019-111317\n    - Master thesis: Meta model structure selection: an algorithm for\n       building polynomial NARX models for regression and classification\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        maxiter: int = 30,\n        alpha: int = 23,\n        g_zero: int = 100,\n        k_agents_percent: int = 2,\n        norm: float = -2,\n        power: int = 2,\n        n_agents: int = 10,\n        p_zeros: float = 0.5,\n        p_ones: float = 0.5,\n        p_value: float = 0.05,\n        xlag: Union[int, list] = 1,\n        ylag: Union[int, list] = 1,\n        elag: Union[int, list] = 1,\n        estimator: Estimators = LeastSquares(),\n        eps: np.float64 = np.finfo(np.float64).eps,\n        estimate_parameter: bool = True,\n        loss_func: str = \"metamss_loss\",\n        model_type: str = \"NARMAX\",\n        basis_function: Polynomial = Polynomial(),\n        steps_ahead: Optional[int] = None,\n        random_state: Optional[int] = None,\n        test_size: float = 0.25,\n    ):\n        super().__init__(\n            estimator=estimator,\n            eps=eps,\n            estimate_parameter=estimate_parameter,\n            model_type=model_type,\n            basis_function=basis_function,\n        )\n\n        BPSOGSA.__init__(\n            self,\n            n_agents=n_agents,\n            maxiter=maxiter,\n            g_zero=g_zero,\n            alpha=alpha,\n            k_agents_percent=k_agents_percent,\n            norm=norm,\n            power=power,\n            p_zeros=p_zeros,\n            p_ones=p_ones,\n        )\n\n        self.xlag = xlag\n        self.ylag = ylag\n        self.elag = elag\n        self.p_value = p_value\n        self.estimator = estimator\n        self.estimate_parameter = estimate_parameter\n        self.loss_func = loss_func\n        self.steps_ahead = steps_ahead\n        self.random_state = random_state\n        self.test_size = test_size\n        self.n_inputs = None\n        self.regressor_code = None\n        self.best_model_history = None\n        self.tested_models = None\n        self.final_model = None\n        self._validate_metamss_params()\n\n    def _validate_metamss_params(self):\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n    def fit(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Fit the polynomial NARMAX model.\n\n        Parameters\n        ----------\n        X : ndarray, optional\n            The input data to be used in the training process.\n        y : ndarray\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        self : returns an instance of self.\n\n        \"\"\"\n        if not isinstance(self.basis_function, Polynomial):\n            raise NotImplementedError(\n                \"Currently MetaMSS only supports polynomial models.\"\n            )\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        if X is not None:\n            check_x_y(X, y)\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.max_lag = self._get_max_lag()\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        self.dimension = self.regressor_code.shape[0]\n        velocity = np.zeros([self.dimension, self.n_agents])\n        self.random_state = check_random_state(self.random_state)\n        population = self.generate_random_population(self.random_state)\n        self.best_by_iter = []\n        self.mean_by_iter = []\n        self.optimal_fitness_value = np.inf\n        self.optimal_model = None\n        self.best_model_history = []\n        self.tested_models = []\n\n        x, x_test, y, y_test = train_test_split(X, y, test_size=self.test_size)\n\n        for i in range(self.maxiter):\n            fitness = self.evaluate_objective_function(x, y, x_test, y_test, population)\n            column_of_best_solution = np.nanargmin(fitness)\n            current_best_fitness = fitness[column_of_best_solution]\n\n            if current_best_fitness &lt; self.optimal_fitness_value:\n                self.optimal_fitness_value = current_best_fitness\n                self.optimal_model = population[:, column_of_best_solution].copy()\n                self.best_model_history.append(self.optimal_model)\n\n            self.best_by_iter.append(self.optimal_fitness_value)\n            self.mean_by_iter.append(np.mean(fitness))\n            agent_mass = self.mass_calculation(fitness)\n            gravitational_constant = self.calculate_gravitational_constant(i)\n            acceleration = self.calculate_acceleration(\n                population, agent_mass, gravitational_constant, i\n            )\n            velocity, population = self.update_velocity_position(\n                population,\n                acceleration,\n                velocity,\n                i,\n            )\n\n        self.final_model = self.regressor_code[self.optimal_model == 1].copy()\n        _ = self.simulate(\n            X_train=x,\n            y_train=y,\n            X_test=x_test,\n            y_test=y_test,\n            model_code=self.final_model,\n            steps_ahead=self.steps_ahead,\n        )\n        self.max_lag = self._get_max_lag()\n        return self\n\n    def evaluate_objective_function(\n        self,\n        x_train: Optional[np.ndarray],\n        y_train: Optional[np.ndarray],\n        x_test: Optional[np.ndarray],\n        y_test: Optional[np.ndarray],\n        population: np.ndarray,\n    ):\n        \"\"\"Fit the polynomial NARMAX model.\n\n        Parameters\n        ----------\n        x_train : ndarray of floats\n            The input data to be used in the training process.\n        y_train : ndarray of floats\n            The output data to be used in the training process.\n        x_test : ndarray of floats\n            The input data to be used in the prediction process.\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        population : ndarray of zeros and ones\n            The initial population of agents.\n\n        Returns\n        -------\n        fitness_value : ndarray\n            The fitness value of each agent.\n        \"\"\"\n        fitness = []\n        for agent in population.T:\n            if np.all(agent == 0):\n                fitness.append(30)  # penalty for cases where there is no terms\n                continue\n\n            m = self.regressor_code[agent == 1].copy()\n            yhat = self.simulate(\n                X_train=x_train,\n                y_train=y_train,\n                X_test=x_test,\n                y_test=y_test,\n                model_code=m,\n                steps_ahead=self.steps_ahead,\n            )\n\n            residues = y_test - yhat\n            self.max_lag = self._get_max_lag()\n            lagged_data = build_lagged_matrix(\n                x_train, y_train, self.xlag, self.ylag, self.model_type\n            )\n\n            psi = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.xlag,\n                self.ylag,\n                self.model_type,\n                predefined_regressors=self.pivv,\n            )\n\n            pos_insignificant_terms, _, _ = self.perform_t_test(\n                psi, self.theta, residues\n            )\n\n            pos_aux = np.where(agent == 1)[0]\n            pos_aux = pos_aux[pos_insignificant_terms]\n            agent[pos_aux] = 0\n\n            m = self.regressor_code[agent == 1].copy()\n\n            if np.all(agent == 0):\n                fitness.append(1000)  # just a big number as penalty\n                continue\n\n            yhat = self.simulate(\n                X_train=x_train,\n                y_train=y_train,\n                X_test=x_test,\n                y_test=y_test,\n                model_code=m,\n                steps_ahead=self.steps_ahead,\n            )\n\n            self.final_model = m.copy()\n            self.tested_models.append(m)\n            if len(self.theta) == 0:\n                print(m)\n            d = getattr(self, self.loss_func)(y_test, yhat, len(self.theta))\n            fitness.append(d)\n\n        return fitness\n\n    def perform_t_test(\n        self, psi: np.ndarray, theta: np.ndarray, residues: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the t-test given the p-value defined by the user.\n\n        Parameters\n        ----------\n        psi : array\n            the data matrix of regressors\n        theta : array\n            the parameters estimated via least squares algorithm\n        residues : array\n            the identification residues of the solution\n\n        Returns\n        -------\n        pos_insignificant_terms : array\n            these regressors in the actual candidate solution are removed\n            from the population since they are insignificant\n        t_test : array\n            the values of the p_value of each regressor of the model\n        tail2p: array\n            The calculated two-tailed p-value.\n\n        \"\"\"\n        sum_of_squared_residues = np.sum(residues**2)\n        variance_of_residues = sum_of_squared_residues / (len(residues) - psi.shape[1])\n        if np.isnan(variance_of_residues):\n            variance_of_residues = 4.3645e05\n\n        skk = np.linalg.pinv(psi.T.dot(psi))\n        skk_diag = np.diag(skk)\n        var_e = variance_of_residues * skk_diag\n        se_theta = np.sqrt(var_e)\n        se_theta = se_theta.reshape(-1, 1)\n        t_test = theta / se_theta\n        degree_of_freedom = psi.shape[0] - psi.shape[1]\n\n        tail2p = 2 * t.cdf(-np.abs(t_test), degree_of_freedom)\n\n        pos_insignificant_terms = np.where(tail2p &gt; self.p_value)[0]\n        pos_insignificant_terms = pos_insignificant_terms.reshape(-1, 1).T\n        if pos_insignificant_terms.shape == 0:\n            return np.array([]), t_test, tail2p\n\n        return pos_insignificant_terms, t_test, tail2p\n\n    def aic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n        \"\"\"Calculate the Akaike Information Criterion.\n\n        Parameters\n        ----------\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        yhat : ndarray of floats\n            The n-steps-ahead predicted values of the model.\n        n_theta : ndarray of floats\n            The number of model parameters.\n\n        Returns\n        -------\n        aic : float\n            The Akaike Information Criterion\n\n        \"\"\"\n        mse = mean_squared_error(y_test, yhat)\n        n = y_test.shape[0]\n        return n * np.log(mse) + 2 * n_theta\n\n    def bic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n        \"\"\"Calculate the Bayesian Information Criterion.\n\n        Parameters\n        ----------\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        yhat : ndarray of floats\n            The n-steps-ahead predicted values of the model.\n        n_theta : ndarray of floats\n            The number of model parameters.\n\n        Returns\n        -------\n        bic : float\n            The Bayesian Information Criterion\n\n        \"\"\"\n        mse = mean_squared_error(y_test, yhat)\n        n = y_test.shape[0]\n        return n * np.log(mse) + n_theta + np.log(n)\n\n    def metamss_loss(self, y_test: np.ndarray, yhat: np.ndarray, n_terms: int) -&gt; float:\n        \"\"\"Calculate the MetaMSS loss function.\n\n        Parameters\n        ----------\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        yhat : ndarray of floats\n            The n-steps-ahead predicted values of the model.\n        n_terms : ndarray of floats\n            The number of model parameters.\n\n        Returns\n        -------\n        metamss_loss : float\n            The MetaMSS loss function\n\n        \"\"\"\n        penalty_count = np.arange(0, self.dimension)\n        penalty_distribution = (np.log(n_terms + 1) ** (-1)) / self.dimension\n        penalty = self.sigmoid_linear_unit_derivative(\n            penalty_count, self.dimension / 2, penalty_distribution\n        )\n\n        penalty = penalty - np.min(penalty)\n        rmse = root_relative_squared_error(y_test, yhat)\n        fitness = rmse * penalty[n_terms]\n        if np.isnan(fitness):\n            fitness = 30\n\n        return fitness\n\n    def sigmoid_linear_unit_derivative(self, x, c, a):\n        \"\"\"Calculate the derivative of the Sigmoid Linear Unit function.\n\n        The derivative of Sigmoid Linear Unit (dSiLU) function can be\n        viewed as a overshooting version of the sigmoid function.\n\n        Parameters\n        ----------\n        x : ndarray\n            The range of the regressors space.\n        a : float\n            The rate of change.\n        c : int\n            Corresponds to the x value where y = 0.5.\n\n        Returns\n        -------\n        penalty : ndarray of floats\n            The values of the penalty function\n\n        \"\"\"\n        return (\n            1\n            / (1 + np.exp(-a * (x - c)))\n            * (1 + (a * (x - c)) * (1 - 1 / (1 + np.exp(-a * (x - c)))))\n        )\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: Optional[np.ndarray] = None,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        raise NotImplementedError(\n            \"MetaMSS doesn't support basis functions other than polynomial yet.\",\n        )\n\n    def _one_step_ahead_prediction(\n        self, x: Optional[np.ndarray], y: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._one_step_ahead_prediction(x, y)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(\n        self, x: Optional[np.ndarray], y_initial: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in MetaMSS for now.\"\n        )\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in MetaMSS for now.\"\n        )\n\n    def _basis_function_n_steps_horizon(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in MetaMSS for now.\"\n        )\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.aic","title":"<code>aic(y_test, yhat, n_theta)</code>","text":"<p>Calculate the Akaike Information Criterion.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> required <code>yhat</code> <code>ndarray of floats</code> <p>The n-steps-ahead predicted values of the model.</p> required <code>n_theta</code> <code>ndarray of floats</code> <p>The number of model parameters.</p> required <p>Returns:</p> Name Type Description <code>aic</code> <code>float</code> <p>The Akaike Information Criterion</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def aic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n    \"\"\"Calculate the Akaike Information Criterion.\n\n    Parameters\n    ----------\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    yhat : ndarray of floats\n        The n-steps-ahead predicted values of the model.\n    n_theta : ndarray of floats\n        The number of model parameters.\n\n    Returns\n    -------\n    aic : float\n        The Akaike Information Criterion\n\n    \"\"\"\n    mse = mean_squared_error(y_test, yhat)\n    n = y_test.shape[0]\n    return n * np.log(mse) + 2 * n_theta\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.bic","title":"<code>bic(y_test, yhat, n_theta)</code>","text":"<p>Calculate the Bayesian Information Criterion.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> required <code>yhat</code> <code>ndarray of floats</code> <p>The n-steps-ahead predicted values of the model.</p> required <code>n_theta</code> <code>ndarray of floats</code> <p>The number of model parameters.</p> required <p>Returns:</p> Name Type Description <code>bic</code> <code>float</code> <p>The Bayesian Information Criterion</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def bic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n    \"\"\"Calculate the Bayesian Information Criterion.\n\n    Parameters\n    ----------\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    yhat : ndarray of floats\n        The n-steps-ahead predicted values of the model.\n    n_theta : ndarray of floats\n        The number of model parameters.\n\n    Returns\n    -------\n    bic : float\n        The Bayesian Information Criterion\n\n    \"\"\"\n    mse = mean_squared_error(y_test, yhat)\n    n = y_test.shape[0]\n    return n * np.log(mse) + n_theta + np.log(n)\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.evaluate_objective_function","title":"<code>evaluate_objective_function(x_train, y_train, x_test, y_test, population)</code>","text":"<p>Fit the polynomial NARMAX model.</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> required <code>y_train</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <code>x_test</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> required <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> required <code>population</code> <code>ndarray of zeros and ones</code> <p>The initial population of agents.</p> required <p>Returns:</p> Name Type Description <code>fitness_value</code> <code>ndarray</code> <p>The fitness value of each agent.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def evaluate_objective_function(\n    self,\n    x_train: Optional[np.ndarray],\n    y_train: Optional[np.ndarray],\n    x_test: Optional[np.ndarray],\n    y_test: Optional[np.ndarray],\n    population: np.ndarray,\n):\n    \"\"\"Fit the polynomial NARMAX model.\n\n    Parameters\n    ----------\n    x_train : ndarray of floats\n        The input data to be used in the training process.\n    y_train : ndarray of floats\n        The output data to be used in the training process.\n    x_test : ndarray of floats\n        The input data to be used in the prediction process.\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    population : ndarray of zeros and ones\n        The initial population of agents.\n\n    Returns\n    -------\n    fitness_value : ndarray\n        The fitness value of each agent.\n    \"\"\"\n    fitness = []\n    for agent in population.T:\n        if np.all(agent == 0):\n            fitness.append(30)  # penalty for cases where there is no terms\n            continue\n\n        m = self.regressor_code[agent == 1].copy()\n        yhat = self.simulate(\n            X_train=x_train,\n            y_train=y_train,\n            X_test=x_test,\n            y_test=y_test,\n            model_code=m,\n            steps_ahead=self.steps_ahead,\n        )\n\n        residues = y_test - yhat\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(\n            x_train, y_train, self.xlag, self.ylag, self.model_type\n        )\n\n        psi = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.xlag,\n            self.ylag,\n            self.model_type,\n            predefined_regressors=self.pivv,\n        )\n\n        pos_insignificant_terms, _, _ = self.perform_t_test(\n            psi, self.theta, residues\n        )\n\n        pos_aux = np.where(agent == 1)[0]\n        pos_aux = pos_aux[pos_insignificant_terms]\n        agent[pos_aux] = 0\n\n        m = self.regressor_code[agent == 1].copy()\n\n        if np.all(agent == 0):\n            fitness.append(1000)  # just a big number as penalty\n            continue\n\n        yhat = self.simulate(\n            X_train=x_train,\n            y_train=y_train,\n            X_test=x_test,\n            y_test=y_test,\n            model_code=m,\n            steps_ahead=self.steps_ahead,\n        )\n\n        self.final_model = m.copy()\n        self.tested_models.append(m)\n        if len(self.theta) == 0:\n            print(m)\n        d = getattr(self, self.loss_func)(y_test, yhat, len(self.theta))\n        fitness.append(d)\n\n    return fitness\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Fit the polynomial NARMAX model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray</code> <p>The output data to be used in the training process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>returns an instance of self.</code> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def fit(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: Optional[np.ndarray] = None,\n):\n    \"\"\"Fit the polynomial NARMAX model.\n\n    Parameters\n    ----------\n    X : ndarray, optional\n        The input data to be used in the training process.\n    y : ndarray\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    self : returns an instance of self.\n\n    \"\"\"\n    if not isinstance(self.basis_function, Polynomial):\n        raise NotImplementedError(\n            \"Currently MetaMSS only supports polynomial models.\"\n        )\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    if X is not None:\n        check_x_y(X, y)\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.max_lag = self._get_max_lag()\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    self.dimension = self.regressor_code.shape[0]\n    velocity = np.zeros([self.dimension, self.n_agents])\n    self.random_state = check_random_state(self.random_state)\n    population = self.generate_random_population(self.random_state)\n    self.best_by_iter = []\n    self.mean_by_iter = []\n    self.optimal_fitness_value = np.inf\n    self.optimal_model = None\n    self.best_model_history = []\n    self.tested_models = []\n\n    x, x_test, y, y_test = train_test_split(X, y, test_size=self.test_size)\n\n    for i in range(self.maxiter):\n        fitness = self.evaluate_objective_function(x, y, x_test, y_test, population)\n        column_of_best_solution = np.nanargmin(fitness)\n        current_best_fitness = fitness[column_of_best_solution]\n\n        if current_best_fitness &lt; self.optimal_fitness_value:\n            self.optimal_fitness_value = current_best_fitness\n            self.optimal_model = population[:, column_of_best_solution].copy()\n            self.best_model_history.append(self.optimal_model)\n\n        self.best_by_iter.append(self.optimal_fitness_value)\n        self.mean_by_iter.append(np.mean(fitness))\n        agent_mass = self.mass_calculation(fitness)\n        gravitational_constant = self.calculate_gravitational_constant(i)\n        acceleration = self.calculate_acceleration(\n            population, agent_mass, gravitational_constant, i\n        )\n        velocity, population = self.update_velocity_position(\n            population,\n            acceleration,\n            velocity,\n            i,\n        )\n\n    self.final_model = self.regressor_code[self.optimal_model == 1].copy()\n    _ = self.simulate(\n        X_train=x,\n        y_train=y,\n        X_test=x_test,\n        y_test=y_test,\n        model_code=self.final_model,\n        steps_ahead=self.steps_ahead,\n    )\n    self.max_lag = self._get_max_lag()\n    return self\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.metamss_loss","title":"<code>metamss_loss(y_test, yhat, n_terms)</code>","text":"<p>Calculate the MetaMSS loss function.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> required <code>yhat</code> <code>ndarray of floats</code> <p>The n-steps-ahead predicted values of the model.</p> required <code>n_terms</code> <code>ndarray of floats</code> <p>The number of model parameters.</p> required <p>Returns:</p> Name Type Description <code>metamss_loss</code> <code>float</code> <p>The MetaMSS loss function</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def metamss_loss(self, y_test: np.ndarray, yhat: np.ndarray, n_terms: int) -&gt; float:\n    \"\"\"Calculate the MetaMSS loss function.\n\n    Parameters\n    ----------\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    yhat : ndarray of floats\n        The n-steps-ahead predicted values of the model.\n    n_terms : ndarray of floats\n        The number of model parameters.\n\n    Returns\n    -------\n    metamss_loss : float\n        The MetaMSS loss function\n\n    \"\"\"\n    penalty_count = np.arange(0, self.dimension)\n    penalty_distribution = (np.log(n_terms + 1) ** (-1)) / self.dimension\n    penalty = self.sigmoid_linear_unit_derivative(\n        penalty_count, self.dimension / 2, penalty_distribution\n    )\n\n    penalty = penalty - np.min(penalty)\n    rmse = root_relative_squared_error(y_test, yhat)\n    fitness = rmse * penalty[n_terms]\n    if np.isnan(fitness):\n        fitness = 30\n\n    return fitness\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.perform_t_test","title":"<code>perform_t_test(psi, theta, residues)</code>","text":"<p>Perform the t-test given the p-value defined by the user.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>array</code> <p>the data matrix of regressors</p> required <code>theta</code> <code>array</code> <p>the parameters estimated via least squares algorithm</p> required <code>residues</code> <code>array</code> <p>the identification residues of the solution</p> required <p>Returns:</p> Name Type Description <code>pos_insignificant_terms</code> <code>array</code> <p>these regressors in the actual candidate solution are removed from the population since they are insignificant</p> <code>t_test</code> <code>array</code> <p>the values of the p_value of each regressor of the model</p> <code>tail2p</code> <code>array</code> <p>The calculated two-tailed p-value.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def perform_t_test(\n    self, psi: np.ndarray, theta: np.ndarray, residues: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the t-test given the p-value defined by the user.\n\n    Parameters\n    ----------\n    psi : array\n        the data matrix of regressors\n    theta : array\n        the parameters estimated via least squares algorithm\n    residues : array\n        the identification residues of the solution\n\n    Returns\n    -------\n    pos_insignificant_terms : array\n        these regressors in the actual candidate solution are removed\n        from the population since they are insignificant\n    t_test : array\n        the values of the p_value of each regressor of the model\n    tail2p: array\n        The calculated two-tailed p-value.\n\n    \"\"\"\n    sum_of_squared_residues = np.sum(residues**2)\n    variance_of_residues = sum_of_squared_residues / (len(residues) - psi.shape[1])\n    if np.isnan(variance_of_residues):\n        variance_of_residues = 4.3645e05\n\n    skk = np.linalg.pinv(psi.T.dot(psi))\n    skk_diag = np.diag(skk)\n    var_e = variance_of_residues * skk_diag\n    se_theta = np.sqrt(var_e)\n    se_theta = se_theta.reshape(-1, 1)\n    t_test = theta / se_theta\n    degree_of_freedom = psi.shape[0] - psi.shape[1]\n\n    tail2p = 2 * t.cdf(-np.abs(t_test), degree_of_freedom)\n\n    pos_insignificant_terms = np.where(tail2p &gt; self.p_value)[0]\n    pos_insignificant_terms = pos_insignificant_terms.reshape(-1, 1).T\n    if pos_insignificant_terms.shape == 0:\n        return np.array([]), t_test, tail2p\n\n    return pos_insignificant_terms, t_test, tail2p\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=1)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: Optional[np.ndarray] = None,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: int = 1,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    raise NotImplementedError(\n        \"MetaMSS doesn't support basis functions other than polynomial yet.\",\n    )\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.sigmoid_linear_unit_derivative","title":"<code>sigmoid_linear_unit_derivative(x, c, a)</code>","text":"<p>Calculate the derivative of the Sigmoid Linear Unit function.</p> <p>The derivative of Sigmoid Linear Unit (dSiLU) function can be viewed as a overshooting version of the sigmoid function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The range of the regressors space.</p> required <code>a</code> <code>float</code> <p>The rate of change.</p> required <code>c</code> <code>int</code> <p>Corresponds to the x value where y = 0.5.</p> required <p>Returns:</p> Name Type Description <code>penalty</code> <code>ndarray of floats</code> <p>The values of the penalty function</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def sigmoid_linear_unit_derivative(self, x, c, a):\n    \"\"\"Calculate the derivative of the Sigmoid Linear Unit function.\n\n    The derivative of Sigmoid Linear Unit (dSiLU) function can be\n    viewed as a overshooting version of the sigmoid function.\n\n    Parameters\n    ----------\n    x : ndarray\n        The range of the regressors space.\n    a : float\n        The rate of change.\n    c : int\n        Corresponds to the x value where y = 0.5.\n\n    Returns\n    -------\n    penalty : ndarray of floats\n        The values of the penalty function\n\n    \"\"\"\n    return (\n        1\n        / (1 + np.exp(-a * (x - c)))\n        * (1 + (a * (x - c)) * (1 - 1 / (1 + np.exp(-a * (x - c)))))\n    )\n</code></pre>"},{"location":"user-guide/API/metrics/","title":"Documentation for <code>Metrics</code>","text":"<p>Common metrics to assess performance on NARX models.</p>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.explained_variance_score","title":"<code>explained_variance_score(y, yhat)</code>","text":"<p>Calculate the Explained Variance Score.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>EVS output is non-negative values. Becoming 1.0 means your model outputs are exactly matched by true target values. Lower values means worse results.</p> References <ul> <li>Wikipedia entry on the Explained Variance    https://en.wikipedia.org/wiki/Explained_variation</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; explained_variance_score(y, yhat)\n0.957\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def explained_variance_score(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Explained Variance Score.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        EVS output is non-negative values. Becoming 1.0 means your\n        model outputs are exactly matched by true target values.\n        Lower values means worse results.\n\n    References\n    ----------\n    - Wikipedia entry on the Explained Variance\n       https://en.wikipedia.org/wiki/Explained_variation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; explained_variance_score(y, yhat)\n    0.957\n\n    \"\"\"\n    y_diff_avg = np.average(y - yhat)\n    numerator = np.average((y - yhat - y_diff_avg) ** 2)\n    y_avg = np.average(y)\n    denominator = np.average((y - y_avg) ** 2)\n    nonzero_numerator = numerator != 0\n    nonzero_denominator = denominator != 0\n    valid_score = nonzero_numerator &amp; nonzero_denominator\n    output_scores = np.ones(y.shape[0])\n    output_scores[valid_score] = 1 - (numerator[valid_score] / denominator[valid_score])\n    output_scores[nonzero_numerator &amp; ~nonzero_denominator] = 0.0\n    return np.average(output_scores)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.forecast_error","title":"<code>forecast_error(y, yhat)</code>","text":"<p>Calculate the forecast error in a regression model.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>ndarray of floats</code> <p>The difference between the true target values and the predicted or forecast value in regression or any other phenomenon.</p> References <ul> <li>Wikipedia entry on the Forecast error    https://en.wikipedia.org/wiki/Forecast_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; forecast_error(y, yhat)\n[0.5, -0.5, 0, -1]\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def forecast_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the forecast error in a regression model.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : ndarray of floats\n        The difference between the true target values and the predicted\n        or forecast value in regression or any other phenomenon.\n\n    References\n    ----------\n    - Wikipedia entry on the Forecast error\n       https://en.wikipedia.org/wiki/Forecast_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; forecast_error(y, yhat)\n    [0.5, -0.5, 0, -1]\n\n    \"\"\"\n    return np.array(y - yhat)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.mean_absolute_error","title":"<code>mean_absolute_error(y, yhat)</code>","text":"<p>Calculate the Mean absolute error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float or ndarray of floats</code> <p>MAE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the Mean absolute error    https://en.wikipedia.org/wiki/Mean_absolute_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; mean_absolute_error(y, yhat)\n0.5\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_absolute_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Mean absolute error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        MAE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Mean absolute error\n       https://en.wikipedia.org/wiki/Mean_absolute_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; mean_absolute_error(y, yhat)\n    0.5\n\n    \"\"\"\n    output_errors = np.average(np.abs(y - yhat))\n    return np.average(output_errors)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.mean_forecast_error","title":"<code>mean_forecast_error(y, yhat)</code>","text":"<p>Calculate the mean of forecast error of a regression model.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>The mean  value of the difference between the true target values and the predicted or forecast value in regression or any other phenomenon.</p> References <ul> <li>Wikipedia entry on the Forecast error    https://en.wikipedia.org/wiki/Forecast_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; mean_forecast_error(y, yhat)\n-0.25\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_forecast_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the mean of forecast error of a regression model.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        The mean  value of the difference between the true target\n        values and the predicted or forecast value in regression\n        or any other phenomenon.\n\n    References\n    ----------\n    - Wikipedia entry on the Forecast error\n       https://en.wikipedia.org/wiki/Forecast_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; mean_forecast_error(y, yhat)\n    -0.25\n\n    \"\"\"\n    return np.average(y - yhat)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.mean_squared_error","title":"<code>mean_squared_error(y, yhat)</code>","text":"<p>Calculate the Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>MSE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the Mean Squared Error    https://en.wikipedia.org/wiki/Mean_squared_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; mean_squared_error(y, yhat)\n0.375\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        MSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Mean Squared Error\n       https://en.wikipedia.org/wiki/Mean_squared_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; mean_squared_error(y, yhat)\n    0.375\n\n    \"\"\"\n    output_error = np.average((y - yhat) ** 2)\n    return np.average(output_error)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.mean_squared_log_error","title":"<code>mean_squared_log_error(y, yhat)</code>","text":"<p>Calculate the Mean Squared Logarithmic Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>MSLE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, 5, 2.5, 7]\n&gt;&gt;&gt; yhat = [2.5, 5, 4, 8]\n&gt;&gt;&gt; mean_squared_log_error(y, yhat)\n0.039\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_squared_log_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Mean Squared Logarithmic Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        MSLE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, 5, 2.5, 7]\n    &gt;&gt;&gt; yhat = [2.5, 5, 4, 8]\n    &gt;&gt;&gt; mean_squared_log_error(y, yhat)\n    0.039\n\n    \"\"\"\n    return mean_squared_error(np.log1p(y), np.log1p(yhat))\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.median_absolute_error","title":"<code>median_absolute_error(y, yhat)</code>","text":"<p>Calculate the Median Absolute Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>MdAE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the Median absolute deviation    https://en.wikipedia.org/wiki/Median_absolute_deviation</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; median_absolute_error(y, yhat)\n0.5\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def median_absolute_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Median Absolute Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        MdAE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Median absolute deviation\n       https://en.wikipedia.org/wiki/Median_absolute_deviation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; median_absolute_error(y, yhat)\n    0.5\n\n    \"\"\"\n    return np.median(np.abs(y - yhat))\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.normalized_root_mean_squared_error","title":"<code>normalized_root_mean_squared_error(y, yhat)</code>","text":"<p>Calculate the normalized Root Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>nRMSE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the normalized Root Mean Squared Error    https://en.wikipedia.org/wiki/Root-mean-square_deviation</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; normalized_root_mean_squared_error(y, yhat)\n0.081\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def normalized_root_mean_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the normalized Root Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        nRMSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the normalized Root Mean Squared Error\n       https://en.wikipedia.org/wiki/Root-mean-square_deviation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; normalized_root_mean_squared_error(y, yhat)\n    0.081\n\n    \"\"\"\n    return root_mean_squared_error(y, yhat) / (y.max() - y.min())\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.r2_score","title":"<code>r2_score(y, yhat)</code>","text":"<p>Calculate the R2 score. Based on sklearn solution.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>R2 output can be non-negative values or negative value. Becoming 1.0 means your model outputs are exactly matched by true target values. Lower values means worse results.</p> Notes <p>This is not a symmetric function.</p> References <ul> <li>Wikipedia entry on the Coefficient of determination    https://en.wikipedia.org/wiki/Coefficient_of_determination</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; explained_variance_score(y, yhat)\n0.948\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def r2_score(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the R2 score. Based on sklearn solution.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        R2 output can be non-negative values or negative value.\n        Becoming 1.0 means your model outputs are exactly\n        matched by true target values. Lower values means worse results.\n\n    Notes\n    -----\n    This is not a symmetric function.\n\n    References\n    ----------\n    - Wikipedia entry on the Coefficient of determination\n       https://en.wikipedia.org/wiki/Coefficient_of_determination\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; explained_variance_score(y, yhat)\n    0.948\n\n    \"\"\"\n    numerator = ((y - yhat) ** 2).sum(axis=0, dtype=np.float64)\n    denominator = ((y - np.average(y, axis=0)) ** 2).sum(axis=0, dtype=np.float64)\n    nonzero_denominator = denominator != 0\n    nonzero_numerator = numerator != 0\n    valid_score = nonzero_denominator &amp; nonzero_numerator\n    output_scores = np.ones([y.shape[1]])\n    output_scores[valid_score] = 1 - (numerator[valid_score] / denominator[valid_score])\n    # arbitrary set to zero to avoid -inf scores, having a constant\n    # y_true is not interesting for scoring a regression anyway\n    output_scores[nonzero_numerator &amp; ~nonzero_denominator] = 0.0\n    return np.average(output_scores)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.root_mean_squared_error","title":"<code>root_mean_squared_error(y, yhat)</code>","text":"<p>Calculate the Root Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>RMSE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the Root Mean Squared Error    https://en.wikipedia.org/wiki/Root-mean-square_deviation</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; root_mean_squared_error(y, yhat)\n0.612\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def root_mean_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Root Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        RMSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Root Mean Squared Error\n       https://en.wikipedia.org/wiki/Root-mean-square_deviation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; root_mean_squared_error(y, yhat)\n    0.612\n\n    \"\"\"\n    return np.sqrt(mean_squared_error(y, yhat))\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.root_relative_squared_error","title":"<code>root_relative_squared_error(y, yhat)</code>","text":"<p>Calculate the Root Relative Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>RRSE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; root_relative_mean_squared_error(y, yhat)\n0.206\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def root_relative_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Root Relative Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        RRSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; root_relative_mean_squared_error(y, yhat)\n    0.206\n\n    \"\"\"\n    numerator = np.sum(np.square((yhat - y)))\n    denominator = np.sum(np.square((y - np.mean(y, axis=0))))\n    return np.sqrt(np.divide(numerator, denominator))\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.symmetric_mean_absolute_percentage_error","title":"<code>symmetric_mean_absolute_percentage_error(y, yhat)</code>","text":"<p>Calculate the SMAPE score.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>SMAPE output is a non-negative value. The results are percentages values.</p> Notes <p>One supposed problem with SMAPE is that it is not symmetric since over-forecasts and under-forecasts are not treated equally.</p> References <ul> <li>Wikipedia entry on the Symmetric mean absolute percentage error    https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; symmetric_mean_absolute_percentage_error(y, yhat)\n57.87\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def symmetric_mean_absolute_percentage_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the SMAPE score.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        SMAPE output is a non-negative value.\n        The results are percentages values.\n\n    Notes\n    -----\n    One supposed problem with SMAPE is that it is not symmetric since\n    over-forecasts and under-forecasts are not treated equally.\n\n    References\n    ----------\n    - Wikipedia entry on the Symmetric mean absolute percentage error\n       https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; symmetric_mean_absolute_percentage_error(y, yhat)\n    57.87\n\n    \"\"\"\n    return 100 / len(y) * np.sum(2 * np.abs(yhat - y) / (np.abs(y) + np.abs(yhat)))\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/","title":"Documentation for <code>Multiobjective Parameter Estimation</code>","text":"<p>Affine Information Least Squares for NARMAX models.</p>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS","title":"<code>AILS</code>","text":"<p>Affine Information Least Squares (AILS) for NARMAX Parameter Estimation.</p> <p>AILS is a non-iterative multiobjective Least Squares technique used for finding Pareto-set solutions in NARMAX (Nonlinear AutoRegressive Moving Average with eXogenous inputs) model parameter estimation. This method is suitable for linear-in-the-parameter model structures.</p> <p>Two types of auxiliary information can be incorporated: static function and steady-state gain.</p> <p>Parameters:</p> Name Type Description Default <code>static_gain</code> <code>bool</code> <p>Flag indicating the presence of data related to steady-state gain.</p> <code>True</code> <code>static_function</code> <code>bool</code> <p>Flag indicating the presence of data concerning static function.</p> <code>True</code> <code>final_model</code> <code>ndarray</code> <p>Model code representation.</p> <code>[[0], [0]]</code> References <ol> <li>Nepomuceno, E. G., Takahashi, R. H. C., &amp; Aguirre, L. A. (2007). \"Multiobjective parameter estimation for nonlinear systems: Affine information and least-squares formulation.\" International Journal of Control, 80, 863-871.</li> </ol> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>class AILS:\n    \"\"\"Affine Information Least Squares (AILS) for NARMAX Parameter Estimation.\n\n    AILS is a non-iterative multiobjective Least Squares technique used for finding\n    Pareto-set solutions in NARMAX (Nonlinear AutoRegressive Moving Average with\n    eXogenous inputs) model parameter estimation. This method is suitable for\n    linear-in-the-parameter model structures.\n\n    Two types of auxiliary information can be incorporated: static function and\n    steady-state gain.\n\n    Parameters\n    ----------\n    static_gain : bool, default=True\n        Flag indicating the presence of data related to steady-state gain.\n    static_function : bool, default=True\n        Flag indicating the presence of data concerning static function.\n    final_model : ndarray, default=[[0], [0]]\n        Model code representation.\n\n    References\n    ----------\n    1. Nepomuceno, E. G., Takahashi, R. H. C., &amp; Aguirre, L. A. (2007).\n    \"Multiobjective parameter estimation for nonlinear systems: Affine information and\n    least-squares formulation.\"\n    International Journal of Control, 80, 863-871.\n    \"\"\"\n\n    def __init__(\n        self,\n        static_gain: bool = True,\n        static_function: bool = True,\n        final_model: np.ndarray = np.zeros((1, 1)),\n        normalize: bool = True,\n    ):\n        self.n_inputs = np.max(final_model // 1000) - 1\n        self.degree = np.shape(final_model)[1]\n        self.max_lag = 1\n        self.final_model = final_model\n        self.static_gain = static_gain\n        self.static_function = static_function\n        self.normalize = normalize\n\n    def build_linear_mapping(self):\n        \"\"\"Assemble the linear mapping matrix R using the regressor-space method.\n\n        This function constructs the linear mapping matrix R, which plays a key role in\n        mapping the parameter vector to the cluster coefficients. It also generates a\n        row matrix qit that assists in locating terms within the linear mapping matrix.\n        This qit matrix is later used in creating the static regressor matrix (Q).\n\n        Returns\n        -------\n        R : ndarray of int\n            A constant matrix of ones and zeros that maps the parameter vector to\n            cluster coefficients.\n        qit : ndarray of int\n            A row matrix that helps locate terms within the linear mapping matrix R and\n            is used in the creation of the static regressor matrix (Q).\n\n        Notes\n        -----\n        The linear mapping matrix R is constructed using the regressor-space method.\n        It plays a crucial role in the parameter estimation process, facilitating the\n        mapping of parameter values to cluster coefficients. The qit matrix aids in\n        term localization within the linear mapping matrix R and is subsequently used\n        to build the static regressor matrix (Q).\n\n        \"\"\"\n        xlag = [1] * self.n_inputs\n\n        object_qit = RegressorDictionary(xlag=xlag, ylag=[1])\n        # Given xlag and ylag equal to 1, there is no repetition of terms, which is\n        # ideal for building qit.\n        qit = object_qit.regressor_space(n_inputs=self.n_inputs) // 1000\n        model = self.final_model // 1000\n        R = np.all(qit[:, None, :] == model, axis=2).astype(int)\n        # Find rows with all zeros in R (sum of row elements is 0)\n        null_rows = list(np.where(np.sum(R, axis=1) == 0)[0])\n\n        R = np.delete(R, null_rows, axis=0)\n        qit = np.delete(qit, null_rows, axis=0)\n        return R, get_term_clustering(qit)\n\n    def build_static_function_information(\n        self, x_static: np.ndarray, y_static: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Construct a matrix of static regressors for a NARMAX model.\n\n        Parameters\n        ----------\n        y_static : array-like, shape (n_samples_static_function,)\n            Output of the static function.\n        x_static : array-like, shape (n_samples_static_function,)\n            Static function input.\n\n        Returns\n        -------\n        Q_dot_R : ndarray of floats, shape (n_samples_static_function, n_parameters)\n            The result of multiplying the matrix of static regressors (Q) with the\n            linear mapping matrix (R), where n_parameters is the number of model\n            parameters.\n        static_covariance: ndarray of floats, shape (n_parameters, n_parameters)\n            The covariance QR'QR\n        static_response: ndarray of floats, shape (n_parameters,)\n            The response QR'y\n\n        Notes\n        -----\n        This function constructs a matrix of static regressors (Q) based on the provided\n        static function outputs (y_static) and inputs (X_static). The linear mapping\n        matrix (R) should be precomputed before calling this function. The result\n        Q_dot_R represents the static regressors for the NARMAX model.\n\n        \"\"\"\n        R, qit = self.build_linear_mapping()\n        Q = y_static ** qit[:, 0]\n        for k in range(self.n_inputs):\n            Q *= x_static ** qit[:, 1 + k]\n\n        Q = Q.reshape(len(y_static), len(qit))\n\n        QR = Q.dot(R)\n        static_covariance = QR.T.dot(QR)\n        static_response = QR.T.dot(y_static)\n        return QR, static_covariance, static_response\n\n    def build_static_gain_information(\n        self, x_static: np.ndarray, y_static: np.ndarray, gain: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Construct a matrix of static regressors referring to the derivative (gain).\n\n        Parameters\n        ----------\n        y_static : array-like, shape (n_samples_static_function,)\n            Output of the static function.\n        x_static : array-like, shape (n_samples_static_function,)\n            Static function input.\n        gain : array-like, shape (n_samples_static_gain,)\n            Static gain input.\n\n        Returns\n        -------\n        HR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n            The matrix of static regressors for the derivative (gain) multiplied by the\n            linear mapping matrix R.\n        gain_covariance : ndarray of floats, shape (n_parameters, n_parameters)\n            The covariance matrix (HR'HR) for the gain-related regressors.\n        gain_response : ndarray of floats, shape (n_parameters,)\n            The response vector (HR'y) for the gain-related regressors.\n\n        Notes\n        -----\n        This function constructs a matrix of static regressors (G+H) for the derivative\n        (gain) based on the provided static function outputs (y_static), inputs\n        (X_static), and gain values. The linear mapping matrix (R) should be\n        precomputed before calling this function.\n\n        \"\"\"\n        R, qit = self.build_linear_mapping()\n        H = np.zeros((len(y_static), len(qit)))\n        G = np.zeros((len(y_static), len(qit)))\n        for i in range(len(y_static)):\n            for j in range(1, len(qit)):\n                if y_static[i, 0] == 0:\n                    if (qit[j, 0]) == 1:\n                        H[i, j] = gain[i][0]\n                    else:\n                        H[i, j] = 0\n                else:\n                    H[i, j] = (gain[i] * qit[j, 0] * y_static[i, 0] ** (qit[j, 0] - 1))[\n                        0\n                    ]\n                for k in range(self.n_inputs):\n                    if x_static[i, k] == 0:\n                        if (qit[j, 1 + k]) == 1:\n                            G[i, j] = 1\n                        else:\n                            G[i, j] = 0\n                    else:\n                        G[i, j] = qit[j, 1 + k] * x_static[i, k] ** (qit[j, 1 + k] - 1)\n\n        HR = (G + H).dot(R)\n        gain_covariance = HR.T.dot(HR)\n        gain_response = HR.T.dot(gain)\n        return HR, gain_covariance, gain_response\n\n    def build_system_data(\n        self,\n        y: np.ndarray,\n        static_gain: np.ndarray,\n        static_function: np.ndarray,\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Construct a list of output data components for the NARMAX system.\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The target data used in the identification process.\n        static_gain : ndarray of floats\n            Static gain output data.\n        static_function : ndarray of floats\n            Static function output data.\n\n        Returns\n        -------\n        system_data : list of ndarrays\n            A list containing data components, including the target data (y),\n            static gain data (if present), and static function data (if present).\n\n        Notes\n        -----\n        This method constructs a list of data components that are used in the NARMAX\n        system identification process. The components may include the target data (y),\n        static gain data (if enabled), and static function data (if enabled).\n\n        \"\"\"\n        if not self.static_gain:\n            return [y] + [static_function]\n\n        if not self.static_function:\n            return [y] + [static_gain]\n\n        return [y] + [static_gain] + [static_function]\n\n    def build_affine_data(\n        self, psi: np.ndarray, HR: np.ndarray, QR: np.ndarray\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Construct a list of affine data components for NARMAX modeling.\n\n        Parameters\n        ----------\n        psi : ndarray of floats, shape (n_samples, n_parameters)\n            The matrix of dynamic regressors.\n        HR : ndarray of floats, shape (n_samples_static_gain, n_parameters)\n            The matrix of static gain regressors.\n        QR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n            The matrix of static function regressors.\n\n        Returns\n        -------\n        affine_data : list of ndarrays\n            A list containing affine data components, including the matrix of static\n            regressors (psi), static gain regressors (if present), and static function\n            regressors (if present).\n\n        Notes\n        -----\n        This method constructs a list of affine data components used in the NARMAX\n        modeling process. The components may include the matrix of static regressors\n        (psi), static gain regressors (if enabled), and static function regressors\n        (if enabled).\n\n        \"\"\"\n        if not self.static_gain:\n            return [psi] + [QR]\n\n        if not self.static_function:\n            return [psi] + [HR]\n\n        return [psi] + [HR] + [QR]\n\n    def build_psi(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build the matrix of dynamic regressor for NARMAX modeling.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        psi : ndarray of floats, shape (n_samples, n_parameters)\n            The matrix of dynamic regressors.\n\n        \"\"\"\n        psi_builder = RegressorDictionary()\n        xlag_code = list_input_regressor_code(self.final_model)\n        ylag_code = list_output_regressor_code(self.final_model)\n        xlag = get_lag_from_regressor_code(xlag_code)\n        ylag = get_lag_from_regressor_code(ylag_code)\n        self.max_lag = get_max_lag_from_model_code(self.final_model)\n        if self.n_inputs != 1:\n            xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n        psi_builder.xlag = xlag\n        psi_builder.ylag = ylag\n        regressor_code = psi_builder.regressor_space(self.n_inputs)\n        pivv = get_index_from_regressor_code(regressor_code, self.final_model)\n        self.final_model = regressor_code[pivv]\n\n        lagged_data = build_input_output_matrix(x=X, y=y, xlag=xlag, ylag=ylag)\n\n        psi = Polynomial(degree=self.degree).fit(\n            lagged_data,\n            max_lag=self.max_lag,\n            ylag=ylag,\n            xlag=xlag,\n            predefined_regressors=pivv,\n        )\n        return psi\n\n    def estimate(\n        self,\n        y_static: np.ndarray = np.zeros(1),\n        X_static: np.ndarray = np.zeros(1),\n        gain: np.ndarray = np.zeros(1),\n        y: np.ndarray = np.zeros(1),\n        X: np.ndarray = np.zeros((1, 1)),\n        weighing_matrix: np.ndarray = np.zeros((1, 1)),\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.int64]:\n        \"\"\"Estimate the parameters via multi-objective techniques.\n\n        Parameters\n        ----------\n        y_static : array-like of shape = n_samples_static_function, default = ([0])\n            Output of static function.\n        X_static : array-like of shape = n_samples_static_function, default = ([0])\n            Static function input.\n        gain : array-like of shape = n_samples_static_gain, default = ([0])\n            Static gain input.\n        y : array-like of shape = n_samples, default = ([0])\n            The target data used in the identification process.\n        X : ndarray of floats, default = ([[0],[0]])\n            Matrix of static regressors.\n        weighing_matrix: ndarray\n            Weighing matrix for defining the weight of each objective.\n\n        Returns\n        -------\n        J : ndarray\n            Matrix referring to the objectives.\n        euclidean_norm : ndarray\n            Matrix of the Euclidean norm.\n        theta : ndarray\n            Matrix with parameters for each weight.\n        HR : ndarray\n            H matrix multiplied by R.\n        QR : ndarray\n            Q matrix multiplied by R.\n        position : ndarray, default = ([[0],[0]])\n            Position of the best theta set.\n\n        \"\"\"\n        psi = self.build_psi(X, y)\n        y = y[self.max_lag :]\n        HR, QR = np.zeros((1, 1)), np.zeros((1, 1))\n        n_parameters = weighing_matrix.shape[1]\n        num_objectives = self.static_function + self.static_gain + 1\n        euclidean_norm = np.zeros(n_parameters)\n        theta = np.zeros((n_parameters, self.final_model.shape[0]))\n        dynamic_covariance = psi.T.dot(psi)\n        dynamic_response = psi.T.dot(y)\n\n        if self.static_function:\n            QR, static_covariance, static_response = (\n                self.build_static_function_information(X_static, y_static)\n            )\n        if self.static_gain:\n            HR, gain_covariance, gain_response = self.build_static_gain_information(\n                X_static, y_static, gain\n            )\n        J = np.zeros((num_objectives, n_parameters))\n        system_data = self.build_system_data(y, gain, y_static)\n        affine_information_data = self.build_affine_data(psi, HR, QR)\n        for i in range(n_parameters):\n            theta1 = weighing_matrix[0, i] * dynamic_covariance\n            theta2 = weighing_matrix[0, i] * dynamic_response\n\n            w = 1\n            if self.static_function:\n                theta1 += weighing_matrix[w, i] * static_covariance\n                theta2 += weighing_matrix[w, i] * static_response.reshape(-1, 1)\n                w += 1\n\n            if self.static_gain:\n                theta1 += weighing_matrix[w, i] * gain_covariance\n                theta2 += weighing_matrix[w, i] * gain_response.reshape(-1, 1)\n                w += 1\n\n            tmp_theta = np.linalg.lstsq(theta1, theta2, rcond=None)[0]\n            theta[i, :] = tmp_theta.T\n\n            for j in range(num_objectives):\n                residuals = get_cost_function(\n                    system_data[j], affine_information_data[j], tmp_theta\n                )\n                J[j, i] = residuals[0]\n\n            euclidean_norm[i] = np.linalg.norm(J[:, i])\n\n        if self.normalize is True:\n            J /= np.max(J, axis=1)[:, np.newaxis]\n            euclidean_norm /= np.max(euclidean_norm)\n\n            euclidean_norm = euclidean_norm / np.max(euclidean_norm)\n\n        position = np.argmin(euclidean_norm)\n        return (\n            J,\n            euclidean_norm,\n            theta,\n            HR,\n            QR,\n            position,\n        )\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_affine_data","title":"<code>build_affine_data(psi, HR, QR)</code>","text":"<p>Construct a list of affine data components for NARMAX modeling.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats, shape (n_samples, n_parameters)</code> <p>The matrix of dynamic regressors.</p> required <code>HR</code> <code>ndarray of floats, shape (n_samples_static_gain, n_parameters)</code> <p>The matrix of static gain regressors.</p> required <code>QR</code> <code>ndarray of floats, shape (n_samples_static_function, n_parameters)</code> <p>The matrix of static function regressors.</p> required <p>Returns:</p> Name Type Description <code>affine_data</code> <code>list of ndarrays</code> <p>A list containing affine data components, including the matrix of static regressors (psi), static gain regressors (if present), and static function regressors (if present).</p> Notes <p>This method constructs a list of affine data components used in the NARMAX modeling process. The components may include the matrix of static regressors (psi), static gain regressors (if enabled), and static function regressors (if enabled).</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_affine_data(\n    self, psi: np.ndarray, HR: np.ndarray, QR: np.ndarray\n) -&gt; List[np.ndarray]:\n    \"\"\"Construct a list of affine data components for NARMAX modeling.\n\n    Parameters\n    ----------\n    psi : ndarray of floats, shape (n_samples, n_parameters)\n        The matrix of dynamic regressors.\n    HR : ndarray of floats, shape (n_samples_static_gain, n_parameters)\n        The matrix of static gain regressors.\n    QR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n        The matrix of static function regressors.\n\n    Returns\n    -------\n    affine_data : list of ndarrays\n        A list containing affine data components, including the matrix of static\n        regressors (psi), static gain regressors (if present), and static function\n        regressors (if present).\n\n    Notes\n    -----\n    This method constructs a list of affine data components used in the NARMAX\n    modeling process. The components may include the matrix of static regressors\n    (psi), static gain regressors (if enabled), and static function regressors\n    (if enabled).\n\n    \"\"\"\n    if not self.static_gain:\n        return [psi] + [QR]\n\n    if not self.static_function:\n        return [psi] + [HR]\n\n    return [psi] + [HR] + [QR]\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_linear_mapping","title":"<code>build_linear_mapping()</code>","text":"<p>Assemble the linear mapping matrix R using the regressor-space method.</p> <p>This function constructs the linear mapping matrix R, which plays a key role in mapping the parameter vector to the cluster coefficients. It also generates a row matrix qit that assists in locating terms within the linear mapping matrix. This qit matrix is later used in creating the static regressor matrix (Q).</p> <p>Returns:</p> Name Type Description <code>R</code> <code>ndarray of int</code> <p>A constant matrix of ones and zeros that maps the parameter vector to cluster coefficients.</p> <code>qit</code> <code>ndarray of int</code> <p>A row matrix that helps locate terms within the linear mapping matrix R and is used in the creation of the static regressor matrix (Q).</p> Notes <p>The linear mapping matrix R is constructed using the regressor-space method. It plays a crucial role in the parameter estimation process, facilitating the mapping of parameter values to cluster coefficients. The qit matrix aids in term localization within the linear mapping matrix R and is subsequently used to build the static regressor matrix (Q).</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_linear_mapping(self):\n    \"\"\"Assemble the linear mapping matrix R using the regressor-space method.\n\n    This function constructs the linear mapping matrix R, which plays a key role in\n    mapping the parameter vector to the cluster coefficients. It also generates a\n    row matrix qit that assists in locating terms within the linear mapping matrix.\n    This qit matrix is later used in creating the static regressor matrix (Q).\n\n    Returns\n    -------\n    R : ndarray of int\n        A constant matrix of ones and zeros that maps the parameter vector to\n        cluster coefficients.\n    qit : ndarray of int\n        A row matrix that helps locate terms within the linear mapping matrix R and\n        is used in the creation of the static regressor matrix (Q).\n\n    Notes\n    -----\n    The linear mapping matrix R is constructed using the regressor-space method.\n    It plays a crucial role in the parameter estimation process, facilitating the\n    mapping of parameter values to cluster coefficients. The qit matrix aids in\n    term localization within the linear mapping matrix R and is subsequently used\n    to build the static regressor matrix (Q).\n\n    \"\"\"\n    xlag = [1] * self.n_inputs\n\n    object_qit = RegressorDictionary(xlag=xlag, ylag=[1])\n    # Given xlag and ylag equal to 1, there is no repetition of terms, which is\n    # ideal for building qit.\n    qit = object_qit.regressor_space(n_inputs=self.n_inputs) // 1000\n    model = self.final_model // 1000\n    R = np.all(qit[:, None, :] == model, axis=2).astype(int)\n    # Find rows with all zeros in R (sum of row elements is 0)\n    null_rows = list(np.where(np.sum(R, axis=1) == 0)[0])\n\n    R = np.delete(R, null_rows, axis=0)\n    qit = np.delete(qit, null_rows, axis=0)\n    return R, get_term_clustering(qit)\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_psi","title":"<code>build_psi(X, y)</code>","text":"<p>Build the matrix of dynamic regressor for NARMAX modeling.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <p>Returns:</p> Name Type Description <code>psi</code> <code>ndarray of floats, shape (n_samples, n_parameters)</code> <p>The matrix of dynamic regressors.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_psi(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the matrix of dynamic regressor for NARMAX modeling.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    psi : ndarray of floats, shape (n_samples, n_parameters)\n        The matrix of dynamic regressors.\n\n    \"\"\"\n    psi_builder = RegressorDictionary()\n    xlag_code = list_input_regressor_code(self.final_model)\n    ylag_code = list_output_regressor_code(self.final_model)\n    xlag = get_lag_from_regressor_code(xlag_code)\n    ylag = get_lag_from_regressor_code(ylag_code)\n    self.max_lag = get_max_lag_from_model_code(self.final_model)\n    if self.n_inputs != 1:\n        xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n    psi_builder.xlag = xlag\n    psi_builder.ylag = ylag\n    regressor_code = psi_builder.regressor_space(self.n_inputs)\n    pivv = get_index_from_regressor_code(regressor_code, self.final_model)\n    self.final_model = regressor_code[pivv]\n\n    lagged_data = build_input_output_matrix(x=X, y=y, xlag=xlag, ylag=ylag)\n\n    psi = Polynomial(degree=self.degree).fit(\n        lagged_data,\n        max_lag=self.max_lag,\n        ylag=ylag,\n        xlag=xlag,\n        predefined_regressors=pivv,\n    )\n    return psi\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_function_information","title":"<code>build_static_function_information(x_static, y_static)</code>","text":"<p>Construct a matrix of static regressors for a NARMAX model.</p> <p>Parameters:</p> Name Type Description Default <code>y_static</code> <code>(array - like, shape(n_samples_static_function))</code> <p>Output of the static function.</p> required <code>x_static</code> <code>(array - like, shape(n_samples_static_function))</code> <p>Static function input.</p> required <p>Returns:</p> Name Type Description <code>Q_dot_R</code> <code>ndarray of floats, shape (n_samples_static_function, n_parameters)</code> <p>The result of multiplying the matrix of static regressors (Q) with the linear mapping matrix (R), where n_parameters is the number of model parameters.</p> <code>static_covariance</code> <code>ndarray of floats, shape (n_parameters, n_parameters)</code> <p>The covariance QR'QR</p> <code>static_response</code> <code>ndarray of floats, shape (n_parameters,)</code> <p>The response QR'y</p> Notes <p>This function constructs a matrix of static regressors (Q) based on the provided static function outputs (y_static) and inputs (X_static). The linear mapping matrix (R) should be precomputed before calling this function. The result Q_dot_R represents the static regressors for the NARMAX model.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_static_function_information(\n    self, x_static: np.ndarray, y_static: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Construct a matrix of static regressors for a NARMAX model.\n\n    Parameters\n    ----------\n    y_static : array-like, shape (n_samples_static_function,)\n        Output of the static function.\n    x_static : array-like, shape (n_samples_static_function,)\n        Static function input.\n\n    Returns\n    -------\n    Q_dot_R : ndarray of floats, shape (n_samples_static_function, n_parameters)\n        The result of multiplying the matrix of static regressors (Q) with the\n        linear mapping matrix (R), where n_parameters is the number of model\n        parameters.\n    static_covariance: ndarray of floats, shape (n_parameters, n_parameters)\n        The covariance QR'QR\n    static_response: ndarray of floats, shape (n_parameters,)\n        The response QR'y\n\n    Notes\n    -----\n    This function constructs a matrix of static regressors (Q) based on the provided\n    static function outputs (y_static) and inputs (X_static). The linear mapping\n    matrix (R) should be precomputed before calling this function. The result\n    Q_dot_R represents the static regressors for the NARMAX model.\n\n    \"\"\"\n    R, qit = self.build_linear_mapping()\n    Q = y_static ** qit[:, 0]\n    for k in range(self.n_inputs):\n        Q *= x_static ** qit[:, 1 + k]\n\n    Q = Q.reshape(len(y_static), len(qit))\n\n    QR = Q.dot(R)\n    static_covariance = QR.T.dot(QR)\n    static_response = QR.T.dot(y_static)\n    return QR, static_covariance, static_response\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_gain_information","title":"<code>build_static_gain_information(x_static, y_static, gain)</code>","text":"<p>Construct a matrix of static regressors referring to the derivative (gain).</p> <p>Parameters:</p> Name Type Description Default <code>y_static</code> <code>(array - like, shape(n_samples_static_function))</code> <p>Output of the static function.</p> required <code>x_static</code> <code>(array - like, shape(n_samples_static_function))</code> <p>Static function input.</p> required <code>gain</code> <code>(array - like, shape(n_samples_static_gain))</code> <p>Static gain input.</p> required <p>Returns:</p> Name Type Description <code>HR</code> <code>ndarray of floats, shape (n_samples_static_function, n_parameters)</code> <p>The matrix of static regressors for the derivative (gain) multiplied by the linear mapping matrix R.</p> <code>gain_covariance</code> <code>ndarray of floats, shape (n_parameters, n_parameters)</code> <p>The covariance matrix (HR'HR) for the gain-related regressors.</p> <code>gain_response</code> <code>ndarray of floats, shape (n_parameters,)</code> <p>The response vector (HR'y) for the gain-related regressors.</p> Notes <p>This function constructs a matrix of static regressors (G+H) for the derivative (gain) based on the provided static function outputs (y_static), inputs (X_static), and gain values. The linear mapping matrix (R) should be precomputed before calling this function.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_static_gain_information(\n    self, x_static: np.ndarray, y_static: np.ndarray, gain: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Construct a matrix of static regressors referring to the derivative (gain).\n\n    Parameters\n    ----------\n    y_static : array-like, shape (n_samples_static_function,)\n        Output of the static function.\n    x_static : array-like, shape (n_samples_static_function,)\n        Static function input.\n    gain : array-like, shape (n_samples_static_gain,)\n        Static gain input.\n\n    Returns\n    -------\n    HR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n        The matrix of static regressors for the derivative (gain) multiplied by the\n        linear mapping matrix R.\n    gain_covariance : ndarray of floats, shape (n_parameters, n_parameters)\n        The covariance matrix (HR'HR) for the gain-related regressors.\n    gain_response : ndarray of floats, shape (n_parameters,)\n        The response vector (HR'y) for the gain-related regressors.\n\n    Notes\n    -----\n    This function constructs a matrix of static regressors (G+H) for the derivative\n    (gain) based on the provided static function outputs (y_static), inputs\n    (X_static), and gain values. The linear mapping matrix (R) should be\n    precomputed before calling this function.\n\n    \"\"\"\n    R, qit = self.build_linear_mapping()\n    H = np.zeros((len(y_static), len(qit)))\n    G = np.zeros((len(y_static), len(qit)))\n    for i in range(len(y_static)):\n        for j in range(1, len(qit)):\n            if y_static[i, 0] == 0:\n                if (qit[j, 0]) == 1:\n                    H[i, j] = gain[i][0]\n                else:\n                    H[i, j] = 0\n            else:\n                H[i, j] = (gain[i] * qit[j, 0] * y_static[i, 0] ** (qit[j, 0] - 1))[\n                    0\n                ]\n            for k in range(self.n_inputs):\n                if x_static[i, k] == 0:\n                    if (qit[j, 1 + k]) == 1:\n                        G[i, j] = 1\n                    else:\n                        G[i, j] = 0\n                else:\n                    G[i, j] = qit[j, 1 + k] * x_static[i, k] ** (qit[j, 1 + k] - 1)\n\n    HR = (G + H).dot(R)\n    gain_covariance = HR.T.dot(HR)\n    gain_response = HR.T.dot(gain)\n    return HR, gain_covariance, gain_response\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_system_data","title":"<code>build_system_data(y, static_gain, static_function)</code>","text":"<p>Construct a list of output data components for the NARMAX system.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The target data used in the identification process.</p> required <code>static_gain</code> <code>ndarray of floats</code> <p>Static gain output data.</p> required <code>static_function</code> <code>ndarray of floats</code> <p>Static function output data.</p> required <p>Returns:</p> Name Type Description <code>system_data</code> <code>list of ndarrays</code> <p>A list containing data components, including the target data (y), static gain data (if present), and static function data (if present).</p> Notes <p>This method constructs a list of data components that are used in the NARMAX system identification process. The components may include the target data (y), static gain data (if enabled), and static function data (if enabled).</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_system_data(\n    self,\n    y: np.ndarray,\n    static_gain: np.ndarray,\n    static_function: np.ndarray,\n) -&gt; List[np.ndarray]:\n    \"\"\"Construct a list of output data components for the NARMAX system.\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The target data used in the identification process.\n    static_gain : ndarray of floats\n        Static gain output data.\n    static_function : ndarray of floats\n        Static function output data.\n\n    Returns\n    -------\n    system_data : list of ndarrays\n        A list containing data components, including the target data (y),\n        static gain data (if present), and static function data (if present).\n\n    Notes\n    -----\n    This method constructs a list of data components that are used in the NARMAX\n    system identification process. The components may include the target data (y),\n    static gain data (if enabled), and static function data (if enabled).\n\n    \"\"\"\n    if not self.static_gain:\n        return [y] + [static_function]\n\n    if not self.static_function:\n        return [y] + [static_gain]\n\n    return [y] + [static_gain] + [static_function]\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.estimate","title":"<code>estimate(y_static=np.zeros(1), X_static=np.zeros(1), gain=np.zeros(1), y=np.zeros(1), X=np.zeros((1, 1)), weighing_matrix=np.zeros((1, 1)))</code>","text":"<p>Estimate the parameters via multi-objective techniques.</p> <p>Parameters:</p> Name Type Description Default <code>y_static</code> <code>array-like of shape = n_samples_static_function</code> <p>Output of static function.</p> <code>= ([0])</code> <code>X_static</code> <code>array-like of shape = n_samples_static_function</code> <p>Static function input.</p> <code>= ([0])</code> <code>gain</code> <code>array-like of shape = n_samples_static_gain</code> <p>Static gain input.</p> <code>= ([0])</code> <code>y</code> <code>array-like of shape = n_samples</code> <p>The target data used in the identification process.</p> <code>= ([0])</code> <code>X</code> <code>ndarray of floats</code> <p>Matrix of static regressors.</p> <code>= ([[0],[0]])</code> <code>weighing_matrix</code> <code>ndarray</code> <p>Weighing matrix for defining the weight of each objective.</p> <code>zeros((1, 1))</code> <p>Returns:</p> Name Type Description <code>J</code> <code>ndarray</code> <p>Matrix referring to the objectives.</p> <code>euclidean_norm</code> <code>ndarray</code> <p>Matrix of the Euclidean norm.</p> <code>theta</code> <code>ndarray</code> <p>Matrix with parameters for each weight.</p> <code>HR</code> <code>ndarray</code> <p>H matrix multiplied by R.</p> <code>QR</code> <code>ndarray</code> <p>Q matrix multiplied by R.</p> <code>position</code> <code>ndarray, default = ([[0],[0]])</code> <p>Position of the best theta set.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def estimate(\n    self,\n    y_static: np.ndarray = np.zeros(1),\n    X_static: np.ndarray = np.zeros(1),\n    gain: np.ndarray = np.zeros(1),\n    y: np.ndarray = np.zeros(1),\n    X: np.ndarray = np.zeros((1, 1)),\n    weighing_matrix: np.ndarray = np.zeros((1, 1)),\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.int64]:\n    \"\"\"Estimate the parameters via multi-objective techniques.\n\n    Parameters\n    ----------\n    y_static : array-like of shape = n_samples_static_function, default = ([0])\n        Output of static function.\n    X_static : array-like of shape = n_samples_static_function, default = ([0])\n        Static function input.\n    gain : array-like of shape = n_samples_static_gain, default = ([0])\n        Static gain input.\n    y : array-like of shape = n_samples, default = ([0])\n        The target data used in the identification process.\n    X : ndarray of floats, default = ([[0],[0]])\n        Matrix of static regressors.\n    weighing_matrix: ndarray\n        Weighing matrix for defining the weight of each objective.\n\n    Returns\n    -------\n    J : ndarray\n        Matrix referring to the objectives.\n    euclidean_norm : ndarray\n        Matrix of the Euclidean norm.\n    theta : ndarray\n        Matrix with parameters for each weight.\n    HR : ndarray\n        H matrix multiplied by R.\n    QR : ndarray\n        Q matrix multiplied by R.\n    position : ndarray, default = ([[0],[0]])\n        Position of the best theta set.\n\n    \"\"\"\n    psi = self.build_psi(X, y)\n    y = y[self.max_lag :]\n    HR, QR = np.zeros((1, 1)), np.zeros((1, 1))\n    n_parameters = weighing_matrix.shape[1]\n    num_objectives = self.static_function + self.static_gain + 1\n    euclidean_norm = np.zeros(n_parameters)\n    theta = np.zeros((n_parameters, self.final_model.shape[0]))\n    dynamic_covariance = psi.T.dot(psi)\n    dynamic_response = psi.T.dot(y)\n\n    if self.static_function:\n        QR, static_covariance, static_response = (\n            self.build_static_function_information(X_static, y_static)\n        )\n    if self.static_gain:\n        HR, gain_covariance, gain_response = self.build_static_gain_information(\n            X_static, y_static, gain\n        )\n    J = np.zeros((num_objectives, n_parameters))\n    system_data = self.build_system_data(y, gain, y_static)\n    affine_information_data = self.build_affine_data(psi, HR, QR)\n    for i in range(n_parameters):\n        theta1 = weighing_matrix[0, i] * dynamic_covariance\n        theta2 = weighing_matrix[0, i] * dynamic_response\n\n        w = 1\n        if self.static_function:\n            theta1 += weighing_matrix[w, i] * static_covariance\n            theta2 += weighing_matrix[w, i] * static_response.reshape(-1, 1)\n            w += 1\n\n        if self.static_gain:\n            theta1 += weighing_matrix[w, i] * gain_covariance\n            theta2 += weighing_matrix[w, i] * gain_response.reshape(-1, 1)\n            w += 1\n\n        tmp_theta = np.linalg.lstsq(theta1, theta2, rcond=None)[0]\n        theta[i, :] = tmp_theta.T\n\n        for j in range(num_objectives):\n            residuals = get_cost_function(\n                system_data[j], affine_information_data[j], tmp_theta\n            )\n            J[j, i] = residuals[0]\n\n        euclidean_norm[i] = np.linalg.norm(J[:, i])\n\n    if self.normalize is True:\n        J /= np.max(J, axis=1)[:, np.newaxis]\n        euclidean_norm /= np.max(euclidean_norm)\n\n        euclidean_norm = euclidean_norm / np.max(euclidean_norm)\n\n    position = np.argmin(euclidean_norm)\n    return (\n        J,\n        euclidean_norm,\n        theta,\n        HR,\n        QR,\n        position,\n    )\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.get_cost_function","title":"<code>get_cost_function(y, psi, theta)</code>","text":"<p>Calculate the cost function based on residuals.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The target data used in the identification process.</p> required <code>psi</code> <code>ndarray of floats, shape (n_samples, n_parameters)</code> <p>The matrix of regressors.</p> required <code>theta</code> <code>ndarray of floats</code> <p>The parameter vector.</p> required <p>Returns:</p> Name Type Description <code>cost_function</code> <code>float</code> <p>The calculated cost function value.</p> Notes <p>This method computes the cost function value based on the residuals between the target data (y) and the predicted values using the regressors (dynamic and static) and parameter vector (theta). It quantifies the error in the model's predictions.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def get_cost_function(y: np.ndarray, psi: np.ndarray, theta: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the cost function based on residuals.\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The target data used in the identification process.\n    psi : ndarray of floats, shape (n_samples, n_parameters)\n        The matrix of regressors.\n    theta : ndarray of floats\n        The parameter vector.\n\n    Returns\n    -------\n    cost_function : float\n        The calculated cost function value.\n\n    Notes\n    -----\n    This method computes the cost function value based on the residuals between\n    the target data (y) and the predicted values using the regressors (dynamic\n    and static) and parameter vector (theta). It quantifies the error in the\n    model's predictions.\n\n    \"\"\"\n    residuals = y - psi.dot(theta)\n    return residuals.T.dot(residuals)\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.get_term_clustering","title":"<code>get_term_clustering(qit)</code>","text":"<p>Get the term clustering of the model.</p> <p>This function takes a matrix <code>qit</code> and compute the term clustering based on their values. It calculates the number of occurrences of each value for each row in the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>qit</code> <code>ndarray</code> <p>Input matrix containing terms clustering to be sorted.</p> required <p>Returns:</p> Name Type Description <code>N_aux</code> <code>ndarray</code> <p>A new matrix with rows representing the number of occurrences of each value for each row in the input matrix <code>qit</code>. The columns correspond to different values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; qit = np.array([[1, 2, 2],\n...                 [1, 3, 1],\n...                 [2, 2, 3]])\n&gt;&gt;&gt; result = get_term_clustering(qit)\n&gt;&gt;&gt; print(result)\n[[1. 2. 0. 0.]\n[2. 0. 1. 0.]\n[0. 2. 1. 0.]]\n</code></pre> Notes <p>The function calculates the number of occurrences of each value (from 1 to the maximum value in the input matrix <code>qit</code>) for each row and returns a matrix where rows represent rows of the input matrix <code>qit</code>, and columns represent different values.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def get_term_clustering(qit: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Get the term clustering of the model.\n\n    This function takes a matrix `qit` and compute the term clustering based\n    on their values. It calculates the number of occurrences of each value\n    for each row in the matrix.\n\n    Parameters\n    ----------\n    qit : ndarray\n        Input matrix containing terms clustering to be sorted.\n\n    Returns\n    -------\n    N_aux : ndarray\n        A new matrix with rows representing the number of occurrences of each value\n        for each row in the input matrix `qit`. The columns correspond to different\n        values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; qit = np.array([[1, 2, 2],\n    ...                 [1, 3, 1],\n    ...                 [2, 2, 3]])\n    &gt;&gt;&gt; result = get_term_clustering(qit)\n    &gt;&gt;&gt; print(result)\n    [[1. 2. 0. 0.]\n    [2. 0. 1. 0.]\n    [0. 2. 1. 0.]]\n\n    Notes\n    -----\n    The function calculates the number of occurrences of each value (from 1 to\n    the maximum value in the input matrix `qit`) for each row and returns a matrix\n    where rows represent rows of the input matrix `qit`, and columns represent\n    different values.\n\n    \"\"\"\n    max_value = int(np.max(qit))\n    counts_matrix = np.zeros((qit.shape[0], max_value))\n\n    for k in range(1, max_value + 1):\n        counts_matrix[:, k - 1] = np.sum(qit == k, axis=1)\n\n    return counts_matrix.astype(int)\n</code></pre>"},{"location":"user-guide/API/narmax-base/","title":"Documentation for <code>narmax-base</code>","text":"<p>Base classes for NARMAX estimator.</p>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.BaseMSS","title":"<code>BaseMSS</code>","text":"<p>               Bases: <code>RegressorDictionary</code></p> <p>Base class for Model Structure Selection.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>class BaseMSS(RegressorDictionary, metaclass=ABCMeta):\n    \"\"\"Base class for Model Structure Selection.\"\"\"\n\n    @abstractmethod\n    def __init__(self):\n        super().__init__(self)\n        self.max_lag = None\n        self.n_inputs = None\n        self.theta = None\n        self.final_model = None\n        self.pivv = None\n\n    @abstractmethod\n    def fit(self, *, X, y):\n        \"\"\"Abstract method.\"\"\"\n\n    @abstractmethod\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Abstract method.\"\"\"\n\n    def _code2exponents(self, *, code: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Convert regressor code to exponents array.\n\n        Parameters\n        ----------\n        code : 1D-array of int\n            Codification of one regressor.\n\n        Returns\n        -------\n        exponents = ndarray of ints\n        \"\"\"\n        regressors = np.array(list(set(code)))\n        regressors_count = Counter(code)\n\n        if np.all(regressors == 0):\n            return np.zeros(self.max_lag * (1 + self.n_inputs))\n\n        exponents = np.array([], dtype=float)\n        elements = np.round(np.divide(regressors, 1000), 0)[(regressors &gt; 0)].astype(\n            int\n        )\n\n        for j in range(1, self.n_inputs + 2):\n            base_exponents = np.zeros(self.max_lag, dtype=float)\n            if j in elements:\n                for i in range(1, self.max_lag + 1):\n                    regressor_code = int(j * 1000 + i)\n                    base_exponents[-i] = regressors_count[regressor_code]\n                exponents = np.append(exponents, base_exponents)\n\n            else:\n                exponents = np.append(exponents, base_exponents)\n\n        return exponents\n\n    def _one_step_ahead_prediction(\n        self,\n        x_base: np.ndarray,\n        y: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        x_base : ndarray of floats of shape = n_samples\n            Regressor matrix with input-output arrays.\n        y : ndarray, optional\n            Unused placeholder to keep signature compatible with subclasses.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        _ = y  # keeps signature aligned with subclasses\n        yhat = np.dot(x_base, self.theta.flatten())\n        return yhat.reshape(-1, 1)\n\n    @abstractmethod\n    def _model_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Model prediction wrapper.\"\"\"\n\n    def _narmax_predict(\n        self,\n        x: np.ndarray,\n        y_initial: np.ndarray,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"narmax_predict method.\"\"\"\n        y_output = np.zeros(forecast_horizon, dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        model_exponents = np.vstack(\n            [self._code2exponents(code=model) for model in self.final_model]\n        )\n        raw_regressor = np.zeros(model_exponents.shape[1], dtype=float)\n        regressor_powers = np.empty_like(model_exponents)\n        theta = self.theta.ravel()\n        for i in range(self.max_lag, forecast_horizon):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n\n            np.power(raw_regressor, model_exponents, out=regressor_powers)\n            regressor_value = np.prod(regressor_powers, axis=1)\n            y_output[i] = regressor_value @ theta\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    @abstractmethod\n    def _nfir_predict(self, x: np.ndarray, y_initial: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Nfir predict method.\"\"\"\n        y_output = np.zeros(x.shape[0], dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n        x = x.reshape(-1, self.n_inputs)\n        model_exponents = np.vstack(\n            [self._code2exponents(code=model) for model in self.final_model]\n        )\n        raw_regressor = np.zeros(model_exponents.shape[1], dtype=float)\n        regressor_powers = np.empty_like(model_exponents)\n        theta = self.theta.ravel()\n        for i in range(self.max_lag, x.shape[0]):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n\n            np.power(raw_regressor, model_exponents, out=regressor_powers)\n            regressor_value = np.prod(regressor_powers, axis=1)\n            y_output[i] = regressor_value @ theta\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    def _nar_step_ahead(self, y: np.ndarray, steps_ahead: int) -&gt; np.ndarray:\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        yhat_length = len(y) + steps_ahead\n        yhat = np.zeros(yhat_length, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    x=None, y_initial=y[step:i], forecast_horizon=steps_ahead\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=None, y_initial=y[steps[-1] : i]\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=None, y_initial=y[0:i], forecast_horizon=steps_ahead\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    def narmax_n_step_ahead(\n        self,\n        x: np.ndarray,\n        y: np.ndarray,\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"n_steps ahead prediction method for NARMAX model.\"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        x = x.reshape(-1, self.n_inputs)\n        yhat = np.zeros(x.shape[0], dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    x=x[step : i + steps_ahead],\n                    y_initial=y[step:i],\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[steps[-1] : i + steps_ahead],\n                y_initial=y[steps[-1] : i],\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[0 : i + steps_ahead],\n                y_initial=y[0:i],\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    @abstractmethod\n    def _n_step_ahead_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            Predicted values for NARMAX and NAR models.\n        \"\"\"\n        if self.model_type == \"NARMAX\":\n            return self.narmax_n_step_ahead(x, y, steps_ahead)\n\n        if self.model_type == \"NAR\":\n            return self._nar_step_ahead(y, steps_ahead)\n\n        raise ValueError(\n            \"n_steps_ahead prediction will be implemented for NFIR models in v0.4.*\"\n        )\n\n    @abstractmethod\n    def _basis_function_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Basis function prediction.\"\"\"\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        # Discard unnecessary initial values\n        analyzed_elements_number = self.max_lag + 1\n\n        for i in range(forecast_horizon - self.max_lag):\n            if self.model_type == \"NARMAX\":\n                lagged_data = build_input_output_matrix(\n                    x[i : i + analyzed_elements_number],\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1),\n                    self.xlag,\n                    self.ylag,\n                )\n            elif self.model_type == \"NAR\":\n                lagged_data = build_output_matrix(\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1), self.ylag\n                )\n            elif self.model_type == \"NFIR\":\n                lagged_data = build_input_matrix(\n                    x[i : i + analyzed_elements_number], self.xlag\n                )\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            x_tmp = self.basis_function.transform(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=self.pivv[: len(self.final_model)],\n            )\n\n            a = x_tmp @ self.theta\n            yhat[i + self.max_lag] = a.item()\n\n        return yhat[self.max_lag :].reshape(-1, 1)\n\n    @abstractmethod\n    def _basis_function_n_step_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: np.ndarray,\n        steps_ahead: int,\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Basis function n step ahead.\"\"\"\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        # Discard unnecessary initial values\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead],\n                    y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        return yhat[self.max_lag :].reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(\n        self,\n        x: Optional[np.ndarray],\n        y: np.ndarray,\n        steps_ahead: int,\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Basis n steps horizon.\"\"\"\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        # Discard unnecessary initial values\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead], y[k : i + steps_ahead], forecast_horizon\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat[self.max_lag :].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.BaseMSS.fit","title":"<code>fit(*, X, y)</code>  <code>abstractmethod</code>","text":"<p>Abstract method.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>@abstractmethod\ndef fit(self, *, X, y):\n    \"\"\"Abstract method.\"\"\"\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.BaseMSS.narmax_n_step_ahead","title":"<code>narmax_n_step_ahead(x, y, steps_ahead)</code>","text":"<p>n_steps ahead prediction method for NARMAX model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def narmax_n_step_ahead(\n    self,\n    x: np.ndarray,\n    y: np.ndarray,\n    steps_ahead: Optional[int],\n) -&gt; np.ndarray:\n    \"\"\"n_steps ahead prediction method for NARMAX model.\"\"\"\n    if len(y) &lt; self.max_lag:\n        raise ValueError(\n            \"Insufficient initial condition elements! Expected at least\"\n            f\" {self.max_lag} elements.\"\n        )\n\n    to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n    x = x.reshape(-1, self.n_inputs)\n    yhat = np.zeros(x.shape[0], dtype=float)\n    yhat.fill(np.nan)\n    yhat[: self.max_lag] = y[: self.max_lag, 0]\n    i = self.max_lag\n    steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n    if len(steps) &gt; 1:\n        for step in steps[:-1]:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[step : i + steps_ahead],\n                y_initial=y[step:i],\n            )[-steps_ahead:].ravel()\n            i += steps_ahead\n\n        steps_ahead = np.sum(np.isnan(yhat))\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            x=x[steps[-1] : i + steps_ahead],\n            y_initial=y[steps[-1] : i],\n        )[-steps_ahead:].ravel()\n    else:\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            x=x[0 : i + steps_ahead],\n            y_initial=y[0:i],\n        )[-steps_ahead:].ravel()\n\n    yhat = yhat.ravel()[self.max_lag : :]\n    return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.BaseMSS.predict","title":"<code>predict(*, X=None, y, steps_ahead=None, forecast_horizon=1)</code>  <code>abstractmethod</code>","text":"<p>Abstract method.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: np.ndarray,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: int = 1,\n) -&gt; np.ndarray:\n    \"\"\"Abstract method.\"\"\"\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary","title":"<code>RegressorDictionary</code>","text":"<p>Base class for Model Structure Selection.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>class RegressorDictionary:\n    \"\"\"Base class for Model Structure Selection.\"\"\"\n\n    def __init__(\n        self,\n        xlag: Union[List[Any], Any] = 1,\n        ylag: Union[List[Any], Any] = 1,\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n    ):\n        self.xlag = xlag\n        self.ylag = ylag\n        self.basis_function = basis_function\n        self.model_type = model_type\n\n    def create_narmax_code(self, n_inputs: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Create the code representation of the regressors.\n\n        This function generates a codification from all possibles\n        regressors given the maximum lag of the input and output.\n        This is used to write the final terms of the model in a\n        readable form. [1001] -&gt; y(k-1).\n        This code format was based on a dissertation from UFMG. See\n        reference below.\n\n        Parameters\n        ----------\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        x_vec : ndarray of int\n            List of the input lags.\n        y_vec : ndarray of int\n            List of the output lags.\n\n        Examples\n        --------\n        The codification is defined as:\n\n        100n = y(k-n)\n        200n = u(k-n)\n        [100n, 100n] = y(k-n)y(k-n)\n        [200n, 200n] = u(k-n)u(k-n)\n\n        References\n        ----------\n        - Master Thesis: Barbosa, Al\u00edpio Monteiro.\n            T\u00e9cnicas de otimiza\u00e7\u00e3o bi-objetivo para a determina\u00e7\u00e3o\n            da estrutura de modelos NARX (2010).\n\n        \"\"\"\n        if self.basis_function.degree &lt; 1:\n            raise ValueError(\n                f\"degree must be integer and &gt; zero. Got {self.basis_function.degree}\"\n            )\n\n        if np.min(np.minimum(self.ylag, 1)) &lt; 1:\n            raise ValueError(\n                f\"ylag must be integer or list and &gt; zero. Got {self.ylag}\"\n            )\n\n        if (\n            np.min(\n                np.min(\n                    np.array(list(chain.from_iterable([[self.xlag]])), dtype=\"object\")\n                )\n            )\n            &lt; 1\n        ):\n            raise ValueError(\n                f\"xlag must be integer or list and &gt; zero. Got {self.xlag}\"\n            )\n\n        y_vec = self.get_y_lag_list()\n\n        if n_inputs == 1:\n            x_vec = self.get_siso_x_lag_list()\n        else:\n            x_vec = self.get_miso_x_lag_list(n_inputs)\n\n        return x_vec, y_vec\n\n    def get_y_lag_list(self) -&gt; np.ndarray:\n        \"\"\"Return y regressor code list.\n\n        Returns\n        -------\n        y_vec = ndarray of ints\n            The y regressor code list given the ylag.\n\n        \"\"\"\n        if isinstance(self.ylag, list):\n            # create only the lags passed from list\n            y_vec = []\n            y_vec.extend([lag + 1000 for lag in self.ylag])\n            return np.array(y_vec)\n\n        # create a range of lags if passed a int value\n        return np.arange(1001, 1001 + self.ylag)\n\n    def get_siso_x_lag_list(self) -&gt; np.ndarray:\n        \"\"\"Return x regressor code list for SISO models.\n\n        Returns\n        -------\n        x_vec_tmp = ndarray of ints\n            The x regressor code list given the xlag for a SISO model.\n\n        \"\"\"\n        if isinstance(self.xlag, list):\n            # create only the lags passed from list\n            x_vec_tmp = []\n            x_vec_tmp.extend([lag + 2000 for lag in self.xlag])\n            return np.array(x_vec_tmp)\n\n        # create a range of lags if passed a int value\n        return np.arange(2001, 2001 + self.xlag)\n\n    def get_miso_x_lag_list(self, n_inputs: int) -&gt; np.ndarray:\n        \"\"\"Return x regressor code list for MISO models.\n\n        Parameters\n        ----------\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        x_vec = ndarray of ints\n            The x regressor code list given the xlag for a MISO model.\n\n        \"\"\"\n        # only list are allowed if n_inputs &gt; 1\n        # the user must entered list of the desired lags explicitly\n        x_vec_tmp = []\n        for i in range(n_inputs):\n            if isinstance(self.xlag[i], list):\n                # create 200n, 300n,..., 400n to describe each input\n                x_vec_tmp.extend([lag + 2000 + i * 1000 for lag in self.xlag[i]])\n            elif isinstance(self.xlag[i], int) and n_inputs &gt; 1:\n                x_vec_tmp.extend(\n                    [np.arange(2001 + i * 1000, 2001 + i * 1000 + self.xlag[i])]\n                )\n\n        # if x_vec is a nested list, ensure all elements are arrays\n        all_arrays = [np.array([i]) if isinstance(i, int) else i for i in x_vec_tmp]\n        return np.concatenate([i for i in all_arrays])\n\n    def regressor_space(self, n_inputs: int) -&gt; np.ndarray:\n        \"\"\"Create regressor code based on model type.\n\n        Parameters\n        ----------\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        regressor_code = ndarray of ints\n            The regressor code list given the xlag and ylag for a MISO model.\n\n        \"\"\"\n        x_vec, y_vec = self.create_narmax_code(n_inputs)\n        reg_aux = np.array([0])\n        if self.model_type == \"NARMAX\":\n            reg_aux = np.concatenate([reg_aux, y_vec, x_vec])\n        elif self.model_type == \"NAR\":\n            reg_aux = np.concatenate([reg_aux, y_vec])\n        elif self.model_type == \"NFIR\":\n            reg_aux = np.concatenate([reg_aux, x_vec])\n        else:\n            raise ValueError(\n                \"Unrecognized model type. Model type should be NARMAX, NAR or NFIR\"\n            )\n\n        regressor_code = list(\n            combinations_with_replacement(reg_aux, self.basis_function.degree)\n        )\n\n        regressor_code = np.array(regressor_code)\n        regressor_code = regressor_code[:, regressor_code.shape[1] :: -1]\n        return regressor_code\n\n    def _get_max_lag(self):\n        \"\"\"Get the max lag defined by the user.\n\n        Returns\n        -------\n        max_lag = int\n            The max lag value defined by the user.\n        \"\"\"\n        ny = np.max(list(chain.from_iterable([[self.ylag]])))\n        nx = np.max(list(chain.from_iterable([[np.array(self.xlag, dtype=object)]])))\n        return np.max([ny, np.max(nx)])\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.create_narmax_code","title":"<code>create_narmax_code(n_inputs)</code>","text":"<p>Create the code representation of the regressors.</p> <p>This function generates a codification from all possibles regressors given the maximum lag of the input and output. This is used to write the final terms of the model in a readable form. [1001] -&gt; y(k-1). This code format was based on a dissertation from UFMG. See reference below.</p> <p>Parameters:</p> Name Type Description Default <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> required <p>Returns:</p> Name Type Description <code>x_vec</code> <code>ndarray of int</code> <p>List of the input lags.</p> <code>y_vec</code> <code>ndarray of int</code> <p>List of the output lags.</p> <p>Examples:</p> <p>The codification is defined as:</p> <p>100n = y(k-n) 200n = u(k-n) [100n, 100n] = y(k-n)y(k-n) [200n, 200n] = u(k-n)u(k-n)</p> References <ul> <li>Master Thesis: Barbosa, Al\u00edpio Monteiro.     T\u00e9cnicas de otimiza\u00e7\u00e3o bi-objetivo para a determina\u00e7\u00e3o     da estrutura de modelos NARX (2010).</li> </ul> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def create_narmax_code(self, n_inputs: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Create the code representation of the regressors.\n\n    This function generates a codification from all possibles\n    regressors given the maximum lag of the input and output.\n    This is used to write the final terms of the model in a\n    readable form. [1001] -&gt; y(k-1).\n    This code format was based on a dissertation from UFMG. See\n    reference below.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input variables.\n\n    Returns\n    -------\n    x_vec : ndarray of int\n        List of the input lags.\n    y_vec : ndarray of int\n        List of the output lags.\n\n    Examples\n    --------\n    The codification is defined as:\n\n    100n = y(k-n)\n    200n = u(k-n)\n    [100n, 100n] = y(k-n)y(k-n)\n    [200n, 200n] = u(k-n)u(k-n)\n\n    References\n    ----------\n    - Master Thesis: Barbosa, Al\u00edpio Monteiro.\n        T\u00e9cnicas de otimiza\u00e7\u00e3o bi-objetivo para a determina\u00e7\u00e3o\n        da estrutura de modelos NARX (2010).\n\n    \"\"\"\n    if self.basis_function.degree &lt; 1:\n        raise ValueError(\n            f\"degree must be integer and &gt; zero. Got {self.basis_function.degree}\"\n        )\n\n    if np.min(np.minimum(self.ylag, 1)) &lt; 1:\n        raise ValueError(\n            f\"ylag must be integer or list and &gt; zero. Got {self.ylag}\"\n        )\n\n    if (\n        np.min(\n            np.min(\n                np.array(list(chain.from_iterable([[self.xlag]])), dtype=\"object\")\n            )\n        )\n        &lt; 1\n    ):\n        raise ValueError(\n            f\"xlag must be integer or list and &gt; zero. Got {self.xlag}\"\n        )\n\n    y_vec = self.get_y_lag_list()\n\n    if n_inputs == 1:\n        x_vec = self.get_siso_x_lag_list()\n    else:\n        x_vec = self.get_miso_x_lag_list(n_inputs)\n\n    return x_vec, y_vec\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_miso_x_lag_list","title":"<code>get_miso_x_lag_list(n_inputs)</code>","text":"<p>Return x regressor code list for MISO models.</p> <p>Parameters:</p> Name Type Description Default <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> required <p>Returns:</p> Type Description <code>x_vec = ndarray of ints</code> <p>The x regressor code list given the xlag for a MISO model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_miso_x_lag_list(self, n_inputs: int) -&gt; np.ndarray:\n    \"\"\"Return x regressor code list for MISO models.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input variables.\n\n    Returns\n    -------\n    x_vec = ndarray of ints\n        The x regressor code list given the xlag for a MISO model.\n\n    \"\"\"\n    # only list are allowed if n_inputs &gt; 1\n    # the user must entered list of the desired lags explicitly\n    x_vec_tmp = []\n    for i in range(n_inputs):\n        if isinstance(self.xlag[i], list):\n            # create 200n, 300n,..., 400n to describe each input\n            x_vec_tmp.extend([lag + 2000 + i * 1000 for lag in self.xlag[i]])\n        elif isinstance(self.xlag[i], int) and n_inputs &gt; 1:\n            x_vec_tmp.extend(\n                [np.arange(2001 + i * 1000, 2001 + i * 1000 + self.xlag[i])]\n            )\n\n    # if x_vec is a nested list, ensure all elements are arrays\n    all_arrays = [np.array([i]) if isinstance(i, int) else i for i in x_vec_tmp]\n    return np.concatenate([i for i in all_arrays])\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_siso_x_lag_list","title":"<code>get_siso_x_lag_list()</code>","text":"<p>Return x regressor code list for SISO models.</p> <p>Returns:</p> Type Description <code>x_vec_tmp = ndarray of ints</code> <p>The x regressor code list given the xlag for a SISO model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_siso_x_lag_list(self) -&gt; np.ndarray:\n    \"\"\"Return x regressor code list for SISO models.\n\n    Returns\n    -------\n    x_vec_tmp = ndarray of ints\n        The x regressor code list given the xlag for a SISO model.\n\n    \"\"\"\n    if isinstance(self.xlag, list):\n        # create only the lags passed from list\n        x_vec_tmp = []\n        x_vec_tmp.extend([lag + 2000 for lag in self.xlag])\n        return np.array(x_vec_tmp)\n\n    # create a range of lags if passed a int value\n    return np.arange(2001, 2001 + self.xlag)\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_y_lag_list","title":"<code>get_y_lag_list()</code>","text":"<p>Return y regressor code list.</p> <p>Returns:</p> Type Description <code>y_vec = ndarray of ints</code> <p>The y regressor code list given the ylag.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_y_lag_list(self) -&gt; np.ndarray:\n    \"\"\"Return y regressor code list.\n\n    Returns\n    -------\n    y_vec = ndarray of ints\n        The y regressor code list given the ylag.\n\n    \"\"\"\n    if isinstance(self.ylag, list):\n        # create only the lags passed from list\n        y_vec = []\n        y_vec.extend([lag + 1000 for lag in self.ylag])\n        return np.array(y_vec)\n\n    # create a range of lags if passed a int value\n    return np.arange(1001, 1001 + self.ylag)\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.regressor_space","title":"<code>regressor_space(n_inputs)</code>","text":"<p>Create regressor code based on model type.</p> <p>Parameters:</p> Name Type Description Default <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> required <p>Returns:</p> Type Description <code>regressor_code = ndarray of ints</code> <p>The regressor code list given the xlag and ylag for a MISO model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def regressor_space(self, n_inputs: int) -&gt; np.ndarray:\n    \"\"\"Create regressor code based on model type.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input variables.\n\n    Returns\n    -------\n    regressor_code = ndarray of ints\n        The regressor code list given the xlag and ylag for a MISO model.\n\n    \"\"\"\n    x_vec, y_vec = self.create_narmax_code(n_inputs)\n    reg_aux = np.array([0])\n    if self.model_type == \"NARMAX\":\n        reg_aux = np.concatenate([reg_aux, y_vec, x_vec])\n    elif self.model_type == \"NAR\":\n        reg_aux = np.concatenate([reg_aux, y_vec])\n    elif self.model_type == \"NFIR\":\n        reg_aux = np.concatenate([reg_aux, x_vec])\n    else:\n        raise ValueError(\n            \"Unrecognized model type. Model type should be NARMAX, NAR or NFIR\"\n        )\n\n    regressor_code = list(\n        combinations_with_replacement(reg_aux, self.basis_function.degree)\n    )\n\n    regressor_code = np.array(regressor_code)\n    regressor_code = regressor_code[:, regressor_code.shape[1] :: -1]\n    return regressor_code\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.house","title":"<code>house(x)</code>","text":"<p>Perform a Householder reflection of vector.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array-like of shape = number_of_training_samples</code> <p>The respective column of the matrix of regressors in each iteration of ERR function.</p> required <p>Returns:</p> Name Type Description <code>v</code> <code>array-like of shape = number_of_training_samples</code> <p>The reflection of the array x.</p> References <ul> <li>Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).     Orthogonal least squares methods and their application to non-linear     system identification.</li> </ul> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def house(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Perform a Householder reflection of vector.\n\n    Parameters\n    ----------\n    x : array-like of shape = number_of_training_samples\n        The respective column of the matrix of regressors in each\n        iteration of ERR function.\n\n    Returns\n    -------\n    v : array-like of shape = number_of_training_samples\n        The reflection of the array x.\n\n    References\n    ----------\n    - Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).\n        Orthogonal least squares methods and their application to non-linear\n        system identification.\n\n    \"\"\"\n    u = np.linalg.norm(x, 2)\n    if u != 0:\n        aux_b = x[0] + np.sign(x[0]) * u\n        x = x[1:] / (aux_b + np.finfo(np.float64).eps)\n        x = np.concatenate((np.array([1]), x))\n    return x\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.rowhouse","title":"<code>rowhouse(RA, v)</code>","text":"<p>Perform a row Householder transformation.</p> <p>Parameters:</p> Name Type Description Default <code>RA</code> <code>array-like of shape = number_of_training_samples</code> <p>The respective column of the matrix of regressors in each iteration of ERR function.</p> required <code>v</code> <code>array-like of shape = number_of_training_samples</code> <p>The reflected vector obtained by using the householder reflection.</p> required <p>Returns:</p> Name Type Description <code>B</code> <code>array-like of shape = number_of_training_samples</code> References <ul> <li>Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).     Orthogonal least squares methods and their application to     non-linear system identification. International Journal of     control, 50(5), 1873-1896.</li> </ul> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def rowhouse(RA: np.ndarray, v: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Perform a row Householder transformation.\n\n    Parameters\n    ----------\n    RA : array-like of shape = number_of_training_samples\n        The respective column of the matrix of regressors in each\n        iteration of ERR function.\n    v : array-like of shape = number_of_training_samples\n        The reflected vector obtained by using the householder reflection.\n\n    Returns\n    -------\n    B : array-like of shape = number_of_training_samples\n\n    References\n    ----------\n    - Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).\n        Orthogonal least squares methods and their application to\n        non-linear system identification. International Journal of\n        control, 50(5), 1873-1896.\n\n    \"\"\"\n    b = -2 / np.dot(v.T, v)\n    w = b * np.dot(RA.T, v)\n    w = w.reshape(1, -1)\n    v = v.reshape(-1, 1)\n    RA = RA + v * w\n    B = RA\n    return B\n</code></pre>"},{"location":"user-guide/API/neural-narx/","title":"Documentation for <code>Neural NARX</code>","text":"<p>Build Polynomial NARMAX Models.</p>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN","title":"<code>NARXNN</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>NARX Neural Network model built on top of Pytorch.</p> <p>Neural networks are models composed of interconnected layers of nodes (neurons) designed for tasks like classification and regression. Each neuron is a basic unit within these networks. Mathematically, a neuron is represented by a function \\(f\\) that takes an input vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]\\) and generates an output \\(y\\). This function usually involves a weighted sum of the inputs, an optional bias term \\(b\\), and an activation function \\(\\phi\\):</p> \\[ y = \\phi \\left( \\sum_{i=1}^{n} w_i x_i + b \\right) \\tag{2.31} \\] <p>where \\(\\mathbf{w} = [w_1, w_2, \\ldots, w_n]\\) are the weights associated with the inputs. The activation function \\(\\phi\\) introduces nonlinearity into the model, allowing the network to learn complex patterns.</p> <p>Currently we support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier, and we convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>basis_function</code> <p>Defines which basis function will be used in the model.</p> <code>Polynomial()</code> <code>model_type</code> <p>The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models</p> <code>'NARMAX'</code> <code>batch_size</code> <code>int</code> <p>Size of mini-batches of data for stochastic optimizers</p> <code>100</code> <code>shuffle_batches</code> <code>bool</code> <p>Whether to shuffle mini-batches during training.</p> <code>False</code> <code>learning_rate</code> <code>float</code> <p>Learning rate schedule for weight updates</p> <code>0.01</code> <code>epochs</code> <code>int</code> <p>Number of training epochs</p> <code>100</code> <code>loss_func</code> <code>str</code> <p>Select the loss function available in torch.nn.functional</p> <code>'mse_loss'</code> <code>optimizer</code> <code>str</code> <p>The solver for weight optimization</p> <code>'SGD'</code> <code>optim_params</code> <code>dict</code> <p>Optional parameters for the optimizer</p> <code>None</code> <code>net</code> <code>default=None</code> <p>The defined network using nn.Module</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Show the training and validation loss at each iteration</p> <code>False</code> <code>random_state</code> <code>int or None</code> <p>Controls the seeding used to reset the neural network parameters before training. When provided, the model weights are reinitialized with the same seed at every call to <code>fit</code> to guarantee deterministic behaviour.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.metrics import mean_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n&gt;&gt;&gt; from sysidentpy.neural_network import NARXNN\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(\n...     n=1000,\n...     colored_noise=False,\n...     sigma=0.01,\n...     train_percentage=80\n... )\n&gt;&gt;&gt; narx_nn = NARXNN(\n...     ylag=2,\n...     xlag=2,\n...     basis_function=basis_function,\n...     model_type=\"NARMAX\",\n...     loss_func='mse_loss',\n...     optimizer='Adam',\n...     epochs=200,\n...     verbose=False,\n...     optim_params={'betas': (0.9, 0.999), 'eps': 1e-05} # for the optimizer\n... )\n&gt;&gt;&gt; class Net(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.lin = nn.Linear(4, 10)\n...         self.lin2 = nn.Linear(10, 10)\n...         self.lin3 = nn.Linear(10, 1)\n...         self.tanh = nn.Tanh()\n&gt;&gt;&gt;\n...     def forward(self, xb):\n...         z = self.lin(xb)\n...         z = self.tanh(z)\n...         z = self.lin2(z)\n...         z = self.tanh(z)\n...         z = self.lin3(z)\n...         return z\n&gt;&gt;&gt;\n&gt;&gt;&gt; narx_nn.net = Net()\n&gt;&gt;&gt; neural_narx.fit(x=x_train, y=y_train)\n&gt;&gt;&gt; yhat = neural_narx.predict(x=x_valid, y=y_valid)\n&gt;&gt;&gt; print(mean_squared_error(y_valid, yhat))\n0.000131\n</code></pre> References <ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf`_</li> </ul> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>class NARXNN(BaseMSS):\n    r\"\"\"NARX Neural Network model built on top of Pytorch.\n\n    Neural networks are models composed of interconnected layers of nodes\n    (neurons) designed for tasks like classification and regression. Each neuron\n    is a basic unit within these networks. Mathematically, a neuron is\n    represented by a function $f$ that takes an input vector\n    $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$ and generates an output $y$.\n    This function usually involves a weighted sum of the inputs, an optional\n    bias term $b$, and an activation function $\\phi$:\n\n    $$\n    y = \\phi \\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n    \\tag{2.31}\n    $$\n\n    where $\\mathbf{w} = [w_1, w_2, \\ldots, w_n]$ are the weights associated with the\n    inputs. The activation function $\\phi$ introduces nonlinearity into the model,\n    allowing the network to learn complex patterns.\n\n    Currently we support a Series-Parallel (open-loop) Feedforward Network training\n    process, which make the training process easier, and we convert the\n    NARX network from Series-Parallel to the Parallel (closed-loop) configuration for\n    prediction.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    basis_function: Polynomial or Fourier basis functions\n        Defines which basis function will be used in the model.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n    batch_size : int, default=100\n        Size of mini-batches of data for stochastic optimizers\n    shuffle_batches : bool, default=False\n        Whether to shuffle mini-batches during training.\n    learning_rate : float, default=0.01\n        Learning rate schedule for weight updates\n    epochs : int, default=100\n        Number of training epochs\n    loss_func : str, default='mse_loss'\n        Select the loss function available in torch.nn.functional\n    optimizer : str, default='SGD'\n        The solver for weight optimization\n    optim_params : dict, default=None\n        Optional parameters for the optimizer\n    net : default=None\n        The defined network using nn.Module\n    verbose : bool, default=False\n        Show the training and validation loss at each iteration\n    random_state : int or None, default=None\n        Controls the seeding used to reset the neural network parameters before\n        training. When provided, the model weights are reinitialized with the\n        same seed at every call to ``fit`` to guarantee deterministic behaviour.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from torch import nn\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.metrics import mean_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; from sysidentpy.neural_network import NARXNN\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(\n    ...     n=1000,\n    ...     colored_noise=False,\n    ...     sigma=0.01,\n    ...     train_percentage=80\n    ... )\n    &gt;&gt;&gt; narx_nn = NARXNN(\n    ...     ylag=2,\n    ...     xlag=2,\n    ...     basis_function=basis_function,\n    ...     model_type=\"NARMAX\",\n    ...     loss_func='mse_loss',\n    ...     optimizer='Adam',\n    ...     epochs=200,\n    ...     verbose=False,\n    ...     optim_params={'betas': (0.9, 0.999), 'eps': 1e-05} # for the optimizer\n    ... )\n    &gt;&gt;&gt; class Net(nn.Module):\n    ...     def __init__(self):\n    ...         super().__init__()\n    ...         self.lin = nn.Linear(4, 10)\n    ...         self.lin2 = nn.Linear(10, 10)\n    ...         self.lin3 = nn.Linear(10, 1)\n    ...         self.tanh = nn.Tanh()\n    &gt;&gt;&gt;\n    ...     def forward(self, xb):\n    ...         z = self.lin(xb)\n    ...         z = self.tanh(z)\n    ...         z = self.lin2(z)\n    ...         z = self.tanh(z)\n    ...         z = self.lin3(z)\n    ...         return z\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; narx_nn.net = Net()\n    &gt;&gt;&gt; neural_narx.fit(x=x_train, y=y_train)\n    &gt;&gt;&gt; yhat = neural_narx.predict(x=x_valid, y=y_valid)\n    &gt;&gt;&gt; print(mean_squared_error(y_valid, yhat))\n    0.000131\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       &lt;https://eprints.soton.ac.uk/251147/1/778742007_content.pdf&gt;`_\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag=1,\n        xlag=1,\n        model_type=\"NARMAX\",\n        basis_function=Polynomial(),\n        batch_size=100,\n        learning_rate=0.01,\n        epochs=200,\n        loss_func=\"mse_loss\",\n        optimizer=\"Adam\",\n        net=None,\n        train_percentage=80,\n        verbose=False,\n        optim_params=None,\n        device=\"cpu\",\n        shuffle_batches=False,\n        random_state: Optional[int] = None,\n    ):\n        self.ylag = ylag\n        self.xlag = xlag\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.non_degree = basis_function.degree\n        self.max_lag = self._get_max_lag()\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.loss_func_name = loss_func\n        self.loss_func = None\n        self.optimizer_name = optimizer\n        self.optimizer = optimizer\n        self.optimizer_cls = None\n        self.net = net\n        self.train_percentage = train_percentage\n        self.verbose = verbose\n        self.shuffle_batches = shuffle_batches\n        self.random_state = random_state\n        if optim_params is None:\n            self.optim_params = {}\n        elif isinstance(optim_params, Mapping):\n            self.optim_params = dict(optim_params)\n        else:\n            self.optim_params = optim_params\n        self.device = _check_cuda(device)\n        self.regressor_code = None\n        self.train_loss = None\n        self.val_loss = None\n        self.ensemble = None\n        self.n_inputs = None\n        self.final_model = None\n        self._validate_params()\n        self.loss_func = getattr(F, self.loss_func_name)\n        self.optimizer_cls = getattr(optim, self.optimizer_name)\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if not isinstance(self.batch_size, int) or self.batch_size &lt; 1:\n            raise ValueError(\n                f\"bacth_size must be integer and &gt; zero. Got {self.batch_size}\"\n            )\n\n        if not isinstance(self.epochs, int) or self.epochs &lt; 1:\n            raise ValueError(f\"epochs must be integer and &gt; zero. Got {self.epochs}\")\n\n        if (\n            not isinstance(self.train_percentage, int)\n            or self.train_percentage &lt;= 0\n            or self.train_percentage &gt; 100\n        ):\n            raise ValueError(\n                \"train_percentage must be an integer between 1 and 100. \"\n                f\"Got {self.train_percentage}\"\n            )\n\n        if not isinstance(self.verbose, bool):\n            raise TypeError(f\"verbose must be False or True. Got {self.verbose}\")\n\n        if not isinstance(self.shuffle_batches, bool):\n            raise TypeError(\n                f\"shuffle_batches must be False or True. Got {self.shuffle_batches}\"\n            )\n\n        self.ylag = self._sanitize_lag(self.ylag, \"ylag\")\n        self.xlag = self._sanitize_lag(self.xlag, \"xlag\")\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n        if not isinstance(self.optim_params, dict):\n            raise TypeError(\n                \"optim_params must be a mapping (e.g. dict). \"\n                f\"Got {type(self.optim_params).__name__}\"\n            )\n\n        if not isinstance(self.loss_func_name, str):\n            raise TypeError(\n                f\"loss_func must be provided as string. Got {self.loss_func_name}\"\n            )\n        if not hasattr(F, self.loss_func_name):\n            raise ValueError(\n                f\"loss_func {self.loss_func_name} not available in torch.nn.functional\"\n            )\n\n        if not isinstance(self.optimizer_name, str):\n            raise TypeError(\n                f\"optimizer must be provided as string. Got {self.optimizer_name}\"\n            )\n        if not hasattr(optim, self.optimizer_name):\n            raise ValueError(\n                f\"optimizer {self.optimizer_name} not available in torch.optim\"\n            )\n\n    @staticmethod\n    def _sanitize_lag(value, name):\n        if isinstance(value, int):\n            if value &lt; 1:\n                raise ValueError(f\"{name} must be &gt;= 1. Got {value}\")\n            return value\n\n        if isinstance(value, (list, tuple, np.ndarray)):\n            if len(value) == 0:\n                raise ValueError(f\"{name} list cannot be empty\")\n            sanitized = []\n            for idx, lag in enumerate(value):\n                if not isinstance(lag, (int, np.integer)):\n                    raise ValueError(\n                        f\"All elements of {name} must be integers. \"\n                        f\"Found {type(lag).__name__} at position {idx}\"\n                    )\n                if lag &lt; 1:\n                    raise ValueError(\n                        f\"All elements of {name} must be &gt;= 1. \"\n                        f\"Found {lag} at position {idx}\"\n                    )\n                sanitized.append(int(lag))\n            return sanitized\n\n        raise ValueError(\n            f\"{name} must be an int or a sequence of ints. Got {type(value).__name__}\"\n        )\n\n    @staticmethod\n    def _as_float_array(array):\n        return np.ascontiguousarray(np.asarray(array, dtype=np.float32))\n\n    def _forward_numpy(self, array):\n        tensor = torch.from_numpy(self._as_float_array(array))\n        if self.device.type != \"cpu\":\n            tensor = tensor.to(self.device, non_blocking=True)\n        return self.net(tensor).detach().cpu().numpy()\n\n    def _scalar_forward(self, array):\n        return float(self._forward_numpy(array).reshape(-1)[0])\n\n    def define_opt(self):\n        \"\"\"Define the optimizer using the user parameters.\"\"\"\n        return self.optimizer_cls(\n            self.net.parameters(), lr=self.learning_rate, **self.optim_params\n        )\n\n    def _seed_torch_generators(self):\n        if self.random_state is None:\n            return\n        torch.manual_seed(self.random_state)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(self.random_state)\n\n    def _reset_network_parameters(self):\n        if self.net is None:\n            raise ValueError(\"The neural network must be defined before training\")\n\n        def _reset_fn(module):\n            if hasattr(module, \"reset_parameters\"):\n                module.reset_parameters()\n\n        self.net.apply(_reset_fn)\n\n    def loss_batch(self, x, y, opt=None):\n        \"\"\"Compute the loss for one batch.\n\n        Parameters\n        ----------\n        x : ndarray of floats\n            The regressor matrix.\n        y : ndarray of floats\n            The output data.\n        opt: Torch optimizer\n            Chosen by the user.\n\n        Returns\n        -------\n        loss : float\n            The loss of one batch.\n\n        \"\"\"\n        loss = self.loss_func(self.net(x), y)\n\n        if opt is not None:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n        return loss.item(), len(x)\n\n    def split_data(self, x, y):\n        \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n        Parameters\n        ----------\n        x : ndarray of floats\n            The input data.\n        y : ndarray of floats\n            The output data.\n\n        Returns\n        -------\n        y : ndarray of floats\n            The y values considering the lags.\n        reg_matrix : ndarray of floats\n            The information matrix of the model.\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n\n        if isinstance(self.basis_function, Polynomial):\n            reg_matrix = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=None,\n            )\n            reg_matrix = reg_matrix[:, 1:]\n        else:\n            reg_matrix = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=None,\n            )\n\n        if x is not None:\n            self.n_inputs = num_features(x)\n        else:\n            self.n_inputs = 1  # only used to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        repetition = len(reg_matrix)\n        if not isinstance(self.basis_function, Polynomial):\n            tmp_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.regressor_code = tmp_code[list(range(len(reg_matrix))), :].copy()\n        else:\n            self.regressor_code = self.regressor_code[\n                1:\n            ]  # removes the column of the constant\n\n        self.final_model = self.regressor_code.copy()\n        reg_matrix = np.atleast_1d(reg_matrix).astype(np.float32)\n\n        y = np.atleast_1d(y[self.max_lag :]).astype(np.float32)\n        return reg_matrix, y\n\n    def get_data(self, train_ds, *, shuffle=None):\n        \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n        Based on Pytorch official docs:\n        https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n        Parameters\n        ----------\n        train_ds: tensor\n            Tensors that have the same size of the first dimension.\n\n        Returns\n        -------\n        Dataloader: dataloader\n            tensors that have the same size of the first dimension.\n\n        \"\"\"\n        pin_memory = False if self.device.type == \"cpu\" else True\n        if shuffle is None:\n            shuffle = self.shuffle_batches\n        return DataLoader(\n            train_ds,\n            batch_size=self.batch_size,\n            pin_memory=pin_memory,\n            shuffle=shuffle,\n        )\n\n    def data_transform(self, x, y, *, shuffle=None):\n        \"\"\"Return the data transformed in tensors using Dataloader.\n\n        Parameters\n        ----------\n        x : ndarray of floats\n            The input data.\n        y : ndarray of floats\n            The output data.\n\n        Returns\n        -------\n        Tensors : Dataloader\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        x_train, y_train = self.split_data(x, y)\n        train_ds = convert_to_tensor(x_train, y_train)\n        train_dl = self.get_data(train_ds, shuffle=shuffle)\n        return train_dl\n\n    def fit(self, *, X=None, y=None, X_test=None, y_test=None):\n        \"\"\"Train a NARX Neural Network model.\n\n        This is an training pipeline that allows a friendly usage\n        by the user. The training pipeline was based on\n        https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n        X_test : ndarray of floats\n            The input data to be used in the prediction process.\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n\n        Returns\n        -------\n        net : nn.Module\n            The model fitted.\n        train_loss: ndarrays of floats\n            The training loss of each batch\n        val_loss: ndarrays of floats\n            The validation loss of each batch\n\n        \"\"\"\n        if self.random_state is not None:\n            self._seed_torch_generators()\n            self._reset_network_parameters()\n\n        train_dl = self.data_transform(X, y, shuffle=self.shuffle_batches)\n        if self.verbose:\n            if X_test is None or y_test is None:\n                raise ValueError(\n                    \"X_test and y_test cannot be None if you set verbose=True\"\n                )\n            valid_dl = self.data_transform(X_test, y_test, shuffle=False)\n\n        opt = self.define_opt()\n        self.val_loss = []\n        self.train_loss = []\n        for epoch in range(self.epochs):\n            self.net.train()\n            epoch_loss = 0.0\n            seen_samples = 0\n            for input_data, output_data in train_dl:\n                X_batch = input_data.to(self.device, non_blocking=True)\n                y_batch = output_data.to(self.device, non_blocking=True)\n                batch_loss, batch_size = self.loss_batch(X_batch, y_batch, opt=opt)\n                if self.verbose:\n                    epoch_loss += batch_loss * batch_size\n                    seen_samples += batch_size\n\n            if self.verbose:\n                train_metric = epoch_loss / max(seen_samples, 1)\n                self.train_loss.append(train_metric)\n\n                self.net.eval()\n                val_loss = 0.0\n                val_count = 0\n                with torch.no_grad():\n                    for X_val, y_val in valid_dl:\n                        loss_val, batch_size = self.loss_batch(\n                            X_val.to(self.device, non_blocking=True),\n                            y_val.to(self.device, non_blocking=True),\n                        )\n                        val_loss += loss_val * batch_size\n                        val_count += batch_size\n                self.val_loss.append(val_loss / max(val_count, 1))\n                logging.info(\n                    \"Train metrics: %s | Validation metrics: %s\",\n                    self.train_loss[epoch],\n                    self.val_loss[epoch],\n                )\n        return self\n\n    def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n        \"\"\"Return the predicted given an input and initial values.\n\n        The predict function allows a friendly usage by the user.\n        Given a trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future).\n\n        Currently, we only support infinity-steps-ahead prediction,\n        but run 1-step-ahead prediction manually is straightforward.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if self.net is None:\n            raise ValueError(\"The neural network must be defined before prediction\")\n\n        was_training = self.net.training\n        self.net.eval()\n        try:\n            with torch.no_grad():\n                if isinstance(self.basis_function, Polynomial):\n                    if steps_ahead is None:\n                        result = self._model_prediction(\n                            X, y, forecast_horizon=forecast_horizon\n                        )\n                    elif steps_ahead == 1:\n                        result = self._one_step_ahead_prediction(X, y)\n                    else:\n                        check_positive_int(steps_ahead, \"steps_ahead\")\n                        result = self._n_step_ahead_prediction(\n                            X, y, steps_ahead=steps_ahead\n                        )\n                else:\n                    if steps_ahead is None:\n                        result = self._basis_function_predict(\n                            X, y, forecast_horizon=forecast_horizon\n                        )\n                    elif steps_ahead == 1:\n                        result = self._one_step_ahead_prediction(X, y)\n                    else:\n                        check_positive_int(steps_ahead, \"steps_ahead\")\n                        result = self._basis_function_n_step_prediction(\n                            X,\n                            y,\n                            steps_ahead=steps_ahead,\n                            forecast_horizon=forecast_horizon,\n                        )\n        finally:\n            if was_training:\n                self.net.train()\n\n        return result\n\n    def _one_step_ahead_prediction(self, x_base, y=None):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        lagged_data = build_lagged_matrix(\n            x_base, y, self.xlag, self.ylag, self.model_type\n        )\n\n        if isinstance(self.basis_function, Polynomial):\n            x_base = self.basis_function.transform(\n                lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n            )\n            x_base = x_base[:, 1:]\n        else:\n            x_base = self.basis_function.transform(\n                lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n            )\n\n        predictions = self._forward_numpy(x_base)\n        yhat = np.concatenate(\n            [y.ravel()[: self.max_lag].flatten(), predictions.ravel()]\n        )\n        return yhat.astype(np.float32).reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, x, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        yhat = np.zeros(x.shape[0], dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n        x = x.reshape(-1, self.n_inputs)\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x[k : i + steps_ahead], y[k : i + steps_ahead]\n            )[-steps_ahead:].ravel()\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat.reshape(-1, 1)\n\n    def _model_prediction(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(self, x, y_initial, forecast_horizon=None):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            x = self._as_float_array(x).reshape(-1, self.n_inputs)\n            forecast_horizon = x.shape[0]\n        else:\n            if forecast_horizon is None:\n                raise ValueError(\n                    \"forecast_horizon cannot be None when x is None for NARXNN prediction\"\n                )\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = np.full(forecast_horizon, np.nan, dtype=np.float32)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        model_exponents = np.vstack(\n            [self._code2exponents(code=model) for model in self.final_model]\n        )\n        raw_regressor = np.zeros(model_exponents.shape[1], dtype=np.float32)\n        regressor_powers = np.empty(model_exponents.shape, dtype=np.float32)\n        regressor_value = np.empty(model_exponents.shape[0], dtype=np.float32)\n        for i in range(self.max_lag, forecast_horizon):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n\n            np.power(raw_regressor, model_exponents, out=regressor_powers)\n            np.prod(regressor_powers, axis=1, out=regressor_value)\n            y_output[i] = self._scalar_forward(regressor_value)\n        return y_output.reshape(-1, 1)\n\n    def _nfir_predict(self, x, y_initial):\n        x = self._as_float_array(x).reshape(-1, self.n_inputs)\n        y_output = np.full(x.shape[0], np.nan, dtype=np.float32)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n        model_exponents = np.vstack(\n            [self._code2exponents(code=model) for model in self.final_model]\n        )\n        raw_regressor = np.zeros(model_exponents.shape[1], dtype=np.float32)\n        regressor_powers = np.empty(model_exponents.shape, dtype=np.float32)\n        regressor_value = np.empty(model_exponents.shape[0], dtype=np.float32)\n        for i in range(self.max_lag, x.shape[0]):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            for j in range(self.n_inputs):\n                raw_regressor[init:final] = x[k:i, j]\n                init += self.max_lag\n                final += self.max_lag\n\n            np.power(raw_regressor, model_exponents, out=regressor_powers)\n            np.prod(regressor_powers, axis=1, out=regressor_value)\n            y_output[i] = self._scalar_forward(regressor_value)\n        return y_output.reshape(-1, 1)\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = np.full(forecast_horizon, np.nan, dtype=np.float32)\n        yhat[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        analyzed_elements_number = self.max_lag + 1\n\n        for i in range(forecast_horizon - self.max_lag):\n            if self.model_type == \"NARMAX\":\n                lagged_data = build_input_output_matrix(\n                    x[i : i + analyzed_elements_number],\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1),\n                    self.xlag,\n                    self.ylag,\n                )\n            elif self.model_type == \"NAR\":\n                lagged_data = build_output_matrix(\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1), self.ylag\n                )\n            elif self.model_type == \"NFIR\":\n                lagged_data = build_input_matrix(\n                    x[i : i + analyzed_elements_number], self.xlag\n                )\n            else:\n                raise ValueError(\n                    \"Unrecognized model type. The model_type should be NARMAX, NAR or\"\n                    \" NFIR.\"\n                )\n\n            x_tmp = self.basis_function.transform(\n                lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n            )\n            yhat[i + self.max_lag] = self._scalar_forward(x_tmp)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead], y[k : i + steps_ahead]\n                )[-steps_ahead:].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                )[-steps_ahead:].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.data_transform","title":"<code>data_transform(x, y, *, shuffle=None)</code>","text":"<p>Return the data transformed in tensors using Dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required <p>Returns:</p> Name Type Description <code>Tensors</code> <code>Dataloader</code> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def data_transform(self, x, y, *, shuffle=None):\n    \"\"\"Return the data transformed in tensors using Dataloader.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    Returns\n    -------\n    Tensors : Dataloader\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    x_train, y_train = self.split_data(x, y)\n    train_ds = convert_to_tensor(x_train, y_train)\n    train_dl = self.get_data(train_ds, shuffle=shuffle)\n    return train_dl\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.define_opt","title":"<code>define_opt()</code>","text":"<p>Define the optimizer using the user parameters.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def define_opt(self):\n    \"\"\"Define the optimizer using the user parameters.\"\"\"\n    return self.optimizer_cls(\n        self.net.parameters(), lr=self.learning_rate, **self.optim_params\n    )\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.fit","title":"<code>fit(*, X=None, y=None, X_test=None, y_test=None)</code>","text":"<p>Train a NARX Neural Network model.</p> <p>This is an training pipeline that allows a friendly usage by the user. The training pipeline was based on https://pytorch.org/tutorials/beginner/nn_tutorial.html</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> <code>None</code> <code>X_test</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>net</code> <code>Module</code> <p>The model fitted.</p> <code>train_loss</code> <code>ndarrays of floats</code> <p>The training loss of each batch</p> <code>val_loss</code> <code>ndarrays of floats</code> <p>The validation loss of each batch</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def fit(self, *, X=None, y=None, X_test=None, y_test=None):\n    \"\"\"Train a NARX Neural Network model.\n\n    This is an training pipeline that allows a friendly usage\n    by the user. The training pipeline was based on\n    https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n    X_test : ndarray of floats\n        The input data to be used in the prediction process.\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n\n    Returns\n    -------\n    net : nn.Module\n        The model fitted.\n    train_loss: ndarrays of floats\n        The training loss of each batch\n    val_loss: ndarrays of floats\n        The validation loss of each batch\n\n    \"\"\"\n    if self.random_state is not None:\n        self._seed_torch_generators()\n        self._reset_network_parameters()\n\n    train_dl = self.data_transform(X, y, shuffle=self.shuffle_batches)\n    if self.verbose:\n        if X_test is None or y_test is None:\n            raise ValueError(\n                \"X_test and y_test cannot be None if you set verbose=True\"\n            )\n        valid_dl = self.data_transform(X_test, y_test, shuffle=False)\n\n    opt = self.define_opt()\n    self.val_loss = []\n    self.train_loss = []\n    for epoch in range(self.epochs):\n        self.net.train()\n        epoch_loss = 0.0\n        seen_samples = 0\n        for input_data, output_data in train_dl:\n            X_batch = input_data.to(self.device, non_blocking=True)\n            y_batch = output_data.to(self.device, non_blocking=True)\n            batch_loss, batch_size = self.loss_batch(X_batch, y_batch, opt=opt)\n            if self.verbose:\n                epoch_loss += batch_loss * batch_size\n                seen_samples += batch_size\n\n        if self.verbose:\n            train_metric = epoch_loss / max(seen_samples, 1)\n            self.train_loss.append(train_metric)\n\n            self.net.eval()\n            val_loss = 0.0\n            val_count = 0\n            with torch.no_grad():\n                for X_val, y_val in valid_dl:\n                    loss_val, batch_size = self.loss_batch(\n                        X_val.to(self.device, non_blocking=True),\n                        y_val.to(self.device, non_blocking=True),\n                    )\n                    val_loss += loss_val * batch_size\n                    val_count += batch_size\n            self.val_loss.append(val_loss / max(val_count, 1))\n            logging.info(\n                \"Train metrics: %s | Validation metrics: %s\",\n                self.train_loss[epoch],\n                self.val_loss[epoch],\n            )\n    return self\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.get_data","title":"<code>get_data(train_ds, *, shuffle=None)</code>","text":"<p>Return the lagged matrix and the y values given the maximum lags.</p> <p>Based on Pytorch official docs: https://pytorch.org/tutorials/beginner/nn_tutorial.html</p> <p>Parameters:</p> Name Type Description Default <code>train_ds</code> <p>Tensors that have the same size of the first dimension.</p> required <p>Returns:</p> Name Type Description <code>Dataloader</code> <code>dataloader</code> <p>tensors that have the same size of the first dimension.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def get_data(self, train_ds, *, shuffle=None):\n    \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n    Based on Pytorch official docs:\n    https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n    Parameters\n    ----------\n    train_ds: tensor\n        Tensors that have the same size of the first dimension.\n\n    Returns\n    -------\n    Dataloader: dataloader\n        tensors that have the same size of the first dimension.\n\n    \"\"\"\n    pin_memory = False if self.device.type == \"cpu\" else True\n    if shuffle is None:\n        shuffle = self.shuffle_batches\n    return DataLoader(\n        train_ds,\n        batch_size=self.batch_size,\n        pin_memory=pin_memory,\n        shuffle=shuffle,\n    )\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.loss_batch","title":"<code>loss_batch(x, y, opt=None)</code>","text":"<p>Compute the loss for one batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The regressor matrix.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required <code>opt</code> <p>Chosen by the user.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>The loss of one batch.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def loss_batch(self, x, y, opt=None):\n    \"\"\"Compute the loss for one batch.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The regressor matrix.\n    y : ndarray of floats\n        The output data.\n    opt: Torch optimizer\n        Chosen by the user.\n\n    Returns\n    -------\n    loss : float\n        The loss of one batch.\n\n    \"\"\"\n    loss = self.loss_func(self.net(x), y)\n\n    if opt is not None:\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n    return loss.item(), len(x)\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted given an input and initial values.</p> <p>The predict function allows a friendly usage by the user. Given a trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future).</p> <p>Currently, we only support infinity-steps-ahead prediction, but run 1-step-ahead prediction manually is straightforward.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n    \"\"\"Return the predicted given an input and initial values.\n\n    The predict function allows a friendly usage by the user.\n    Given a trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future).\n\n    Currently, we only support infinity-steps-ahead prediction,\n    but run 1-step-ahead prediction manually is straightforward.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if self.net is None:\n        raise ValueError(\"The neural network must be defined before prediction\")\n\n    was_training = self.net.training\n    self.net.eval()\n    try:\n        with torch.no_grad():\n            if isinstance(self.basis_function, Polynomial):\n                if steps_ahead is None:\n                    result = self._model_prediction(\n                        X, y, forecast_horizon=forecast_horizon\n                    )\n                elif steps_ahead == 1:\n                    result = self._one_step_ahead_prediction(X, y)\n                else:\n                    check_positive_int(steps_ahead, \"steps_ahead\")\n                    result = self._n_step_ahead_prediction(\n                        X, y, steps_ahead=steps_ahead\n                    )\n            else:\n                if steps_ahead is None:\n                    result = self._basis_function_predict(\n                        X, y, forecast_horizon=forecast_horizon\n                    )\n                elif steps_ahead == 1:\n                    result = self._one_step_ahead_prediction(X, y)\n                else:\n                    check_positive_int(steps_ahead, \"steps_ahead\")\n                    result = self._basis_function_n_step_prediction(\n                        X,\n                        y,\n                        steps_ahead=steps_ahead,\n                        forecast_horizon=forecast_horizon,\n                    )\n    finally:\n        if was_training:\n            self.net.train()\n\n    return result\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.split_data","title":"<code>split_data(x, y)</code>","text":"<p>Return the lagged matrix and the y values given the maximum lags.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray of floats</code> <p>The y values considering the lags.</p> <code>reg_matrix</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def split_data(self, x, y):\n    \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    Returns\n    -------\n    y : ndarray of floats\n        The y values considering the lags.\n    reg_matrix : ndarray of floats\n        The information matrix of the model.\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n\n    if isinstance(self.basis_function, Polynomial):\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n        reg_matrix = reg_matrix[:, 1:]\n    else:\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n    if x is not None:\n        self.n_inputs = num_features(x)\n    else:\n        self.n_inputs = 1  # only used to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    repetition = len(reg_matrix)\n    if not isinstance(self.basis_function, Polynomial):\n        tmp_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.regressor_code = tmp_code[list(range(len(reg_matrix))), :].copy()\n    else:\n        self.regressor_code = self.regressor_code[\n            1:\n        ]  # removes the column of the constant\n\n    self.final_model = self.regressor_code.copy()\n    reg_matrix = np.atleast_1d(reg_matrix).astype(np.float32)\n\n    y = np.atleast_1d(y[self.max_lag :]).astype(np.float32)\n    return reg_matrix, y\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.convert_to_tensor","title":"<code>convert_to_tensor(reg_matrix, y)</code>","text":"<p>Return the lagged matrix and the y values given the maximum lags.</p> <p>Based on Pytorch official docs: https://pytorch.org/tutorials/beginner/nn_tutorial.html</p> <p>Parameters:</p> Name Type Description Default <code>reg_matrix</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>tensor</code> <p>tensors that have the same size of the first dimension.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def convert_to_tensor(reg_matrix, y):\n    \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n    Based on Pytorch official docs:\n    https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n    Parameters\n    ----------\n    reg_matrix : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats\n        The output data\n\n    Returns\n    -------\n    Tensor: tensor\n        tensors that have the same size of the first dimension.\n\n    \"\"\"\n    reg_matrix = np.ascontiguousarray(np.asarray(reg_matrix, dtype=np.float32))\n    y = np.ascontiguousarray(np.asarray(y, dtype=np.float32))\n    return TensorDataset(torch.from_numpy(reg_matrix), torch.from_numpy(y))\n</code></pre>"},{"location":"user-guide/API/ofr-base/","title":"Documentation for <code>OFRBase</code>","text":"<p>Base methods for Orthogonal Forward Regression algorithm.</p>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase","title":"<code>OFRBase</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Base class for Model Structure Selection.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>class OFRBase(BaseMSS, metaclass=ABCMeta):\n    \"\"\"Base class for Model Structure Selection.\"\"\"\n\n    @abstractmethod\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        elag: Union[int, list] = 2,\n        order_selection: bool = True,\n        info_criteria: str = \"aic\",\n        n_terms: Union[int, None] = None,\n        n_info_values: int = 15,\n        estimator: Estimators = RecursiveLeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n        eps: np.float64 = np.finfo(np.float64).eps,\n        alpha: float = 0,\n        err_tol: Optional[float] = None,\n    ):\n        self.order_selection = order_selection\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.info_criteria = info_criteria\n        self.info_criteria_function = get_info_criteria(info_criteria)\n        self.n_info_values = n_info_values\n        self.n_terms = n_terms\n        self.estimator = estimator\n        self.elag = elag\n        self.model_type = model_type\n        self.basis_function = basis_function\n        self.eps = eps\n        if isinstance(self.estimator, RidgeRegression):\n            self.alpha = self.estimator.alpha\n        else:\n            self.alpha = alpha\n\n        self.err_tol = err_tol\n        self._validate_params()\n        self.n_inputs = None\n        self.regressor_code = None\n        self.info_values = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if not isinstance(self.n_info_values, int) or self.n_info_values &lt; 1:\n            raise ValueError(\n                f\"n_info_values must be integer and &gt; zero. Got {self.n_info_values}\"\n            )\n\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if not isinstance(self.order_selection, bool):\n            raise TypeError(\n                f\"order_selection must be False or True. Got {self.order_selection}\"\n            )\n\n        if self.info_criteria not in [\"aic\", \"aicc\", \"bic\", \"fpe\", \"lilc\"]:\n            raise ValueError(\n                f\"info_criteria must be aic, bic, fpe or lilc. Got {self.info_criteria}\"\n            )\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n        if (\n            not isinstance(self.n_terms, int) or self.n_terms &lt; 1\n        ) and self.n_terms is not None:\n            raise ValueError(f\"n_terms must be integer and &gt; zero. Got {self.n_terms}\")\n\n        if not isinstance(self.eps, float) or self.eps &lt; 0:\n            raise ValueError(f\"eps must be float and &gt; zero. Got {self.eps}\")\n\n    @abstractmethod\n    def run_mss_algorithm(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        return self.error_reduction_ratio(psi, y, process_term_number)\n\n    def error_reduction_ratio(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the Error Reduction Ration algorithm.\n\n        Parameters\n        ----------\n        y : array-like of shape = n_samples\n            The target data used in the identification process.\n        psi : ndarray of floats\n            The information matrix of the model.\n        process_term_number : int\n            Number of Process Terms defined by the user.\n\n        Returns\n        -------\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        psi_orthogonal : ndarray of floats\n            The updated and orthogonal information matrix.\n\n        References\n        ----------\n        - Manuscript: Orthogonal least squares methods and their application\n           to non-linear system identification\n           https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n        - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n           Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n           e Novos Resultados\n\n        \"\"\"\n        squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n        squared_y = float(np.maximum(squared_y, np.finfo(np.float64).eps))\n        tmp_psi = psi.copy()\n        y = y[self.max_lag :, 0].reshape(-1, 1)\n        tmp_y = y.copy()\n        dimension = tmp_psi.shape[1]\n        piv = np.arange(dimension)\n        tmp_err = np.zeros(dimension)\n        err = np.zeros(dimension)\n\n        for i in np.arange(0, dimension):\n            tmp_err[i:] = _compute_err_slice(\n                tmp_psi,\n                tmp_y,\n                i,\n                squared_y,\n                self.alpha,\n                self.eps,\n            )\n\n            piv_index = np.argmax(tmp_err[i:]) + i\n            err[i] = tmp_err[piv_index]\n            if i == process_term_number:\n                break\n\n            if (self.err_tol is not None) and (err.cumsum()[i] &gt;= self.err_tol):\n                self.n_terms = i + 1\n                process_term_number = i + 1\n                break\n\n            tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n            piv[[piv_index, i]] = piv[[i, piv_index]]\n            v = house(tmp_psi[i:, i])\n            row_result = rowhouse(tmp_psi[i:, i:], v)\n            tmp_y[i:] = rowhouse(tmp_y[i:], v)\n            tmp_psi[i:, i:] = np.copy(row_result)\n\n        tmp_piv = piv[0:process_term_number]\n        psi_orthogonal = psi[:, tmp_piv]\n        return err, tmp_piv, psi_orthogonal\n\n    def information_criterion(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Determine the model order.\n\n        This function uses a information criterion to determine the model size.\n        'Akaike'-  Akaike's Information Criterion with\n                   critical value 2 (AIC) (default).\n        'Bayes' -  Bayes Information Criterion (BIC).\n        'FPE'   -  Final Prediction Error (FPE).\n        'LILC'  -  Khundrin's law ofiterated logarithm criterion (LILC).\n\n        Parameters\n        ----------\n        y : array-like of shape = n_samples\n            Target values of the system.\n        x : array-like of shape = n_samples\n            Input system values measured by the user.\n\n        Returns\n        -------\n        output_vector : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        if self.n_info_values is not None and self.n_info_values &gt; x.shape[1]:\n            self.n_info_values = x.shape[1]\n            warnings.warn(\n                \"n_info_values is greater than the maximum number of all\"\n                \" regressors space considering the chosen y_lag, u_lag, and\"\n                f\" non_degree. We set as {x.shape[1]}\",\n                stacklevel=2,\n            )\n\n        output_vector = np.zeros(self.n_info_values)\n        output_vector[:] = np.nan\n\n        n_samples = len(y) - self.max_lag\n\n        for i in range(self.n_info_values):\n            n_theta = i + 1\n            regressor_matrix = self.run_mss_algorithm(x, y, n_theta)[2]\n\n            tmp_theta = self.estimator.optimize(\n                regressor_matrix, y[self.max_lag :, 0].reshape(-1, 1)\n            )\n\n            tmp_yhat = np.dot(regressor_matrix, tmp_theta)\n            tmp_residual = y[self.max_lag :] - tmp_yhat\n            e_var = np.var(tmp_residual, ddof=1)\n            output_vector[i] = self.info_criteria_function(n_theta, n_samples, e_var)\n\n        return output_vector\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n        \"\"\"Fit polynomial NARMAX model.\n\n        This is an 'alpha' version of the 'fit' function which allows\n        a friendly usage by the user. Given two arguments, x and y, fit\n        training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n        if X is not None:\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n\n        if self.order_selection is True:\n            self.info_values = self.information_criterion(reg_matrix, y)\n\n        if self.n_terms is None and self.order_selection is True:\n            model_length = get_min_info_value(self.info_values)\n            self.n_terms = model_length\n        elif self.n_terms is None and self.order_selection is not True:\n            raise ValueError(\n                \"If order_selection is False, you must define n_terms value.\"\n            )\n        else:\n            model_length = self.n_terms\n\n        (self.err, self.pivv, psi) = self.run_mss_algorithm(reg_matrix, y, model_length)\n\n        tmp_piv = self.pivv[0:model_length]\n        repetition = len(reg_matrix)\n        if isinstance(self.basis_function, Polynomial):\n            self.final_model = self.regressor_code[tmp_piv, :].copy()\n        else:\n            self.regressor_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.final_model = self.regressor_code[tmp_piv, :].copy()\n\n        self.theta = self.estimator.optimize(psi, y[self.max_lag :, 0].reshape(-1, 1))\n        if self.estimator.unbiased is True:\n            self.theta = self.estimator.unbiased_estimator(\n                psi,\n                y[self.max_lag :, 0].reshape(-1, 1),\n                self.theta,\n                self.elag,\n                self.max_lag,\n                self.estimator,\n                self.basis_function,\n                self.estimator.uiter,\n            )\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead, forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(\n        self, x: Optional[np.ndarray], y: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n\n        x_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(\n        self, x: Optional[np.ndarray], y: Optional[np.ndarray], steps_ahead: int\n    ) -&gt; float:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(\n        self, x: Optional[np.ndarray], y_initial: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = super()._basis_function_predict(x, y_initial, forecast_horizon)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: np.ndarray,\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = super()._basis_function_n_step_prediction(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        yhat = super()._basis_function_n_steps_horizon(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase.error_reduction_ratio","title":"<code>error_reduction_ratio(psi, y, process_term_number)</code>","text":"<p>Perform the Error Reduction Ration algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = n_samples</code> <p>The target data used in the identification process.</p> required <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>process_term_number</code> <code>int</code> <p>Number of Process Terms defined by the user.</p> required <p>Returns:</p> Name Type Description <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>psi_orthogonal</code> <code>ndarray of floats</code> <p>The updated and orthogonal information matrix.</p> References <ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def error_reduction_ratio(\n    self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the Error Reduction Ration algorithm.\n\n    Parameters\n    ----------\n    y : array-like of shape = n_samples\n        The target data used in the identification process.\n    psi : ndarray of floats\n        The information matrix of the model.\n    process_term_number : int\n        Number of Process Terms defined by the user.\n\n    Returns\n    -------\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    psi_orthogonal : ndarray of floats\n        The updated and orthogonal information matrix.\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n    squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n    squared_y = float(np.maximum(squared_y, np.finfo(np.float64).eps))\n    tmp_psi = psi.copy()\n    y = y[self.max_lag :, 0].reshape(-1, 1)\n    tmp_y = y.copy()\n    dimension = tmp_psi.shape[1]\n    piv = np.arange(dimension)\n    tmp_err = np.zeros(dimension)\n    err = np.zeros(dimension)\n\n    for i in np.arange(0, dimension):\n        tmp_err[i:] = _compute_err_slice(\n            tmp_psi,\n            tmp_y,\n            i,\n            squared_y,\n            self.alpha,\n            self.eps,\n        )\n\n        piv_index = np.argmax(tmp_err[i:]) + i\n        err[i] = tmp_err[piv_index]\n        if i == process_term_number:\n            break\n\n        if (self.err_tol is not None) and (err.cumsum()[i] &gt;= self.err_tol):\n            self.n_terms = i + 1\n            process_term_number = i + 1\n            break\n\n        tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n        piv[[piv_index, i]] = piv[[i, piv_index]]\n        v = house(tmp_psi[i:, i])\n        row_result = rowhouse(tmp_psi[i:, i:], v)\n        tmp_y[i:] = rowhouse(tmp_y[i:], v)\n        tmp_psi[i:, i:] = np.copy(row_result)\n\n    tmp_piv = piv[0:process_term_number]\n    psi_orthogonal = psi[:, tmp_piv]\n    return err, tmp_piv, psi_orthogonal\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase.fit","title":"<code>fit(*, X=None, y)</code>","text":"<p>Fit polynomial NARMAX model.</p> <p>This is an 'alpha' version of the 'fit' function which allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>info_values</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n    \"\"\"Fit polynomial NARMAX model.\n\n    This is an 'alpha' version of the 'fit' function which allows\n    a friendly usage by the user. Given two arguments, x and y, fit\n    training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n\n    reg_matrix = self.basis_function.fit(\n        lagged_data,\n        self.max_lag,\n        self.ylag,\n        self.xlag,\n        self.model_type,\n        predefined_regressors=None,\n    )\n\n    if X is not None:\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n\n    if self.order_selection is True:\n        self.info_values = self.information_criterion(reg_matrix, y)\n\n    if self.n_terms is None and self.order_selection is True:\n        model_length = get_min_info_value(self.info_values)\n        self.n_terms = model_length\n    elif self.n_terms is None and self.order_selection is not True:\n        raise ValueError(\n            \"If order_selection is False, you must define n_terms value.\"\n        )\n    else:\n        model_length = self.n_terms\n\n    (self.err, self.pivv, psi) = self.run_mss_algorithm(reg_matrix, y, model_length)\n\n    tmp_piv = self.pivv[0:model_length]\n    repetition = len(reg_matrix)\n    if isinstance(self.basis_function, Polynomial):\n        self.final_model = self.regressor_code[tmp_piv, :].copy()\n    else:\n        self.regressor_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.final_model = self.regressor_code[tmp_piv, :].copy()\n\n    self.theta = self.estimator.optimize(psi, y[self.max_lag :, 0].reshape(-1, 1))\n    if self.estimator.unbiased is True:\n        self.theta = self.estimator.unbiased_estimator(\n            psi,\n            y[self.max_lag :, 0].reshape(-1, 1),\n            self.theta,\n            self.elag,\n            self.max_lag,\n            self.estimator,\n            self.basis_function,\n            self.estimator.uiter,\n        )\n    return self\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase.information_criterion","title":"<code>information_criterion(x, y)</code>","text":"<p>Determine the model order.</p> <p>This function uses a information criterion to determine the model size. 'Akaike'-  Akaike's Information Criterion with            critical value 2 (AIC) (default). 'Bayes' -  Bayes Information Criterion (BIC). 'FPE'   -  Final Prediction Error (FPE). 'LILC'  -  Khundrin's law ofiterated logarithm criterion (LILC).</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = n_samples</code> <p>Target values of the system.</p> required <code>x</code> <code>array-like of shape = n_samples</code> <p>Input system values measured by the user.</p> required <p>Returns:</p> Name Type Description <code>output_vector</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def information_criterion(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Determine the model order.\n\n    This function uses a information criterion to determine the model size.\n    'Akaike'-  Akaike's Information Criterion with\n               critical value 2 (AIC) (default).\n    'Bayes' -  Bayes Information Criterion (BIC).\n    'FPE'   -  Final Prediction Error (FPE).\n    'LILC'  -  Khundrin's law ofiterated logarithm criterion (LILC).\n\n    Parameters\n    ----------\n    y : array-like of shape = n_samples\n        Target values of the system.\n    x : array-like of shape = n_samples\n        Input system values measured by the user.\n\n    Returns\n    -------\n    output_vector : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    if self.n_info_values is not None and self.n_info_values &gt; x.shape[1]:\n        self.n_info_values = x.shape[1]\n        warnings.warn(\n            \"n_info_values is greater than the maximum number of all\"\n            \" regressors space considering the chosen y_lag, u_lag, and\"\n            f\" non_degree. We set as {x.shape[1]}\",\n            stacklevel=2,\n        )\n\n    output_vector = np.zeros(self.n_info_values)\n    output_vector[:] = np.nan\n\n    n_samples = len(y) - self.max_lag\n\n    for i in range(self.n_info_values):\n        n_theta = i + 1\n        regressor_matrix = self.run_mss_algorithm(x, y, n_theta)[2]\n\n        tmp_theta = self.estimator.optimize(\n            regressor_matrix, y[self.max_lag :, 0].reshape(-1, 1)\n        )\n\n        tmp_yhat = np.dot(regressor_matrix, tmp_theta)\n        tmp_residual = y[self.max_lag :] - tmp_yhat\n        e_var = np.var(tmp_residual, ddof=1)\n        output_vector[i] = self.info_criteria_function(n_theta, n_samples, e_var)\n\n    return output_vector\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase.predict","title":"<code>predict(*, X=None, y, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> required <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: np.ndarray,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead, forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.aic","title":"<code>aic(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Akaike information criteria value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>info_criteria_value</code> <code>float</code> <p>The computed value given the information criteria selected by the user.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def aic(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Akaike information criteria value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = 2 * n_theta\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.aicc","title":"<code>aicc(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Akaike information Criteria corrected value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>aicc</code> <code>float</code> <p>The computed aicc value.</p> References <ul> <li>https://www.mathworks.com/help/ident/ref/idmodel.aic.html</li> </ul> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def aicc(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Akaike information Criteria corrected value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    aicc : float\n        The computed aicc value.\n\n    References\n    ----------\n    - https://www.mathworks.com/help/ident/ref/idmodel.aic.html\n\n    \"\"\"\n    aic_values = aic(n_theta, n_samples, e_var)\n    aicc_values = aic_values + (2 * n_theta * (n_theta + 1) / (n_samples - n_theta - 1))\n\n    return aicc_values\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.bic","title":"<code>bic(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Bayesian information criteria value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>info_criteria_value</code> <code>float</code> <p>The computed value given the information criteria selected by the user.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def bic(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Bayesian information criteria value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = n_theta * np.log(n_samples)\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.fpe","title":"<code>fpe(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Final Error Prediction value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>info_criteria_value</code> <code>float</code> <p>The computed value given the information criteria selected by the user.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def fpe(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Final Error Prediction value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = n_samples * np.log((n_samples + n_theta) / (n_samples - n_theta))\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.get_info_criteria","title":"<code>get_info_criteria(info_criteria)</code>","text":"<p>Get info criteria.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def get_info_criteria(info_criteria: str):\n    \"\"\"Get info criteria.\"\"\"\n    info_criteria_options = {\n        \"aic\": aic,\n        \"aicc\": aicc,\n        \"bic\": bic,\n        \"fpe\": fpe,\n        \"lilc\": lilc,\n    }\n    return info_criteria_options.get(info_criteria)\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.get_min_info_value","title":"<code>get_min_info_value(info_values)</code>","text":"<p>Find the index of the first increasing value in an array.</p> <p>Parameters:</p> Name Type Description Default <code>info_values</code> <code>array - like</code> <p>A sequence of numeric values to be analyzed.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The index of the first element where the values start to increase monotonically. If no such element exists, the length of <code>info_values</code> is returned.</p> Notes <ul> <li>The function assumes that <code>info_values</code> is a 1-dimensional array-like structure.</li> <li>The function uses <code>np.diff</code> to compute the difference between consecutive elements in the sequence.</li> <li>The function checks if any differences are positive, indicating an increase in value.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyClass:\n...     def __init__(self, values):\n...         self.info_values = values\n...     def get_min_info_value(self):\n...         is_monotonique = np.diff(self.info_values) &gt; 0\n...         if any(is_monotonique):\n...             return np.where(is_monotonique)[0][0] + 1\n...         return len(self.info_values)\n&gt;&gt;&gt; instance = MyClass([3, 2, 1, 4, 5])\n&gt;&gt;&gt; instance.get_min_info_value()\n3\n</code></pre> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def get_min_info_value(info_values):\n    \"\"\"Find the index of the first increasing value in an array.\n\n    Parameters\n    ----------\n    info_values : array-like\n        A sequence of numeric values to be analyzed.\n\n    Returns\n    -------\n    int\n        The index of the first element where the values start to increase\n        monotonically. If no such element exists, the length of\n        `info_values` is returned.\n\n    Notes\n    -----\n    - The function assumes that `info_values` is a 1-dimensional array-like\n    structure.\n    - The function uses `np.diff` to compute the difference between consecutive\n    elements in the sequence.\n    - The function checks if any differences are positive, indicating an increase\n    in value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; class MyClass:\n    ...     def __init__(self, values):\n    ...         self.info_values = values\n    ...     def get_min_info_value(self):\n    ...         is_monotonique = np.diff(self.info_values) &gt; 0\n    ...         if any(is_monotonique):\n    ...             return np.where(is_monotonique)[0][0] + 1\n    ...         return len(self.info_values)\n    &gt;&gt;&gt; instance = MyClass([3, 2, 1, 4, 5])\n    &gt;&gt;&gt; instance.get_min_info_value()\n    3\n    \"\"\"\n    is_monotonique = np.diff(info_values) &gt; 0\n    if any(is_monotonique):\n        return np.where(is_monotonique)[0][0] + 1\n    return len(info_values)\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.lilc","title":"<code>lilc(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Lilc information criteria value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>info_criteria_value</code> <code>float</code> <p>The computed value given the information criteria selected by the user.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def lilc(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Lilc information criteria value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = 2 * n_theta * np.log(np.log(n_samples))\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/","title":"Documentation for <code>Parameters Estimation</code>","text":"<p>Methods for parameter estimation.</p>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares","title":"<code>AffineLeastMeanSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Affine Least Mean Squares (ALMS) filter for parameter estimation.</p> <p>The ALMS filter is an adaptive filter used to estimate the parameters of a model. It incorporates an offset covariance factor to improve the stability and convergence of the parameter estimation process.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>offset_covariance</code> <code>float</code> <p>The offset covariance factor of the affine least mean squares filter.</p> <code>0.2</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>offset_covariance</code> <code>float</code> <p>The offset covariance factor of the affine least mean squares filter.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the ALMS filter.</p> References <ul> <li>Poularikas, A. D. (2017). Adaptive filtering: Fundamentals of least mean squares with MATLAB\u00ae. CRC Press.</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class AffineLeastMeanSquares(BaseEstimator):\n    \"\"\"Affine Least Mean Squares (ALMS) filter for parameter estimation.\n\n    The ALMS filter is an adaptive filter used to estimate the parameters of a model.\n    It incorporates an offset covariance factor to improve the stability and convergence\n    of the parameter estimation process.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    offset_covariance : float, default=0.2\n        The offset covariance factor of the affine least mean squares filter.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    offset_covariance : float\n        The offset covariance factor of the affine least mean squares filter.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the ALMS filter.\n\n    References\n    ----------\n    - Poularikas, A. D. (2017). Adaptive filtering: Fundamentals of least mean squares\n    with MATLAB\u00ae. CRC Press.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        offset_covariance: float = 0.2,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.offset_covariance = offset_covariance\n        self.uiter = uiter\n        self.unbiased = unbiased\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Affine Least Mean Squares.\n\n        The ALMS method updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi = y - \\psi \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\psi (\\psi^T \\psi + \\text{offset_covariance}\n           \\cdot I)^{-1} \\xi\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned references.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            self.xi = y - psi.dot(theta[:, i - 1].reshape(-1, 1))\n            aux = (\n                self.mu\n                * psi\n                @ np.linalg.pinv(psi.T @ psi + self.offset_covariance * np.eye(n_theta))\n            )\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + aux.T.dot(self.xi)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Affine Least Mean Squares.</p> <p>The ALMS method updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi = y - \\psi \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\psi (\\psi^T \\psi + \\text{offset_covariance}    \\cdot I)^{-1} \\xi    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Notes <p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Affine Least Mean Squares.\n\n    The ALMS method updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi = y - \\psi \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\psi (\\psi^T \\psi + \\text{offset_covariance}\n       \\cdot I)^{-1} \\xi\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned references.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        self.xi = y - psi.dot(theta[:, i - 1].reshape(-1, 1))\n        aux = (\n            self.mu\n            * psi\n            @ np.linalg.pinv(psi.T @ psi + self.offset_covariance * np.eye(n_theta))\n        )\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + aux.T.dot(self.xi)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares","title":"<code>BoundedVariableLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Solve a linear least-squares problem with bounds on the variables.</p> <p>This is a wrapper class for the <code>scipy.optimize.lsq_linear</code> method.</p> <p>Given a m-by-n design matrix A and a target vector b with m elements, <code>lsq_linear</code> solves the following optimization problem::</p> <pre><code>minimize 0.5 * ||A x - b||**2\nsubject to lb &lt;= x &lt;= ub\n</code></pre> <p>This optimization problem is convex, hence a found minimum (if iterations have converged) is guaranteed to be global.</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>30</code> <code>method</code> <code>trf or bvls</code> <p>Method to perform minimization.</p> <pre><code>* 'trf' : Trust Region Reflective algorithm adapted for a linear\n  least-squares problem. This is an interior-point-like method\n  and the required number of iterations is weakly correlated with\n  the number of variables.\n* 'bvls' : Bounded-variable least-squares algorithm. This is\n  an active set method, which requires the number of iterations\n  comparable to the number of variables. Can't be used when `A` is\n  sparse or LinearOperator.\n</code></pre> <p>Default is 'trf'.</p> <code>'trf'</code> <code>tol</code> <code>float</code> <p>Tolerance parameter. The algorithm terminates if a relative change of the cost function is less than <code>tol</code> on the last iteration. Additionally, the first-order optimality measure is considered:</p> <pre><code>* ``method='trf'`` terminates if the uniform norm of the gradient,\n  scaled to account for the presence of the bounds, is less than\n  `tol`.\n* ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n  are satisfied within `tol` tolerance.\n</code></pre> <code>1e-10</code> <code>lsq_solver</code> <code>(None, exact, lsmr)</code> <p>Method of solving unbounded least-squares problems throughout iterations:</p> <pre><code>* 'exact' : Use dense QR or SVD decomposition approach. Can't be\n  used when `A` is sparse or LinearOperator.\n* 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n  which requires only matrix-vector product evaluations. Can't\n  be used with ``method='bvls'``.\n</code></pre> <p>If None (default), the solver is chosen based on type of <code>A</code>.</p> <code>None</code> <code>lsmr_tol</code> <code>(None, float or auto)</code> <p>Tolerance parameters 'atol' and 'btol' for <code>scipy.sparse.linalg.lsmr</code> If None (default), it is set to <code>1e-2 * tol</code>. If 'auto', the tolerance will be adjusted based on the optimality of the current iterate, which can speed up the optimization process, but is not always reliable.</p> <code>None</code> <code>max_iter</code> <code>None or int</code> <p>Maximum number of iterations before termination. If None (default), it is set to 100 for <code>method='trf'</code> or to the number of variables for <code>method='bvls'</code> (not counting iterations for 'bvls' initialization).</p> <code>None</code> <code>verbose</code> <code>(0, 1, 2)</code> <p>Level of algorithm's verbosity:</p> <pre><code>* 0 : work silently (default).\n* 1 : display a termination report.\n* 2 : display progress during iterations.\n</code></pre> <code>0</code> <code>lsmr_maxiter</code> <code>None or int</code> <p>Maximum number of iterations for the lsmr least squares solver, if it is used (by setting <code>lsq_solver='lsmr'</code>). If None (default), it uses lsmr's default of <code>min(m, n)</code> where <code>m</code> and <code>n</code> are the number of rows and columns of <code>A</code>, respectively. Has no effect if <code>lsq_solver='exact'</code>.</p> <code>None</code> References <p>M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,     and Conjugate Gradient Method for Large-Scale Bound-Constrained     Minimization Problems,\" SIAM Journal on Scientific Computing,     Vol. 21, Number 1, pp 1-23, 1999. P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:     an Algorithm and Applications\", Computational Statistics, 10,     129-141, 1995.</p> Notes <p>This docstring is adapted from the <code>scipy.optimize.lsq_linear</code> method.</p> <p>Examples:</p> <p>In this example, a problem with a large sparse matrix and bounds on the variables is solved.</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.sparse import rand\n&gt;&gt;&gt; from sysidentpy.parameter_estimation import BoundedVariableLeastSquares\n&gt;&gt;&gt; rng = np.random.default_rng()\n...\n&gt;&gt;&gt; m = 20000\n&gt;&gt;&gt; n = 10000\n...\n&gt;&gt;&gt; A = rand(m, n, density=1e-4, random_state=rng)\n&gt;&gt;&gt; b = rng.standard_normal(m)\n...\n&gt;&gt;&gt; lb = rng.standard_normal(n)\n&gt;&gt;&gt; ub = lb + 1\n...\n&gt;&gt;&gt; res = BoundedVariableLeastSquares(A, b, bounds=(lb, ub), lsmr_tol='auto',\nverbose=1)\nThe relative change of the cost function is less than `tol`.\nNumber of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\nfirst-order optimality 4.66e-08.\n</code></pre> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class BoundedVariableLeastSquares(BaseEstimator):\n    \"\"\"Solve a linear least-squares problem with bounds on the variables.\n\n    This is a wrapper class for the `scipy.optimize.lsq_linear` method.\n\n    Given a m-by-n design matrix A and a target vector b with m elements,\n    `lsq_linear` solves the following optimization problem::\n\n        minimize 0.5 * ||A x - b||**2\n        subject to lb &lt;= x &lt;= ub\n\n    This optimization problem is convex, hence a found minimum (if iterations\n    have converged) is guaranteed to be global.\n\n    Parameters\n    ----------\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    method : 'trf' or 'bvls', optional\n        Method to perform minimization.\n\n            * 'trf' : Trust Region Reflective algorithm adapted for a linear\n              least-squares problem. This is an interior-point-like method\n              and the required number of iterations is weakly correlated with\n              the number of variables.\n            * 'bvls' : Bounded-variable least-squares algorithm. This is\n              an active set method, which requires the number of iterations\n              comparable to the number of variables. Can't be used when `A` is\n              sparse or LinearOperator.\n\n        Default is 'trf'.\n    tol : float, optional\n        Tolerance parameter. The algorithm terminates if a relative change\n        of the cost function is less than `tol` on the last iteration.\n        Additionally, the first-order optimality measure is considered:\n\n            * ``method='trf'`` terminates if the uniform norm of the gradient,\n              scaled to account for the presence of the bounds, is less than\n              `tol`.\n            * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n              are satisfied within `tol` tolerance.\n\n    lsq_solver : {None, 'exact', 'lsmr'}, optional\n        Method of solving unbounded least-squares problems throughout\n        iterations:\n\n            * 'exact' : Use dense QR or SVD decomposition approach. Can't be\n              used when `A` is sparse or LinearOperator.\n            * 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n              which requires only matrix-vector product evaluations. Can't\n              be used with ``method='bvls'``.\n\n        If None (default), the solver is chosen based on type of `A`.\n    lsmr_tol : None, float or 'auto', optional\n        Tolerance parameters 'atol' and 'btol' for `scipy.sparse.linalg.lsmr`\n        If None (default), it is set to ``1e-2 * tol``. If 'auto', the\n        tolerance will be adjusted based on the optimality of the current\n        iterate, which can speed up the optimization process, but is not always\n        reliable.\n    max_iter : None or int, optional\n        Maximum number of iterations before termination. If None (default), it\n        is set to 100 for ``method='trf'`` or to the number of variables for\n        ``method='bvls'`` (not counting iterations for 'bvls' initialization).\n    verbose : {0, 1, 2}, optional\n        Level of algorithm's verbosity:\n\n            * 0 : work silently (default).\n            * 1 : display a termination report.\n            * 2 : display progress during iterations.\n    lsmr_maxiter : None or int, optional\n        Maximum number of iterations for the lsmr least squares solver,\n        if it is used (by setting ``lsq_solver='lsmr'``). If None (default), it\n        uses lsmr's default of ``min(m, n)`` where ``m`` and ``n`` are the\n        number of rows and columns of `A`, respectively. Has no effect if\n        ``lsq_solver='exact'``.\n\n    References\n    ----------\n    M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n        and Conjugate Gradient Method for Large-Scale Bound-Constrained\n        Minimization Problems,\" SIAM Journal on Scientific Computing,\n        Vol. 21, Number 1, pp 1-23, 1999.\n    P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:\n        an Algorithm and Applications\", Computational Statistics, 10,\n        129-141, 1995.\n\n    Notes\n    -----\n    This docstring is adapted from the `scipy.optimize.lsq_linear` method.\n\n    Examples\n    --------\n    In this example, a problem with a large sparse matrix and bounds on the\n    variables is solved.\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from scipy.sparse import rand\n    &gt;&gt;&gt; from sysidentpy.parameter_estimation import BoundedVariableLeastSquares\n    &gt;&gt;&gt; rng = np.random.default_rng()\n    ...\n    &gt;&gt;&gt; m = 20000\n    &gt;&gt;&gt; n = 10000\n    ...\n    &gt;&gt;&gt; A = rand(m, n, density=1e-4, random_state=rng)\n    &gt;&gt;&gt; b = rng.standard_normal(m)\n    ...\n    &gt;&gt;&gt; lb = rng.standard_normal(n)\n    &gt;&gt;&gt; ub = lb + 1\n    ...\n    &gt;&gt;&gt; res = BoundedVariableLeastSquares(A, b, bounds=(lb, ub), lsmr_tol='auto',\n    verbose=1)\n    The relative change of the cost function is less than `tol`.\n    Number of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\n    first-order optimality 4.66e-08.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        unbiased: bool = False,\n        uiter: int = 30,\n        bounds=(-np.inf, np.inf),\n        method=\"trf\",\n        tol=1e-10,\n        lsq_solver=None,\n        lsmr_tol=None,\n        max_iter=None,\n        verbose=0,\n        lsmr_maxiter=None,\n    ):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self.max_iter = max_iter\n        self.bounds = bounds\n        self.method = method\n        self.tol = tol\n        self.lsq_solver = lsq_solver\n        self.lsmr_tol = lsmr_tol\n        self.verbose = verbose\n        self.lsmr_maxiter = lsmr_maxiter\n\n    def optimize(self, psi, y):\n        \"\"\"Parameter estimation using the BoundedVariableLeastSquares algorithm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        This is a wrapper class for the `scipy.optimize.lsq_linear` method.\n\n        References\n        ----------\n        .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html\n        \"\"\"\n        theta = lsq_linear(\n            psi,\n            y.ravel(),\n            bounds=self.bounds,\n            method=self.method,\n            tol=self.tol,\n            lsq_solver=self.lsq_solver,\n            lsmr_tol=self.lsmr_tol,\n            max_iter=self.max_iter,\n            verbose=self.verbose,\n            lsmr_maxiter=self.lsmr_maxiter,\n        )\n        return theta.x.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the BoundedVariableLeastSquares algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> Notes <p>This is a wrapper class for the <code>scipy.optimize.lsq_linear</code> method.</p> References <p>.. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi, y):\n    \"\"\"Parameter estimation using the BoundedVariableLeastSquares algorithm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    This is a wrapper class for the `scipy.optimize.lsq_linear` method.\n\n    References\n    ----------\n    .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html\n    \"\"\"\n    theta = lsq_linear(\n        psi,\n        y.ravel(),\n        bounds=self.bounds,\n        method=self.method,\n        tol=self.tol,\n        lsq_solver=self.lsq_solver,\n        lsmr_tol=self.lsmr_tol,\n        max_iter=self.max_iter,\n        verbose=self.verbose,\n        lsmr_maxiter=self.lsmr_maxiter,\n    )\n    return theta.x.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.EstimatorError","title":"<code>EstimatorError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Generic Python-exception-derived object raised by estimator functions.</p> <p>General purpose exception class, derived from Python's ValueError class, programmatically raised in estimators functions when a Estimator-related condition would prevent further correct execution of the function.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> required Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class EstimatorError(Exception):\n    \"\"\"Generic Python-exception-derived object raised by estimator functions.\n\n    General purpose exception class, derived from Python's ValueError\n    class, programmatically raised in estimators functions when a Estimator-related\n    condition would prevent further correct execution of the function.\n\n    Parameters\n    ----------\n    None\n\n    \"\"\"\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm","title":"<code>LeastMeanSquareMixedNorm</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Square Mixed Norm (LMS-MN) Adaptive Filter.</p> <p>This class implements the Mixed-norm Least Mean Square (LMS) adaptive filter algorithm, which incorporates an additional weight factor to control the proportions of the error norms, thus providing an extra degree of freedom in the adaptation process.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The adaptation step size. Default is 0.01.</p> <code>0.01</code> <code>weight</code> <code>float</code> <p>The weight factor for mixed-norm control. This factor controls the proportions of the error norms and offers an extra degree of freedom within the adaptation of the LMS mixed norm method.</p> <code>0.02</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The adaptation step size.</p> <code>weight</code> <code>float</code> <p>The weight factor for mixed-norm control.</p> <code>xi</code> <code>ndarray or None</code> <p>The error signal, initialized to None.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMSF filter.</p> References <ul> <li>Chambers, J. A., Tanrikulu, O., &amp; Constantinides, A. G. (1994).   Least mean mixed-norm adaptive filtering.   Electronics letters, 30(19), 1574-1575.   https://ieeexplore.ieee.org/document/326382</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,   an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo   vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares   https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquareMixedNorm(BaseEstimator):\n    \"\"\"Least Mean Square Mixed Norm (LMS-MN) Adaptive Filter.\n\n    This class implements the Mixed-norm Least Mean Square (LMS) adaptive filter\n    algorithm, which incorporates an additional weight factor to control the\n    proportions of the error norms, thus providing an extra degree of freedom\n    in the adaptation process.\n\n    Parameters\n    ----------\n    mu : float, optional\n        The adaptation step size. Default is 0.01.\n    weight : float, optional\n        The weight factor for mixed-norm control. This factor controls the\n        proportions of the error norms and offers an extra degree of freedom\n        within the adaptation of the LMS mixed norm method.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The adaptation step size.\n    weight : float\n        The weight factor for mixed-norm control.\n    xi : ndarray or None\n        The error signal, initialized to None.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMSF filter.\n\n    References\n    ----------\n    - Chambers, J. A., Tanrikulu, O., &amp; Constantinides, A. G. (1994).\n      Least mean mixed-norm adaptive filtering.\n      Electronics letters, 30(19), 1574-1575.\n      https://ieeexplore.ieee.org/document/326382\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n      an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n      vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n      https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        weight: float = 0.02,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.weight = weight\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n        The LMS-MN algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i (\\text{weight}\n           + (1 - \\text{weight}) \\xi_i^2)\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameter estimation\n        will be available soon. For now, please refer to the mentioned references.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[\n                i, 0\n            ] * (self.weight + (1 - self.weight) * self.xi[i, 0] ** 2)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Mixed-norm LMS filter.</p> <p>The LMS-MN algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i (\\text{weight}    + (1 - \\text{weight}) \\xi_i^2)    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Notes <p>A more in-depth documentation of all methods for parameter estimation will be available soon. For now, please refer to the mentioned references.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n    The LMS-MN algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i (\\text{weight}\n       + (1 - \\text{weight}) \\xi_i^2)\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameter estimation\n    will be available soon. For now, please refer to the mentioned references.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[\n            i, 0\n        ] * (self.weight + (1 - self.weight) * self.xi[i, 0] ** 2)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares","title":"<code>LeastMeanSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares (LMS) filter for parameter estimation in adaptive filtering.</p> <p>The LMS algorithm is an adaptive filter used to estimate the parameters of a model by minimizing the mean square error between the observed and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMS filter.</p> References <ul> <li>Haykin, S., &amp; Widrow, B. (Eds.). (2003). Least-mean-square adaptive filters (Vol. 31). John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquares(BaseEstimator):\n    \"\"\"Least Mean Squares (LMS) filter for parameter estimation in adaptive filtering.\n\n    The LMS algorithm is an adaptive filter used to estimate the parameters of a model\n    by minimizing the mean square error between the observed and predicted values.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n\n    References\n    ----------\n    - Haykin, S., &amp; Widrow, B. (Eds.). (2003). Least-mean-square adaptive filters\n    (Vol. 31). John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n    algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Least Mean Squares filter.\n\n        The LMS algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\psi_i\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * psi_tmp\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Least Mean Squares filter.</p> <p>The LMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\psi_i    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Least Mean Squares filter.\n\n    The LMS algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\psi_i\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * psi_tmp\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth","title":"<code>LeastMeanSquaresFourth</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares Fourth (LMSF) filter for parameter estimation.</p> <p>The LMSF algorithm is an adaptive filter used to estimate the parameters of a model by using the mean fourth error cost function to eliminate the noise effectively.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.5</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMSF filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Gui, G., Mehbodniya, A., &amp; Adachi, F. (2013). Least mean square/fourth algorithm   with application to sparse channel estimation. arXiv preprint arXiv:1304.3911.   https://arxiv.org/pdf/1304.3911.pdf</li> <li>Nascimento, V. H., &amp; Bermudez, J. C. M. (2005, March). When is the least-mean   fourth algorithm mean-square stable? In Proceedings.(ICASSP'05). IEEE   International Conference on Acoustics, Speech, and Signal Processing, 2005.   (Vol. 4, pp. iv-341). IEEE. http://www.lps.usp.br/vitor/artigos/icassp05.pdf</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresFourth(BaseEstimator):\n    \"\"\"Least Mean Squares Fourth (LMSF) filter for parameter estimation.\n\n    The LMSF algorithm is an adaptive filter used to estimate the parameters of a model\n    by using the mean fourth error cost function to eliminate the noise effectively.\n\n    Parameters\n    ----------\n    mu : float, default=0.5\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMSF filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Gui, G., Mehbodniya, A., &amp; Adachi, F. (2013). Least mean square/fourth algorithm\n      with application to sparse channel estimation. arXiv preprint arXiv:1304.3911.\n      https://arxiv.org/pdf/1304.3911.pdf\n    - Nascimento, V. H., &amp; Bermudez, J. C. M. (2005, March). When is the least-mean\n      fourth algorithm mean-square stable? In Proceedings.(ICASSP'05). IEEE\n      International Conference on Acoustics, Speech, and Signal Processing, 2005.\n      (Vol. 4, pp. iv-341). IEEE. http://www.lps.usp.br/vitor/artigos/icassp05.pdf\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.5, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the LMS Fourth filter.\n\n        The LMSF algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i^3\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : ndarray of floats of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[i, 0] ** 3\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the LMS Fourth filter.</p> <p>The LMSF algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i^3    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>ndarray of floats of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the LMS Fourth filter.\n\n    The LMSF algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i^3\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : ndarray of floats of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[i, 0] ** 3\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky","title":"<code>LeastMeanSquaresLeaky</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares Leaky (LMSL) filter for parameter estimation.</p> <p>The LMSL algorithm is an adaptive filter used to estimate the parameters of a model by minimizing the mean square error between the observed and predicted values. The leakage factor helps to prevent coefficient drift.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>gama</code> <code>float</code> <p>The leakage factor of the Leaky LMS method.</p> <code>0.2</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>gama</code> <code>float, default=0.2</code> <p>The leakage factor of the Leaky LMS method.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMSL filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresLeaky(BaseEstimator):\n    \"\"\"Least Mean Squares Leaky (LMSL) filter for parameter estimation.\n\n    The LMSL algorithm is an adaptive filter used to estimate the parameters of a model\n    by minimizing the mean square error between the observed and predicted values. The\n    leakage factor helps to prevent coefficient drift.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMSL filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        gama: float = 0.001,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.gama = gama\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Leaky LMS filter.\n\n        The LMSL algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\xi_i \\psi_i\n           $$\n\n        When the leakage factor, $\\gamma$, is set to 0, there is no leakage in the\n        estimation process.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n          John Wiley &amp; Sons.\n        - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n          de algoritmos LMS de passo vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1) * (1 - self.mu * self.gama)\n                + self.mu * self.xi[i, 0] * psi_tmp\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Leaky LMS filter.</p> <p>The LMSL algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\xi_i \\psi_i    $$</p> <p>When the leakage factor, \\(\\gamma\\), is set to 0, there is no leakage in the estimation process.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias   de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Leaky LMS filter.\n\n    The LMSL algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\xi_i \\psi_i\n       $$\n\n    When the leakage factor, $\\gamma$, is set to 0, there is no leakage in the\n    estimation process.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n      de algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1) * (1 - self.mu * self.gama)\n            + self.mu * self.xi[i, 0] * psi_tmp\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky","title":"<code>LeastMeanSquaresNormalizedLeaky</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares Leaky (NLMSL) filter for parameter estimation.</p> <p>The NLMSL algorithm is an adaptive filter used to estimate the parameters of a model by minimizing the mean square error between the observed and predicted values. The normalization is used to avoid numerical instability when updating the estimated parameters, and the leakage factor helps to prevent coefficient drift.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <code>gama</code> <code>float</code> <p>The leakage factor of the Leaky LMS method.</p> <code>0.2</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float, default=np.finfo(np.float64).eps</code> <p>Normalization factor of the normalized filters.</p> <code>gama</code> <code>float, default=0.2</code> <p>The leakage factor of the Leaky LMS method.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the NLMSL filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresNormalizedLeaky(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares Leaky (NLMSL) filter for parameter estimation.\n\n    The NLMSL algorithm is an adaptive filter used to estimate the parameters of a model\n    by minimizing the mean square error between the observed and predicted values. The\n    normalization is used to avoid numerical instability when updating the estimated\n    parameters, and the leakage factor helps to prevent coefficient drift.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the NLMSL filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        gama: float = 0.2,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.gama = gama\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Leaky LMS filter.\n\n        The NLMSL algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\frac{\\xi_i \\psi_i}{\\epsilon\n           + \\psi_i^T \\psi_i}\n           $$\n\n        When the leakage factor, $\\gamma$, is set to 0, there is no leakage in the\n        estimation process.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n          John Wiley &amp; Sons.\n        - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n          de algoritmos LMS de passo vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) * (\n                1 - self.mu * self.gama\n            ) + self.mu * self.xi[i, 0] * psi_tmp / (\n                self.eps + np.dot(psi_tmp.T, psi_tmp)\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Leaky LMS filter.</p> <p>The NLMSL algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\frac{\\xi_i \\psi_i}{\\epsilon    + \\psi_i^T \\psi_i}    $$</p> <p>When the leakage factor, \\(\\gamma\\), is set to 0, there is no leakage in the estimation process.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias   de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Leaky LMS filter.\n\n    The NLMSL algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\frac{\\xi_i \\psi_i}{\\epsilon\n       + \\psi_i^T \\psi_i}\n       $$\n\n    When the leakage factor, $\\gamma$, is set to 0, there is no leakage in the\n    estimation process.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n      de algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) * (\n            1 - self.mu * self.gama\n        ) + self.mu * self.xi[i, 0] * psi_tmp / (\n            self.eps + np.dot(psi_tmp.T, psi_tmp)\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor","title":"<code>LeastMeanSquaresNormalizedSignRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares SignRegressor filter for parameter estimation.</p> <p>The Normalized Sign-Regressor LMS algorithm updates the parameter estimates recursively by normalizing the input signal to avoid numerical instability and using the sign of the information matrix to adjust the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float, default=np.finfo(np.float64).eps</code> <p>Normalization factor of the normalized filters.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the Normalized Sign-Regressor LMS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares   https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresNormalizedSignRegressor(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares SignRegressor filter for parameter estimation.\n\n    The Normalized Sign-Regressor LMS algorithm updates the parameter estimates\n    recursively by normalizing the input signal to avoid numerical instability\n    and using the sign of the information matrix to adjust the filter coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the Normalized Sign-Regressor LMS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n      https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Sign-Regressor LMS filter.\n\n        The Normalized Sign-Regressor LMS algorithm updates the parameter estimates\n        recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot\n           \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}\n           $$\n\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters and the sign of the information matrix is\n        used to change the filter coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[i, 0] * (\n                np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Sign-Regressor LMS filter.</p> <p>The Normalized Sign-Regressor LMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot    \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}    $$</p> <p>The normalization is used to avoid numerical instability when updating the estimated parameters and the sign of the information matrix is used to change the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Sign-Regressor LMS filter.\n\n    The Normalized Sign-Regressor LMS algorithm updates the parameter estimates\n    recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot\n       \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}\n       $$\n\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters and the sign of the information matrix is\n    used to change the filter coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[i, 0] * (\n            np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign","title":"<code>LeastMeanSquaresNormalizedSignSign</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares SignSign (NLMSSS) filter for parameter estimation.</p> <p>The NLMSSS algorithm updates the parameter estimates recursively by normalizing the input signal to avoid numerical instability and using both the sign of the information matrix and the sign of the error vector to adjust the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the NLMSSS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresNormalizedSignSign(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares SignSign (NLMSSS) filter for parameter estimation.\n\n    The NLMSSS algorithm updates the parameter estimates recursively by normalizing\n    the input signal to avoid numerical instability and using both the sign of the\n    information matrix and the sign of the error vector to adjust the filter\n    coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the NLMSSS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Sign-Sign LMS filter.\n\n        The NLMSSS algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot\n           \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}\n           $$\n\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters and both the sign of the information matrix\n        and the sign of the error vector are used to change the filter\n        coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n          John Wiley &amp; Sons.\n        - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n        de algoritmos LMS de passo vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n                self.xi[i, 0]\n            ) * (np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Sign-Sign LMS filter.</p> <p>The NLMSSS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot    \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}    $$</p> <p>The normalization is used to avoid numerical instability when updating the estimated parameters and both the sign of the information matrix and the sign of the error vector are used to change the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Sign-Sign LMS filter.\n\n    The NLMSSS algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot\n       \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}\n       $$\n\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters and both the sign of the information matrix\n    and the sign of the error vector are used to change the filter\n    coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n    de algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n            self.xi[i, 0]\n        ) * (np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError","title":"<code>LeastMeanSquaresSignError</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares (LMS) filter for parameter estimation using sign-error.</p> <p>The sign-error LMS algorithm uses the sign of the error vector to update the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling. John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresSignError(BaseEstimator):\n    \"\"\"Least Mean Squares (LMS) filter for parameter estimation using sign-error.\n\n    The sign-error LMS algorithm uses the sign of the error vector to update the filter\n    coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n    John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n    algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.uiter = uiter\n        self.unbiased = unbiased\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Sign-Error Least Mean Squares filter.\n\n        The sign-error LMS algorithm updates the parameter estimates recursively as\n        follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\cdot \\text{sign}(\\xi_i) \\cdot \\psi_i\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1)\n                + self.mu * np.sign(self.xi[i, 0]) * psi_tmp\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Sign-Error Least Mean Squares filter.</p> <p>The sign-error LMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\cdot \\text{sign}(\\xi_i) \\cdot \\psi_i    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Sign-Error Least Mean Squares filter.\n\n    The sign-error LMS algorithm updates the parameter estimates recursively as\n    follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\cdot \\text{sign}(\\xi_i) \\cdot \\psi_i\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1)\n            + self.mu * np.sign(self.xi[i, 0]) * psi_tmp\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor","title":"<code>LeastMeanSquaresSignRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares (LMSSR) filter for parameter estimation.</p> <p>The sign-regressor LMS algorithm uses the sign of the matrix information to change the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresSignRegressor(BaseEstimator):\n    \"\"\"Least Mean Squares (LMSSR) filter for parameter estimation.\n\n    The sign-regressor LMS algorithm uses the sign of the matrix\n    information to change the filter coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Sign-Regressor LMS filter.\n\n        The sign-regressor LMS algorithm updates the parameter estimates recursively\n        as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot \\text{sign}(\\psi_i)\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[\n                i, 0\n            ] * np.sign(psi_tmp)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Sign-Regressor LMS filter.</p> <p>The sign-regressor LMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot \\text{sign}(\\psi_i)    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Sign-Regressor LMS filter.\n\n    The sign-regressor LMS algorithm updates the parameter estimates recursively\n    as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot \\text{sign}(\\psi_i)\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[\n            i, 0\n        ] * np.sign(psi_tmp)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign","title":"<code>LeastMeanSquaresSignSign</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares Sign-Sign (LMSSS) filter for parameter estimation.</p> <p>The LMSSS algorithm uses both the sign of the matrix information and the sign of the error vector to update the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMSSS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling. John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresSignSign(BaseEstimator):\n    \"\"\"Least Mean Squares Sign-Sign (LMSSS) filter for parameter estimation.\n\n    The LMSSS algorithm uses both the sign of the matrix information and the sign of\n    the error vector to update the filter coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMSSS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n    John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n    algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Sign-Sign LMS filter.\n\n        The LMSSS algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2* \\mu \\cdot \\text{sign}(\\xi_i)\n           \\cdot \\text{sign}(\\psi_i)\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n                self.xi[i, 0]\n            ) * np.sign(psi_tmp)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Sign-Sign LMS filter.</p> <p>The LMSSS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2* \\mu \\cdot \\text{sign}(\\xi_i)    \\cdot \\text{sign}(\\psi_i)    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Sign-Sign LMS filter.\n\n    The LMSSS algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2* \\mu \\cdot \\text{sign}(\\xi_i)\n       \\cdot \\text{sign}(\\psi_i)\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n            self.xi[i, 0]\n        ) * np.sign(psi_tmp)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquares","title":"<code>LeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Ordinary Least Squares for linear parameter estimation.</p> <p>The Least Squares method minimizes the sum of the squared differences between the observed and predicted values. It is used to estimate the parameters of a linear model.</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 20.</p> <code>20</code> References <ul> <li>Sorenson, H. W. (1970). Least-squares estimation: from Gauss to Kalman.   IEEE spectrum, 7(7), 63-68.   http://pzs.dstu.dp.ua/DataMining/mls/bibl/Gauss2Kalman.pdf</li> <li>Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o de sistemas: t\u00e9cnicas   lineares e n\u00e3o-lineares aplicadas a sistemas reais. Editora da UFMG. 3a edi\u00e7\u00e3o.</li> <li>Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least-squares methods.   Signal processing, 87(10), 2283-2302.   https://eprints.soton.ac.uk/263855/1/tls_overview.pdf</li> <li>Wikipedia entry on Least Squares   https://en.wikipedia.org/wiki/Least_squares</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastSquares(BaseEstimator):\n    \"\"\"Ordinary Least Squares for linear parameter estimation.\n\n    The Least Squares method minimizes the sum of the squared differences\n    between the observed and predicted values. It is used to estimate the\n    parameters of a linear model.\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 20.\n\n    References\n    ----------\n    - Sorenson, H. W. (1970). Least-squares estimation: from Gauss to Kalman.\n      IEEE spectrum, 7(7), 63-68.\n      http://pzs.dstu.dp.ua/DataMining/mls/bibl/Gauss2Kalman.pdf\n    - Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o de sistemas: t\u00e9cnicas\n      lineares e n\u00e3o-lineares aplicadas a sistemas reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n    - Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least-squares methods.\n      Signal processing, 87(10), 2283-2302.\n      https://eprints.soton.ac.uk/263855/1/tls_overview.pdf\n    - Wikipedia entry on Least Squares\n      https://en.wikipedia.org/wiki/Least_squares\n    \"\"\"\n\n    def __init__(self, *, unbiased: bool = False, uiter: int = 20):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Least Squares method.\n\n        The Least Squares method solves the following optimization problem:\n\n        $$\n        \\min_{\\theta} \\| \\psi \\theta - y \\|_2^2\n        $$\n\n        where $\\psi$ is the information matrix, $y$ is the observed data,\n        and $\\theta$ are the model parameters to be estimated.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        check_linear_dependence_rows(psi)\n        theta = np.linalg.lstsq(psi, y, rcond=None)[0]\n        return theta\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Least Squares method.</p> <p>The Least Squares method solves the following optimization problem:</p> \\[ \\min_{\\theta} \\| \\psi \\theta - y \\|_2^2 \\] <p>where \\(\\psi\\) is the information matrix, \\(y\\) is the observed data, and \\(\\theta\\) are the model parameters to be estimated.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Least Squares method.\n\n    The Least Squares method solves the following optimization problem:\n\n    $$\n    \\min_{\\theta} \\| \\psi \\theta - y \\|_2^2\n    $$\n\n    where $\\psi$ is the information matrix, $y$ is the observed data,\n    and $\\theta$ are the model parameters to be estimated.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    check_linear_dependence_rows(psi)\n    theta = np.linalg.lstsq(psi, y, rcond=None)[0]\n    return theta\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual","title":"<code>LeastSquaresMinimalResidual</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Iterative solver for least-squares minimal residual problems.</p> <p>This is a wrapper class for the <code>scipy.sparse.linalg.lsmr</code> method.</p> <p>lsmr solves the system of linear equations <code>Ax = b</code>. If the system is inconsistent, it solves the least-squares problem <code>min ||b - Ax||_2</code>. <code>A</code> is a rectangular matrix of dimension m-by-n, where all cases are allowed: m = n, m &gt; n, or m &lt; n. <code>b</code> is a vector of length m. The matrix A may be dense or sparse (usually sparse).</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>damp</code> <code>float</code> <p>Damping factor for regularized least-squares. <code>lsmr</code> solves the regularized least-squares problem::</p> <p>min ||(b) - (  A   )x||      ||(0)   (damp*I) ||_2</p> <p>where damp is a scalar.  If damp is None or 0, the system is solved without regularization. Default is 0.</p> <code>atol, btol</code> <code>(float, optional)</code> <p>Stopping tolerances. <code>lsmr</code> continues iterations until a certain backward error estimate is smaller than some quantity depending on atol and btol.  Let <code>r = b - Ax</code> be the residual vector for the current approximate solution <code>x</code>. If <code>Ax = b</code> seems to be consistent, <code>lsmr</code> terminates when <code>norm(r) &lt;= atol * norm(A) * norm(x) + btol * norm(b)</code>. Otherwise, <code>lsmr</code> terminates when <code>norm(A^H r) &lt;= atol * norm(A) * norm(r)</code>.  If both tolerances are 1.0e-6 (default), the final <code>norm(r)</code> should be accurate to about 6 digits. (The final <code>x</code> will usually have fewer correct digits, depending on <code>cond(A)</code> and the size of LAMBDA.)  If <code>atol</code> or <code>btol</code> is None, a default value of 1.0e-6 will be used. Ideally, they should be estimates of the relative error in the entries of <code>A</code> and <code>b</code> respectively.  For example, if the entries of <code>A</code> have 7 correct digits, set <code>atol = 1e-7</code>. This prevents the algorithm from doing unnecessary work beyond the uncertainty of the input data.</p> <code>conlim</code> <code>(float, optional)</code> <p><code>lsmr</code> terminates if an estimate of <code>cond(A)</code> exceeds <code>conlim</code>.  For compatible systems <code>Ax = b</code>, conlim could be as large as 1.0e+12 (say).  For least-squares problems, <code>conlim</code> should be less than 1.0e+8. If <code>conlim</code> is None, the default value is 1e+8.  Maximum precision can be obtained by setting <code>atol = btol = conlim = 0</code>, but the number of iterations may then be excessive. Default is 1e8.</p> <code>maxiter</code> <code>(int, optional)</code> <p><code>lsmr</code> terminates if the number of iterations reaches <code>maxiter</code>.  The default is <code>maxiter = min(m, n)</code>.  For ill-conditioned systems, a larger value of <code>maxiter</code> may be needed. Default is False.</p> <code>show</code> <code>(bool, optional)</code> <p>Print iterations logs if <code>show=True</code>. Default is False.</p> <code>x0</code> <code>(array_like, shape(n), optional)</code> <p>Initial guess of <code>x</code>, if None zeros are used. Default is None.</p> References <p>.. [1] D. C.-L. Fong and M. A. Saunders,        \"LSMR: An iterative algorithm for sparse least-squares problems\",        SIAM J. Sci. Comput., vol. 33, pp. 2950-2971, 2011.        :arxiv:<code>1006.0758</code> .. [2] LSMR Software, https://web.stanford.edu/group/SOL/software/lsmr/</p> Notes <p>This docstring is adapted from the <code>scipy.sparse.linalg.lsmr</code> method.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastSquaresMinimalResidual(BaseEstimator):\n    \"\"\"Iterative solver for least-squares minimal residual problems.\n\n    This is a wrapper class for the `scipy.sparse.linalg.lsmr` method.\n\n    lsmr solves the system of linear equations ``Ax = b``. If the system\n    is inconsistent, it solves the least-squares problem ``min ||b - Ax||_2``.\n    ``A`` is a rectangular matrix of dimension m-by-n, where all cases are\n    allowed: m = n, m &gt; n, or m &lt; n. ``b`` is a vector of length m.\n    The matrix A may be dense or sparse (usually sparse).\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    damp : float\n        Damping factor for regularized least-squares. `lsmr` solves\n        the regularized least-squares problem::\n\n         min ||(b) - (  A   )x||\n             ||(0)   (damp*I) ||_2\n\n        where damp is a scalar.  If damp is None or 0, the system\n        is solved without regularization. Default is 0.\n    atol, btol : float, optional\n        Stopping tolerances. `lsmr` continues iterations until a\n        certain backward error estimate is smaller than some quantity\n        depending on atol and btol.  Let ``r = b - Ax`` be the\n        residual vector for the current approximate solution ``x``.\n        If ``Ax = b`` seems to be consistent, `lsmr` terminates\n        when ``norm(r) &lt;= atol * norm(A) * norm(x) + btol * norm(b)``.\n        Otherwise, `lsmr` terminates when ``norm(A^H r) &lt;=\n        atol * norm(A) * norm(r)``.  If both tolerances are 1.0e-6 (default),\n        the final ``norm(r)`` should be accurate to about 6\n        digits. (The final ``x`` will usually have fewer correct digits,\n        depending on ``cond(A)`` and the size of LAMBDA.)  If `atol`\n        or `btol` is None, a default value of 1.0e-6 will be used.\n        Ideally, they should be estimates of the relative error in the\n        entries of ``A`` and ``b`` respectively.  For example, if the entries\n        of ``A`` have 7 correct digits, set ``atol = 1e-7``. This prevents\n        the algorithm from doing unnecessary work beyond the\n        uncertainty of the input data.\n    conlim : float, optional\n        `lsmr` terminates if an estimate of ``cond(A)`` exceeds\n        `conlim`.  For compatible systems ``Ax = b``, conlim could be\n        as large as 1.0e+12 (say).  For least-squares problems,\n        `conlim` should be less than 1.0e+8. If `conlim` is None, the\n        default value is 1e+8.  Maximum precision can be obtained by\n        setting ``atol = btol = conlim = 0``, but the number of\n        iterations may then be excessive. Default is 1e8.\n    maxiter : int, optional\n        `lsmr` terminates if the number of iterations reaches\n        `maxiter`.  The default is ``maxiter = min(m, n)``.  For\n        ill-conditioned systems, a larger value of `maxiter` may be\n        needed. Default is False.\n    show : bool, optional\n        Print iterations logs if ``show=True``. Default is False.\n    x0 : array_like, shape (n,), optional\n        Initial guess of ``x``, if None zeros are used. Default is None.\n\n    References\n    ----------\n    .. [1] D. C.-L. Fong and M. A. Saunders,\n           \"LSMR: An iterative algorithm for sparse least-squares problems\",\n           SIAM J. Sci. Comput., vol. 33, pp. 2950-2971, 2011.\n           :arxiv:`1006.0758`\n    .. [2] LSMR Software, https://web.stanford.edu/group/SOL/software/lsmr/\n\n    Notes\n    -----\n    This docstring is adapted from the `scipy.sparse.linalg.lsmr` method.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        unbiased: bool = False,\n        uiter: int = 30,\n        damp=0.0,\n        atol=1e-6,\n        btol=1e-6,\n        conlim=1e8,\n        maxiter=None,\n        show=False,\n        x0=None,\n    ):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self.damp = damp\n        self.atol = atol\n        self.btol = btol\n        self.conlim = conlim\n        self.maxiter = maxiter\n        self.show = show\n        self.x0 = x0\n\n    def optimize(self, psi, y):\n        \"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        This is a wrapper class for the `scipy.sparse.linalg.lsmr` method.\n\n        References\n        ----------\n        .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html\n        \"\"\"\n        theta = lsmr(\n            psi,\n            y.ravel(),\n            damp=self.damp,\n            atol=self.atol,\n            btol=self.btol,\n            conlim=self.conlim,\n            maxiter=self.maxiter,\n            show=self.show,\n            x0=self.x0,\n        )[0]\n        return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Mixed-norm LMS filter.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> Notes <p>This is a wrapper class for the <code>scipy.sparse.linalg.lsmr</code> method.</p> References <p>.. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi, y):\n    \"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    This is a wrapper class for the `scipy.sparse.linalg.lsmr` method.\n\n    References\n    ----------\n    .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html\n    \"\"\"\n    theta = lsmr(\n        psi,\n        y.ravel(),\n        damp=self.damp,\n        atol=self.atol,\n        btol=self.btol,\n        conlim=self.conlim,\n        maxiter=self.maxiter,\n        show=self.show,\n        x0=self.x0,\n    )[0]\n    return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares","title":"<code>NonNegativeLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Solve <code>argmin_x || Ax - b ||_2</code> for <code>x &gt;= 0</code>.</p> <p>This is a wrapper class for the <code>scipy.optimize.nnls</code> method.</p> <p>This problem, often called NonNegative Least Squares (NNLS), is a convex optimization problem with convex constraints. It typically arises when the <code>x</code> models quantities for which only nonnegative values are attainable; such as weights of ingredients, component costs, and so on.</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <code>maxiter</code> <code>int</code> <p>Maximum number of iterations. Default value is <code>3 * n</code> where <code>n</code> is the number of features.</p> <code>None</code> <code>atol</code> <code>float</code> <p>Tolerance value used in the algorithm to assess closeness to zero in the projected residual <code>(A.T @ (A x - b))</code> entries. Increasing this value relaxes the solution constraints. A typical relaxation value can be selected as <code>max(m, n) * np.linalg.norm(A, 1) * np.spacing(1.)</code>. Default is None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>maxiter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>atol</code> <code>float</code> <p>Tolerance value for the algorithm.</p> References <p>Lawson C., Hanson R.J., \"Solving Least Squares Problems\", SIAM,    1995, :doi:<code>10.1137/1.9781611971217</code> Bro, Rasmus and de Jong, Sijmen, \"A Fast Non-Negativity-Constrained Least     Squares Algorithm\", Journal Of Chemometrics, 1997,     :doi:<code>10.1002/(SICI)1099-128X(199709/10)11:5&lt;393::AID-CEM483&gt;3.0.CO;2-L</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sysidentpy.parameter_estimation import NonNegativeLeastSquares\n...\n&gt;&gt;&gt; A = np.array([[1, 0], [1, 0], [0, 1]])\n&gt;&gt;&gt; b = np.array([2, 1, 1])\n&gt;&gt;&gt; nnls_solver = NonNegativeLeastSquares()\n&gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n&gt;&gt;&gt; print(x)\n[[1.5]\n [1. ]]\n</code></pre> <pre><code>&gt;&gt;&gt; b = np.array([-1, -1, -1])\n&gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n&gt;&gt;&gt; print(x)\n[[0.]\n [0.]]\n</code></pre> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class NonNegativeLeastSquares(BaseEstimator):\n    \"\"\"Solve ``argmin_x || Ax - b ||_2`` for ``x &gt;= 0``.\n\n    This is a wrapper class for the `scipy.optimize.nnls` method.\n\n    This problem, often called NonNegative Least Squares (NNLS), is a convex\n    optimization problem with convex constraints. It typically arises when\n    the ``x`` models quantities for which only nonnegative values are\n    attainable; such as weights of ingredients, component costs, and so on.\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n    maxiter : int, optional\n        Maximum number of iterations. Default value is ``3 * n`` where ``n``\n        is the number of features.\n    atol : float, optional\n        Tolerance value used in the algorithm to assess closeness to zero in\n        the projected residual ``(A.T @ (A x - b))`` entries. Increasing this\n        value relaxes the solution constraints. A typical relaxation value can\n        be selected as ``max(m, n) * np.linalg.norm(A, 1) * np.spacing(1.)``.\n        Default is None.\n\n    Attributes\n    ----------\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    maxiter : int\n        Maximum number of iterations.\n    atol : float\n        Tolerance value for the algorithm.\n\n    References\n    ----------\n    Lawson C., Hanson R.J., \"Solving Least Squares Problems\", SIAM,\n       1995, :doi:`10.1137/1.9781611971217`\n    Bro, Rasmus and de Jong, Sijmen, \"A Fast Non-Negativity-Constrained Least\n        Squares Algorithm\", Journal Of Chemometrics, 1997,\n        :doi:`10.1002/(SICI)1099-128X(199709/10)11:5&lt;393::AID-CEM483&gt;3.0.CO;2-L`\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sysidentpy.parameter_estimation import NonNegativeLeastSquares\n    ...\n    &gt;&gt;&gt; A = np.array([[1, 0], [1, 0], [0, 1]])\n    &gt;&gt;&gt; b = np.array([2, 1, 1])\n    &gt;&gt;&gt; nnls_solver = NonNegativeLeastSquares()\n    &gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n    &gt;&gt;&gt; print(x)\n    [[1.5]\n     [1. ]]\n\n    &gt;&gt;&gt; b = np.array([-1, -1, -1])\n    &gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n    &gt;&gt;&gt; print(x)\n    [[0.]\n     [0.]]\n    \"\"\"\n\n    def __init__(\n        self, unbiased: bool = False, uiter: int = 30, maxiter=None, atol=None\n    ):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self.maxiter = maxiter\n        self.atol = atol\n\n    def optimize(self, psi, y):\n        \"\"\"Parameter estimation using the NonNegativeLeastSquares algorithm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        This is a wrapper class for the `scipy.optimize.nnls` method.\n\n        References\n        ----------\n        .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html\n        \"\"\"\n        theta, _ = nnls(psi, y.ravel(), maxiter=self.maxiter, atol=self.atol)\n        return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the NonNegativeLeastSquares algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> Notes <p>This is a wrapper class for the <code>scipy.optimize.nnls</code> method.</p> References <p>.. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi, y):\n    \"\"\"Parameter estimation using the NonNegativeLeastSquares algorithm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    This is a wrapper class for the `scipy.optimize.nnls` method.\n\n    References\n    ----------\n    .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html\n    \"\"\"\n    theta, _ = nnls(psi, y.ravel(), maxiter=self.maxiter, atol=self.atol)\n    return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares","title":"<code>NormalizedLeastMeanSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares (NLMS) filter for parameter estimation.</p> <p>The NLMS algorithm is an adaptive filter used to estimate the parameters of a model by minimizing the mean square error between the observed and predicted values. The normalization is used to avoid numerical instability when updating the estimated parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the NLMS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class NormalizedLeastMeanSquares(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares (NLMS) filter for parameter estimation.\n\n    The NLMS algorithm is an adaptive filter used to estimate the parameters of a model\n    by minimizing the mean square error between the observed and predicted values. The\n    normalization is used to avoid numerical instability when updating the estimated\n    parameters.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the NLMS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Least Mean Squares filter.\n\n        The NLMS algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\frac{\\psi_i}{\\epsilon +\n           \\psi_i^T \\psi_i}\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * (\n                psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Least Mean Squares filter.</p> <p>The NLMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\frac{\\psi_i}{\\epsilon +    \\psi_i^T \\psi_i}    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Least Mean Squares filter.\n\n    The NLMS algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\frac{\\psi_i}{\\epsilon +\n       \\psi_i^T \\psi_i}\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * (\n            psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError","title":"<code>NormalizedLeastMeanSquaresSignError</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares SignError (NLMSSE) filter for parameter estimation.</p> <p>The NLMSSE algorithm updates the parameter estimates recursively by normalizing the input signal to avoid numerical instability and using the sign of the error vector to adjust the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the NLMSSE filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class NormalizedLeastMeanSquaresSignError(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares SignError (NLMSSE) filter for parameter estimation.\n\n    The NLMSSE algorithm updates the parameter estimates recursively by normalizing\n    the input signal to avoid numerical instability and using the sign of the error\n    vector to adjust the filter coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the NLMSSE filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Sign-Error LMS filter.\n\n        The NLMSSE algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot\n           \\frac{\\psi_i}{\\epsilon + \\psi_i^T \\psi_i}\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters and the sign of the error vector is used to\n        change the filter coefficients.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n                self.xi[i, 0]\n            ) * (psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Sign-Error LMS filter.</p> <p>The NLMSSE algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot    \\frac{\\psi_i}{\\epsilon + \\psi_i^T \\psi_i}    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Notes <p>The normalization is used to avoid numerical instability when updating the estimated parameters and the sign of the error vector is used to change the filter coefficients.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Sign-Error LMS filter.\n\n    The NLMSSE algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot\n       \\frac{\\psi_i}{\\epsilon + \\psi_i^T \\psi_i}\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters and the sign of the error vector is used to\n    change the filter coefficients.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n            self.xi[i, 0]\n        ) * (psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares","title":"<code>RecursiveLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Recursive Least Squares (RLS) filter for parameter estimation.</p> <p>The Recursive Least Squares method is used to estimate the parameters of a model by minimizing the sum of the squares of the differences between the observed and predicted values. This method incorporates a forgetting factor to give more weight to recent observations.</p> <p>Parameters:</p> Name Type Description Default <code>lam</code> <code>float</code> <p>Forgetting factor of the Recursive Least Squares method.</p> <code>0.98</code> <code>delta</code> <code>float</code> <p>Normalization factor of the P matrix.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>lam</code> <code>float</code> <p>Forgetting factor of the Recursive Least Squares method.</p> <code>delta</code> <code>float</code> <p>Normalization factor of the P matrix.</p> <code>xi</code> <code>ndarray</code> <p>The estimation error at each iteration.</p> <code>theta_evolution</code> <code>ndarray</code> <p>Evolution of the estimated parameters over iterations.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the Recursive Least Squares method.</p> References <ul> <li>Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o    de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas    reais. Editora da UFMG. 3a edi\u00e7\u00e3o.</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class RecursiveLeastSquares(BaseEstimator):\n    \"\"\"Recursive Least Squares (RLS) filter for parameter estimation.\n\n    The Recursive Least Squares method is used to estimate the parameters of a model\n    by minimizing the sum of the squares of the differences between the observed and\n    predicted values. This method incorporates a forgetting factor to give more weight\n    to recent observations.\n\n    Parameters\n    ----------\n    lam : float, default=0.98\n        Forgetting factor of the Recursive Least Squares method.\n    delta : float, default=0.01\n        Normalization factor of the P matrix.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    lam : float\n        Forgetting factor of the Recursive Least Squares method.\n    delta : float\n        Normalization factor of the P matrix.\n    xi : np.ndarray\n        The estimation error at each iteration.\n    theta_evolution : np.ndarray\n        Evolution of the estimated parameters over iterations.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the Recursive Least Squares method.\n\n    References\n    ----------\n    - Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o\n       de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas\n       reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        delta: float = 0.01,\n        lam: float = 0.98,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.delta = delta\n        self.lam = lam\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n        self.theta_evolution: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Recursive Least Squares method.\n\n        The implementation considers the forgetting factor.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        The RLS algorithm updates the parameter estimates recursively as follows:\n\n        1. Initialize the parameter vector `theta` and the covariance matrix `P`:\n\n           $$\n           \\\\theta_0 = \\\\mathbf{0}, \\\\quad P_0 = \\\\frac{1}{\\\\delta} I\n           $$\n\n        2. For each new observation `(psi_i, y_i)`, update the estimates:\n\n           $$\n           k_i = \\\\frac{\\\\lambda^{-1} P_{i-1} \\\\psi_i}{1 +\n           \\\\lambda^{-1} \\\\psi_i^T P_{i-1} \\\\psi_i}\n           $$\n\n           $$\n           \\\\theta_i = \\\\theta_{i-1} + k_i (y_i - \\\\psi_i^T \\\\theta_{i-1})\n           $$\n\n           $$\n           P_i = \\\\lambda^{-1} (P_{i-1} - k_i \\\\psi_i^T P_{i-1})\n           $$\n\n        References\n        ----------\n        - Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o\n           de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas\n           reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n        p = np.eye(n_theta) / self.delta\n\n        for i in range(2, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            k_numerator = self.lam ** (-1) * p.dot(psi_tmp)\n            k_denominator = 1 + self.lam ** (-1) * psi_tmp.T.dot(p).dot(psi_tmp)\n            k = np.divide(k_numerator, k_denominator)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + k.dot(self.xi[i, 0])\n            theta[:, i] = tmp_list.flatten()\n\n            p1 = p.dot(psi[i, :].reshape(-1, 1)).dot(psi[i, :].reshape(-1, 1).T).dot(p)\n            p2 = (\n                psi[i, :].reshape(-1, 1).T.dot(p).dot(psi[i, :].reshape(-1, 1))\n                + self.lam\n            )\n\n            p_numerator = p - np.divide(p1, p2)\n            p = np.divide(p_numerator, self.lam)\n\n        self.theta_evolution = theta.copy()\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Recursive Least Squares method.</p> <p>The implementation considers the forgetting factor.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape = y_training</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> Notes <p>The RLS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Initialize the parameter vector <code>theta</code> and the covariance matrix <code>P</code>:</li> </ol> <p>$$    \\theta_0 = \\mathbf{0}, \\quad P_0 = \\frac{1}{\\delta} I    $$</p> <ol> <li>For each new observation <code>(psi_i, y_i)</code>, update the estimates:</li> </ol> <p>$$    k_i = \\frac{\\lambda^{-1} P_{i-1} \\psi_i}{1 +    \\lambda^{-1} \\psi_i^T P_{i-1} \\psi_i}    $$</p> <p>$$    \\theta_i = \\theta_{i-1} + k_i (y_i - \\psi_i^T \\theta_{i-1})    $$</p> <p>$$    P_i = \\lambda^{-1} (P_{i-1} - k_i \\psi_i^T P_{i-1})    $$</p> References <ul> <li>Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o    de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas    reais. Editora da UFMG. 3a edi\u00e7\u00e3o.</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Recursive Least Squares method.\n\n    The implementation considers the forgetting factor.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    The RLS algorithm updates the parameter estimates recursively as follows:\n\n    1. Initialize the parameter vector `theta` and the covariance matrix `P`:\n\n       $$\n       \\\\theta_0 = \\\\mathbf{0}, \\\\quad P_0 = \\\\frac{1}{\\\\delta} I\n       $$\n\n    2. For each new observation `(psi_i, y_i)`, update the estimates:\n\n       $$\n       k_i = \\\\frac{\\\\lambda^{-1} P_{i-1} \\\\psi_i}{1 +\n       \\\\lambda^{-1} \\\\psi_i^T P_{i-1} \\\\psi_i}\n       $$\n\n       $$\n       \\\\theta_i = \\\\theta_{i-1} + k_i (y_i - \\\\psi_i^T \\\\theta_{i-1})\n       $$\n\n       $$\n       P_i = \\\\lambda^{-1} (P_{i-1} - k_i \\\\psi_i^T P_{i-1})\n       $$\n\n    References\n    ----------\n    - Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o\n       de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas\n       reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n    p = np.eye(n_theta) / self.delta\n\n    for i in range(2, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        k_numerator = self.lam ** (-1) * p.dot(psi_tmp)\n        k_denominator = 1 + self.lam ** (-1) * psi_tmp.T.dot(p).dot(psi_tmp)\n        k = np.divide(k_numerator, k_denominator)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + k.dot(self.xi[i, 0])\n        theta[:, i] = tmp_list.flatten()\n\n        p1 = p.dot(psi[i, :].reshape(-1, 1)).dot(psi[i, :].reshape(-1, 1).T).dot(p)\n        p2 = (\n            psi[i, :].reshape(-1, 1).T.dot(p).dot(psi[i, :].reshape(-1, 1))\n            + self.lam\n        )\n\n        p_numerator = p - np.divide(p1, p2)\n        p = np.divide(p_numerator, self.lam)\n\n    self.theta_evolution = theta.copy()\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression","title":"<code>RidgeRegression</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Ridge Regression estimator using classic and SVD methods.</p> <p>This class implements Ridge Regression, a type of linear regression that includes an L2 penalty to prevent overfitting. The implementation offers two methods for parameter estimation: a classic approach and an approach based on Singular Value Decomposition (SVD).</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>(float64, optional(default=eps))</code> <p>Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. If the input is a noisy signal, the ridge parameter is likely to be set close to the noise level, at least as a starting point. Entered through the self data structure.</p> <code>eps</code> <code>solver</code> <code>(str, optional(default=svd))</code> <p>Solver to use in the parameter estimation procedure.</p> <code>'svd'</code> <p>Methods:</p> Name Description <code>ridge_regression_classic</code> <p>Estimate the model parameters using the classic ridge regression method.</p> <code>ridge_regression</code> <p>Estimate the model parameters using the SVD-based ridge regression method.</p> <code>optimize</code> <p>Optimize the model parameters using the chosen method (SVD or classic).</p> References <ul> <li>Wikipedia entry on ridge regression   https://en.wikipedia.org/wiki/Ridge_regression</li> <li>D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation   reservoir computing,' Nat. Commun. 12, 5564 (2021).   https://www.nature.com/articles/s41467-021-25801-2</li> <li>Hoerl, A. E.; Kennard, R. W. Ridge regression: applications to nonorthogonal   problems. Technometrics, Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.</li> <li>StackExchange: whuber. The proof of shrinking coefficients using ridge regression   through \"spectral decomposition\".   Cross Validated, accessed 21 September 2023,   https://stats.stackexchange.com/q/220324</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>@deprecated(\n    version=\"v0.6.0\",\n    future_version=\"v1.0.0\",\n    message=(\n        \" `solver` is deprecated in v0.5.4 and will be removed in v1.0.0.\"\n        \" A single solver option will be retained moving forward.\"\n    ),\n)\nclass RidgeRegression(BaseEstimator):\n    \"\"\"Ridge Regression estimator using classic and SVD methods.\n\n    This class implements Ridge Regression, a type of linear regression that includes\n    an L2 penalty to prevent overfitting. The implementation offers two methods for\n    parameter estimation: a classic approach and an approach based on Singular Value\n    Decomposition (SVD).\n\n    Parameters\n    ----------\n    alpha : np.float64, optional (default=np.finfo(np.float64).eps)\n        Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger\n        values specify stronger regularization. If the input is a noisy signal,\n        the ridge parameter is likely to be set close to the noise level, at least as\n        a starting point. Entered through the self data structure.\n    solver : str, optional (default=\"svd\")\n        Solver to use in the parameter estimation procedure.\n\n    Methods\n    -------\n    ridge_regression_classic(psi, y)\n        Estimate the model parameters using the classic ridge regression method.\n    ridge_regression(psi, y)\n        Estimate the model parameters using the SVD-based ridge regression method.\n    optimize(psi, y)\n        Optimize the model parameters using the chosen method (SVD or classic).\n\n    References\n    ----------\n    - Wikipedia entry on ridge regression\n      https://en.wikipedia.org/wiki/Ridge_regression\n    - D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation\n      reservoir computing,' Nat. Commun. 12, 5564 (2021).\n      https://www.nature.com/articles/s41467-021-25801-2\n    - Hoerl, A. E.; Kennard, R. W. Ridge regression: applications to nonorthogonal\n      problems. Technometrics, Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.\n    - StackExchange: whuber. The proof of shrinking coefficients using ridge regression\n      through \"spectral decomposition\".\n      Cross Validated, accessed 21 September 2023,\n      https://stats.stackexchange.com/q/220324\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        alpha: np.float64 = np.finfo(np.float64).eps,\n        solver: str = \"svd\",\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.alpha = alpha\n        self.solver = solver\n        self.uiter = uiter\n        self.unbiased = unbiased\n        _validate_params(vars(self))\n\n    def ridge_regression_classic(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using ridge regression.\n\n           Based on the least_squares module and uses the same data format but you need\n           to pass alpha in the call to FROLS.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Wikipedia entry on ridge regression\n          https://en.wikipedia.org/wiki/Ridge_regression\n\n        alpha multiplied by the identity matrix (np.eye) favors models (theta) that\n        have small size using an L2 norm.  This prevents over fitting of the model.\n        For applications where preventing overfitting is important, see, for example,\n        D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation\n        reservoir computing,' Nat. Commun. 12, 5564 (2021).\n        https://www.nature.com/articles/s41467-021-25801-2\n\n        \"\"\"\n        check_linear_dependence_rows(psi)\n\n        theta = (\n            np.linalg.pinv(psi.T @ psi + self.alpha * np.eye(psi.shape[1])) @ psi.T @ y\n        )\n        return theta\n\n    def ridge_regression(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using SVD and Ridge Regression method.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Manuscript: Hoerl, A. E.; Kennard, R. W. Ridge regression:\n                      applications to nonorthogonal problems. Technometrics,\n                      Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.\n\n        - StackExchange: whuber. The proof of shrinking coefficients using ridge\n                         regression through \"spectral decomposition\".\n                         Cross Validated, accessed 21 September 2023,\n                         https://stats.stackexchange.com/q/220324\n        \"\"\"\n        check_linear_dependence_rows(psi)\n        try:\n            U, S, Vh = np.linalg.svd(psi, full_matrices=False)\n            S = np.diag(S)\n            i = np.identity(len(S))\n            theta = Vh.T @ np.linalg.inv(S**2 + self.alpha * i) @ S @ U.T @ y\n        except EstimatorError:\n            warnings.warn(\n                \"The SVD computation did not converge.\"\n                \"Theta values will be calculated with the classic algorithm.\",\n                stacklevel=2,\n            )\n\n            theta = self.ridge_regression_classic(psi, y)\n\n        return theta\n\n    def optimize(self, psi: np.ndarray, y):\n        if self.solver == \"svd\":\n            return self.ridge_regression(psi, y)\n\n        return self.ridge_regression_classic(psi, y)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression","title":"<code>ridge_regression(psi, y)</code>","text":"<p>Estimate the model parameters using SVD and Ridge Regression method.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape = y_training</code> <p>The data used to training the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> References <ul> <li> <p>Manuscript: Hoerl, A. E.; Kennard, R. W. Ridge regression:               applications to nonorthogonal problems. Technometrics,               Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.</p> </li> <li> <p>StackExchange: whuber. The proof of shrinking coefficients using ridge                  regression through \"spectral decomposition\".                  Cross Validated, accessed 21 September 2023,                  https://stats.stackexchange.com/q/220324</p> </li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def ridge_regression(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using SVD and Ridge Regression method.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Manuscript: Hoerl, A. E.; Kennard, R. W. Ridge regression:\n                  applications to nonorthogonal problems. Technometrics,\n                  Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.\n\n    - StackExchange: whuber. The proof of shrinking coefficients using ridge\n                     regression through \"spectral decomposition\".\n                     Cross Validated, accessed 21 September 2023,\n                     https://stats.stackexchange.com/q/220324\n    \"\"\"\n    check_linear_dependence_rows(psi)\n    try:\n        U, S, Vh = np.linalg.svd(psi, full_matrices=False)\n        S = np.diag(S)\n        i = np.identity(len(S))\n        theta = Vh.T @ np.linalg.inv(S**2 + self.alpha * i) @ S @ U.T @ y\n    except EstimatorError:\n        warnings.warn(\n            \"The SVD computation did not converge.\"\n            \"Theta values will be calculated with the classic algorithm.\",\n            stacklevel=2,\n        )\n\n        theta = self.ridge_regression_classic(psi, y)\n\n    return theta\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression_classic","title":"<code>ridge_regression_classic(psi, y)</code>","text":"<p>Estimate the model parameters using ridge regression.</p> <p>Based on the least_squares module and uses the same data format but you need    to pass alpha in the call to FROLS.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape = y_training</code> <p>The data used to training the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> References <ul> <li>Wikipedia entry on ridge regression   https://en.wikipedia.org/wiki/Ridge_regression</li> </ul> <p>alpha multiplied by the identity matrix (np.eye) favors models (theta) that have small size using an L2 norm.  This prevents over fitting of the model. For applications where preventing overfitting is important, see, for example, D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation reservoir computing,' Nat. Commun. 12, 5564 (2021). https://www.nature.com/articles/s41467-021-25801-2</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def ridge_regression_classic(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using ridge regression.\n\n       Based on the least_squares module and uses the same data format but you need\n       to pass alpha in the call to FROLS.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Wikipedia entry on ridge regression\n      https://en.wikipedia.org/wiki/Ridge_regression\n\n    alpha multiplied by the identity matrix (np.eye) favors models (theta) that\n    have small size using an L2 norm.  This prevents over fitting of the model.\n    For applications where preventing overfitting is important, see, for example,\n    D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation\n    reservoir computing,' Nat. Commun. 12, 5564 (2021).\n    https://www.nature.com/articles/s41467-021-25801-2\n\n    \"\"\"\n    check_linear_dependence_rows(psi)\n\n    theta = (\n        np.linalg.pinv(psi.T @ psi + self.alpha * np.eye(psi.shape[1])) @ psi.T @ y\n    )\n    return theta\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares","title":"<code>TotalLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Estimate the model parameters using the Total Least Squares (TLS) method.</p> <p>The Total Least Squares method is used to solve the problem of fitting a model to data when both the independent variables (psi) and the dependent variable (y) are subject to errors. This method minimizes the orthogonal distances from the data points to the fitted model, which is more appropriate when errors are present in all variables.</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> References <ul> <li>Golub, G. H., &amp; Van Loan, C. F. (1980). An analysis of the total least squares problem.   SIAM journal on numerical analysis, 17(6), 883-893.</li> <li>Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least-squares methods.   Signal processing, 87(10), 2283-2302. https://eprints.soton.ac.uk/263855/1/tls_overview.pdf</li> <li>Wikipedia entry on Total Least Squares: https://en.wikipedia.org/wiki/Total_least_squares</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class TotalLeastSquares(BaseEstimator):\n    \"\"\"Estimate the model parameters using the Total Least Squares (TLS) method.\n\n    The Total Least Squares method is used to solve the problem of fitting a model\n    to data when both the independent variables (psi) and the dependent variable (y)\n    are subject to errors. This method minimizes the orthogonal distances from the\n    data points to the fitted model, which is more appropriate when errors are present\n    in all variables.\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    References\n    ----------\n    - Golub, G. H., &amp; Van Loan, C. F. (1980). An analysis of the total least squares\n    problem.\n      SIAM journal on numerical analysis, 17(6), 883-893.\n    - Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least-squares methods.\n      Signal processing, 87(10), 2283-2302. https://eprints.soton.ac.uk/263855/1/tls_overview.pdf\n    - Wikipedia entry on Total Least Squares: https://en.wikipedia.org/wiki/Total_least_squares\n    \"\"\"\n\n    def __init__(self, *, unbiased: bool = False, uiter: int = 30):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Total Least Squares method.\n\n        The TLS method solves the following problem:\n\n        $$\n            \\min_{E, f} \\| [E, f] \\|_F \\quad \\text{subject to}\n            \\quad (psi + E) \\theta = y + f\n        $$\n\n        where $E$ and $f$ are the error matrices for $psi$ and $y$ respectively,\n        and $\\| \\cdot \\|_F$ denotes the Frobenius norm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        check_linear_dependence_rows(psi)\n        full = np.hstack((psi, y))\n        n = psi.shape[1]\n        _, _, v = np.linalg.svd(full, full_matrices=True)\n        theta = -v.T[:n, n:] / v.T[n:, n:]\n        return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Total Least Squares method.</p> <p>The TLS method solves the following problem:</p> \\[     \\min_{E, f} \\| [E, f] \\|_F \\quad \\text{subject to}     \\quad (psi + E) \\theta = y + f \\] <p>where \\(E\\) and \\(f\\) are the error matrices for \\(psi\\) and \\(y\\) respectively, and \\(\\| \\cdot \\|_F\\) denotes the Frobenius norm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Total Least Squares method.\n\n    The TLS method solves the following problem:\n\n    $$\n        \\min_{E, f} \\| [E, f] \\|_F \\quad \\text{subject to}\n        \\quad (psi + E) \\theta = y + f\n    $$\n\n    where $E$ and $f$ are the error matrices for $psi$ and $y$ respectively,\n    and $\\| \\cdot \\|_F$ denotes the Frobenius norm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    check_linear_dependence_rows(psi)\n    full = np.hstack((psi, y))\n    n = psi.shape[1]\n    _, _, v = np.linalg.svd(full, full_matrices=True)\n    theta = -v.T[:n, n:] / v.T[n:, n:]\n    return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/residues/","title":"Documentation for <code>Residual Analysis</code>","text":""},{"location":"user-guide/API/residues/#sysidentpy.residues.residues_correlation.calculate_residues","title":"<code>calculate_residues(y, yhat)</code>","text":"<p>Calculate the residues (errors) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape (n_samples,)</code> <p>True values.</p> required <code>yhat</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>residues</code> <code>ndarray of shape (n_samples,)</code> <p>Residues (errors) between true and predicted values.</p> Source code in <code>sysidentpy/residues/residues_correlation.py</code> <pre><code>def calculate_residues(y, yhat):\n    \"\"\"Calculate the residues (errors) between true and predicted values.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        True values.\n    yhat : array-like of shape (n_samples,)\n        Predicted values.\n\n    Returns\n    -------\n    residues : ndarray of shape (n_samples,)\n        Residues (errors) between true and predicted values.\n    \"\"\"\n    return (y - yhat).flatten()\n</code></pre>"},{"location":"user-guide/API/residues/#sysidentpy.residues.residues_correlation.compute_cross_correlation","title":"<code>compute_cross_correlation(y, yhat, arr)</code>","text":"<p>Compute the cross-correlation between the residues and another array.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape (n_samples,)</code> <p>True values.</p> required <code>yhat</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <code>arr</code> <code>array-like of shape (n_samples,)</code> <p>Another array to compute the cross-correlation with.</p> required <p>Returns:</p> Name Type Description <code>ccf</code> <code>ndarray of shape (n_samples,)</code> <p>Cross-correlation function.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound for the confidence interval.</p> <code>lower_bound</code> <code>float</code> <p>Lower bound for the confidence interval.</p> Source code in <code>sysidentpy/residues/residues_correlation.py</code> <pre><code>def compute_cross_correlation(y, yhat, arr):\n    \"\"\"Compute the cross-correlation between the residues and another array.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        True values.\n    yhat : array-like of shape (n_samples,)\n        Predicted values.\n    arr : array-like of shape (n_samples,)\n        Another array to compute the cross-correlation with.\n\n    Returns\n    -------\n    ccf : ndarray of shape (n_samples,)\n        Cross-correlation function.\n    upper_bound : float\n        Upper bound for the confidence interval.\n    lower_bound : float\n        Lower bound for the confidence interval.\n    \"\"\"\n    e = calculate_residues(y, yhat)\n    n = len(e) * 2 - 1\n    ccf, upper_bound, lower_bound = _input_ccf(e, arr, n)\n    return ccf, upper_bound, lower_bound\n</code></pre>"},{"location":"user-guide/API/residues/#sysidentpy.residues.residues_correlation.compute_residues_autocorrelation","title":"<code>compute_residues_autocorrelation(y, yhat)</code>","text":"<p>Compute the autocorrelation of the residues.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape (n_samples,)</code> <p>True values.</p> required <code>yhat</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>e_acf</code> <code>ndarray of shape (n_samples,)</code> <p>Autocorrelation of the residues.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound for the confidence interval.</p> <code>lower_bound</code> <code>float</code> <p>Lower bound for the confidence interval.</p> Source code in <code>sysidentpy/residues/residues_correlation.py</code> <pre><code>def compute_residues_autocorrelation(y, yhat):\n    \"\"\"Compute the autocorrelation of the residues.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        True values.\n    yhat : array-like of shape (n_samples,)\n        Predicted values.\n\n    Returns\n    -------\n    e_acf : ndarray of shape (n_samples,)\n        Autocorrelation of the residues.\n    upper_bound : float\n        Upper bound for the confidence interval.\n    lower_bound : float\n        Lower bound for the confidence interval.\n    \"\"\"\n    e = calculate_residues(y, yhat)\n    unnormalized_e_acf = get_unnormalized_e_acf(e)\n    half_of_symmetry_autocorr = int(np.floor(unnormalized_e_acf.size / 2))\n\n    e_acf = (\n        unnormalized_e_acf[half_of_symmetry_autocorr:]\n        / unnormalized_e_acf[half_of_symmetry_autocorr]\n    )\n\n    upper_bound = 1.96 / np.sqrt(len(unnormalized_e_acf))\n    lower_bound = upper_bound * (-1)\n    return e_acf, upper_bound, lower_bound\n</code></pre>"},{"location":"user-guide/API/residues/#sysidentpy.residues.residues_correlation.get_unnormalized_e_acf","title":"<code>get_unnormalized_e_acf(e)</code>","text":"<p>Compute the unnormalized autocorrelation function of the residues.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>array-like of shape (n_samples,)</code> <p>Residues (errors).</p> required <p>Returns:</p> Name Type Description <code>unnormalized_e_acf</code> <code>ndarray of shape (2*n_samples-1,)</code> <p>Unnormalized autocorrelation function of the residues.</p> Source code in <code>sysidentpy/residues/residues_correlation.py</code> <pre><code>def get_unnormalized_e_acf(e):\n    \"\"\"Compute the unnormalized autocorrelation function of the residues.\n\n    Parameters\n    ----------\n    e : array-like of shape (n_samples,)\n        Residues (errors).\n\n    Returns\n    -------\n    unnormalized_e_acf : ndarray of shape (2*n_samples-1,)\n        Unnormalized autocorrelation function of the residues.\n    \"\"\"\n    return np.correlate(e, e, mode=\"full\")\n</code></pre>"},{"location":"user-guide/API/simulation/","title":"Documentation for <code>Simulation</code>","text":"<p>Simulation methods for NARMAX models.</p>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX","title":"<code>SimulateNARMAX</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Simulates a Polynomial NARMAX model.</p> <p>The NARMAX (Nonlinear AutoRegressive Moving Average with eXogenous inputs) model is described as:</p> \\[ y_k = \\mathcal{F}^\\ell \\Big[y_{k-1}, \\dotsc, y_{k-n_y}, x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e} \\Big] + e_k \\] <p>where:</p> <ul> <li>$ n_y \\in \\mathbb{N}^* $, $ n_x \\in \\mathbb{N} $, and $ n_e \\in \\mathbb{N} $ are     the maximum lags for the system output, input, and noise, respectively.</li> <li>$ x_k \\in \\mathbb{R}^{n_x} $ is the system input, and $ y_k \\in \\mathbb{R}^{n_y} $     is the system output at discrete time $ k \\in \\mathbb{N} $.</li> <li>$ e_k \\in \\mathbb{R}^{n_e} $ represents uncertainties and possible noise at     discrete time $ k $.</li> <li>$ \\mathcal{F}^\\ell $ is a nonlinear function of the input and output regressors     with nonlinearity degree $ \\ell \\in \\mathbb{N} $.</li> <li>$ d $ is a time delay, typically set to $ d=1 $.</li> </ul> <p>This class provides tools for simulating NARMAX models using a chosen basis function and estimation method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Estimators</code> <p>The parameter estimation method used for model identification.</p> <code>RecursiveLeastSquares()</code> <code>elag</code> <code>int or list</code> <p>Specifies the maximum lags for the error variables. If an integer, it applies to both input and output lags. If a list, it should contain specific lag values for different variables.</p> <code>2</code> <code>estimate_parameter</code> <code>bool</code> <p>Whether to estimate model parameters. Set to <code>True</code> unless pre-estimated parameters are provided.</p> <code>True</code> <code>calculate_err</code> <code>bool</code> <p>If <code>True</code>, uses the Error Reduction Ratio (ERR) algorithm to select regressors.</p> <code>False</code> <code>model_type</code> <code>str</code> <p>Defines the model type. Supported values: <code>\"NARMAX\"</code>, <code>\"ARX\"</code>, <code>\"OE\"</code>, etc.</p> <code>\"NARMAX\"</code> <code>basis_function</code> <code>Polynomial or Fourier</code> <p>The basis function used to define the model's nonlinear terms.</p> <code>Polynomial()</code> <code>eps</code> <code>float</code> <p>A small numerical constant used for normalization.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>xlag</code> <code>int or list</code> <p>Lags for the input variables.</p> <code>ylag</code> <code>int or list</code> <p>Lags for the output variables.</p> <code>n_terms</code> <code>int</code> <p>Number of terms in the final model.</p> <code>err</code> <code>array - like</code> <p>Error Reduction Ratio (ERR) values for the selected regressors.</p> <code>final_model</code> <code>array - like</code> <p>The structure of the identified model.</p> <code>theta</code> <code>array - like</code> <p>Estimated parameters of the model.</p> <code>pivv</code> <code>array - like</code> <p>Pivot vector for variable selection.</p> <code>non_degree</code> <code>int</code> <p>Degree of nonlinearity used in the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; x_train = np.random.rand(1000, 1)\n&gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n&gt;&gt;&gt; model = np.array([\n...     [1001, 0],       # y(k-1)\n...     [2001, 1001],    # x1(k-1)y(k-1)\n...     [2002, 0]        # x1(k-2)\n... ])\n&gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Model parameters\n&gt;&gt;&gt; y_pred = simulator.simulate(\n...     X_test=x_train, y_test=y_train,\n...     model_code=model, theta=theta\n... )\n</code></pre> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>class SimulateNARMAX(BaseMSS):\n    r\"\"\"Simulates a Polynomial NARMAX model.\n\n    The NARMAX (Nonlinear AutoRegressive Moving Average with eXogenous inputs) model\n    is described as:\n\n    $$\n    y_k = \\mathcal{F}^\\ell \\Big[y_{k-1}, \\dotsc, y_{k-n_y}, x_{k-d}, x_{k-d-1},\n    \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e} \\Big] + e_k\n    $$\n\n    where:\n\n    - $ n_y \\in \\mathbb{N}^* $, $ n_x \\in \\mathbb{N} $, and $ n_e \\in \\mathbb{N} $ are\n        the maximum lags for the system output, input, and noise, respectively.\n    - $ x_k \\in \\mathbb{R}^{n_x} $ is the system input, and $ y_k \\in \\mathbb{R}^{n_y} $\n        is the system output at discrete time $ k \\in \\mathbb{N} $.\n    - $ e_k \\in \\mathbb{R}^{n_e} $ represents uncertainties and possible noise at\n        discrete time $ k $.\n    - $ \\mathcal{F}^\\ell $ is a nonlinear function of the input and output regressors\n        with nonlinearity degree $ \\ell \\in \\mathbb{N} $.\n    - $ d $ is a time delay, typically set to $ d=1 $.\n\n    This class provides tools for simulating NARMAX models using a chosen basis function\n    and estimation method.\n\n    Parameters\n    ----------\n    estimator : Estimators, default=RecursiveLeastSquares()\n        The parameter estimation method used for model identification.\n    elag : int or list, default=2\n        Specifies the maximum lags for the error variables.\n        If an integer, it applies to both input and output lags.\n        If a list, it should contain specific lag values for different variables.\n    estimate_parameter : bool, default=True\n        Whether to estimate model parameters. Set to `True` unless pre-estimated\n        parameters are provided.\n    calculate_err : bool, default=False\n        If `True`, uses the Error Reduction Ratio (ERR) algorithm to select regressors.\n    model_type : str, default=\"NARMAX\"\n        Defines the model type. Supported values: `\"NARMAX\"`, `\"ARX\"`, `\"OE\"`, etc.\n    basis_function : Polynomial or Fourier, default=Polynomial()\n        The basis function used to define the model's nonlinear terms.\n    eps : float, default=np.finfo(np.float64).eps\n        A small numerical constant used for normalization.\n\n    Attributes\n    ----------\n    n_inputs : int\n        Number of input variables.\n    xlag : int or list\n        Lags for the input variables.\n    ylag : int or list\n        Lags for the output variables.\n    n_terms : int\n        Number of terms in the final model.\n    err : array-like\n        Error Reduction Ratio (ERR) values for the selected regressors.\n    final_model : array-like\n        The structure of the identified model.\n    theta : array-like\n        Estimated parameters of the model.\n    pivv : array-like\n        Pivot vector for variable selection.\n    non_degree : int\n        Degree of nonlinearity used in the model.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; x_train = np.random.rand(1000, 1)\n    &gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n    &gt;&gt;&gt; model = np.array([\n    ...     [1001, 0],       # y(k-1)\n    ...     [2001, 1001],    # x1(k-1)y(k-1)\n    ...     [2002, 0]        # x1(k-2)\n    ... ])\n    &gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Model parameters\n    &gt;&gt;&gt; y_pred = simulator.simulate(\n    ...     X_test=x_train, y_test=y_train,\n    ...     model_code=model, theta=theta\n    ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        estimator: Estimators = RecursiveLeastSquares(),\n        elag: Union[int, list] = 2,\n        estimate_parameter: bool = True,\n        calculate_err: bool = False,\n        model_type: str = \"NARMAX\",\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        eps: np.float64 = np.finfo(np.float64).eps,\n    ):\n        self.elag = elag\n        self.model_type = model_type\n        self.basis_function = basis_function\n        self.estimator = estimator\n        self.estimate_parameter = estimate_parameter\n        self.calculate_err = calculate_err\n        self.eps = eps\n        self.n_inputs = None\n        self.xlag = None\n        self.ylag = None\n        self.n_terms = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n        self.non_degree = None\n        self._validate_simulate_params()\n\n    def _validate_simulate_params(self):\n        if not isinstance(self.estimate_parameter, bool):\n            raise TypeError(\n                \"estimate_parameter must be False or True. Got\"\n                f\" {self.estimate_parameter}\"\n            )\n\n        if not isinstance(self.calculate_err, bool):\n            raise TypeError(\n                f\"calculate_err must be False or True. Got {self.calculate_err}\"\n            )\n\n        if self.basis_function is None:\n            raise TypeError(f\"basis_function can't be. Got {self.basis_function}\")\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR, or NFIR. Got {self.model_type}\"\n            )\n\n    def _check_simulate_params(self, y_train, y_test, model_code, steps_ahead, theta):\n        if not isinstance(self.basis_function, Polynomial):\n            raise NotImplementedError(\n                \"Currently, SimulateNARMAX only works for polynomial models.\"\n            )\n\n        if y_test is None:\n            raise ValueError(\"y_test cannot be None\")\n\n        if not isinstance(model_code, np.ndarray):\n            raise TypeError(f\"model_code must be an np.np.ndarray. Got {model_code}\")\n\n        if not isinstance(steps_ahead, (int, type(None))):\n            raise ValueError(\n                f\"steps_ahead must be None or integer &gt; zero. Got {steps_ahead}\"\n            )\n\n        if not isinstance(theta, np.ndarray) and not self.estimate_parameter:\n            raise TypeError(\n                \"If estimate_parameter is False, theta must be an np.ndarray. Got\"\n                f\" {theta}\"\n            )\n\n        if self.estimate_parameter:\n            if not all(isinstance(i, np.ndarray) for i in [y_train]):\n                raise TypeError(\n                    \"If estimate_parameter is True, X_train and y_train must be an\"\n                    f\" np.ndarray. Got {type(y_train)}\"\n                )\n\n    def simulate(\n        self,\n        *,\n        X_train=None,\n        y_train=None,\n        X_test=None,\n        y_test=None,\n        model_code=None,\n        steps_ahead=None,\n        theta=None,\n        forecast_horizon=None,\n    ):\n        \"\"\"Simulate the response of a NARMAX model based on user-defined parameters.\n\n        This method simulates the system's response using a predefined model structure\n        (`model_code`) and estimated parameters (`theta`). It allows for both\n        training-based parameter estimation and direct simulation using precomputed\n        parameters.\n\n        Parameters\n        ----------\n        X_train : array-like, shape (n_samples, n_features), optional\n            Input data used for parameter estimation during training.\n            Required if `estimate_parameter=True`.\n        y_train : array-like, shape (n_samples, 1), optional\n            Output (target) data used for parameter estimation during training.\n            Required if `estimate_parameter=True`.\n        X_test : array-like, shape (n_samples, n_features), optional\n            Input data used for simulation (prediction).\n        y_test : array-like, shape (n_samples, 1), optional\n            Output data used as initial conditions for simulation.\n        model_code : array-like, shape (n_terms, n_columns)\n            Encoded representation of the model's regressors, defining\n            the input-output relationships in the system.\n        steps_ahead : int, optional\n            Number of steps ahead for multi-step prediction. If `None`, defaults to\n            one-step-ahead prediction.\n        theta : array-like, shape (n_terms, 1), optional\n            Precomputed model parameters. Required if `estimate_parameter=False`.\n        forecast_horizon : int, optional\n            Number of time steps to predict in open-loop forecasting.\n            Used mainly for NAR and NARMA-type models.\n\n        Returns\n        -------\n        yhat : array-like, shape (n_samples, 1)\n            Predicted output values of the system based on the given inputs.\n\n        Raises\n        ------\n        ValueError\n            If necessary parameters are missing, such as `y_train` when\n            `estimate_parameter=True` or `theta` when `estimate_parameter=False`.\n\n        Notes\n        -----\n        - If `estimate_parameter=True`, the method first estimates the parameters using\n        the provided training data (`X_train`, `y_train`) and the chosen basis function.\n        - If `estimate_parameter=False`, the method assumes `theta` contains the model\n            parameters.\n        - The forecast horizon is automatically adjusted for NAR models if not provided.\n        - The method internally computes the lag structure based on `model_code` to\n            define regressors.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n        &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n        &gt;&gt;&gt; X_train = np.random.rand(1000, 1)\n        &gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n        &gt;&gt;&gt; X_test = np.random.rand(200, 1)\n        &gt;&gt;&gt; y_test = np.random.rand(200, 1)\n        &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n        &gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n        &gt;&gt;&gt; model = np.array([\n        ...     [1001, 0],       # y(k-1)\n        ...     [2001, 1001],    # x1(k-1)y(k-1)\n        ...     [2002, 0]        # x1(k-2)\n        ... ])\n        &gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Precomputed model parameters\n        &gt;&gt;&gt; y_pred = simulator.simulate(\n        ...     X_train=X_train, y_train=y_train,\n        ...     X_test=X_test, y_test=y_test,\n        ...     model_code=model, theta=theta\n        ... )\n\n        \"\"\"\n        self._check_simulate_params(y_train, y_test, model_code, steps_ahead, theta)\n\n        if X_test is not None:\n            self.n_inputs = num_features(X_test)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        xlag_code = list_input_regressor_code(model_code)\n        ylag_code = list_output_regressor_code(model_code)\n        self.xlag = get_lag_from_regressor_code(xlag_code)\n        self.ylag = get_lag_from_regressor_code(ylag_code)\n        self.max_lag = max(self.xlag, self.ylag)\n        if self.n_inputs != 1:\n            self.xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n        # for MetaMSS NAR modelling\n        if self.model_type == \"NAR\" and forecast_horizon is None:\n            forecast_horizon = y_test.shape[0] - self.max_lag\n\n        self.non_degree = model_code.shape[1]\n        regressor_code = self.regressor_space(self.n_inputs)\n\n        self.pivv = get_index_from_regressor_code(regressor_code, model_code)\n        self.final_model = regressor_code[self.pivv]\n        # to use in the predict function\n        self.n_terms = self.final_model.shape[0]\n        if self.estimate_parameter and not self.calculate_err:\n            self.max_lag = self._get_max_lag()\n            lagged_data = build_lagged_matrix(\n                X_train, y_train, self.xlag, self.ylag, self.model_type\n            )\n            psi = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=self.pivv,\n            )\n\n            self.theta = self.estimator.optimize(\n                psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n            )\n            if self.estimator.unbiased is True:\n                self.theta = self.estimator.unbiased_estimator(\n                    psi,\n                    y_train[self.max_lag :, 0].reshape(-1, 1),\n                    self.theta,\n                    self.elag,\n                    self.max_lag,\n                    self.estimator,\n                    self.basis_function,\n                    self.estimator.uiter,\n                )\n\n            self.err = self.n_terms * [0]\n        elif not self.estimate_parameter:\n            self.theta = theta\n            self.err = self.n_terms * [0]\n        else:\n            self.max_lag = self._get_max_lag()\n            lagged_data = build_lagged_matrix(\n                X_train, y_train, self.xlag, self.ylag, self.model_type\n            )\n            psi = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=self.pivv,\n            )\n\n            _, self.err, _, _ = self.error_reduction_ratio(\n                psi, y_train, self.n_terms, self.final_model\n            )\n            self.theta = self.estimator.optimize(\n                psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n            )\n            if self.estimator.unbiased is True:\n                self.theta = self.estimator.unbiased_estimator(\n                    psi,\n                    y_train[self.max_lag :, 0].reshape(-1, 1),\n                    self.theta,\n                    self.elag,\n                    self.max_lag,\n                    self.estimator,\n                    self.basis_function,\n                    self.estimator.uiter,\n                )\n\n        return self.predict(\n            X=X_test,\n            y=y_test,\n            steps_ahead=steps_ahead,\n            forecast_horizon=forecast_horizon,\n        )\n\n    def error_reduction_ratio(self, psi, y, process_term_number, regressor_code):\n        \"\"\"Perform the Error Reduction Ration algorithm.\n\n        Parameters\n        ----------\n        psi : array_like\n            The information matrix of the model.\n        y : array-like\n            The target data used in the identification process.\n        process_term_number : int\n            Number of Process Terms defined by the user.\n        regressor_code : array_like\n            The regressor code list given the xlag and ylag for a MISO model.\n\n        Returns\n        -------\n        model_code : array_like\n            Model defined by the user to simulate.\n        err : array-like\n            The respective ERR calculated for each regressor.\n        piv : array-like\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        psi_orthogonal : array_like\n            The updated and orthogonal information matrix.\n\n        References\n        ----------\n        - Manuscript: Orthogonal least squares methods and their application\n           to non-linear system identification\n           https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n        - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n           Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n           e Novos Resultados\n\n        \"\"\"\n        squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n        tmp_psi = psi.copy()\n        y = y[self.max_lag :, 0].reshape(-1, 1)\n        tmp_y = y.copy()\n        dimension = tmp_psi.shape[1]\n        piv = np.arange(dimension)\n        tmp_err = np.zeros(dimension)\n        err = np.zeros(dimension)\n\n        for i in np.arange(0, dimension):\n            for j in np.arange(i, dimension):\n                # Add `eps` in the denominator to omit division by zero if\n                # denominator is zero\n                tmp_err[j] = (\n                    (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                    / (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) * squared_y + self.eps)\n                )[0, 0]\n\n            if i == process_term_number:\n                break\n\n            piv_index = np.argmax(tmp_err[i:]) + i\n            err[i] = tmp_err[piv_index]\n            tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n            piv[[piv_index, i]] = piv[[i, piv_index]]\n\n            v = house(tmp_psi[i:, i])\n\n            row_result = rowhouse(tmp_psi[i:, i:], v)\n\n            tmp_y[i:] = rowhouse(tmp_y[i:], v)\n\n            tmp_psi[i:, i:] = np.copy(row_result)\n\n        tmp_piv = piv[0:process_term_number]\n        psi_orthogonal = psi[:, tmp_piv]\n        model_code = regressor_code[tmp_piv, :].copy()\n        return model_code, err, piv, psi_orthogonal\n\n    def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : array_like\n            The input data to be used in the prediction process.\n        y : array_like\n            The output data to be used in the prediction process.\n        steps_ahead : int\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction. The default is None\n        forecast_horizon : int\n            The number of predictions over the time. The default is None\n\n        Returns\n        -------\n        yhat : array_like\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(self, x, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : array_like of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : array_like\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n        x_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, x, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : array_like of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : array_like\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : array_like of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : array_like\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(self, x, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(self, x, y_initial):\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in SimulateNARMAX for now.\"\n        )\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in SimulateNARMAX for now.\"\n        )\n\n    def _basis_function_n_steps_horizon(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in SimulateNARMAX for now.\"\n        )\n\n    def fit(self, *, X=None, y=None):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"There is no fit method in Simulate because the model is predefined.\"\n        )\n</code></pre>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.error_reduction_ratio","title":"<code>error_reduction_ratio(psi, y, process_term_number, regressor_code)</code>","text":"<p>Perform the Error Reduction Ration algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>array_like</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array - like</code> <p>The target data used in the identification process.</p> required <code>process_term_number</code> <code>int</code> <p>Number of Process Terms defined by the user.</p> required <code>regressor_code</code> <code>array_like</code> <p>The regressor code list given the xlag and ylag for a MISO model.</p> required <p>Returns:</p> Name Type Description <code>model_code</code> <code>array_like</code> <p>Model defined by the user to simulate.</p> <code>err</code> <code>array - like</code> <p>The respective ERR calculated for each regressor.</p> <code>piv</code> <code>array - like</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>psi_orthogonal</code> <code>array_like</code> <p>The updated and orthogonal information matrix.</p> References <ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def error_reduction_ratio(self, psi, y, process_term_number, regressor_code):\n    \"\"\"Perform the Error Reduction Ration algorithm.\n\n    Parameters\n    ----------\n    psi : array_like\n        The information matrix of the model.\n    y : array-like\n        The target data used in the identification process.\n    process_term_number : int\n        Number of Process Terms defined by the user.\n    regressor_code : array_like\n        The regressor code list given the xlag and ylag for a MISO model.\n\n    Returns\n    -------\n    model_code : array_like\n        Model defined by the user to simulate.\n    err : array-like\n        The respective ERR calculated for each regressor.\n    piv : array-like\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    psi_orthogonal : array_like\n        The updated and orthogonal information matrix.\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n    squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n    tmp_psi = psi.copy()\n    y = y[self.max_lag :, 0].reshape(-1, 1)\n    tmp_y = y.copy()\n    dimension = tmp_psi.shape[1]\n    piv = np.arange(dimension)\n    tmp_err = np.zeros(dimension)\n    err = np.zeros(dimension)\n\n    for i in np.arange(0, dimension):\n        for j in np.arange(i, dimension):\n            # Add `eps` in the denominator to omit division by zero if\n            # denominator is zero\n            tmp_err[j] = (\n                (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                / (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) * squared_y + self.eps)\n            )[0, 0]\n\n        if i == process_term_number:\n            break\n\n        piv_index = np.argmax(tmp_err[i:]) + i\n        err[i] = tmp_err[piv_index]\n        tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n        piv[[piv_index, i]] = piv[[i, piv_index]]\n\n        v = house(tmp_psi[i:, i])\n\n        row_result = rowhouse(tmp_psi[i:, i:], v)\n\n        tmp_y[i:] = rowhouse(tmp_y[i:], v)\n\n        tmp_psi[i:, i:] = np.copy(row_result)\n\n    tmp_piv = piv[0:process_term_number]\n    psi_orthogonal = psi[:, tmp_piv]\n    model_code = regressor_code[tmp_piv, :].copy()\n    return model_code, err, piv, psi_orthogonal\n</code></pre>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Not implemented.</p> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def fit(self, *, X=None, y=None):\n    \"\"\"Not implemented.\"\"\"\n    raise NotImplementedError(\n        \"There is no fit method in Simulate because the model is predefined.\"\n    )\n</code></pre>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array_like</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>array_like</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction. The default is None</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time. The default is None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>array_like</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : array_like\n        The input data to be used in the prediction process.\n    y : array_like\n        The output data to be used in the prediction process.\n    steps_ahead : int\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction. The default is None\n    forecast_horizon : int\n        The number of predictions over the time. The default is None\n\n    Returns\n    -------\n    yhat : array_like\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.simulate","title":"<code>simulate(*, X_train=None, y_train=None, X_test=None, y_test=None, model_code=None, steps_ahead=None, theta=None, forecast_horizon=None)</code>","text":"<p>Simulate the response of a NARMAX model based on user-defined parameters.</p> <p>This method simulates the system's response using a predefined model structure (<code>model_code</code>) and estimated parameters (<code>theta</code>). It allows for both training-based parameter estimation and direct simulation using precomputed parameters.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Input data used for parameter estimation during training. Required if <code>estimate_parameter=True</code>.</p> <code>None</code> <code>y_train</code> <code>(array - like, shape(n_samples, 1))</code> <p>Output (target) data used for parameter estimation during training. Required if <code>estimate_parameter=True</code>.</p> <code>None</code> <code>X_test</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Input data used for simulation (prediction).</p> <code>None</code> <code>y_test</code> <code>(array - like, shape(n_samples, 1))</code> <p>Output data used as initial conditions for simulation.</p> <code>None</code> <code>model_code</code> <code>(array - like, shape(n_terms, n_columns))</code> <p>Encoded representation of the model's regressors, defining the input-output relationships in the system.</p> <code>None</code> <code>steps_ahead</code> <code>int</code> <p>Number of steps ahead for multi-step prediction. If <code>None</code>, defaults to one-step-ahead prediction.</p> <code>None</code> <code>theta</code> <code>(array - like, shape(n_terms, 1))</code> <p>Precomputed model parameters. Required if <code>estimate_parameter=False</code>.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>Number of time steps to predict in open-loop forecasting. Used mainly for NAR and NARMA-type models.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>(array - like, shape(n_samples, 1))</code> <p>Predicted output values of the system based on the given inputs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If necessary parameters are missing, such as <code>y_train</code> when <code>estimate_parameter=True</code> or <code>theta</code> when <code>estimate_parameter=False</code>.</p> Notes <ul> <li>If <code>estimate_parameter=True</code>, the method first estimates the parameters using the provided training data (<code>X_train</code>, <code>y_train</code>) and the chosen basis function.</li> <li>If <code>estimate_parameter=False</code>, the method assumes <code>theta</code> contains the model     parameters.</li> <li>The forecast horizon is automatically adjusted for NAR models if not provided.</li> <li>The method internally computes the lag structure based on <code>model_code</code> to     define regressors.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; X_train = np.random.rand(1000, 1)\n&gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n&gt;&gt;&gt; X_test = np.random.rand(200, 1)\n&gt;&gt;&gt; y_test = np.random.rand(200, 1)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n&gt;&gt;&gt; model = np.array([\n...     [1001, 0],       # y(k-1)\n...     [2001, 1001],    # x1(k-1)y(k-1)\n...     [2002, 0]        # x1(k-2)\n... ])\n&gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Precomputed model parameters\n&gt;&gt;&gt; y_pred = simulator.simulate(\n...     X_train=X_train, y_train=y_train,\n...     X_test=X_test, y_test=y_test,\n...     model_code=model, theta=theta\n... )\n</code></pre> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def simulate(\n    self,\n    *,\n    X_train=None,\n    y_train=None,\n    X_test=None,\n    y_test=None,\n    model_code=None,\n    steps_ahead=None,\n    theta=None,\n    forecast_horizon=None,\n):\n    \"\"\"Simulate the response of a NARMAX model based on user-defined parameters.\n\n    This method simulates the system's response using a predefined model structure\n    (`model_code`) and estimated parameters (`theta`). It allows for both\n    training-based parameter estimation and direct simulation using precomputed\n    parameters.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features), optional\n        Input data used for parameter estimation during training.\n        Required if `estimate_parameter=True`.\n    y_train : array-like, shape (n_samples, 1), optional\n        Output (target) data used for parameter estimation during training.\n        Required if `estimate_parameter=True`.\n    X_test : array-like, shape (n_samples, n_features), optional\n        Input data used for simulation (prediction).\n    y_test : array-like, shape (n_samples, 1), optional\n        Output data used as initial conditions for simulation.\n    model_code : array-like, shape (n_terms, n_columns)\n        Encoded representation of the model's regressors, defining\n        the input-output relationships in the system.\n    steps_ahead : int, optional\n        Number of steps ahead for multi-step prediction. If `None`, defaults to\n        one-step-ahead prediction.\n    theta : array-like, shape (n_terms, 1), optional\n        Precomputed model parameters. Required if `estimate_parameter=False`.\n    forecast_horizon : int, optional\n        Number of time steps to predict in open-loop forecasting.\n        Used mainly for NAR and NARMA-type models.\n\n    Returns\n    -------\n    yhat : array-like, shape (n_samples, 1)\n        Predicted output values of the system based on the given inputs.\n\n    Raises\n    ------\n    ValueError\n        If necessary parameters are missing, such as `y_train` when\n        `estimate_parameter=True` or `theta` when `estimate_parameter=False`.\n\n    Notes\n    -----\n    - If `estimate_parameter=True`, the method first estimates the parameters using\n    the provided training data (`X_train`, `y_train`) and the chosen basis function.\n    - If `estimate_parameter=False`, the method assumes `theta` contains the model\n        parameters.\n    - The forecast horizon is automatically adjusted for NAR models if not provided.\n    - The method internally computes the lag structure based on `model_code` to\n        define regressors.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; X_train = np.random.rand(1000, 1)\n    &gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n    &gt;&gt;&gt; X_test = np.random.rand(200, 1)\n    &gt;&gt;&gt; y_test = np.random.rand(200, 1)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n    &gt;&gt;&gt; model = np.array([\n    ...     [1001, 0],       # y(k-1)\n    ...     [2001, 1001],    # x1(k-1)y(k-1)\n    ...     [2002, 0]        # x1(k-2)\n    ... ])\n    &gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Precomputed model parameters\n    &gt;&gt;&gt; y_pred = simulator.simulate(\n    ...     X_train=X_train, y_train=y_train,\n    ...     X_test=X_test, y_test=y_test,\n    ...     model_code=model, theta=theta\n    ... )\n\n    \"\"\"\n    self._check_simulate_params(y_train, y_test, model_code, steps_ahead, theta)\n\n    if X_test is not None:\n        self.n_inputs = num_features(X_test)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    xlag_code = list_input_regressor_code(model_code)\n    ylag_code = list_output_regressor_code(model_code)\n    self.xlag = get_lag_from_regressor_code(xlag_code)\n    self.ylag = get_lag_from_regressor_code(ylag_code)\n    self.max_lag = max(self.xlag, self.ylag)\n    if self.n_inputs != 1:\n        self.xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n    # for MetaMSS NAR modelling\n    if self.model_type == \"NAR\" and forecast_horizon is None:\n        forecast_horizon = y_test.shape[0] - self.max_lag\n\n    self.non_degree = model_code.shape[1]\n    regressor_code = self.regressor_space(self.n_inputs)\n\n    self.pivv = get_index_from_regressor_code(regressor_code, model_code)\n    self.final_model = regressor_code[self.pivv]\n    # to use in the predict function\n    self.n_terms = self.final_model.shape[0]\n    if self.estimate_parameter and not self.calculate_err:\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(\n            X_train, y_train, self.xlag, self.ylag, self.model_type\n        )\n        psi = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv,\n        )\n\n        self.theta = self.estimator.optimize(\n            psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if self.estimator.unbiased is True:\n            self.theta = self.estimator.unbiased_estimator(\n                psi,\n                y_train[self.max_lag :, 0].reshape(-1, 1),\n                self.theta,\n                self.elag,\n                self.max_lag,\n                self.estimator,\n                self.basis_function,\n                self.estimator.uiter,\n            )\n\n        self.err = self.n_terms * [0]\n    elif not self.estimate_parameter:\n        self.theta = theta\n        self.err = self.n_terms * [0]\n    else:\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(\n            X_train, y_train, self.xlag, self.ylag, self.model_type\n        )\n        psi = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv,\n        )\n\n        _, self.err, _, _ = self.error_reduction_ratio(\n            psi, y_train, self.n_terms, self.final_model\n        )\n        self.theta = self.estimator.optimize(\n            psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if self.estimator.unbiased is True:\n            self.theta = self.estimator.unbiased_estimator(\n                psi,\n                y_train[self.max_lag :, 0].reshape(-1, 1),\n                self.theta,\n                self.elag,\n                self.max_lag,\n                self.estimator,\n                self.basis_function,\n                self.estimator.uiter,\n            )\n\n    return self.predict(\n        X=X_test,\n        y=y_test,\n        steps_ahead=steps_ahead,\n        forecast_horizon=forecast_horizon,\n    )\n</code></pre>"},{"location":"user-guide/API/uofr/","title":"Documentation for <code>UOFR</code>","text":"<p>Build NARMAX Models using UOFR algorithm.</p>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR","title":"<code>UOFR</code>","text":"<p>               Bases: <code>OFRBase</code></p> <p>Ultra Orthogonal Forward Regression algorithm.</p> <p>This class uses the UOFR algorithm ([1]) to build NARMAX models. The NARMAX model is described as:</p> \\[     y_k= F[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},     \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}\\) is some nonlinear function of the input and output regressors and \\(d\\) is a time delay typically set to  \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>elag</code> <code>int</code> <p>The maximum lag of the residues regressors.</p> <code>2</code> <code>order_selection</code> <code>bool</code> <p>Whether to use information criteria for order selection.</p> <code>True</code> <code>info_criteria</code> <code>str</code> <p>The information criteria method to be used.</p> <code>\"aic\"</code> <code>n_terms</code> <code>int</code> <p>The number of the model terms to be selected. Note that n_terms overwrite the information criteria values.</p> <code>None</code> <code>n_info_values</code> <code>int</code> <p>The number of iterations of the information criteria method.</p> <code>10</code> <code>estimator</code> <code>str</code> <p>The parameter estimation method.</p> <code>\"least_squares\"</code> <code>model_type</code> <code>str</code> <p>The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models</p> <code>'NARMAX'</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <code>alpha</code> <code>float</code> <p>Regularization parameter used in ridge regression. Ridge regression parameter that regularizes the algorithm to prevent over fitting. If the input is a noisy signal, the ridge parameter is likely to be set close to the noise level, at least as a starting point. Entered through the self data structure.</p> <code>np.finfo(np.float64).eps</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import FROLS\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n...                                                    colored_noise=True,\n...                                                    sigma=0.2,\n...                                                    train_percentage=90)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = UOFR(basis_function=basis_function,\n...               order_selection=True,\n...               n_info_values=10,\n...               extended_least_squares=False,\n...               ylag=2,\n...               xlag=2,\n...               info_criteria='aic',\n...               )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Manuscript: Ultra-Orthogonal Forward Regression Algorithms for the     Identification of Non-Linear Dynamic Systems    https://eprints.whiterose.ac.uk/107310/1/UOFR%20Algorithms%20R1.pdf</li> </ul> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>class UOFR(OFRBase):\n    r\"\"\"Ultra Orthogonal Forward Regression algorithm.\n\n    This class uses the UOFR algorithm ([1]) to build NARMAX models.\n    The NARMAX model is described as:\n\n    $$\n        y_k= F[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},\n        \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}$ is some nonlinear function\n    of the input and output regressors and $d$ is a time delay typically set to\n     $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    elag : int, default=2\n        The maximum lag of the residues regressors.\n    order_selection: bool, default=False\n        Whether to use information criteria for order selection.\n    info_criteria : str, default=\"aic\"\n        The information criteria method to be used.\n    n_terms : int, default=None\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_info_values : int, default=10\n        The number of iterations of the information\n        criteria method.\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    alpha : float, default=np.finfo(np.float64).eps\n        Regularization parameter used in ridge regression.\n        Ridge regression parameter that regularizes the algorithm to prevent over\n        fitting. If the input is a noisy signal, the ridge parameter is likely to be\n        set close to the noise level, at least as a starting point.\n        Entered through the self data structure.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import FROLS\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = UOFR(basis_function=basis_function,\n    ...               order_selection=True,\n    ...               n_info_values=10,\n    ...               extended_least_squares=False,\n    ...               ylag=2,\n    ...               xlag=2,\n    ...               info_criteria='aic',\n    ...               )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Ultra-Orthogonal Forward Regression Algorithms for the\n        Identification of Non-Linear Dynamic Systems\n       https://eprints.whiterose.ac.uk/107310/1/UOFR%20Algorithms%20R1.pdf\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        elag: Union[int, list] = 2,\n        order_selection: bool = True,\n        info_criteria: str = \"aic\",\n        n_terms: Union[int, None] = None,\n        n_info_values: int = 15,\n        estimator: Estimators = RecursiveLeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n        eps: np.float64 = np.finfo(np.float64).eps,\n        alpha: float = 0,\n        err_tol: Optional[float] = None,\n    ):\n        self.order_selection = order_selection\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.info_criteria = info_criteria\n        self.info_criteria_function = get_info_criteria(info_criteria)\n        self.n_info_values = n_info_values\n        self.n_terms = n_terms\n        self.estimator = estimator\n        self.elag = elag\n        self.model_type = model_type\n        self.basis_function = basis_function\n        self.eps = eps\n        if isinstance(self.estimator, RidgeRegression):\n            self.alpha = self.estimator.alpha\n        else:\n            self.alpha = alpha\n\n        self.err_tol = err_tol\n        self._validate_params()\n        self.n_inputs = None\n        self.regressor_code = None\n        self.info_values = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n\n    def gaussian_test_function(self, t: np.ndarray, order: int) -&gt; np.ndarray:\n        \"\"\"Generate Gaussian-like test function derivatives.\"\"\"\n        sigma = 1.0  # Adjust based on signal characteristics\n        gaussian = np.exp(-(t**2) / (2 * sigma**2))\n        derivative = np.gradient(gaussian, t)\n        for _ in range(order - 1):\n            derivative = np.gradient(derivative, t)\n        return derivative\n\n    def normalize_test_function(self, phi_j: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize derivatives.\"\"\"\n        norm = np.linalg.norm(phi_j, ord=2)\n        return phi_j / norm if norm != 0 else phi_j\n\n    def compute_modulated_signal(\n        self, signal: np.ndarray, phi_bar_j: np.ndarray\n    ) -&gt; np.ndarray:\n        modulated = np.convolve(signal.flatten(), phi_bar_j, mode=\"valid\")\n        return modulated  # Length = len(signal) - len(phi_bar_j) + 1\n\n    def augment_uls_terms(\n        self, y: np.ndarray, psi: np.ndarray, m: int = 2, test_support: int = 5\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Augment signals for ULS with matching row counts.\"\"\"\n        modulated_length = len(y) - test_support + 1\n        num_terms = psi.shape[1]\n        t = np.linspace(-3, 3, test_support)\n\n        # Initialize y_augmented and psi_augmented with original truncated signals\n        y_augmented = y[:modulated_length].reshape(-1, 1)\n        psi_augmented = psi[:modulated_length, :]\n\n        for j in range(1, m + 1):\n            phi_j = self.gaussian_test_function(t, order=j)\n            phi_bar_j = self.normalize_test_function(phi_j)\n            y_j = self.compute_modulated_signal(y, phi_bar_j).reshape(-1, 1)\n            y_augmented = np.vstack([y_augmented, y_j])\n            modulated_terms = np.zeros((modulated_length, num_terms))\n            for term in range(num_terms):\n                x_j = self.compute_modulated_signal(psi[:, term], phi_bar_j)\n                modulated_terms[:, term] = x_j\n\n            psi_augmented = np.vstack([psi_augmented, modulated_terms])\n\n        return y_augmented, psi_augmented\n\n    def sobolev_error_reduction_ratio(\n        self,\n        psi: np.ndarray,\n        y: np.ndarray,\n        process_term_number: int,\n        m: int = 2,\n        test_support: int = 5,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Define Ultra Orthogonal Least Squares.\"\"\"\n        y = y[self.max_lag :, 0].reshape(-1, 1)\n        y_augmented, psi_augmented = self.augment_uls_terms(y, psi, m, test_support)\n        y_augmented = y_augmented.reshape(-1, 1)\n        # Compute ERR on the augmented ULS matrix\n        squared_y = np.dot(y_augmented.T, y_augmented)\n        squared_y = float(np.maximum(squared_y, np.finfo(np.float64).eps))\n        psi_working = psi_augmented.copy()\n        y_working = y_augmented.copy()\n        num_terms = psi_working.shape[1]\n        piv = np.arange(num_terms)\n        candidate_err = np.zeros(num_terms)\n        err = np.zeros(num_terms)\n\n        for step_idx in np.arange(0, num_terms):\n            candidate_err[step_idx:] = _compute_err_slice(\n                psi_working,\n                y_working,\n                step_idx,\n                squared_y,\n                self.alpha,\n                self.eps,\n            )\n\n            max_err_idx = np.argmax(candidate_err[step_idx:]) + step_idx\n            err[step_idx] = candidate_err[max_err_idx]\n            if step_idx == process_term_number:\n                break\n\n            if (self.err_tol is not None) and (err.cumsum()[step_idx] &gt;= self.err_tol):\n                self.n_terms = step_idx + 1\n                process_term_number = step_idx + 1\n                break\n\n            psi_working[:, [max_err_idx, step_idx]] = psi_working[\n                :, [step_idx, max_err_idx]\n            ]\n            piv[[max_err_idx, step_idx]] = piv[[step_idx, max_err_idx]]\n            reflector = house(psi_working[step_idx:, step_idx])\n            row_result = rowhouse(psi_working[step_idx:, step_idx:], reflector)\n            y_working[step_idx:] = rowhouse(y_working[step_idx:], reflector)\n            psi_working[step_idx:, step_idx:] = np.copy(row_result)\n\n        tmp_piv = piv[0:process_term_number]\n        psi_orthogonal = psi[:, tmp_piv]\n        return err, tmp_piv, psi_orthogonal\n\n    def run_mss_algorithm(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        return self.sobolev_error_reduction_ratio(psi, y, process_term_number)\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n        \"\"\"Fit polynomial NARMAX model.\n\n        This is an 'alpha' version of the 'fit' function which allows\n        a friendly usage by the user. Given two arguments, x and y, fit\n        training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        super().fit(X=X, y=y)\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        yhat = super().predict(\n            X=X, y=y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        return yhat\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.augment_uls_terms","title":"<code>augment_uls_terms(y, psi, m=2, test_support=5)</code>","text":"<p>Augment signals for ULS with matching row counts.</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def augment_uls_terms(\n    self, y: np.ndarray, psi: np.ndarray, m: int = 2, test_support: int = 5\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Augment signals for ULS with matching row counts.\"\"\"\n    modulated_length = len(y) - test_support + 1\n    num_terms = psi.shape[1]\n    t = np.linspace(-3, 3, test_support)\n\n    # Initialize y_augmented and psi_augmented with original truncated signals\n    y_augmented = y[:modulated_length].reshape(-1, 1)\n    psi_augmented = psi[:modulated_length, :]\n\n    for j in range(1, m + 1):\n        phi_j = self.gaussian_test_function(t, order=j)\n        phi_bar_j = self.normalize_test_function(phi_j)\n        y_j = self.compute_modulated_signal(y, phi_bar_j).reshape(-1, 1)\n        y_augmented = np.vstack([y_augmented, y_j])\n        modulated_terms = np.zeros((modulated_length, num_terms))\n        for term in range(num_terms):\n            x_j = self.compute_modulated_signal(psi[:, term], phi_bar_j)\n            modulated_terms[:, term] = x_j\n\n        psi_augmented = np.vstack([psi_augmented, modulated_terms])\n\n    return y_augmented, psi_augmented\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.fit","title":"<code>fit(*, X=None, y)</code>","text":"<p>Fit polynomial NARMAX model.</p> <p>This is an 'alpha' version of the 'fit' function which allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>info_values</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n    \"\"\"Fit polynomial NARMAX model.\n\n    This is an 'alpha' version of the 'fit' function which allows\n    a friendly usage by the user. Given two arguments, x and y, fit\n    training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    super().fit(X=X, y=y)\n    return self\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.gaussian_test_function","title":"<code>gaussian_test_function(t, order)</code>","text":"<p>Generate Gaussian-like test function derivatives.</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def gaussian_test_function(self, t: np.ndarray, order: int) -&gt; np.ndarray:\n    \"\"\"Generate Gaussian-like test function derivatives.\"\"\"\n    sigma = 1.0  # Adjust based on signal characteristics\n    gaussian = np.exp(-(t**2) / (2 * sigma**2))\n    derivative = np.gradient(gaussian, t)\n    for _ in range(order - 1):\n        derivative = np.gradient(derivative, t)\n    return derivative\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.normalize_test_function","title":"<code>normalize_test_function(phi_j)</code>","text":"<p>Normalize derivatives.</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def normalize_test_function(self, phi_j: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Normalize derivatives.\"\"\"\n    norm = np.linalg.norm(phi_j, ord=2)\n    return phi_j / norm if norm != 0 else phi_j\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.sobolev_error_reduction_ratio","title":"<code>sobolev_error_reduction_ratio(psi, y, process_term_number, m=2, test_support=5)</code>","text":"<p>Define Ultra Orthogonal Least Squares.</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def sobolev_error_reduction_ratio(\n    self,\n    psi: np.ndarray,\n    y: np.ndarray,\n    process_term_number: int,\n    m: int = 2,\n    test_support: int = 5,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Define Ultra Orthogonal Least Squares.\"\"\"\n    y = y[self.max_lag :, 0].reshape(-1, 1)\n    y_augmented, psi_augmented = self.augment_uls_terms(y, psi, m, test_support)\n    y_augmented = y_augmented.reshape(-1, 1)\n    # Compute ERR on the augmented ULS matrix\n    squared_y = np.dot(y_augmented.T, y_augmented)\n    squared_y = float(np.maximum(squared_y, np.finfo(np.float64).eps))\n    psi_working = psi_augmented.copy()\n    y_working = y_augmented.copy()\n    num_terms = psi_working.shape[1]\n    piv = np.arange(num_terms)\n    candidate_err = np.zeros(num_terms)\n    err = np.zeros(num_terms)\n\n    for step_idx in np.arange(0, num_terms):\n        candidate_err[step_idx:] = _compute_err_slice(\n            psi_working,\n            y_working,\n            step_idx,\n            squared_y,\n            self.alpha,\n            self.eps,\n        )\n\n        max_err_idx = np.argmax(candidate_err[step_idx:]) + step_idx\n        err[step_idx] = candidate_err[max_err_idx]\n        if step_idx == process_term_number:\n            break\n\n        if (self.err_tol is not None) and (err.cumsum()[step_idx] &gt;= self.err_tol):\n            self.n_terms = step_idx + 1\n            process_term_number = step_idx + 1\n            break\n\n        psi_working[:, [max_err_idx, step_idx]] = psi_working[\n            :, [step_idx, max_err_idx]\n        ]\n        piv[[max_err_idx, step_idx]] = piv[[step_idx, max_err_idx]]\n        reflector = house(psi_working[step_idx:, step_idx])\n        row_result = rowhouse(psi_working[step_idx:, step_idx:], reflector)\n        y_working[step_idx:] = rowhouse(y_working[step_idx:], reflector)\n        psi_working[step_idx:, step_idx:] = np.copy(row_result)\n\n    tmp_piv = piv[0:process_term_number]\n    psi_orthogonal = psi[:, tmp_piv]\n    return err, tmp_piv, psi_orthogonal\n</code></pre>"},{"location":"user-guide/API/utils/","title":"Documentation for <code>Neural NARX</code>","text":"<p>Utilities fo data validation.</p> <p>Display results formatted for the user.</p> <p>Utilities for data generation.</p> <p>Utils methods for NARMAX modeling.</p> <p>Plotting methods.</p>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_dimension","title":"<code>check_dimension(x, y)</code>","text":"<p>Check if x and y have only real values.</p> <p>If there is any string or object samples a ValueError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_dimension(x, y):\n    \"\"\"Check if x and y have only real values.\n\n    If there is any string or object samples a ValueError is raised.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if x.ndim == 0:\n        raise ValueError(\n            \"Input must be a 2d array, got scalar instead. Reshape your data using\"\n            \" array.reshape(-1, 1)\"\n        )\n\n    if x.ndim == 1:\n        raise ValueError(\n            \"Input must be a 2d array, got 1d array instead. \"\n            \"Reshape your data using array.reshape(-1, 1)\"\n        )\n\n    if y.ndim == 0:\n        raise ValueError(\n            \"Output must be a 2d array, got scalar instead. \"\n            \"Reshape your data using array.reshape(-1, 1)\"\n        )\n\n    if y.ndim == 1:\n        raise ValueError(\n            \"Output must be a 2d array, got 1d array instead. \"\n            \"Reshape your data using array.reshape(-1, 1)\"\n        )\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_infinity","title":"<code>check_infinity(x, y)</code>","text":"<p>Check that x and y have no NaN or Inf samples.</p> <p>If there is any NaN or Inf samples a ValueError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_infinity(x, y):\n    \"\"\"Check that x and y have no NaN or Inf samples.\n\n    If there is any NaN or Inf samples a ValueError is raised.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if np.isinf(x).any():\n        msg_error = (\n            \"Input contains invalid values (e.g. NaN, Inf) on \"\n            f\"index {np.argwhere(np.isinf(x))}\"\n        )\n        raise ValueError(msg_error)\n\n    if np.isinf(y).any():\n        msg_error = (\n            \"Output contains invalid values (e.g Inf) on \"\n            f\"index {np.argwhere(np.isinf(y))}\"\n        )\n        raise ValueError(msg_error)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_length","title":"<code>check_length(x, y)</code>","text":"<p>Check that x and y have the same number of samples.</p> <p>If the length of x and y are different a ValueError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_length(x, y):\n    \"\"\"Check that x and y have the same number of samples.\n\n    If the length of x and y are different a ValueError is raised.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if x.shape[0] != y.shape[0]:\n        msg_error = (\n            \"Input and output data must have the same number of \"\n            f\"samples. x has dimension {x.shape} and \"\n            f\"y has dimension {y.shape}\"\n        )\n        raise ValueError(msg_error)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_linear_dependence_rows","title":"<code>check_linear_dependence_rows(psi)</code>","text":"<p>Check for linear dependence in the rows of the Psi matrix.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the Psi matrix has linearly dependent rows.</p> Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_linear_dependence_rows(psi):\n    \"\"\"Check for linear dependence in the rows of the Psi matrix.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n\n    Warns\n    -----\n    UserWarning\n        If the Psi matrix has linearly dependent rows.\n    \"\"\"\n    if np.linalg.matrix_rank(psi) != psi.shape[1]:\n        warn(\n            \"Psi matrix might have linearly dependent rows.\"\n            \"Be careful and check your data\",\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_nan","title":"<code>check_nan(x, y)</code>","text":"<p>Check that x and y have no NaN or Inf samples.</p> <p>If there is any NaN or Inf samples a ValueError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_nan(x, y):\n    \"\"\"Check that x and y have no NaN or Inf samples.\n\n    If there is any NaN or Inf samples a ValueError is raised.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if np.isnan(x).any():\n        msg_error = (\n            \"Input contains invalid values (e.g. NaN, Inf) on \"\n            f\"index {np.argwhere(np.isnan(x))}\"\n        )\n        raise ValueError(msg_error)\n\n    if not ~np.isnan(y).any():\n        msg_error = (\n            \"Output contains invalid values (e.g. NaN, Inf) on \"\n            f\"index {np.argwhere(np.isnan(y))}\"\n        )\n        raise ValueError(msg_error)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_random_state","title":"<code>check_random_state(seed)</code>","text":"<p>Turn <code>seed</code> into a <code>np.random.RandomState</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>{None, int, `numpy.random.Generator`,</code> <pre><code>`numpy.random.RandomState`}, optional\n</code></pre> <p>If <code>seed</code> is None (or <code>np.random</code>), the <code>numpy.random.RandomState</code> singleton is used. If <code>seed</code> is an int, a new <code>RandomState</code> instance is used, seeded with <code>seed</code>. If <code>seed</code> is already a <code>Generator</code> or <code>RandomState</code> instance then that instance is used.</p> required <p>Returns:</p> Name Type Description <code>seed</code> <code>{`numpy.random.Generator`, `numpy.random.RandomState`}</code> <p>Random number generator.</p> Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_random_state(seed):\n    \"\"\"Turn `seed` into a `np.random.RandomState` instance.\n\n    Parameters\n    ----------\n    seed : {None, int, `numpy.random.Generator`,\n            `numpy.random.RandomState`}, optional\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n        that instance is used.\n\n    Returns\n    -------\n    seed : {`numpy.random.Generator`, `numpy.random.RandomState`}\n        Random number generator.\n\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, (numbers.Integral, np.integer)):\n        return np.random.default_rng(seed)\n    if isinstance(seed, (np.random.RandomState, np.random.Generator)):\n        return seed\n\n    raise ValueError(\n        \"%r cannot be used to seed a numpy.random.RandomState instance\" % seed\n    )\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_x_y","title":"<code>check_x_y(x, y)</code>","text":"<p>Validate input and output data using some crucial tests.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_x_y(x, y):\n    \"\"\"Validate input and output data using some crucial tests.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    check_length(x, y)\n    check_dimension(x, y)\n    check_infinity(x, y)\n    check_nan(x, y)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.deprecation.deprecated","title":"<code>deprecated(version, future_version=None, message=None, alternative=None, **kwargs)</code>","text":"<p>Decorate deprecated methods.</p> <p>This decorator is adapted from astroML decorator: https://github.com/astroML/astroML/blob/f66558232f6d33cb34ecd1bed8a80b9db7ae1c30/astroML/utils/decorators.py#L120</p> Source code in <code>sysidentpy/utils/deprecation.py</code> <pre><code>def deprecated(version, future_version=None, message=None, alternative=None, **kwargs):\n    \"\"\"Decorate deprecated methods.\n\n    This decorator is adapted from astroML decorator:\n    https://github.com/astroML/astroML/blob/f66558232f6d33cb34ecd1bed8a80b9db7ae1c30/astroML/utils/decorators.py#L120\n\n    \"\"\"\n\n    def deprecate_function(\n        func,\n        version=version,\n        future_version=future_version,\n        message=message,\n        alternative=alternative,\n    ):\n        if message is None:\n            message = f\"Function {func.__name__} has been deprecated since {version}.\"\n            if alternative is not None:\n                message += (\n                    f\"\\n You'll have to use {alternative} instead.\"\n                    \"This module was deprecated in favor of \"\n                    f\"{alternative} module into which all the refactored \"\n                    \"classes and functions are moved.\"\n                )\n            if future_version is not None:\n                message += (\n                    f\"\\n This change will be applied in version {future_version}.\"\n                )\n\n        @functools.wraps(func)\n        def deprecated_func(*args, **kwargs):\n            warnings.warn(message, FutureWarning, stacklevel=1)\n            return func(*args, **kwargs)\n\n        return deprecated_func\n\n    def deprecate_class(\n        cls,\n        version=version,\n        future_version=future_version,\n        message=message,\n        alternative=alternative,\n    ):\n        if message is None:\n            message = f\"Class {cls.__name__} has been deprecated since {version}.\"\n            if alternative is not None:\n                message += alternative\n            if future_version is not None:\n                message += (\n                    f\"\\n This change will be applied in version {future_version}.\"\n                )\n\n        cls.__init__ = deprecate_function(cls.__init__, message=message)\n\n        return cls\n\n    def deprecate_warning(obj):\n        if isinstance(obj, type):\n            return deprecate_class(obj)\n\n        return deprecate_function(obj)\n\n    return deprecate_warning\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.display_results.results","title":"<code>results(final_model=None, theta=None, err=None, n_terms=None, theta_precision=4, err_precision=8, dtype='dec')</code>","text":"<p>Return the model regressors, parameters and ERR values.</p> <p>Generates a formatted string matrix containing model regressors, their corresponding parameters, and error reduction ratio (ERR) values.</p> <p>This function constructs a structured output where each row represents a model regressor, its estimated parameter, and the associated ERR value. The numerical values can be displayed in either decimal or scientific notation.</p> <p>Parameters:</p> Name Type Description Default <code>final_model</code> <code>array - like</code> <p>The identified model structure, where each row corresponds to a regressor represented by numerical codes.</p> <code>None</code> <code>theta</code> <code>array - like</code> <p>A column vector containing the estimated parameters for each regressor.</p> <code>None</code> <code>err</code> <code>array - like</code> <p>A vector containing the error reduction ratio (ERR) values for each regressor.</p> <code>None</code> <code>n_terms</code> <code>int</code> <p>Number of terms (regressors) in the model.</p> <code>None</code> <code>theta_precision</code> <code>int</code> <p>Number of decimal places for displaying parameter values.</p> <code>4</code> <code>err_precision</code> <code>int</code> <p>Number of decimal places for displaying ERR values.</p> <code>8</code> <code>dtype</code> <code>(dec, sci)</code> <p>Format for displaying numerical values: - 'dec' : Decimal notation. - 'sci' : Scientific notation.</p> <code>'dec'</code> <p>Returns:</p> Name Type Description <code>output_matrix</code> <code>list of lists</code> <p>A structured matrix where: - The first column contains regressor representations as strings. - The second column contains the corresponding estimated parameter values. - The third column contains the associated ERR values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>theta_precision</code> or <code>err_precision</code> is not a positive integer. If <code>dtype</code> is not 'dec' or 'sci'.</p> Source code in <code>sysidentpy/utils/display_results.py</code> <pre><code>def results(\n    final_model=None,\n    theta=None,\n    err=None,\n    n_terms=None,\n    theta_precision=4,\n    err_precision=8,\n    dtype=\"dec\",\n):\n    \"\"\"Return the model regressors, parameters and ERR values.\n\n    Generates a formatted string matrix containing model regressors,\n    their corresponding parameters, and error reduction ratio (ERR) values.\n\n    This function constructs a structured output where each row represents\n    a model regressor, its estimated parameter, and the associated ERR value.\n    The numerical values can be displayed in either decimal or scientific notation.\n\n    Parameters\n    ----------\n    final_model : array-like, optional\n        The identified model structure, where each row corresponds to\n        a regressor represented by numerical codes.\n\n    theta : array-like, optional\n        A column vector containing the estimated parameters for each regressor.\n\n    err : array-like, optional\n        A vector containing the error reduction ratio (ERR) values for each regressor.\n\n    n_terms : int, optional\n        Number of terms (regressors) in the model.\n\n    theta_precision : int, default=4\n        Number of decimal places for displaying parameter values.\n\n    err_precision : int, default=8\n        Number of decimal places for displaying ERR values.\n\n    dtype : {'dec', 'sci'}, default='dec'\n        Format for displaying numerical values:\n        - 'dec' : Decimal notation.\n        - 'sci' : Scientific notation.\n\n    Returns\n    -------\n    output_matrix : list of lists\n        A structured matrix where:\n        - The first column contains regressor representations as strings.\n        - The second column contains the corresponding estimated parameter values.\n        - The third column contains the associated ERR values.\n\n    Raises\n    ------\n    ValueError\n        If `theta_precision` or `err_precision` is not a positive integer.\n        If `dtype` is not 'dec' or 'sci'.\n\n    \"\"\"\n    if not isinstance(theta_precision, int) or theta_precision &lt; 1:\n        raise ValueError(\n            f\"theta_precision must be integer and &gt; zero. Got {theta_precision}.\"\n        )\n\n    if not isinstance(err_precision, int) or err_precision &lt; 1:\n        raise ValueError(\n            f\"err_precision must be integer and &gt; zero. Got {err_precision}.\"\n        )\n\n    if dtype not in (\"dec\", \"sci\"):\n        raise ValueError(f\"dtype must be dec or sci. Got {dtype}.\")\n\n    output_matrix = []\n    theta_output_format = \"{:.\" + str(theta_precision)\n    err_output_format = \"{:.\" + str(err_precision)\n\n    if dtype == \"dec\":\n        theta_output_format = theta_output_format + \"f}\"\n        err_output_format = err_output_format + \"f}\"\n    else:\n        theta_output_format = theta_output_format + \"E}\"\n        err_output_format = err_output_format + \"E}\"\n\n    for i in range(n_terms):\n        if np.max(final_model[i]) &lt; 1:\n            tmp_regressor = str(1)\n        else:\n            regressor_dic = Counter(final_model[i])\n            regressor_string = []\n            for j in range(len(list(regressor_dic.keys()))):\n                regressor_key = list(regressor_dic.keys())[j]\n                if regressor_key &lt; 1:\n                    translated_key = \"\"\n                    translated_exponent = \"\"\n                else:\n                    delay_string = str(\n                        int(regressor_key - np.floor(regressor_key / 1000) * 1000)\n                    )\n                    if int(regressor_key / 1000) &lt; 2:\n                        translated_key = \"y(k-\" + delay_string + \")\"\n                    else:\n                        translated_key = (\n                            \"x\"\n                            + str(int(regressor_key / 1000) - 1)\n                            + \"(k-\"\n                            + delay_string\n                            + \")\"\n                        )\n                    if regressor_dic[regressor_key] &lt; 2:\n                        translated_exponent = \"\"\n                    else:\n                        translated_exponent = \"^\" + str(regressor_dic[regressor_key])\n                regressor_string.append(translated_key + translated_exponent)\n            tmp_regressor = \"\".join(regressor_string)\n\n        current_parameter = theta_output_format.format(theta[i, 0])\n        current_err = err_output_format.format(err[i])\n        current_output = [tmp_regressor, current_parameter, current_err]\n        output_matrix.append(current_output)\n\n    return output_matrix\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.generate_data.get_miso_data","title":"<code>get_miso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90)</code>","text":"<p>Generate synthetic data for Multiple-Input Single-Output system identification.</p> <p>This function simulates input-output data for a nonlinear MISO system using two input signals. The system output is influenced by both inputs and can be affected by either white or colored (autoregressive) noise based on the <code>colored_noise</code> flag.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>(int, optional(default=5000))</code> <p>Number of samples to generate.</p> <code>5000</code> <code>colored_noise</code> <code>(bool, optional(default=False))</code> <p>If True, adds colored (autoregressive) noise to the system; otherwise, white noise is used.</p> <code>False</code> <code>sigma</code> <code>(float, optional(default=0.05))</code> <p>Standard deviation of the noise distribution.</p> <code>0.05</code> <code>train_percentage</code> <code>(int, optional(default=90))</code> <p>Percentage of the dataset allocated for training. The remainder is used for validation.</p> <code>90</code> <p>Returns:</p> Name Type Description <code>x_train</code> <code>ndarray</code> <p>Input data matrix (features) for system identification (training).</p> <code>x_valid</code> <code>ndarray</code> <p>Input data matrix (features) for system validation (testing).</p> <code>y_train</code> <code>ndarray</code> <p>Output data corresponding to <code>x_train</code>.</p> <code>y_valid</code> <code>ndarray</code> <p>Output data corresponding to <code>x_valid</code>.</p> Notes <ul> <li>The system follows the nonlinear difference equation:</li> </ul> <p>y[k] = 0.4 * y[k-1]\u00b2 + 0.1 * y[k-1] * x1[k-1] + 0.6 * x2[k-1]          - 0.3 * x1[k-1] * x2[k-2] + e[k]</p> <p>where <code>e[k]</code> is either white or colored noise.</p> <ul> <li>The inputs <code>x1</code> and <code>x2</code> are independently sampled from a uniform distribution in   the range [-1, 1].</li> <li>The dataset is split into training and validation sets based on <code>train_percentage</code>   , ensuring a clear separation between them.</li> <li>The function returns <code>x_train</code> and <code>x_valid</code> as stacked arrays, where each row   represents a sample and each column corresponds to an input variable   (<code>x1</code> or <code>x2</code>).</li> </ul> Source code in <code>sysidentpy/utils/generate_data.py</code> <pre><code>def get_miso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90):\n    \"\"\"Generate synthetic data for Multiple-Input Single-Output system identification.\n\n    This function simulates input-output data for a nonlinear MISO system using two\n    input signals. The system output is influenced by both inputs and can be affected\n    by either white or colored (autoregressive) noise based on the `colored_noise` flag.\n\n    Parameters\n    ----------\n    n : int, optional (default=5000)\n        Number of samples to generate.\n    colored_noise : bool, optional (default=False)\n        If True, adds colored (autoregressive) noise to the system; otherwise, white\n        noise is used.\n    sigma : float, optional (default=0.05)\n        Standard deviation of the noise distribution.\n    train_percentage : int, optional (default=90)\n        Percentage of the dataset allocated for training. The remainder is used\n        for validation.\n\n    Returns\n    -------\n    x_train : ndarray\n        Input data matrix (features) for system identification (training).\n    x_valid : ndarray\n        Input data matrix (features) for system validation (testing).\n    y_train : ndarray\n        Output data corresponding to `x_train`.\n    y_valid : ndarray\n        Output data corresponding to `x_valid`.\n\n    Notes\n    -----\n    - The system follows the nonlinear difference equation:\n\n      y[k] = 0.4 * y[k-1]\u00b2 + 0.1 * y[k-1] * x1[k-1] + 0.6 * x2[k-1]\n             - 0.3 * x1[k-1] * x2[k-2] + e[k]\n\n      where `e[k]` is either white or colored noise.\n\n    - The inputs `x1` and `x2` are independently sampled from a uniform distribution in\n      the range [-1, 1].\n    - The dataset is split into training and validation sets based on `train_percentage`\n      , ensuring a clear separation between them.\n    - The function returns `x_train` and `x_valid` as stacked arrays, where each row\n      represents a sample and each column corresponds to an input variable\n      (`x1` or `x2`).\n\n    \"\"\"\n    if train_percentage &lt; 0 or train_percentage &gt; 100:\n        raise ValueError(\"train_percentage must be smaller than 100\")\n\n    mu = 0  # mean of the distribution\n    nu = np.random.normal(mu, sigma, n).T\n    e = np.zeros((n, 1))\n\n    lag = 2\n    if colored_noise is True:\n        for k in range(lag, len(e)):\n            e[k] = 0.8 * nu[k - 1] + nu[k]\n    else:\n        e = nu\n\n    x1 = np.random.uniform(-1, 1, n).T\n    x2 = np.random.uniform(-1, 1, n).T\n    y = np.zeros((n, 1))\n    theta = np.array([[0.4], [0.1], [0.6], [-0.3]])\n\n    lag = 2\n    for k in range(lag, len(e)):\n        y[k] = (\n            theta[0] * y[k - 1] ** 2\n            + theta[1] * y[k - 1] * x1[k - 1]\n            + theta[2] * x2[k - 1]\n            + theta[3] * x1[k - 1] * x2[k - 2]\n            + e[k]\n        )\n\n    split_data = int(len(x1) * (train_percentage / 100))\n    x1_train = x1[0:split_data].reshape(-1, 1)\n    x2_train = x2[0:split_data].reshape(-1, 1)\n    x1_valid = x1[split_data::].reshape(-1, 1)\n    x2_valid = x2[split_data::].reshape(-1, 1)\n\n    x_train = np.hstack([x1_train, x2_train])\n    x_valid = np.hstack([x1_valid, x2_valid])\n\n    y_train = y[0:split_data].reshape(-1, 1)\n    y_valid = y[split_data::].reshape(-1, 1)\n\n    return x_train, x_valid, y_train, y_valid\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.generate_data.get_siso_data","title":"<code>get_siso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90)</code>","text":"<p>Generate synthetic data for Single-Input Single-Output system identification.</p> <p>This function simulates input-output data for a SISO system based on a predefined nonlinear difference equation. The system output is affected by either white noise or colored noise (autoregressive noise) depending on the <code>colored_noise</code> flag.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>(int, optional(default=5000))</code> <p>Number of samples to generate.</p> <code>5000</code> <code>colored_noise</code> <code>(bool, optional(default=False))</code> <p>If True, adds colored (autoregressive) noise to the system; otherwise, white noise is used.</p> <code>False</code> <code>sigma</code> <code>(float, optional(default=0.05))</code> <p>Standard deviation of the noise distribution.</p> <code>0.05</code> <code>train_percentage</code> <code>(int, optional(default=90))</code> <p>Percentage of the dataset allocated for training. The rest is used for validation.</p> <code>90</code> <p>Returns:</p> Name Type Description <code>x_train</code> <code>ndarray</code> <p>Input data for system identification (training).</p> <code>x_valid</code> <code>ndarray</code> <p>Input data for system validation (testing).</p> <code>y_train</code> <code>ndarray</code> <p>Output data corresponding to <code>x_train</code>.</p> <code>y_valid</code> <code>ndarray</code> <p>Output data corresponding to <code>x_valid</code>.</p> Notes <ul> <li>The system follows the nonlinear difference equation:</li> </ul> <p>y[k] = 0.2 * y[k-1] + 0.1 * y[k-1] * x[k-1] + 0.9 * x[k-2] + e[k]</p> <p>where <code>e[k]</code> is either white or colored noise.</p> <ul> <li>The input <code>x</code> is uniformly sampled from the range [-1, 1].</li> <li>The dataset is split based on <code>train_percentage</code>, ensuring a clear separation   between training and validation data.</li> </ul> Source code in <code>sysidentpy/utils/generate_data.py</code> <pre><code>def get_siso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90):\n    r\"\"\"Generate synthetic data for Single-Input Single-Output system identification.\n\n    This function simulates input-output data for a SISO system based on a predefined\n    nonlinear difference equation. The system output is affected by either white noise\n    or colored noise (autoregressive noise) depending on the `colored_noise` flag.\n\n    Parameters\n    ----------\n    n : int, optional (default=5000)\n        Number of samples to generate.\n    colored_noise : bool, optional (default=False)\n        If True, adds colored (autoregressive) noise to the system; otherwise, white\n        noise is used.\n    sigma : float, optional (default=0.05)\n        Standard deviation of the noise distribution.\n    train_percentage : int, optional (default=90)\n        Percentage of the dataset allocated for training. The rest is used for\n        validation.\n\n    Returns\n    -------\n    x_train : ndarray\n        Input data for system identification (training).\n    x_valid : ndarray\n        Input data for system validation (testing).\n    y_train : ndarray\n        Output data corresponding to `x_train`.\n    y_valid : ndarray\n        Output data corresponding to `x_valid`.\n\n    Notes\n    -----\n    - The system follows the nonlinear difference equation:\n\n      y[k] = 0.2 * y[k-1] + 0.1 * y[k-1] * x[k-1] + 0.9 * x[k-2] + e[k]\n\n      where `e[k]` is either white or colored noise.\n\n    - The input `x` is uniformly sampled from the range [-1, 1].\n    - The dataset is split based on `train_percentage`, ensuring a clear separation\n      between training and validation data.\n\n    \"\"\"\n    if train_percentage &lt; 0 or train_percentage &gt; 100:\n        raise ValueError(\"train_percentage must be smaller than 100\")\n\n    mu = 0  # mean of the distribution\n    nu = np.random.normal(mu, sigma, n).T\n    e = np.zeros((n, 1))\n\n    lag = 2\n    if colored_noise is True:\n        for k in range(lag, len(e)):\n            e[k] = 0.8 * nu[k - 1] + nu[k]\n    else:\n        e = nu\n\n    x = np.random.uniform(-1, 1, n).T\n    y = np.zeros((n, 1))\n    theta = np.array([[0.2], [0.1], [0.9]])\n    lag = 2\n    for k in range(lag, len(x)):\n        y[k] = (\n            theta[0] * y[k - 1]\n            + theta[1] * y[k - 1] * x[k - 1]\n            + theta[2] * x[k - 2]\n            + e[k]\n        )\n\n    split_data = int(len(x) * (train_percentage / 100))\n\n    x_train = x[0:split_data].reshape(-1, 1)\n    x_valid = x[split_data::].reshape(-1, 1)\n\n    y_train = y[0:split_data].reshape(-1, 1)\n    y_valid = y[split_data::].reshape(-1, 1)\n\n    return x_train, x_valid, y_train, y_valid\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.build_input_matrix","title":"<code>build_input_matrix(x, xlag)</code>","text":"<p>Build the information matrix of input values.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree entered by the user.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>Input data used during the training phase.</p> required <code>xlag</code> <code>int, list of int, or nested list of int</code> <p>Input that can be a single integer, a list, or a nested list.</p> required <p>Returns:</p> Type Description <code>data = ndarray of floats</code> <p>The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def build_input_matrix(x, xlag: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the information matrix of input values.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree entered by the user.\n\n    Parameters\n    ----------\n    x : array-like\n        Input data used during the training phase.\n    xlag : int, list of int, or nested list of int\n        Input that can be a single integer, a list, or a nested list.\n\n    Returns\n    -------\n    data = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # Generate a lagged data which each column is a input or output\n    # related to its respective lags. With this approach we can create\n    # the information matrix by using all possible combination of\n    # the columns as a product in the iterations\n\n    n_inputs, xlag = _process_xlag(x, xlag)\n    x_lagged = _create_lagged_x(x, n_inputs, xlag)\n    constant = np.ones([x_lagged.shape[0], 1])\n    data = np.concatenate([constant, x_lagged], axis=1)\n    return data\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.build_input_output_matrix","title":"<code>build_input_output_matrix(x, y, xlag, ylag)</code>","text":"<p>Build the information matrix.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree entered by the user.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>Input data used on training phase.</p> required <code>y</code> <code>array - like</code> <p>Target data used on training phase.</p> required <code>xlag</code> <code>int, list of int, or nested list of int</code> <p>Input that can be a single integer, a list, or a nested list.</p> required <code>ylag</code> <code>int or list of int</code> <p>The range of lags according to user definition.</p> required <p>Returns:</p> Type Description <code>data = ndarray of floats</code> <p>The constructed information matrix.</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def build_input_output_matrix(x: np.ndarray, y: np.ndarray, xlag, ylag) -&gt; np.ndarray:\n    \"\"\"Build the information matrix.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree entered by the user.\n\n    Parameters\n    ----------\n    x : array-like\n        Input data used on training phase.\n    y : array-like\n        Target data used on training phase.\n    xlag : int, list of int, or nested list of int\n        Input that can be a single integer, a list, or a nested list.\n    ylag : int or list of int\n        The range of lags according to user definition.\n\n    Returns\n    -------\n    data = ndarray of floats\n        The constructed information matrix.\n\n    \"\"\"\n    # Generate a lagged data which each column is a input or output\n    # related to its respective lags. With this approach we can create\n    # the information matrix by using all possible combination of\n    # the columns as a product in the iterations\n    lagged_data = initial_lagged_matrix(x, y, xlag, ylag)\n    constant = np.ones([lagged_data.shape[0], 1])\n    data = np.concatenate([constant, lagged_data], axis=1)\n    return data\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.build_output_matrix","title":"<code>build_output_matrix(y, ylag)</code>","text":"<p>Build the information matrix of output values.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree entered by the user.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array - like</code> <p>Output data used during the training phase.</p> required <code>ylag</code> <code>int or list of int</code> <p>The range of lags according to user definition.</p> required <p>Returns:</p> Type Description <code>data = ndarray of floats</code> <p>The constructed output regressor matrix.</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def build_output_matrix(y, ylag: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the information matrix of output values.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree entered by the user.\n\n    Parameters\n    ----------\n    y : array-like\n        Output data used during the training phase.\n    ylag : int or list of int\n        The range of lags according to user definition.\n\n    Returns\n    -------\n    data = ndarray of floats\n        The constructed output regressor matrix.\n\n    \"\"\"\n    # Generate a lagged data which each column is an input or output\n    # related to its respective lags. With this approach we can create\n    # the information matrix by using all possible combination of\n    # the columns as a product in the iterations\n    ylag = _process_ylag(ylag)\n    y_lagged = _create_lagged_y(y, ylag)\n    constant = np.ones([y_lagged.shape[0], 1])\n    data = np.concatenate([constant, y_lagged], axis=1)\n    return data\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.count_model_regressors","title":"<code>count_model_regressors(*, x, y, xlag, ylag, model_type, basis_function, is_neural_narx=False)</code>","text":"<p>Compute the number of model regressors after applying the basis function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data.</p> required <code>y</code> <code>ndarray</code> <p>Output data.</p> required <code>xlag</code> <code>int</code> <p>Number of lags for input variables.</p> required <code>ylag</code> <code>int</code> <p>Number of lags for output variables.</p> required <code>model_type</code> <code>str</code> <p>The type of model ('NARMAX', 'NAR', 'NFIR', etc.).</p> required <code>basis_function</code> <code>object</code> <p>The basis function used for feature transformation.</p> required <code>is_neural_narx</code> <code>bool</code> <p>Whether to adjust for a neural NARX model, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of regressors/features after transformation.</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def count_model_regressors(\n    *,\n    x: np.ndarray,\n    y: np.ndarray,\n    xlag: int,\n    ylag: int,\n    model_type: str,\n    basis_function,\n    is_neural_narx: bool = False,\n) -&gt; int:\n    \"\"\"\n    Compute the number of model regressors after applying the basis function.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input data.\n    y : np.ndarray\n        Output data.\n    xlag : int\n        Number of lags for input variables.\n    ylag : int\n        Number of lags for output variables.\n    model_type : str\n        The type of model ('NARMAX', 'NAR', 'NFIR', etc.).\n    basis_function : object\n        The basis function used for feature transformation.\n    is_neural_narx : bool, optional\n        Whether to adjust for a neural NARX model, by default False.\n\n    Returns\n    -------\n    int\n        The number of regressors/features after transformation.\n    \"\"\"\n    data = build_lagged_matrix(x, y, xlag, ylag, model_type)\n    n_features = basis_function.fit(data[:3, :]).shape[1]\n    if is_neural_narx:\n        return n_features - 1\n\n    return n_features\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.get_build_io_method","title":"<code>get_build_io_method(model_type)</code>","text":"<p>Get info criteria method.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <p>The type of the model (NARMAX, NAR or NFIR)</p> required <p>Returns:</p> Type Description <code>build_method = Self</code> <p>Method to build the input-output matrix</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def get_build_io_method(model_type):\n    \"\"\"Get info criteria method.\n\n    Parameters\n    ----------\n    model_type = str\n        The type of the model (NARMAX, NAR or NFIR)\n\n    Returns\n    -------\n    build_method = Self\n        Method to build the input-output matrix\n    \"\"\"\n    build_matrix_options = {\n        \"NARMAX\": build_input_output_matrix,\n        \"NFIR\": build_input_matrix,\n        \"NAR\": build_output_matrix,\n    }\n    return build_matrix_options.get(model_type, None)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.initial_lagged_matrix","title":"<code>initial_lagged_matrix(x, y, xlag, ylag)</code>","text":"<p>Construct a matrix with lagged versions of input and output variables.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>Input data used during the training phase.</p> required <code>y</code> <code>array - like</code> <p>Output data used during the training phase.</p> required <code>xlag</code> <code>int, list of int, or nested list of int</code> <p>Input that can be a single integer, a list, or a nested list.</p> required <code>ylag</code> <code>int or list of int</code> <p>The range of lags according to user definition.</p> required <p>Returns:</p> Name Type Description <code>lagged_data</code> <code>ndarray</code> <p>The combined matrix containing lagged input and output values.</p> <p>Examples:</p> <p>If <code>xlag=2</code> and <code>ylag=2</code>, the resulting matrix will contain columns: Y[k-1], Y[k-2], x[k-1], x[k-2].</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def initial_lagged_matrix(x: np.ndarray, y: np.ndarray, xlag, ylag) -&gt; np.ndarray:\n    \"\"\"Construct a matrix with lagged versions of input and output variables.\n\n    Parameters\n    ----------\n    x : array-like\n        Input data used during the training phase.\n    y : array-like\n        Output data used during the training phase.\n    xlag : int, list of int, or nested list of int\n        Input that can be a single integer, a list, or a nested list.\n    ylag : int or list of int\n        The range of lags according to user definition.\n\n    Returns\n    -------\n    lagged_data : ndarray\n        The combined matrix containing lagged input and output values.\n\n    Examples\n    --------\n    If `xlag=2` and `ylag=2`, the resulting matrix will contain columns:\n    Y[k-1], Y[k-2], x[k-1], x[k-2].\n\n    \"\"\"\n    n_inputs, xlag = _process_xlag(x, xlag)\n    ylag = _process_ylag(ylag)\n    x_lagged = _create_lagged_x(x, n_inputs, xlag)\n    y_lagged = _create_lagged_y(y, ylag)\n    lagged_data = np.concatenate([y_lagged, x_lagged], axis=1)\n    return lagged_data\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.shift_column","title":"<code>shift_column(col_to_shift, lag)</code>","text":"<p>Shift an array by a specified lag, introducing zeros for missing values.</p> <p>Parameters:</p> Name Type Description Default <code>col_to_shift</code> <code>array-like of shape (n_samples,)</code> <p>The input or output time-series data to be lagged.</p> required <code>lag</code> <code>int</code> <p>The number of time steps to shift the data.</p> required <p>Returns:</p> Name Type Description <code>tmp_column</code> <code>ndarray of shape (n_samples, 1)</code> <p>The shifted array, where the first <code>lag</code> values are replaced with zeros.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; shift_column(y, 1)\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])\n</code></pre> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def shift_column(col_to_shift: np.ndarray, lag: int) -&gt; np.ndarray:\n    \"\"\"Shift an array by a specified lag, introducing zeros for missing values.\n\n    Parameters\n    ----------\n    col_to_shift : array-like of shape (n_samples,)\n        The input or output time-series data to be lagged.\n    lag : int\n        The number of time steps to shift the data.\n\n    Returns\n    -------\n    tmp_column : ndarray of shape (n_samples, 1)\n        The shifted array, where the first `lag` values are replaced with zeros.\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n    &gt;&gt;&gt; shift_column(y, 1)\n    array([[0],\n           [1],\n           [2],\n           [3],\n           [4]])\n\n    \"\"\"\n    if lag &lt; 0:\n        raise ValueError(\"lag must be non-negative\")\n\n    n_samples = col_to_shift.shape[0]\n    tmp_column = np.zeros((n_samples, 1))\n    if lag == 0:\n        return col_to_shift.copy()\n\n    if lag &lt; n_samples:\n        tmp_column[lag:, 0] = col_to_shift[: n_samples - lag, 0]\n    return tmp_column\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.lags.get_lag_from_regressor_code","title":"<code>get_lag_from_regressor_code(regressors)</code>","text":"<p>Get the maximum lag from array of regressors.</p> <p>Parameters:</p> Name Type Description Default <code>regressors</code> <code>ndarray of int</code> <p>Flattened list of input or output regressors.</p> required <p>Returns:</p> Name Type Description <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> Source code in <code>sysidentpy/utils/lags.py</code> <pre><code>def get_lag_from_regressor_code(regressors):\n    \"\"\"Get the maximum lag from array of regressors.\n\n    Parameters\n    ----------\n    regressors : ndarray of int\n        Flattened list of input or output regressors.\n\n    Returns\n    -------\n    max_lag : int\n        Maximum lag of list of regressors.\n\n    \"\"\"\n    lag_list = [int(i) for i in regressors.astype(\"str\") for i in [np.sum(int(i[2:]))]]\n    if len(lag_list) != 0:\n        return max(lag_list)\n\n    return 1\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.lags.get_max_lag_from_model_code","title":"<code>get_max_lag_from_model_code(model_code)</code>","text":"<p>Create a flattened array of input regressors.</p> <p>Parameters:</p> Name Type Description Default <code>model_code</code> <code>ndarray of int</code> <p>Model defined by the user to simulate.</p> required <p>Returns:</p> Name Type Description <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> Source code in <code>sysidentpy/utils/lags.py</code> <pre><code>def get_max_lag_from_model_code(model_code: List[int]) -&gt; int:\n    \"\"\"Create a flattened array of input regressors.\n\n    Parameters\n    ----------\n    model_code : ndarray of int\n        Model defined by the user to simulate.\n\n    Returns\n    -------\n    max_lag : int\n        Maximum lag of list of regressors.\n\n    \"\"\"\n    xlag_code = list_input_regressor_code(model_code)\n    ylag_code = list_output_regressor_code(model_code)\n    xlag = get_lag_from_regressor_code(xlag_code)\n    ylag = get_lag_from_regressor_code(ylag_code)\n    return max(xlag, ylag)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.lags.get_max_xlag","title":"<code>get_max_xlag(xlag=1)</code>","text":"<p>Get maximum value from various xlag structures.</p> <p>Parameters:</p> Name Type Description Default <code>xlag</code> <code>int, list of int, or nested list of int</code> <p>Input that can be a single integer, a list, or a nested list.</p> <code>1</code> <p>Returns:</p> Type Description <code>int</code> <p>Maximum value found.</p> Source code in <code>sysidentpy/utils/lags.py</code> <pre><code>def get_max_xlag(xlag: int = 1):\n    \"\"\"Get maximum value from various xlag structures.\n\n    Parameters\n    ----------\n    xlag : int, list of int, or nested list of int\n        Input that can be a single integer, a list, or a nested list.\n\n    Returns\n    -------\n    int\n        Maximum value found.\n    \"\"\"\n    if isinstance(xlag, int):  # Case 1: Single integer\n        return xlag\n\n    if isinstance(xlag, list):\n        # Case 2: Flat list of integers\n        if all(isinstance(i, int) for i in xlag):\n            return max(xlag)\n        # Case 3: Nested list\n        return max(chain.from_iterable(xlag))\n\n    raise ValueError(\"Unsupported data type for xlag\")\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.lags.get_max_ylag","title":"<code>get_max_ylag(ylag=1)</code>","text":"<p>Get maximum ylag.</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>ny</code> <code>list</code> <p>Maximum value of ylag.</p> Source code in <code>sysidentpy/utils/lags.py</code> <pre><code>def get_max_ylag(ylag: int = 1):\n    \"\"\"Get maximum ylag.\n\n    Parameters\n    ----------\n    ylag : ndarray of int\n        The range of lags according to user definition.\n\n    Returns\n    -------\n    ny : list\n        Maximum value of ylag.\n\n    \"\"\"\n    ny = np.max(list(chain.from_iterable([[ylag]])))\n    return ny\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.narmax_tools.regressor_code","title":"<code>regressor_code(*, X=None, xlag=2, ylag=2, model_type='NARMAX', model_representation=None, basis_function=Polynomial())</code>","text":"<p>Generate a regressor code based on the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input feature matrix.</p> <code>None</code> <code>xlag</code> <code>int</code> <p>The number of lags for the input features.</p> <code>2</code> <code>ylag</code> <code>int</code> <p>The number of lags for the target variable.</p> <code>2</code> <code>model_type</code> <code>str</code> <p>The type of model to be used. Default is \"NARMAX\".</p> <code>'NARMAX'</code> <code>model_representation</code> <code>str</code> <p>The model representation to be used.</p> <code>None</code> <code>basis_function</code> <code>object</code> <p>The basis function object used to transform the regressor space.</p> <code>Polynomial()</code> <p>Returns:</p> Name Type Description <code>encoding</code> <code>ndarray</code> <p>The generated regressor encoding.</p> Source code in <code>sysidentpy/utils/narmax_tools.py</code> <pre><code>@deprecated(\n    version=\"v0.6.0\",\n    future_version=\"v1.0.0\",\n    message=(\n        \" `regressor_code` is deprecated in v0.6.0 and will be removed in v1.0.0.\"\n        \" Use the `count_model_regressors` from sysidentpy.utils instead.\"\n    ),\n)\ndef regressor_code(\n    *,\n    X: Optional[np.ndarray] = None,\n    xlag: int = 2,\n    ylag: int = 2,\n    model_type: str = \"NARMAX\",\n    model_representation: Optional[str] = None,\n    basis_function: Polynomial = Polynomial(),\n) -&gt; np.ndarray:\n    \"\"\"Generate a regressor code based on the provided parameters.\n\n    Parameters\n    ----------\n    X : np.ndarray, optional\n        The input feature matrix.\n    xlag : int, optional\n        The number of lags for the input features.\n    ylag : int, optional\n        The number of lags for the target variable.\n    model_type : str, optional\n        The type of model to be used. Default is \"NARMAX\".\n    model_representation : str, optional\n        The model representation to be used.\n    basis_function : object, optional\n        The basis function object used to transform the regressor space.\n\n    Returns\n    -------\n    encoding : np.ndarray\n        The generated regressor encoding.\n    \"\"\"\n    if X is not None:\n        n_inputs = num_features(X)\n    else:\n        n_inputs = 1  # only used to create the regressor space base\n\n    encoding = RegressorDictionary(\n        xlag=xlag, ylag=ylag, model_type=model_type, basis_function=basis_function\n    ).regressor_space(n_inputs)\n\n    if not isinstance(basis_function, Polynomial) and basis_function.ensemble:\n        repetition = basis_function.n * 2\n        basis_code = np.sort(\n            np.tile(encoding[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        encoding = np.concatenate([encoding[1:], basis_code])\n    elif (\n        not isinstance(basis_function, Polynomial) and basis_function.ensemble is False\n    ):\n        repetition = basis_function.n * 2\n        encoding = np.sort(\n            np.tile(encoding[1:, :], (repetition, 1)),\n            axis=0,\n        )\n\n    if (\n        isinstance(basis_function, Polynomial)\n        and model_representation == \"neural_network\"\n    ):\n        return encoding[1:]\n    if isinstance(basis_function, Polynomial) and model_representation is None:\n        return encoding\n\n    return encoding\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.narmax_tools.set_weights","title":"<code>set_weights(*, static_function=True, static_gain=True, start=-0.01, stop=-5, num=50, base=2.71)</code>","text":"<p>Set log-spaced weights assigned to each objective in the MO optimization.</p> <p>Parameters:</p> Name Type Description Default <code>static_function</code> <code>bool</code> <p>Indicator for the presence of static function data. Default is True.</p> <code>True</code> <code>static_gain</code> <code>bool</code> <p>Indicator for the presence of static gain data. Default is True.</p> <code>True</code> <code>start</code> <code>float</code> <p>The starting exponent for the log-spaced weights. Default is -0.01.</p> <code>-0.01</code> <code>stop</code> <code>float</code> <p>The stopping exponent for the log-spaced weights. Default is -5.</p> <code>-5</code> <code>num</code> <code>int</code> <p>The number of weights to generate. Default is 50.</p> <code>50</code> <code>base</code> <code>float</code> <p>The base of the logarithm used to generate weights. Default is 2.71.</p> <code>2.71</code> <p>Returns:</p> Name Type Description <code>weights</code> <code>ndarray of floats</code> <p>An array containing the weights for each objective.</p> Notes <p>This method calculates the weights to be assigned to different objectives in multi-objective optimization. The choice of weights depends on the presence of static function and static gain data. If both are present, a set of weights for dynamic, gain, and static objectives is computed. If either static function or static gain is absent, a simplified set of weights is generated.</p> Source code in <code>sysidentpy/utils/narmax_tools.py</code> <pre><code>def set_weights(\n    *,\n    static_function: bool = True,\n    static_gain: bool = True,\n    start: float = -0.01,\n    stop: float = -5,\n    num: int = 50,\n    base: float = 2.71,\n) -&gt; np.ndarray:\n    \"\"\"Set log-spaced weights assigned to each objective in the MO optimization.\n\n    Parameters\n    ----------\n    static_function : bool, optional\n        Indicator for the presence of static function data. Default is True.\n    static_gain : bool, optional\n        Indicator for the presence of static gain data. Default is True.\n    start : float, optional\n        The starting exponent for the log-spaced weights. Default is -0.01.\n    stop : float, optional\n        The stopping exponent for the log-spaced weights. Default is -5.\n    num : int, optional\n        The number of weights to generate. Default is 50.\n    base : float, optional\n        The base of the logarithm used to generate weights. Default is 2.71.\n\n    Returns\n    -------\n    weights : ndarray of floats\n        An array containing the weights for each objective.\n\n    Notes\n    -----\n    This method calculates the weights to be assigned to different objectives in\n    multi-objective optimization. The choice of weights depends on the presence\n    of static function and static gain data. If both are present, a set of weights\n    for dynamic, gain, and static objectives is computed. If either static function\n    or static gain is absent, a simplified set of weights is generated.\n\n    \"\"\"\n    w1 = np.logspace(start=start, stop=stop, num=num, base=base)\n    if static_function is False or static_gain is False:\n        w2 = 1 - w1\n        return np.vstack([w1, w2])\n\n    w2 = w1[::-1]\n    w1_grid, w2_grid = np.meshgrid(w1, w2)\n    w3_grid = 1 - (w1_grid + w2_grid)\n    mask = w1_grid + w2_grid &lt;= 1\n    dynamic_weight = np.flip(w1_grid[mask])\n    gain_weight = np.flip(w2_grid[mask])\n    static_weight = np.flip(w3_grid[mask])\n    return np.vstack([dynamic_weight, gain_weight, static_weight])\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.narmax_tools.train_test_split","title":"<code>train_test_split(X, y, test_size=0.25)</code>","text":"<p>Split the time series dataset into training and testing sets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. Can be None if there are no features.</p> required <code>y</code> <code>ndarray</code> <p>The target vector.</p> required <code>test_size</code> <code>float</code> <p>The proportion of the dataset to include in the test split. Default is 0.25.</p> <code>0.25</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>ndarray or None</code> <p>The training set feature matrix, or None if x is None.</p> <code>X_test</code> <code>ndarray or None</code> <p>The testing set feature matrix, or None if x is None.</p> <code>y_train</code> <code>ndarray</code> <p>The training set target vector.</p> <code>y_test</code> <code>ndarray</code> <p>The testing set target vector.</p> Source code in <code>sysidentpy/utils/narmax_tools.py</code> <pre><code>def train_test_split(\n    X: Optional[np.ndarray], y: np.ndarray, test_size: float = 0.25\n) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray], np.ndarray, np.ndarray]:\n    \"\"\"Split the time series dataset into training and testing sets.\n\n    Parameters\n    ----------\n    X : np.ndarray, optional\n        The feature matrix. Can be None if there are no features.\n    y : np.ndarray\n        The target vector.\n    test_size : float, optional\n        The proportion of the dataset to include in the test split. Default is 0.25.\n\n    Returns\n    -------\n    X_train : np.ndarray or None\n        The training set feature matrix, or None if x is None.\n    X_test : np.ndarray or None\n        The testing set feature matrix, or None if x is None.\n    y_train : np.ndarray\n        The training set target vector.\n    y_test : np.ndarray\n        The testing set target vector.\n    \"\"\"\n    if not 0 &lt; test_size &lt; 1:\n        raise ValueError(\"test_size should be between 0 and 1\")\n\n    # Determine the split index\n    split_index = int(len(y) * (1 - test_size))\n\n    y_train, y_test = y[:split_index], y[split_index:]\n\n    if X is None:\n        return None, None, y_train, y_test\n\n    X_train, X_test = X[:split_index], X[split_index:]\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.plotting.plot_residues_correlation","title":"<code>plot_residues_correlation(data=None, *, figsize=(10, 6), n=100, style='default', facecolor='white', title='Residual Analysis', ylabel='Correlation')</code>","text":"<p>Plot the residual validation.</p> Source code in <code>sysidentpy/utils/plotting.py</code> <pre><code>def plot_residues_correlation(\n    data=None,\n    *,\n    figsize: Tuple[int, int] = (10, 6),\n    n: int = 100,\n    style: str = \"default\",\n    facecolor: str = \"white\",\n    title: str = \"Residual Analysis\",\n    ylabel: str = \"Correlation\",\n) -&gt; None:\n    \"\"\"Plot the residual validation.\"\"\"\n    plt.style.use(style)\n    plt.rcParams[\"axes.facecolor\"] = facecolor\n    _, ax = plt.subplots(figsize=figsize, facecolor=facecolor)\n    ax.plot(data[0][:n], color=\"#1f77b4\")\n    ax.axhspan(data[1], data[2], color=\"#ccd9ff\", alpha=0.5, lw=0)\n    ax.set_xlabel(\"Lag\", fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    ax.tick_params(labelsize=14)\n    ax.set_ylim([-1, 1])\n    ax.set_title(title, fontsize=18)\n    plt.show()\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.plotting.plot_results","title":"<code>plot_results(y, *, yhat, n=100, title='Free run simulation', xlabel='Samples', ylabel='y, $\\\\hat{y}$', data_color='#1f77b4', model_color='#ff7f0e', marker='o', model_marker='*', linewidth=1.5, figsize=(10, 6), style='default', facecolor='white')</code>","text":"<p>Plot the results of a simulation.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>True data values.</p> required <code>yhat</code> <code>ndarray</code> <p>Model predictions.</p> required <code>n</code> <code>int</code> <p>Number of samples to plot.</p> <code>100</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Free run simulation'</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis.</p> <code>'Samples'</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis.</p> <code>'y, $\\\\hat{y}$'</code> <code>data_color</code> <code>str</code> <p>Color for the data line.</p> <code>'#1f77b4'</code> <code>model_color</code> <code>str</code> <p>Color for the model line.</p> <code>'#ff7f0e'</code> <code>marker</code> <code>str</code> <p>Marker style for the data line.</p> <code>'o'</code> <code>model_marker</code> <code>str</code> <p>Marker style for the model line.</p> <code>'*'</code> <code>linewidth</code> <code>float</code> <p>Line width for both lines.</p> <code>1.5</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size (width, height).</p> <code>(10, 6)</code> <code>style</code> <code>str</code> <p>Matplotlib style.</p> <code>'default'</code> <code>facecolor</code> <code>str</code> <p>Figure facecolor.</p> <code>'white'</code> Source code in <code>sysidentpy/utils/plotting.py</code> <pre><code>def plot_results(\n    y: np.ndarray,\n    *,\n    yhat: np.ndarray,\n    n: int = 100,\n    title: str = \"Free run simulation\",\n    xlabel: str = \"Samples\",\n    ylabel: str = r\"y, $\\hat{y}$\",\n    data_color: str = \"#1f77b4\",\n    model_color: str = \"#ff7f0e\",\n    marker: str = \"o\",\n    model_marker: str = \"*\",\n    linewidth: float = 1.5,\n    figsize: Tuple[int, int] = (10, 6),\n    style: str = \"default\",\n    facecolor: str = \"white\",\n) -&gt; None:\n    \"\"\"Plot the results of a simulation.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        True data values.\n    yhat : np.ndarray\n        Model predictions.\n    n : int\n        Number of samples to plot.\n    title : str\n        Plot title.\n    xlabel : str\n        Label for the x-axis.\n    ylabel : str\n        Label for the y-axis.\n    data_color : str\n        Color for the data line.\n    model_color : str\n        Color for the model line.\n    marker : str\n        Marker style for the data line.\n    model_marker : str\n        Marker style for the model line.\n    linewidth : float\n        Line width for both lines.\n    figsize : Tuple[int, int]\n        Figure size (width, height).\n    style : str\n        Matplotlib style.\n    facecolor : str\n        Figure facecolor.\n\n    \"\"\"\n    if len(y) == 0 or len(yhat) == 0:\n        raise ValueError(\"Arrays must have at least 1 samples.\")\n\n    # Set Matplotlib style and figure properties\n    plt.style.use(style)\n    plt.rcParams[\"axes.facecolor\"] = facecolor\n\n    _, ax = plt.subplots(figsize=figsize, facecolor=facecolor)\n    ax.plot(\n        y[:n], c=data_color, alpha=1, marker=marker, label=\"Data\", linewidth=linewidth\n    )\n    ax.plot(\n        yhat[:n], c=model_color, marker=model_marker, label=\"Model\", linewidth=linewidth\n    )\n\n    # Customize plot properties\n    ax.set_title(title, fontsize=18)\n    ax.legend()\n    ax.tick_params(labelsize=14)\n    ax.set_xlabel(xlabel, fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    plt.show()\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.save_load.load_model","title":"<code>load_model(*, file_name='model', path=None)</code>","text":"<p>Load the model from file \"file_name.syspy\" located at path \"path\".</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <p>model to be loaded</p> <code>'model'</code> <code>path</code> <code>None</code> <p>Returns:</p> Name Type Description <code>model_loaded</code> <code>model loaded, as a variable, containing model and its attributes</code> Source code in <code>sysidentpy/utils/save_load.py</code> <pre><code>def load_model(\n    *,\n    file_name=\"model\",\n    path=None,\n):\n    \"\"\"Load the model from file \"file_name.syspy\" located at path \"path\".\n\n    Parameters\n    ----------\n    file_name: file name (str), along with .syspy extension of the file containing\n        model to be loaded\n    path: location where \"file_name.syspy\" is (optional).\n\n    Returns\n    -------\n    model_loaded: model loaded, as a variable, containing model and its attributes\n\n    \"\"\"\n    # Checking if path is provided\n    if path is not None:\n\n        # Composing file_name with path\n        file_name = os.path.join(path, file_name)\n\n    # Loading the model\n    with open(file_name, \"rb\") as fp:\n        model_loaded = pk.load(fp)\n\n    return model_loaded\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.save_load.save_model","title":"<code>save_model(*, model=None, file_name='model', path=None)</code>","text":"<p>Save the model \"model\" in folder \"folder\" using an extension .syspy.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>None</code> <code>file_name</code> <code>'model'</code> <code>path</code> <code>None</code> <p>Returns:</p> Type Description <code>file file_name.syspy located at \"path\", containing the estimated model.</code> Source code in <code>sysidentpy/utils/save_load.py</code> <pre><code>def save_model(\n    *,\n    model=None,\n    file_name=\"model\",\n    path=None,\n):\n    \"\"\"Save the model \"model\" in folder \"folder\" using an extension .syspy.\n\n    Parameters\n    ----------\n    model: the model variable to be saved\n    file_name: file name, along with .syspy extension\n    path: location where the model will be saved (optional)\n\n    Returns\n    -------\n    file file_name.syspy located at \"path\", containing the estimated model.\n\n    \"\"\"\n    if model is None:\n        raise TypeError(\"model cannot be None.\")\n\n    # Checking if path is provided\n    if path is not None:\n\n        # Composing file_name with path\n        file_name = os.path.join(path, file_name)\n\n    # Saving model\n    with open(file_name, \"wb\") as fp:\n        pk.dump(model, fp)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.simulation.get_index_from_regressor_code","title":"<code>get_index_from_regressor_code(regressor_code, model_code)</code>","text":"<p>Get the index of user regressor in regressor space.</p> <p>Took from: https://stackoverflow.com/questions/38674027/find-the-row-indexes-of-several-values-in-a-numpy-array/38674038#38674038</p> <p>Parameters:</p> Name Type Description Default <code>regressor_code</code> <code>ndarray of int</code> <p>Matrix codification of all possible regressors.</p> required <code>model_code</code> <code>ndarray of int</code> <p>Model defined by the user to simulate.</p> required <p>Returns:</p> Name Type Description <code>model_index</code> <code>ndarray of int</code> <p>Index of model code in the regressor space.</p> Source code in <code>sysidentpy/utils/simulation.py</code> <pre><code>def get_index_from_regressor_code(regressor_code: np.ndarray, model_code: List[int]):\n    \"\"\"Get the index of user regressor in regressor space.\n\n    Took from: https://stackoverflow.com/questions/38674027/find-the-row-indexes-of-several-values-in-a-numpy-array/38674038#38674038\n\n    Parameters\n    ----------\n    regressor_code : ndarray of int\n        Matrix codification of all possible regressors.\n    model_code : ndarray of int\n        Model defined by the user to simulate.\n\n    Returns\n    -------\n    model_index : ndarray of int\n        Index of model code in the regressor space.\n\n    \"\"\"\n    dims = regressor_code.max(0) + 1\n    model_index = np.where(\n        np.in1d(\n            np.ravel_multi_index(regressor_code.T, dims),\n            np.ravel_multi_index(model_code.T, dims),\n        )\n    )[0]\n    return model_index\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.simulation.list_input_regressor_code","title":"<code>list_input_regressor_code(model_code)</code>","text":"<p>Create a flattened array of input regressors.</p> <p>Parameters:</p> Name Type Description Default <code>model_code</code> <code>ndarray of int</code> <p>Model defined by the user to simulate.</p> required <p>Returns:</p> Name Type Description <code>regressor_code</code> <code>ndarray of int</code> <p>Flattened list of output regressors.</p> Source code in <code>sysidentpy/utils/simulation.py</code> <pre><code>def list_input_regressor_code(model_code: List[int]) -&gt; np.ndarray:\n    \"\"\"Create a flattened array of input regressors.\n\n    Parameters\n    ----------\n    model_code : ndarray of int\n        Model defined by the user to simulate.\n\n    Returns\n    -------\n    regressor_code : ndarray of int\n        Flattened list of output regressors.\n\n    \"\"\"\n    regressor_code = [\n        code for code in model_code.ravel() if (code != 0) and (str(code)[0] != \"1\")\n    ]\n    return np.asarray(regressor_code)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.simulation.list_output_regressor_code","title":"<code>list_output_regressor_code(model_code)</code>","text":"<p>Create a flattened array of output regressors.</p> <p>Parameters:</p> Name Type Description Default <code>model_code</code> <code>ndarray of int</code> <p>Model defined by the user to simulate.</p> required <p>Returns:</p> Name Type Description <code>regressor_code</code> <code>ndarray of int</code> <p>Flattened list of output regressors.</p> Source code in <code>sysidentpy/utils/simulation.py</code> <pre><code>def list_output_regressor_code(model_code: List[int]) -&gt; np.ndarray:\n    \"\"\"Create a flattened array of output regressors.\n\n    Parameters\n    ----------\n    model_code : ndarray of int\n        Model defined by the user to simulate.\n\n    Returns\n    -------\n    regressor_code : ndarray of int\n        Flattened list of output regressors.\n\n    \"\"\"\n    regressor_code = [\n        code for code in model_code.ravel() if (code != 0) and (str(code)[0] == \"1\")\n    ]\n\n    return np.asarray(regressor_code)\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/","title":"Create a NARX Neural Network","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#series-parallel-training-and-parallel-prediction","title":"Series-Parallel Training and Parallel prediction","text":"<p>Currently SysIdentPy support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier. We convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction. </p> <p>Series-Parallel allows us to use Pytorch directly for training, so we can use all the power of the Pytorch library to build our NARX Neural Network model! </p> <p></p> <p>The reader is referred to the following paper for a more in depth discussion about Series-Parallel and Parallel configurations regarding NARX neural network:</p> <p>Parallel Training Considered Harmful?: Comparing series-parallel and parallel feedforward network training</p>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#building-a-narx-neural-network","title":"Building a NARX Neural Network","text":"<p>First, just import the necessary packages</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>from torch import nn\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.utils.narmax_tools import regressor_code\nimport torch\n</code></pre> <pre><code>torch.cuda.is_available()\n</code></pre> <pre><code>False\n</code></pre> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</code></pre> <pre><code>Using cpu device\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#getting-the-data","title":"Getting the data","text":"<p>The data is generated by simulating the following model:</p> <p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\).</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\),</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#choosing-the-narx-parameters-loss-function-and-optimizer","title":"Choosing the NARX parameters, loss function and optimizer","text":"<p>One can create a NARXNN object and choose the maximum lag of both input and output for building the regressor matrix to serve as input of the network.</p> <p>In addition, you can choose the loss function, the optimizer, the optional parameters of the optimizer, the number of epochs.</p> <p>Because we built this feature on top of Pytorch, you can choose any of the loss function of the torch.nn.functional. Click here for a list of the loss functions you can use. You just need to pass the name of the loss function you want.</p> <p>Similarly, you can choose any of the optimizers of the torch.optim. Click here for a list of optimizers available.</p> <pre><code>basis_function = Polynomial(degree=1)\n\nnarx_net = NARXNN(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    device=\"cuda\",\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\neural_network\\narx_nn.py:156: UserWarning: No CUDA available. We set the device as CPU\n  self.device = self._check_cuda(device)\n</code></pre> <p>Since we have defined our NARXNN using \\(ylag=2\\), \\(xlag=2\\) and a polynomial basis function with \\(degree=1\\), we have a regressor matrix with 4 features. We need the size of the regressor matrix to build the layers of our network. Our input data(x_train) have only one feature, but since we are creating an NARX network, a regressor matrix is built behind the scenes with new features based on the xlag and ylag.</p> <p>If you need help finding how many regressors are created behind the scenes you can use the narmax_tools function regressor_code and take the size of the regressor code generated:</p> <pre><code>basis_function = Polynomial(degree=1)\n\nregressors = regressor_code(\n    X=x_train,\n    xlag=2,\n    ylag=2,\n    model_type=\"NARMAX\",\n    model_representation=\"neural_network\",\n    basis_function=basis_function,\n)\n</code></pre> <pre><code>n_features = regressors.shape[0]  # the number of features of the NARX net\nn_features\n</code></pre> <pre><code>4\n</code></pre> <pre><code>regressors\n</code></pre> <pre><code>array([[1001],\n       [1002],\n       [2001],\n       [2002]])\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#building-the-narx-neural-network","title":"Building the NARX Neural Network","text":"<p>The configuration of your network follows exactly the same pattern of a network defined in Pytorch. The following representing our NARX neural network.</p> <pre><code>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n</code></pre> <p>We have to pass the defined network to our NARXNN estimator.</p> <pre><code>narx_net.net = NARX()\n</code></pre> <pre><code>if device == \"cuda\":\n    narx_net.net.to(torch.device(\"cuda\"))\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#fit-and-predict","title":"Fit and Predict","text":"<p>Because we have a fit (for training) and predict function for Polynomial NARMAX, we create the same pattern for the NARX net. So, you only have to fit and predict using the following:</p> <pre><code>narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid)\n</code></pre> <pre><code>&lt;sysidentpy.neural_network.narx_nn.NARXNN at 0x19ddfff3890&gt;\n</code></pre> <pre><code>yhat = narx_net.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#results","title":"Results","text":"<p>Now we show the results</p> <pre><code>print(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>MSE:  0.00013103585914746256\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#note","title":"Note","text":"<p>If you built the net configuration before calling the NARXNN, you can just pass the model to the NARXNN as follows:</p> <pre><code>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n\n\nnarx_net2 = NARXNN(\n    net=NARX(),\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n\nnarx_net2.fit(X=x_train, y=y_train)\nyhat = narx_net2.predict(X=x_valid, y=y_valid)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>MSE:  0.00010086796658327408\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#note_1","title":"Note","text":"<p>Remember you can use n-steps-ahead prediction and NAR and NFIR models. Check how to use it in their respective examples.</p>"},{"location":"user-guide/how-to/create-custom-basis-function/","title":"Create a Custom Basis Function","text":"<p>This walkthrough mirrors the example provided in <code>examples/custom-basis-function.ipynb</code> and shows how easy it is to plug your own feature generator into SysIdentPy.</p> <p>In this how-to we extend <code>BaseBasisFunction</code> to create a harmonic feature map powered only by NumPy. The new class works exactly like the built-in basis functions, so it can be reused across any estimator that expects the same interface.</p>"},{"location":"user-guide/how-to/create-custom-basis-function/#requirements","title":"Requirements","text":"<p>You can reuse the project environment or install a minimal set of packages:</p> <pre><code>sysidentpy\nnumpy\nmatplotlib\n</code></pre> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>The example runs entirely on CPU.</li> <li>No additional datasets are required.</li> </ul>"},{"location":"user-guide/how-to/create-custom-basis-function/#generate-a-synthetic-dataset","title":"Generate a synthetic dataset","text":"<p>We build a simple SISO system with a strong sinusoidal component driven by the input. The first 1600 samples are used for training and the remainder for validation.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function.basis_function_base import BaseBasisFunction\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=50\n)\n</code></pre>"},{"location":"user-guide/how-to/create-custom-basis-function/#implement-the-custom-basis-function","title":"Implement the custom basis function","text":"<p>The new <code>HarmonicBasis</code> class only needs to implement <code>fit</code> and <code>transform</code>. Internally we create a matrix that contains the raw signals plus sine/cosine transforms for the requested harmonics. Because the class inherits from <code>BaseBasisFunction</code>, SysIdentPy can use it just like any built-in option.</p> <pre><code>class HarmonicBasis(BaseBasisFunction):\n    \"\"\"Map lagged regressors to sine/cosine features.\"\"\"\n\n    def __init__(self, harmonics=(1,), include_linear=True, scale=np.pi):\n        super().__init__(degree=1)\n        self.harmonics = tuple(harmonics)\n        self.include_linear = include_linear\n        self.scale = scale\n\n    def _build_matrix(self, data, predefined_regressors):\n        features = []\n        if self.include_linear:\n            features.append(data)\n        for harmonic in self.harmonics:\n            scaled = self.scale * harmonic * data\n            features.append(np.sin(scaled))\n            features.append(np.cos(scaled))\n        if not features:\n            raise ValueError(\"The basis needs at least one active transformation.\")\n        psi = np.hstack(features)\n        if predefined_regressors is not None:\n            idx = np.asarray(predefined_regressors, dtype=int)\n            psi = psi[:, idx]\n        return psi\n\n    def fit(\n        self,\n        data,\n        max_lag=1,\n        ylag=1,\n        xlag=1,\n        model_type=\"NARMAX\",\n        predefined_regressors=None,\n    ):\n        psi = self._build_matrix(data, predefined_regressors)\n        return psi[max_lag:, :]\n\n    def transform(\n        self,\n        data,\n        max_lag=1,\n        ylag=1,\n        xlag=1,\n        model_type=\"NARMAX\",\n        predefined_regressors=None,\n    ):\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/how-to/create-custom-basis-function/#train-with-the-custom-basis","title":"Train with the custom basis","text":"<p>The workflow is identical to every other example. We simply pass an instance of <code>HarmonicBasis</code> to <code>FROLS</code> and proceed with training, evaluation, and plotting.</p> <pre><code>basis_function = HarmonicBasis(harmonics=(1, 2, 3), include_linear=True, scale=np.pi)\n\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    order_selection=True,\n    n_info_values=20,\n    info_criteria=\"aic\",\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NARX\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid[model.max_lag:], yhat[model.max_lag:])\nprint(f\"RRSE (validation): {rrse:.4f}\")\n</code></pre> <pre><code>plot_results(\n    y=y_valid[model.max_lag:],\n    yhat=yhat[model.max_lag:],\n    n=400,\n    figsize=(12, 4),\n    title=\"Validation results with HarmonicBasis\",\n)\n</code></pre>"},{"location":"user-guide/how-to/create-custom-basis-function/#wrap-up","title":"Wrap up","text":"<p>With only a few lines of code we built a drop-in replacement for the stock basis functions. Any NumPy/SciPy/Scikit-Learn transformation can be exported to a class like <code>HarmonicBasis</code>, enabling you to reuse bespoke feature maps across every SysIdentPy estimator without touching the rest of your workflow.</p>"},{"location":"user-guide/how-to/save-and-load-models/","title":"Save and Load Models","text":"<p>Example created by Samir Angelo Milani Martins</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/how-to/save-and-load-models/#obtaining-the-model-using-frols","title":"Obtaining the model using FROLS.","text":"<pre><code>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.save_load import save_model, load_model\n\n# Generating 1 input 1 output sample data from a benchmark system\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nyhat = model.predict(X=x_valid, y=y_valid)\n\n# Gathering results\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n</code></pre>"},{"location":"user-guide/how-to/save-and-load-models/#saving-obtained-model-in-file-model_namesyspy","title":"Saving obtained model in file \"model_name.syspy\"","text":"<pre><code># save_model(model_variable, file_name.syspy, path (optional))\nsave_model(model=model, file_name=\"model_name.syspy\")\n</code></pre>"},{"location":"user-guide/how-to/save-and-load-models/#loading-model-and-checking-if-everything-went-smoothly","title":"Loading model and checking if everything went smoothly","text":"<pre><code># load_model(file_name.syspy, path (optional))\nloaded_model = load_model(file_name=\"model_name.syspy\")\n\n# Predicting output with loaded_model\nyhat_loaded = loaded_model.predict(X=x_valid, y=y_valid)\n\nr_loaded = pd.DataFrame(\n    results(\n        loaded_model.final_model,\n        loaded_model.theta,\n        loaded_model.err,\n        loaded_model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\n# Printing both: original model and model loaded from file\nprint(\"\\n Original model \\n\", r)\nprint(\"\\n Model Loaded from file \\n\", r_loaded)\n\n# Checking predictions from both: original model and model loaded from file\nif (yhat == yhat_loaded).all():\n    print(\"\\n Predictions are the same!\")\n\n# Ploting results\nplot_results(y=y_valid, yhat=yhat_loaded, n=1000)\n</code></pre> <pre><code> Original model \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56631676E-01\n1         y(k-1)  1.9999E-01  3.99688899E-02\n2  x1(k-1)y(k-1)  1.0000E-01  3.39940092E-03\n\n Model Loaded from file \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56631676E-01\n1         y(k-1)  1.9999E-01  3.99688899E-02\n2  x1(k-1)y(k-1)  1.0000E-01  3.39940092E-03\n\n Predictions are the same!\n</code></pre>"},{"location":"user-guide/how-to/set-specific-lags/","title":"Set Specific Lags","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Different ways to set the maximum lag for input and output</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n</code></pre>"},{"location":"user-guide/how-to/set-specific-lags/#setting-lags-using-a-range-of-values","title":"Setting lags using a range of values","text":"<p>If you pass int values for ylag and xlag, the lags are defined as a range from 1-ylag and 1-xlag. </p> <p>For example: if ylag=4 then the candidate regressors are \\(y_{k-1}, y_{k-2}, y_{k-3}, y_{k-4}\\)</p> <pre><code>basis_function = Polynomial(degree=1)\n\nmodel = FROLS(\n    order_selection=True,\n    ylag=4,\n    xlag=4,\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n</code></pre>"},{"location":"user-guide/how-to/set-specific-lags/#setting-specific-lags-using-lists","title":"Setting specific lags using lists","text":"<p>If you pass the ylag and xlag as a list, only the lags related to values in the list will be created. \\(y_{k-1}, y_{k-4}\\),  \\(x_{k-1}, x_{k-4}\\)</p> <pre><code>model = FROLS(\n    order_selection=True,\n    ylag=[1, 4],\n    xlag=[1, 4],\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n</code></pre>"},{"location":"user-guide/how-to/set-specific-lags/#setting-lags-for-multiple-input-single-output-miso-models","title":"Setting lags for Multiple Input Single Output (MISO) models","text":"<p>The following example shows how to define specific lags for each input. One should notice that we have to use a nested list in that case.</p> <pre><code># The example considers a model with 2 inputs, but you can use the same for any amount of inputs.\n\nmodel = FROLS(\n    order_selection=True,\n    ylag=[1, 4],\n    xlag=[[1, 2, 3, 4], [1, 7]],\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n# The lags defined are:\n# x1(k-1), x1(k-2), x(k-3), x(k-4)\n# x2(k-1), x1(k-7)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/","title":"Simulate Existing Models","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.simulation import SimulateNARMAX\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data","text":""},{"location":"user-guide/how-to/simulating-existing-models/#the-data-is-generated-by-simulating-the-following-model","title":"The data is generated by simulating the following model:","text":"<p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/#defining-the-model","title":"Defining the model","text":"<p>We already know that the generated data is a result of the model  \\(\ud835\udc66_\ud835\udc58=0.2\ud835\udc66_{\ud835\udc58\u22121}+0.1\ud835\udc66_{\ud835\udc58\u22121}\ud835\udc65_{\ud835\udc58\u22121}+0.9\ud835\udc65_{\ud835\udc58\u22122}+\ud835\udc52_\ud835\udc58\\) . Thus, we can create a model with those regressors follwing a codification pattern: - \\(0\\) is the constant term, - \\([1001] = y_{k-1}\\) - \\([100n] = y_{k-n}\\) - \\([200n] = x1_{k-n}\\) - \\([300n] = x2_{k-n}\\) - \\([1011, 1001] = y_{k-11} \\times y_{k-1}\\) - \\([100n, 100m] = y_{k-n} \\times y_{k-m}\\) - \\([12001, 1003, 1001] = x11_{k-1} \\times y_{k-3} \\times y_{k-1}\\) - and so on</p>"},{"location":"user-guide/how-to/simulating-existing-models/#important-note","title":"Important Note","text":"<p>The order of the arrays matter. </p> <p>If you use [2001, 1001], it will work, but [1001, 2001] will not (the regressor will be ignored). Always put the highest value first: - \\([2003, 2001]\\) works - \\([2001, 2003]\\) do not work</p> <p>We will handle this limitation in upcoming update.</p> <pre><code>s = SimulateNARMAX(\n    basis_function=Polynomial(), calculate_err=True, estimate_parameter=False\n)\n\n# the model must be a numpy array\nmodel = np.array(\n    [\n        [1001, 0],  # y(k-1)\n        [2001, 1001],  # x1(k-1)y(k-1)\n        [2002, 0],  # x1(k-2)\n    ]\n)\n# theta must be a numpy array of shape (n, 1) where n is the number of regressors\ntheta = np.array([[0.2, 0.9, 0.1]]).T\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/#simulating-the-model","title":"Simulating the model","text":"<p>After defining the model and theta we just need to use the simulate method.</p> <p>The simulate method returns the predicted values and the results where we can look at regressors, parameters and ERR values.</p> <pre><code>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0         y(k-1)  2.0000E-01  0.00000000E+00\n1        x1(k-2)  9.0000E-01  0.00000000E+00\n2  x1(k-1)y(k-1)  1.0000E-01  0.00000000E+00\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/how-to/simulating-existing-models/#options","title":"Options","text":"<p>You can set the <code>steps_ahead</code> to run the prediction/simulation:</p> <pre><code>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=1,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.001980394341423956\n</code></pre> <pre><code>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=21,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.0019394741034286557\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/#estimating-the-parameters","title":"Estimating the parameters","text":"<p>If you have only the model strucuture, you can create an object with <code>estimate_parameter=True</code> and choose the methed for estimation using <code>estimator</code>. In this case, you have to pass the training data for parameters estimation. </p> <p>When <code>estimate_parameter=True</code>, we also computate the ERR considering only the regressors defined by the user. </p> <pre><code>s = SimulateNARMAX(\n    basis_function=Polynomial(),\n    estimate_parameter=True,\n    estimator=LeastSquares(),\n    calculate_err=True,\n)\n\nyhat = s.simulate(\n    X_train=x_train,\n    y_train=y_train,\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    # theta will be estimated using the defined estimator\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0         y(k-1)  1.9999E-01  9.57682046E-01\n1        x1(k-2)  9.0003E-01  3.87716434E-02\n2  x1(k-1)y(k-1)  1.0009E-01  3.54306118E-03\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/how-to/use-extended-least-squares/","title":"Use Extended Least Squares","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>To use the Extended Least Squares (ELS) algorithm, set the <code>unbiased</code> parameter to <code>True</code> when defining the parameter estimator algorithm.</p> <pre><code>from sysidentpy.parameter_estimation import LeastSquares\n\nestimator = LeastSquares(unbiased=True)\n</code></pre> <p>The <code>unbiased</code> hyperparameter is available in all parameter estimation algorithms, with a default value of <code>False</code>.</p> <p>Additionally, the Extended Least Squares algorithm is iterative. In SysIdentPy, the default number of iterations is set to 20 (<code>uiter=20</code>), as studies in the literature indicate that the algorithm typically converges within 10 to 20 iterations. However, you can adjust this value to any number of iterations you prefer.</p> <pre><code>from sysidentpy.parameter_estimation import LeastSquares\n\nestimator = LeastSquares(unbiased=True, uiter=40)\n</code></pre> <p>A simple yet complete code example demonstrating parameter estimation using the Extended Least Squares (ELS) algorithm is shown below.</p> <p>(Simulated data is used for illustrative purposes.)</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model = FROLS(\n        order_selection=False,\n        n_terms=3,\n        ylag=2,\n        xlag=2,\n        elag=2,\n        info_criteria=\"aic\",\n        estimator=estimator,\n        basis_function=basis_function,\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\nplt.figure(figsize=(14, 4))\n\n# Compute and plot KDE for each parameter using scipy's gaussian_kde\nx_grid = np.linspace(np.min(parameters), np.max(parameters), 1000)\n\nfor i, label in enumerate([\"Parameter 1\", \"Parameter 2\", \"Parameter 3\"]):\n    kde = gaussian_kde(parameters[i, :])\n    plt.plot(x_grid, kde(x_grid), label=label)\n\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters (Matplotlib only)\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/NFIR-model-overview/","title":"NFIR Model - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>This example shows how to use SysIdentPy to build NFIR models. NFIR models are models with no output feedback. In other words, there are no \\(y(k-n_y)\\) regressors, only \\(x(k-n_x)\\).</p> <p>The NFIR model can be described as:</p> \\[     y_k= F^\\ell[x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_x \\in \\mathbb{N}\\) is the maximum lag for the system input; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>It is important to note that NFIR model size is generally significantly higher compared to their counterpart NARMAX model size. This drawback can be noted in linear models dimensionality and it leads to even more complex scenarios in the nonlinear case.</p> <p>So, if you are looking for parsimonious and compact models, consider using NARMAX models. However, when comparing NFIR and NARMAX models, it's generally more challenging to establish stability, particularly in a control-oriented context, with NARMAX models than with NFIR models.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import pandas as pd\nimport numpy as np\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.model_structure_selection import AOLS, FROLS\n</code></pre> <p>NFIR x NARMAX</p> <p>We will reproduce the same example provided in the \"presenting main functionality section\". In that example, we use a NARX model with xlag and ylag equal to 2 and a nonlinearity degree equal to 2. That resulted in a model with 3 regressors and a RRSE (validation metric) equal to \\(0.000184\\)</p> Regressors Parameters ERR x1(k-2) 9.0001E-01 9.57505011E-01 y(k-1) 2.0001E-01 3.89117583E-02 x1(k-1)y(k-1) 9.9992E-02 3.58319976E-03"},{"location":"user-guide/tutorials/NFIR-model-overview/#so-what-happens-if-i-use-a-nfir-model-with-the-same-configuration","title":"So, what happens if I use a NFIR model with the same configuration?","text":"<pre><code>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:618: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 6\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n0.2129700627690414\n  Regressors  Parameters             ERR\n0    x1(k-2)  8.9017E-01  9.55432286E-01\n</code></pre> <p>In the NFIR case, we got a model with 1 regressor, but with significantly worse RRSE (\\(0.21\\))</p> Regressors Parameters ERR x1(k-2) 8.9017E-01 9.55432286E-01 <p>So, to get a better NFIR model, we have to set a higher order model. In other words, we have to set a higher maximum lag to build the model.</p> <p>Lets set xlag=3.</p> <pre><code>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=3,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.04314951932710626\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9980E-01  9.55367779E-01\n1         x1(k-3)  1.7832E-01  3.94348076E-02\n2  x1(k-3)x1(k-1)  9.1104E-02  3.33315478E-03\n</code></pre> <p></p> <p>Now, the model have 3 regressors, but the RRSE is still worse (\\(0.04\\)).</p> Regressors Parameters ERR x1(k-2) 8.9980E-01 9.55367779E-01 x1(k-3) 1.7832E-01 3.94348076E-02 x1(k-3)x1(k-1) 9.1104E-02 3.33315478E-03 <p>Lets set xlag=5.</p> <pre><code>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=5,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.004209451216121233\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9978E-01  9.55485306E-01\n1         x1(k-3)  1.7979E-01  3.93181813E-02\n2  x1(k-3)x1(k-1)  8.9706E-02  3.33141271E-03\n3         x1(k-4)  3.5772E-02  1.54789285E-03\n4  x1(k-4)x1(k-2)  1.7615E-02  1.09675506E-04\n5  x1(k-4)x1(k-1)  1.7871E-02  1.13215338E-04\n6         x1(k-5)  6.9594E-03  6.23773643E-05\n7  x1(k-5)x1(k-1)  4.1353E-03  6.10794551E-06\n8  x1(k-5)x1(k-3)  3.4007E-03  3.98364615E-06\n9  x1(k-5)x1(k-2)  2.9798E-03  3.42693984E-06\n</code></pre> <p></p> <p>Now the RRSE is closer to the NARMAX model, but the NFIR model have 10 regressors. So, as mentioned before, the order of NFIR models is generally higher than NARMAX model to get comparable results.</p> Regressors Parameters ERR x1(k-2) 8.9978E-01 9.55485306E-01 x1(k-3) 1.7979E-01 3.93181813E-02 x1(k-3)x1(k-1) 8.9706E-02 3.33141271E-03 x1(k-4) 3.5772E-02 1.54789285E-03 x1(k-4)x1(k-2) 1.7615E-02 1.09675506E-04 x1(k-4)x1(k-1) 1.7871E-02 1.13215338E-04 x1(k-5) 6.9594E-03 6.23773643E-05 x1(k-5)x1(k-1) 4.1353E-03 6.10794551E-06 x1(k-5)x1(k-3) 3.4007E-03 3.98364615E-06 x1(k-5)x1(k-2) 2.9798E-03 3.42693984E-06 <p>xlag = 35</p> <pre><code>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=35,\n    n_info_values=200,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.0033427508754120074\n        Regressors  Parameters             ERR\n0          x1(k-2)  9.0009E-01  9.55386378E-01\n1          x1(k-3)  1.8001E-01  3.94178379E-02\n2   x1(k-3)x1(k-1)  9.0886E-02  3.32874170E-03\n3          x1(k-4)  3.5412E-02  1.54540871E-03\n4   x1(k-4)x1(k-2)  1.8743E-02  1.12751104E-04\n5   x1(k-4)x1(k-1)  1.8378E-02  1.13878189E-04\n6          x1(k-5)  6.7236E-03  6.27406151E-05\n7   x1(k-5)x1(k-1)  4.4974E-03  6.32909200E-06\n8   x1(k-5)x1(k-3)  3.5420E-03  3.95779051E-06\n9   x1(k-5)x1(k-2)  5.5656E-03  3.39231220E-06\n10         x1(k-6)  1.5079E-03  2.37202762E-06\n11  x1(k-6)x1(k-2)  1.8768E-03  3.65196792E-07\n12  x1(k-6)x1(k-3)  1.0685E-03  2.92529290E-07\n13  x1(k-6)x1(k-1)  6.3191E-04  2.55676107E-07\n</code></pre> <p></p> <p>Now the RRSE is closer to the NARMAX model, but the NFIR model have 14 regressors, with <code>RRSE=0.0033</code>. So, as you can check in these examples, the order of NFIR models is generally higher than NARMAX model to get comparable results, even trying different parameter estimation algorithms</p> Regressors Parameters ERR x1(k-2) 9.0009E-01 9.55386378E-01 x1(k-3) 1.8001E-01 3.94178379E-02 x1(k-3)x1(k-1) 9.0886E-02 3.32874170E-03 x1(k-4) 3.5412E-02 1.54540871E-03 x1(k-4)x1(k-2) 1.8743E-02 1.12751104E-04 x1(k-4)x1(k-1) 1.8378E-02 1.13878189E-04 x1(k-5) 6.7236E-03 6.27406151E-05 x1(k-5)x1(k-1) 4.4974E-03 6.32909200E-06 x1(k-5)x1(k-3) 3.5420E-03 3.95779051E-06 x1(k-5)x1(k-2) 5.5656E-03 3.39231220E-06 x1(k-6) 1.5079E-03 2.37202762E-06 x1(k-6)x1(k-2) 1.8768E-03 3.65196792E-07 x1(k-6)x1(k-3) 1.0685E-03 2.92529290E-07 x1(k-6)x1(k-1) 6.3191E-04 2.55676107E-07"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/","title":"PV Forecasting Benchmark","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#note","title":"Note","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained against neural prophet library.</p> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS and FROLS (with polynomial base function) methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p> <p>We will compare a 1-step ahead forecaster on solar irradiance data (that can be a proxy for solar PV production). The config of the neuralprophet model was taken from the neuralprophet documentation (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>The training will occur on 80% of the data, reserving the last 20% for the validation.</p> <p>Note: the data used in this example can be found in neuralprophet github.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</code></pre>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#frols","title":"FROLS","text":"<pre><code>raw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_pv_ghi/SanFrancisco_PV_GHI.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    ylag=24,\n    xlag=24,\n    info_criteria=\"bic\",\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    estimator=LeastSquares(),\n)\n\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(sysidentpy_loss)\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <pre><code>2204.333646698544\n</code></pre>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#metamss","title":"MetaMSS","text":"<pre><code>set_random_seed(42)\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_pv_ghi/SanFrancisco_PV_GHI.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nestimator = LeastSquares()\nsysidentpy_metamss = MetaMSS(\n    basis_function=basis_function,\n    xlag=24,\n    ylag=24,\n    estimator=estimator,\n    maxiter=10,\n    steps_ahead=1,\n    n_agents=15,\n    loss_func=\"metamss_loss\",\n    model_type=\"NARMAX\",\n    random_state=42,\n)\nsysidentpy_metamss.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\n\nyhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1)\nmetamss_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <pre><code>2157.7700127350877\n</code></pre>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#aols","title":"AOLS","text":"<pre><code>set_random_seed(42)\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_pv_ghi/SanFrancisco_PV_GHI.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(\n    ylag=24, xlag=24, k=2, L=1, model_type=\"NARMAX\", basis_function=basis_function\n)\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <pre><code>2361.561682547365\n</code></pre>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\n\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_pv_ghi/SanFrancisco_PV_GHI.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n    n_lags=24,\n    ar_sparsity=0.5,\n    # num_hidden_layers = 2,\n    # d_hidden=20,\n)\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\n\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\n# fig = m.plot(forecast)\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n</code></pre> <pre><code>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\nINFO: nprophet.utils - set_auto_seasonalities: Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 32\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 7\n 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 87/100 [00:00&lt;00:00, 644.82it/s]\nINFO: nprophet - _lr_range_test: learning rate range test found optimal lr: 1.23E-01\nEpoch[7/7]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02&lt;00:00,  2.58it/s, SmoothL1Loss=0.00415, MAE=58.8, RegLoss=0.0112]\nINFO: nprophet - _evaluate: Validation metrics:    SmoothL1Loss    MAE\n1         0.003 48.746\n\n\n4642.234763049609\n</code></pre> <pre><code>plt.plot(forecast[\"y\"][-104:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-104:], \"k*-\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x2618e76ebe0&gt;]\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/","title":"Air Passenger Benchmark","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#note","title":"Note","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained using the sktime and neural prophet library.</p> <p>From sktime, the following models will be used:</p> <ul> <li> <p>AutoARIMA</p> </li> <li> <p>BATS</p> </li> <li> <p>TBATS</p> </li> <li> <p>Exponential Smoothing</p> </li> <li> <p>Prophet</p> </li> <li> <p>AutoETS</p> </li> </ul> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS, FROLS (with polynomial base function) and NARXNN methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport scipy.signal.signaltools\n\n\ndef _centered(arr, newsize):\n    # Return the center newsize portion of the array.\n    newsize = np.asarray(newsize)\n    currsize = np.array(arr.shape)\n    startind = (currsize - newsize) // 2\n    endind = startind + newsize\n    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n    return arr[tuple(myslice)]\n\n\nscipy.signal.signaltools._centered = _centered\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom torch import nn\n\n# from sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.ets import AutoETS\nfrom sktime.forecasting.arima import ARIMA, AutoARIMA\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.fbprophet import Prophet\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.bats import BATS\n\n# from sktime.forecasting.model_evaluation import evaluate\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.performance_metrics.forecasting import mean_squared_error\nfrom sktime.utils.plotting import plot_series\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_series\\_check.py:43: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:45: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:46: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\utils\\validation\\series.py:18: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\forecasting\\base\\_fh.py:18: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  RELATIVE_TYPES = (pd.Int64Index, pd.RangeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\forecasting\\base\\_fh.py:19: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  ABSOLUTE_TYPES = (pd.Int64Index, pd.RangeIndex, pd.DatetimeIndex, pd.PeriodIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#air-passengers-data","title":"Air passengers data","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</code></pre> <pre><code>121 23\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#results","title":"Results","text":"No. Package Mean Squared Error 1 SysIdentPy (Neural Model) 316.54 2 SysIdentPy (MetaMSS) 450.99 3 SysIdentPy (AOLS) 476.64 4 NeuralProphet 501.24 5 SysIdentPy (FROLS) 805.95 6 Exponential Smoothing 910.52 7 Prophet 1186.00 8 AutoArima 1714.47 9 Manual Arima 2085.42 10 ETS 2590.05 11 BATS 7286.64 12 TBATS 7448.43"},{"location":"user-guide/tutorials/air-passenger-benchmark/#sysidentpy-frols","title":"SysIdentPy FROLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    ylag=13,  # the lags for all models will be 13\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\nsysidentpy.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(y=y_test, forecast_horizon=23)\nfrols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:619: UserWarning:\n\nn_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 14\n\n\n\n805.9521186338106\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#sysidentpy-aols","title":"SysIdentPy AOLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\ndf_train, df_test = temporal_train_test_split(y, test_size=23)\ndf_train = df_train.reset_index()\ndf_train.columns = [\"ds\", \"y\"]\ndf_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"].astype(str))\ndf_test = df_test.reset_index()\ndf_test.columns = [\"ds\", \"y\"]\ndf_test[\"ds\"] = pd.to_datetime(df_test[\"ds\"].astype(str))\n\nsysidentpy_AOLS = AOLS(\n    ylag=13, k=2, L=1, model_type=\"NAR\", basis_function=basis_function\n)\nsysidentpy_AOLS.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\nplot_results(y=y_test[sysidentpy_AOLS.max_lag :], yhat=yhat[sysidentpy_AOLS.max_lag :])\n</code></pre> <pre><code>476.64996316992523\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#sysidentpy-metamss","title":"SysIdentPy MetaMSS","text":"<pre><code>set_random_seed(42)\n\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nsysidentpy_metamss = MetaMSS(\n    basis_function=basis_function, ylag=13, model_type=\"NAR\", test_size=0.17\n)\nsysidentpy_metamss.fit(y=y_train)\n\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\n\nyhat = sysidentpy_metamss.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\nmetamss_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\nplot_results(\n    y=y_test[sysidentpy_metamss.max_lag :], yhat=yhat[sysidentpy_metamss.max_lag :]\n)\n</code></pre> <pre><code>450.992127624293\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#sysidentpy-neural-narx","title":"SysIdentPy Neural NARX","text":"<pre><code>import torch\n\ntorch.manual_seed(42)\n\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=36)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nx_train = np.zeros_like(y_train)\nx_test = np.zeros_like(y_test)\n\n\nclass NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(13, 20)\n        self.lin2 = nn.Linear(20, 20)\n        self.lin3 = nn.Linear(20, 20)\n        self.lin4 = nn.Linear(20, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.relu(z)\n        z = self.lin2(z)\n        z = self.relu(z)\n        z = self.lin3(z)\n        z = self.relu(z)\n        z = self.lin4(z)\n        return z\n\n\nnarx_net = NARXNN(\n    net=NARX(),\n    ylag=13,\n    model_type=\"NAR\",\n    basis_function=Polynomial(degree=1),\n    epochs=900,\n    verbose=False,\n    learning_rate=2.5e-02,\n    optim_params={},  # optional parameters of the optimizer\n)\n\nnarx_net.fit(y=y_train)\nyhat = narx_net.predict(y=y_test, forecast_horizon=23)\nnarxnet_loss = loss(\n    pd.Series(y_test.flatten()[narx_net.max_lag :]),\n    pd.Series(yhat.flatten()[narx_net.max_lag :]),\n)\nprint(narxnet_loss)\nplot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :])\n</code></pre> <pre><code>316.54086775668776\n</code></pre> <pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</code></pre> <pre><code>121 23\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#exponential-smoothing","title":"Exponential Smoothing","text":"<pre><code>es = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nes.fit(y_train)\ny_pred_es = es.predict(fh)\n\nplot_series(y_test, y_pred_es, labels=[\"y_test\", \"y_pred\"])\nes_loss = loss(y_test, y_pred_es)\nes_loss\n</code></pre> <pre><code>910.462659260655\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#autoets","title":"AutoETS","text":"<pre><code>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nets = AutoETS(auto=True, sp=12, n_jobs=-1)\nets.fit(y_train)\ny_pred_ets = ets.predict(fh)\n\nplot_series(y_test, y_pred_ets, labels=[\"y_test\", \"y_pred\"])\nets_loss = loss(y_test, y_pred_ets)\nets_loss\n</code></pre> <pre><code>1739.117296439066\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#autoarima","title":"AutoArima","text":"<pre><code>auto_arima = AutoARIMA(sp=12, suppress_warnings=True)\ny = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nauto_arima.fit(y_train)\ny_pred_auto_arima = auto_arima.predict(fh)\n\nplot_series(y_test, y_pred_auto_arima, labels=[\"y_test\", \"y_pred\"])\nautoarima_loss = loss(y_test, y_pred_auto_arima)\nautoarima_loss\n</code></pre> <pre><code>1714.4753226965322\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#arima","title":"Arima","text":"<pre><code>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nmanual_arima = ARIMA(\n    order=(13, 1, 0), suppress_warnings=True\n)  # seasonal_order=(0, 1, 0, 12)\nmanual_arima.fit(y_train)\ny_pred_manual_arima = manual_arima.predict(fh)\nplot_series(y_test, y_pred_manual_arima, labels=[\"y_test\", \"y_pred\"])\nmanualarima_loss = loss(y_test, y_pred_manual_arima)\nmanualarima_loss\n</code></pre> <pre><code>2085.425167938668\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#bats","title":"BATS","text":"<pre><code>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nbats = BATS(sp=12, use_trend=True, use_box_cox=False)\nbats.fit(y_train)\ny_pred_bats = bats.predict(fh)\n\nplot_series(y_test, y_pred_bats, labels=[\"y_test\", \"y_pred\"])\nbats_loss = loss(y_test, y_pred_bats)\nbats_loss\n</code></pre> <pre><code>7286.6484525676415\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#tbats","title":"TBATS","text":"<pre><code>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ntbats = TBATS(sp=12, use_trend=True, use_box_cox=False)\ntbats.fit(y_train)\ny_pred_tbats = tbats.predict(fh)\nplot_series(y_test, y_pred_tbats, labels=[\"y_test\", \"y_pred\"])\ntbats_loss = loss(y_test, y_pred_tbats)\ntbats_loss\n</code></pre> <pre><code>7448.434672875093\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#prophet","title":"Prophet","text":"<pre><code>set_random_seed(42)\n\ny = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nz = y.copy()\nz = z.to_timestamp(freq=\"M\")\nz_train, z_test = temporal_train_test_split(z, test_size=23)\n\n\nprophet = Prophet(\n    seasonality_mode=\"multiplicative\",\n    n_changepoints=int(len(y_train) / 12),\n    add_country_holidays={\"country_name\": \"Germany\"},\n    yearly_seasonality=True,\n    weekly_seasonality=False,\n    daily_seasonality=False,\n)\nprophet.fit(z_train)\ny_pred_prophet = prophet.predict(fh.to_relative(cutoff=y_train.index[-1]))\n\ny_pred_prophet.index = y_test.index\nplot_series(y_test, y_pred_prophet, labels=[\"y_test\", \"y_pred\"])\nprophet_loss = loss(y_test, y_pred_prophet)\nprophet_loss\n</code></pre> <pre><code>1186.0045566050442\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\n\ndf = pd.read_csv(r\".\\datasets\\air_passengers.csv\")\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\ndf_train = df.iloc[:-23, :].copy()\ndf_test = df.iloc[-23:, :].copy()\n\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\n\nmetrics = m.fit(df_train, freq=\"MS\")\n\nfuture = m.make_future_dataframe(\n    df_train, periods=23, n_historic_predictions=len(df_train)\n)\n\nforecast = m.predict(future)\nplt.plot(forecast[\"yhat1\"].values[-23:])\nplt.plot(df_test[\"y\"].values)\nneuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values)\nneuralprophet_loss\n</code></pre> <pre><code>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n\n\n11-21 20:57:55 - WARNING - Parts of code may break if using other than daily data.\n\n\nINFO: nprophet.utils - set_auto_seasonalities: Disabling weekly seasonality. Run NeuralProphet with weekly_seasonality=True to override this.\n\n\n11-21 20:57:55 - INFO - Disabling weekly seasonality. Run NeuralProphet with weekly_seasonality=True to override this.\n\n\nINFO: nprophet.utils - set_auto_seasonalities: Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n\n\n11-21 20:57:55 - INFO - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n\n\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 8\n\n\n11-21 20:57:55 - INFO - Auto-set batch_size to 8\n\n\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 264\n\n\n11-21 20:57:55 - INFO - Auto-set epochs to 264\n\n\n 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 83/100 [00:00&lt;00:00, 1034.46it/s]\nINFO: nprophet - _lr_range_test: learning rate range test found optimal lr: 1.87E-01\n\n\n11-21 20:57:55 - INFO - learning rate range test found optimal lr: 1.87E-01\n\n\nEpoch[264/264]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 264/264 [00:03&lt;00:00, 66.42it/s, SmoothL1Loss=0.000325, MAE=6.38, RegLoss=0]\n\n\n\n\n\n501.24794023767436\n</code></pre> <pre><code>results = {\n    \"Exponential Smoothing\": es_loss,\n    \"ETS\": ets_loss,\n    \"AutoArima\": autoarima_loss,\n    \"Manual Arima\": manualarima_loss,\n    \"BATS\": bats_loss,\n    \"TBATS\": tbats_loss,\n    \"Prophet\": prophet_loss,\n    \"SysIdentPy (Polynomial Model)\": frols_loss,\n    \"SysIdentPy (Neural Model)\": narxnet_loss,\n    \"SysIdentPy (AOLS)\": aols_loss,\n    \"SysIdentPy (MetaMSS)\": metamss_loss,\n    \"NeuralProphet\": neuralprophet_loss,\n}\n\nsorted(results.items(), key=lambda result: result[1])\n</code></pre> <pre><code>[('SysIdentPy (Neural Model)', 316.54086775668776),\n ('SysIdentPy (MetaMSS)', 450.992127624293),\n ('SysIdentPy (AOLS)', 476.64996316992523),\n ('NeuralProphet', 501.24794023767436),\n ('SysIdentPy (Polynomial Model)', 805.9521186338106),\n ('Exponential Smoothing', 910.462659260655),\n ('Prophet', 1186.0045566050442),\n ('AutoArima', 1714.4753226965322),\n ('ETS', 1739.117296439066),\n ('Manual Arima', 2085.425167938668),\n ('BATS', 7286.6484525676415),\n ('TBATS', 7448.434672875093)]\n</code></pre>"},{"location":"user-guide/tutorials/aols-overview/","title":"AOLS Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <pre><code>import pandas as pd\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.model_structure_selection import AOLS\n\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre> <pre><code>basis_function = Polynomial(degree=2)\nmodel = AOLS(xlag=3, ylag=3, k=5, L=1, basis_function=basis_function)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS at 0x25cd3b406d0&gt;\n</code></pre> <pre><code>yhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0018996279285613828\n      Regressors   Parameters             ERR\n0         y(k-1)   1.9999E-01  0.00000000E+00\n1        x1(k-2)   9.0003E-01  0.00000000E+00\n2  x1(k-1)y(k-1)   9.9954E-02  0.00000000E+00\n3  x1(k-3)y(k-1)  -2.1442E-04  0.00000000E+00\n4      x1(k-1)^2   3.3714E-04  0.00000000E+00\n</code></pre> <pre><code>plot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/tutorials/basis-function-overview/","title":"Basis Functions - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>This notebook is not intended to find the best possible models for system identification. Instead, it serves as a simple demonstration of the basis functions available in SysIdentPy. The goal is to showcase each basis function with minimal code to illustrate how to use them within the SysIdentPy framework.</p> <p>We use basic settings for model structure selection and parameter estimation, but for real-world applications, you may need to fine-tune the hyperparameters and explore more advanced methods to achieve optimal results.</p> <p>For more details on SysIdentPy and how to fully leverage its capabilities, please refer to the official documentation and the companion book.</p>"},{"location":"user-guide/tutorials/basis-function-overview/#introduction","title":"Introduction","text":"<p>In this example, we'll explore how to use SysIdentPy to apply various basis functions for system identification and model structure selection. We'll use a simulated dataset and apply the FROLS algorithm with different basis functions. Each basis function will be evaluated, and results will be plotted to compare their performance.</p> <p>You can learn more about SysIdentPy's basis functions by referring to the official documentation.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy import basis_function\n</code></pre>"},{"location":"user-guide/tutorials/basis-function-overview/#generating-simulated-data","title":"Generating Simulated Data","text":"<p>We begin by generating simulated Single-Input Single-Output (SISO) data using the get_siso_data function. This utility allows us to create realistic data for system identification tasks. For more details about how to customize the data generation process, visit the data utilities documentation.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n</code></pre>"},{"location":"user-guide/tutorials/basis-function-overview/#basis-functions-in-sysidentpy","title":"Basis Functions in SysIdentPy","text":"<p>SysIdentPy provides several basis functions that can be used in system identification. Basis functions transform input data into a feature space, enabling the identification of non-linear systems.</p> <p>The following code snippet dynamically loads and instantiates each available basis function. You can explore the full list of basis functions available in SysIdentPy by visiting the basis functions documentation.</p> <pre><code>basis_function.__all__\n</code></pre> <pre><code>['Bernstein',\n 'Bilinear',\n 'Fourier',\n 'Hermite',\n 'HermiteNormalized',\n 'Laguerre',\n 'Legendre',\n 'Polynomial']\n</code></pre> <pre><code>import inspect\nfrom sysidentpy import basis_function\n\nfor basis_name, bf in inspect.getmembers(basis_function):\n    if inspect.isclass(bf):\n        estimator = LeastSquares()\n        model = FROLS(\n            order_selection=True,\n            n_info_values=15,\n            ylag=2,\n            xlag=2,\n            info_criteria=\"aic\",\n            estimator=estimator,\n            err_tol=None,\n            basis_function=bf(degree=5),\n        )\n\n        model.fit(X=x_train, y=y_train)\n        yhat = model.predict(X=x_valid, y=y_valid)\n\n        plot_results(\n            y=y_valid,\n            yhat=yhat,\n            n=100,\n            title=f\"{basis_name}\",\n            xlabel=\"Samples\",\n            ylabel=r\"y, $\\hat{y}$\",\n            data_color=\"#1f77b4\",\n            model_color=\"#ff7f0e\",\n            marker=\"o\",\n            model_marker=\"*\",\n            linewidth=1.5,\n            figsize=(10, 6),\n            style=\"seaborn-v0_8-notebook\",\n            facecolor=\"white\",\n        )\n</code></pre> <pre><code>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning:  `bias` and `n` are deprecated in 0.5.0 and will be removed in 1.0.0. Use `include_bias` and `degree`, respectively, instead.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"user-guide/tutorials/coupled-eletric-device/","title":"Coupled Eletric Device","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The CE8 coupled electric drives dataset - Nonlinear Benchmark presents a compelling use case for demonstrating the performance of SysIdentPy. This system involves two electric motors driving a pulley with a flexible belt, creating a dynamic environment ideal for testing system identification tools.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#system-overview","title":"System Overview","text":"<p>The CE8 system, illustrated in Figure 1, features: - Two Electric Motors: These motors independently control the tension and speed of the belt, providing symmetrical control around zero. This enables both clockwise and counterclockwise movements. - Pulley Mechanism: The pulley is supported by a spring, introducing a lightly damped dynamic mode that adds complexity to the system. - Speed Control Focus: The primary focus is on the speed control system. The pulley\u2019s angular speed is measured using a pulse counter, which is insensitive to the direction of the velocity.</p> <p></p> <p>Figure 1. CE8 system design.</p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#sensor-and-filtering","title":"Sensor and Filtering","text":"<p>The measurement process involves: - Pulse Counter: This sensor measures the angular speed of the pulley without regard to the direction. - Analogue Low Pass Filtering: This reduces high-frequency noise, followed by anti-aliasing filtering to prepare the signal for digital processing. The dynamic effects are mainly influenced by the electric drive time constants and the spring, with the low pass filtering having a minimal impact on the output.</p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#sota-results","title":"SOTA Results","text":"<p>SysIdentPy can be used to build robust models for identifying and modeling the complex dynamics of the CE8 system. The performance will be compared against a benchmark provided by Max D. Champneys, Gerben I. Beintema, Roland T\u00f3th, Maarten Schoukens, and Timothy J. Rogers -\u00a0Baselines for Nonlinear Benchmarks,\u00a0Workshop on Nonlinear System Identification Benchmarks, 2024.</p> <p></p> <p>The benchmark evaluate the average metric between the two experiments. That's why the SOTA method do not have the better metric for <code>test 1</code>, but it is still the best overall.  The goal of this case study is not only to showcase the robustness of SysIdentPy but also provides valuable insights into its practical applications in real-world dynamic systems.</p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/coupled-eletric-device/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the CE8 coupled electric drives dataset. This example showcases the robust performance of SysIdentPy in modeling and identifying complex dynamic systems. The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for CE8 system.</p> <p>This practical example will help users understand how to effectively utilize SysIdentPy for their own system identification tasks, leveraging its advanced features to handle the complexities of real-world dynamic systems. Let's dive into the code and explore the capabilities of SysIdentPy.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.CED(atleast_2d=True)\ndata_train_1, data_train_2 = train_val\ndata_test_1, data_test_2 = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation GerbenBeintema - nonlinear_benchmarks: The official dataload for nonlinear benchmark datasets to check the details of how to use it.</p> <p>The following plot detail the training and testing data of both experiments. Here we are trying to get two models, one for each experiment, that have a better performance than the mentioned baselines.</p> <pre><code>plt.plot(data_train_1.u)\nplt.plot(data_train_1.y)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(data_test_1.u)\nplt.plot(data_test_1.y)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(data_train_2.u)\nplt.plot(data_train_2.y)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(data_test_2.u)\nplt.plot(data_test_2.y)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#results","title":"Results","text":"<p>First, we will set the exactly same configuration to built models for both experiments. We can have better models by optimizing the configurations individually, but we will start simple.</p> <p>A basic configuration of FROLS using a polynomial basis function with degree equal 2 is defined. The information criteria will be the default one, the <code>aic</code>. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example.</p> <p>Model for experiment 1:</p> <pre><code>y_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n    n_info_values=120,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\parameter_estimation\\estimators.py:75: UserWarning: Psi matrix might have linearly dependent rows.Be careful and check your data\n  self._check_linear_dependence_rows(psi)\n</code></pre> <p></p> <p>Model for experiment 2:</p> <pre><code>y_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n    n_info_values=120,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\parameter_estimation\\estimators.py:75: UserWarning: Psi matrix might have linearly dependent rows.Be careful and check your data\n  self._check_linear_dependence_rows(psi)\n</code></pre> <p></p> <p>The first configuration for experiment 1 is already better than the LTI ARX, LTI SS, GRU, LSTM, MLP NARX, MLP FIR, OLSTM, and the SOTA models shown in the benchmark table. Better than 8 out 11 models shown in the benchmark. For experiment 2, its better than LTI ARX, LTI SS, GRU, RNN, LSTM, OLSTM, and pNARX (7 out 11). It's a good start, but let's check if the performance improves if we set a higher lag for both <code>xlag</code> and <code>ylag</code>.</p> <p>The average metric is \\((0.1131 + 0.1059)/2 = 0.1095\\), which is very good, but worse than the SOTA (\\(0.0945\\)). We will now increase the lags for <code>x</code> and <code>y</code> to check if we get a better model. Before increasing the lags, the information criteria is shown:</p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <p></p> <p>It can be observed that after 22 regressors, adding new regressors do not improve the model performance (considering the configuration defined for that model). Because we want to try models with higher lags and higher nonlinearity degree, the stopping criteria will be changed to <code>err_tol</code> instead of information criteria. This will made the algorithm runs considerably faster.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9996,\n    n_terms=22,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>(22, 2) 0.9970964868326048\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aicc\",\n    err_tol=0.9996,\n    n_terms=22,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <p></p> <p>In the first experiment, the model showed a slight improvement, while the performance of the second experiment experienced a minor decline. Increasing the lag settings with these configurations did not result in significant changes. Therefore, let's set the polynomial degree to \\(3\\) and increase the number of terms to build the model to <code>n_terms=40</code> if the <code>err_tol</code> is not reached. It's important to note that these values are chosen empirically. We could also adjust the parameter estimation technique, the <code>err_tol</code>, the model structure selection algorithm, and the basis function, among other factors. Users are encouraged to employ hyperparameter tuning techniques to find the optimal combinations of hyperparameters.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9996,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>(40, 3) 0.9982136069197526\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aicc\",\n    err_tol=0.9996,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <p></p> <p>As shown in the plot, we have surpassed the state-of-the-art (SOTA) results with an average metric of \\((0.0969 + 0.0731)/2 = 0.0849\\). Additionally, the metric for the first experiment matches the best model in the benchmark, and the metric for the second experiment slightly exceeds the benchmark's best model. Using the same configuration for both models, we achieved the best overall results!</p>"},{"location":"user-guide/tutorials/electromechanical-system-identification-entropic-regression/","title":"Electromechanical System Identification - Entropic Regression","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>More details about this data can be found in the following paper (in Portuguese): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>df1 = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/generator/x_cc.csv\"\n)\ndf2 = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/generator/y_cc.csv\"\n)\n</code></pre> <pre><code>df2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code># we will decimate the data using d=500 in this example\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-entropic-regression/#building-a-polynomial-narx-model-using-entropic-regression-algorithm","title":"Building a Polynomial NARX model using Entropic Regression Algorithm","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = ER(\n    ylag=6,\n    xlag=6,\n    n_perm=2,\n    k=2,\n    skip_forward=True,\n    estimator=estimator,\n    basis_function=basis_function,\n)\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use ER(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_20912\\4260657624.py:1: UserWarning: Given the higher number of possible regressors (91), the Entropic Regression algorithm may take long time to run. Consider reducing the number of regressors \n  model.fit(X=x_train, y=y_train)\n\n\n0.03276775133089435\n        Regressors   Parameters             ERR\n0                1  -6.7052E+02  0.00000000E+00\n1           y(k-1)   9.6022E-01  0.00000000E+00\n2           y(k-5)  -3.0769E-02  0.00000000E+00\n3          x1(k-2)   7.3733E+02  0.00000000E+00\n4         y(k-1)^2   1.5897E-04  0.00000000E+00\n5     y(k-2)y(k-1)  -2.2080E-04  0.00000000E+00\n6     y(k-3)y(k-1)   2.9946E-06  0.00000000E+00\n7     y(k-5)y(k-1)   4.9779E-06  0.00000000E+00\n8    x1(k-1)y(k-1)  -1.7036E-01  0.00000000E+00\n9    x1(k-2)y(k-1)  -2.0748E-01  0.00000000E+00\n10   x1(k-4)y(k-1)   8.3724E-03  0.00000000E+00\n11        y(k-2)^2   7.3635E-05  0.00000000E+00\n12   x1(k-1)y(k-2)   1.2028E-01  0.00000000E+00\n13   x1(k-2)y(k-2)   8.0270E-02  0.00000000E+00\n14   x1(k-3)y(k-2)  -3.0208E-03  0.00000000E+00\n15   x1(k-4)y(k-2)  -8.8307E-03  0.00000000E+00\n16   x1(k-1)y(k-3)  -4.9095E-02  0.00000000E+00\n17   x1(k-1)y(k-4)   1.2375E-02  0.00000000E+00\n18       x1(k-1)^2   1.1682E+02  0.00000000E+00\n19  x1(k-3)x1(k-2)   5.2777E+00  0.00000000E+00\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-metamss/","title":"Electromechanical System Identification - MetaMSS","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import MetaMSS, FROLS\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>df1 = pd.read_csv(\"./datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"./datasets/y_cc.csv\")\n\ndf2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code>df1.iloc[::500].values.shape\n</code></pre> <pre><code>(1000, 1)\n</code></pre> <p>We will decimate the data using d=500 in this example. Besides, we separate the MetaMSS data to use the same amount of samples in the prediction validation. Because MetaMSS need a train and test data to optimize the parameters of the model, in this case, we'll use 400 samples to train instead of 500 samples used for the other models. </p> <pre><code># we will decimate the data using d=500 in this example\nx_train, x_test = np.split(df1.iloc[::500].values, 2)\ny_train, y_test = np.split(df2.iloc[::500].values, 2)\n</code></pre> <pre><code>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = MetaMSS(\n    xlag=5,\n    ylag=5,\n    estimator=estimator,\n    maxiter=5,\n    n_agents=15,\n    basis_function=basis_function,\n    random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:455: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:465: RuntimeWarning: invalid value encountered in sqrt\n  se_theta = np.sqrt(var_e)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n\n\n\n\n\n&lt;sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS at 0x229e13e3150&gt;\n</code></pre> <pre><code>yhat = model.predict(X=x_test, y=y_test, steps_ahead=None)\nrrse = root_relative_squared_error(y_test[model.max_lag :, :], yhat[model.max_lag :, :])\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.035919583498004094\n        Regressors   Parameters             ERR\n0                1  -6.1606E+02  0.00000000E+00\n1           y(k-1)   1.3117E+00  0.00000000E+00\n2           y(k-2)  -3.0579E-01  0.00000000E+00\n3          x1(k-1)   5.7920E+02  0.00000000E+00\n4          x1(k-3)  -1.8750E-01  0.00000000E+00\n5    x1(k-1)y(k-1)  -1.7305E-01  0.00000000E+00\n6    x1(k-2)y(k-1)  -1.1660E-01  0.00000000E+00\n7    x1(k-1)y(k-2)   1.2182E-01  0.00000000E+00\n8    x1(k-2)y(k-2)   3.4112E-02  0.00000000E+00\n9    x1(k-1)y(k-3)  -4.8970E-02  0.00000000E+00\n10   x1(k-1)y(k-4)   1.3846E-02  0.00000000E+00\n11       x1(k-2)^2   1.0290E+02  0.00000000E+00\n12  x1(k-3)x1(k-2)   8.6745E-01  0.00000000E+00\n13  x1(k-4)x1(k-2)   3.4336E-01  0.00000000E+00\n14  x1(k-5)x1(k-2)   2.7815E-01  0.00000000E+00\n15       x1(k-3)^2  -9.3749E-01  0.00000000E+00\n16  x1(k-4)x1(k-3)   6.1039E-01  0.00000000E+00\n17  x1(k-5)x1(k-3)   3.9361E-02  0.00000000E+00\n18       x1(k-4)^2  -4.6335E-01  0.00000000E+00\n19  x1(k-5)x1(k-4)  -9.5668E-02  0.00000000E+00\n20       x1(k-5)^2   3.6922E-01  0.00000000E+00\n</code></pre> <p></p> <p></p> <p></p> <pre><code># Plotting the evolution of the agents\nplt.plot(model.best_by_iter)\nmodel.best_by_iter[-1]\n</code></pre> <pre><code>0.0017530517788608157\n</code></pre> <p></p> <pre><code># You have access to all tested models\n# model.tested_models\n</code></pre> <pre><code>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nxlag = ylag = 5\n\nestimators = [\n    (\n        \"NARX_KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX_ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            ylag=ylag,\n            xlag=xlag,\n            basis_function=basis_function,\n            info_criteria=\"bic\",\n            err_tol=None,\n        ),\n    ),\n    (\n        \"MetaMSS\",\n        MetaMSS(\n            norm=-2,\n            xlag=xlag,\n            ylag=ylag,\n            estimator=estimator,\n            maxiter=5,\n            n_agents=15,\n            loss_func=\"metamss_loss\",\n            basis_function=basis_function,\n            random_state=42,\n        ),\n    ),\n]\n\n\nall_results = {}\nfor model_name, modelo in estimators:\n    all_results[\"%s\" % model_name] = []\n    modelo.fit(X=x_train, y=y_train)\n    yhat = modelo.predict(X=x_test, y=y_test)\n    if model_name in [\"FROLS-Polynomial_NARX\", \"MetaMSS\"]:\n        result = root_relative_squared_error(\n            y_test[modelo.max_lag :], yhat[modelo.max_lag :]\n        )\n    else:\n        result = root_relative_squared_error(y_test, yhat)\n    all_results[\"%s\" % model_name].append(result)\n    print(model_name, \"%.3f\" % np.mean(result))\n</code></pre> <pre><code>NARX_KNeighborsRegressor 1.158\nNARX_DecisionTreeRegressor 0.203\nNARX_RandomForestRegressor 0.146\nNARX_Catboost 0.120\nNARX_ARD 0.083\nFROLS-Polynomial_NARX 0.057\n\n\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:455: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:465: RuntimeWarning: invalid value encountered in sqrt\n  se_theta = np.sqrt(var_e)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n\n\nMetaMSS 0.036\n</code></pre> <pre><code>for model_name, metric in sorted(\n    all_results.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(model_name, np.mean(metric))\n</code></pre> <pre><code>MetaMSS 0.035919583498004094\nFROLS-Polynomial_NARX 0.05729765719062527\nNARX_ARD 0.08265856190495872\nNARX_Catboost 0.12034851661643597\nNARX_RandomForestRegressor 0.14557973585496042\nNARX_DecisionTreeRegressor 0.203057724881072\nNARX_KNeighborsRegressor 1.157787546845798\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-overview/","title":"Electromechanical System Identification - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>More details about this data can be found in the following paper (in Portuguese): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>df1 = pd.read_csv(\"../examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"../examples/datasets/y_cc.csv\")\n</code></pre> <pre><code>df2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code># we will decimate the data using d=500 in this example\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-overview/#building-a-polynomial-narx-model","title":"Building a Polynomial NARX model","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=100,\n    ylag=5,\n    xlag=5,\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:618: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 66\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n0.05681502501595064\n        Regressors   Parameters             ERR\n0           y(k-1)   1.5935E+00  9.86000310E-01\n1        x1(k-1)^2   1.1202E+02  7.94813324E-03\n2         y(k-2)^2  -1.7469E-05  2.50921747E-03\n3    x1(k-1)y(k-1)  -1.5994E-01  1.43297462E-03\n4           y(k-2)  -7.4013E-01  1.02774988E-03\n5    x1(k-1)y(k-2)   1.0771E-01  5.35195948E-04\n6     y(k-3)y(k-1)   4.2578E-05  3.46258211E-04\n7        x1(k-4)^2  -6.1823E+00  6.91218347E-05\n8    x1(k-1)y(k-3)  -3.0064E-02  2.83751722E-05\n9     y(k-4)y(k-1)  -1.4505E-05  2.01620114E-05\n10  x1(k-4)x1(k-1)  -2.7490E+00  1.09189469E-05\n11    y(k-4)y(k-2)   7.2062E-06  1.27131624E-05\n12   x1(k-5)y(k-1)  -8.5557E-04  6.53111914E-06\n13  x1(k-3)x1(k-2)  -9.8645E-01  4.24331903E-06\n14  x1(k-2)x1(k-1)  -2.3609E+00  6.41299982E-06\n15         x1(k-3)  -2.0121E+02  6.43059002E-06\n16   x1(k-1)y(k-5)   3.0338E-03  2.76577885E-06\n17   x1(k-3)y(k-1)   3.2426E-02  2.79523223E-06\n18   x1(k-4)y(k-1)   5.9510E-03  1.62218750E-06\n19               1  -4.2071E+01  1.13359933E-06\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-overview/#testing-different-autoregressive-models","title":"Testing different autoregressive models","text":"<pre><code>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    AdaBoostRegressor,\n    GradientBoostingRegressor,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nbasis_function = Polynomial(degree=2)\nxlag = 5\nylag = 5\n\nestimators = [\n    (\n        \"KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            model_type=\"NARMAX\",\n        ),\n    ),\n    (\n        \"NARX-DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX-ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            ylag=xlag,\n            xlag=ylag,\n            info_criteria=\"bic\",\n            estimator=estimator,\n            basis_function=basis_function,\n            err_tol=None,\n        ),\n    ),\n]\n\nall_results = {}\nfor model_name, modelo in estimators:\n    all_results[\"%s\" % model_name] = []\n    modelo.fit(X=x_train, y=y_train)\n    yhat = modelo.predict(X=x_valid, y=y_valid)\n    result = root_relative_squared_error(\n        y_valid[modelo.max_lag :], yhat[modelo.max_lag :]\n    )\n    all_results[\"%s\" % model_name].append(result)\n    print(model_name, \"%.3f\" % np.mean(result))\n</code></pre> <pre><code>KNeighborsRegressor 1.168\nNARX-DecisionTreeRegressor 0.190\nNARX-RandomForestRegressor 0.151\nNARX-Catboost 0.121\nNARX-ARD 0.083\nFROLS-Polynomial_NARX 0.057\n</code></pre> <pre><code>for model_name, metric in sorted(\n    all_results.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(model_name, np.mean(metric))\n</code></pre> <pre><code>FROLS-Polynomial_NARX 0.05729765719062527\nNARX-ARD 0.08336072971138789\nNARX-Catboost 0.12137085298392238\nNARX-RandomForestRegressor 0.15102205613876338\nNARX-DecisionTreeRegressor 0.19018792321900427\nKNeighborsRegressor 1.1676227184643708\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/","title":"F-16 Aircraft - N-steps Ahead Prediction","text":"<p>Note: The following examples do not try to replicate the results of the cited manuscripts. Even the model parameters such as ylag and xlag and size of identification and validation data are not the same of the cited papers. Moreover, sampling rate adjustment and other different data preparation are not handled here.</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>The following text was taken from the link http://www.nonlinearbenchmark.org/#F16. </p> <p>Note: The reader is reffered to the mentioned website for a complete reference concerning the experiment. For now, this notebook is just a simple example of the performance of SysIdentPy on a real world dataset. A more detailed study of this system will be published in the future.  </p> <p>The F-16 Ground Vibration Test benchmark features a high order system with clearance and friction nonlinearities at the mounting interface of the payloads.</p> <p>The experimental data made available to the Workshop participants were acquired on a full-scale F-16 aircraft on the occasion of the Siemens LMS Ground Vibration Testing Master Class, held in September 2014 at the Saffraanberg military basis, Sint-Truiden, Belgium.</p> <p>During the test campaign, two dummy payloads were mounted at the wing tips to simulate the mass and inertia properties of real devices typically equipping an F-16 in \ufb02ight. The aircraft structure was instrumented with accelerometers. One shaker was attached underneath the right wing to apply input signals. The dominant source of nonlinearity in the structural dynamics was expected to originate from the mounting interfaces of the two payloads. These interfaces consist of T-shaped connecting elements on the payload side, slid through a rail attached to the wing side. A preliminary investigation showed that the back connection of the right-wing-to-payload interface was the predominant source of nonlinear distortions in the aircraft dynamics, and is therefore the focus of this benchmark study.</p> <p>A detailed formulation of the identification problem can be found here. All the provided files and information on the F-16 aircraft benchmark system are available for download here. This zip-file contains a detailed system description, the estimation and test data sets, and some pictures of the setup. The data is available in the .csv and .mat file format.</p> <p>Please refer to the F16 benchmark as:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/#preparing-the-data","title":"Preparing the data","text":"<pre><code>f_16 = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/f_16_vibration_test/f-16.txt\",\n    header=None,\n    names=[\"x1\", \"x2\", \"y\"],\n)\n</code></pre> <pre><code>f_16.shape\n</code></pre> <pre><code>(32768, 3)\n</code></pre> <pre><code>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</code></pre> <pre><code>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</code></pre> <pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/#building-the-model","title":"Building the model","text":"<pre><code>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x25d368a8910&gt;\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/#defining-the-forecasting-horizon","title":"Defining the forecasting horizon","text":"<p>To perform a n-steps-ahead prediction you just need to set the \"steps_ahead\" argument.</p> <p>Note The default value for steps_ahead is None and it performs a infinity-steps-ahead prediction</p> <pre><code>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=1)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</code></pre> <pre><code>0.09610207940697202\n   Regressors   Parameters             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</code></pre> <p></p> <pre><code>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=5)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</code></pre> <pre><code>0.2168472873799118\n</code></pre> <p></p> <pre><code>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=None)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</code></pre> <pre><code>0.2910089654603829\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/f-16-aircraft/","title":"F-16 Aircraft","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Note: The following examples do not try to replicate the results of the cited manuscripts. Even the model parameters such as ylag and xlag and size of identification and validation data are not the same of the cited papers. Moreover, sampling rate adjustment and other different data preparation are not handled here.</p>"},{"location":"user-guide/tutorials/f-16-aircraft/#reference","title":"Reference","text":"<p>The following text was taken from the link http://www.nonlinearbenchmark.org/#F16. </p> <p>Note: The reader is referred to the mentioned website for a complete reference concerning the experiment. For now, this notebook is just a simple example of the performance of SysIdentPy on a real world dataset. A more detailed study of this system will be published in the future.  </p> <p>The F-16 Ground Vibration Test benchmark features a high order system with clearance and friction nonlinearities at the mounting interface of the payloads.</p> <p>The experimental data made available to the Workshop participants were acquired on a full-scale F-16 aircraft on the occasion of the Siemens LMS Ground Vibration Testing Master Class, held in September 2014 at the Saffraanberg military basis, Sint-Truiden, Belgium.</p> <p>During the test campaign, two dummy payloads were mounted at the wing tips to simulate the mass and inertia properties of real devices typically equipping an F-16 in \ufb02ight. The aircraft structure was instrumented with accelerometers. One shaker was attached underneath the right wing to apply input signals. The dominant source of nonlinearity in the structural dynamics was expected to originate from the mounting interfaces of the two payloads. These interfaces consist of T-shaped connecting elements on the payload side, slid through a rail attached to the wing side. A preliminary investigation showed that the back connection of the right-wing-to-payload interface was the predominant source of nonlinear distortions in the aircraft dynamics, and is therefore the focus of this benchmark study.</p> <p>A detailed formulation of the identification problem can be found here. All the provided files and information on the F-16 aircraft benchmark system are available for download here. This zip-file contains a detailed system description, the estimation and test data sets, and some pictures of the setup. The data is available in the .csv and .mat file format.</p> <p>Please refer to the F16 benchmark as:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>f_16 = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/f_16_vibration_test/f-16.txt\",\n    header=None,\n    names=[\"x1\", \"x2\", \"y\"],\n)\n</code></pre> <pre><code>f_16.shape\n</code></pre> <pre><code>(32768, 3)\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#visualizating-the-data","title":"Visualizating the data","text":"<pre><code>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#spliting-the-data","title":"Spliting the data","text":"<pre><code>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#setting-the-input-lags","title":"Setting the input lags","text":"<pre><code>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</code></pre> <pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#model-training-and-evalutation","title":"Model training and evalutation","text":"<pre><code>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\ny_hat = model.predict(X=x_val, y=y_val)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.2910089654603829\n   Regressors   Parameters             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</code></pre> <pre><code>plot_results(y=y_val, yhat=y_hat, n=1000)\nee = compute_residues_autocorrelation(y_val, y_hat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0])\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#information-criteria-plot","title":"Information Criteria plot","text":"<pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n\n# You can use the plot below to choose the \"n_terms\" and run the model again with the most adequate value of terms.\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/","title":"Fourier NARX - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>This example shows how changing or adding a new basis function could improve the model</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\n\nnp.seterr(all=\"ignore\")\nnp.random.seed(1)\n\n%matplotlib inline\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#defining-the-system","title":"Defining the system","text":"<pre><code># Simulated system\ndef system_equation(y, u):\n    yk = (\n        (0.2 - 0.75 * np.cos(-y[0] ** 2)) * np.cos(y[0])\n        - (0.15 + 0.45 * np.cos(-y[0] ** 2)) * np.cos(y[1])\n        + np.cos(u[0])\n        + 0.2 * u[1]\n        + 0.7 * u[0] * u[1]\n    )\n    return yk\n\n\nrepetition = 5\nrandom_samples = 200\ntotal_time = repetition * random_samples\nn = np.arange(0, total_time)\n\n# Generating input\nx = np.random.normal(size=(random_samples,)).repeat(repetition)\n\n\n_, ax = plt.subplots(figsize=(12, 6))\nax.step(n, x)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$x[n]$\", fontsize=18)\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#simulate-the-system","title":"Simulate the system","text":"<pre><code>y = np.empty_like(x)\n# Initial Conditions\ny0 = [0, 0]\n\n# Simulate it\ny[0:2] = y0\nfor i in range(2, len(y)):\n    y[i] = system_equation(\n        [y[i - 1], y[i - 2]], [x[i - 1], x[i - 2]]\n    ) + np.random.normal(scale=0.1)\n\n# Plot\n_, ax = plt.subplots(figsize=(12, 6))\nax.plot(n, y)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.grid()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#adding-noise-to-the-system","title":"Adding noise to the system","text":"<pre><code># Noise free data\nynoise_free = y.copy()\n\n# Generate noise\nv = np.random.normal(scale=0.5, size=y.shape)\n\n# Data corrupted with noise\nynoisy = ynoise_free + v\n\n# Plot\n_, ax = plt.subplots(figsize=(14, 8))\nax.plot(n, ynoise_free, label=\"Noise-free data\")\nax.plot(n, ynoisy, label=\"Corrupted data\")\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.legend(fontsize=18)\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#generating-training-and-test-data","title":"Generating training and test data","text":"<pre><code>n_train = 700\n\n# Identification data\ny_train = ynoisy[:n_train].reshape(-1, 1)\nx_train = x[:n_train].reshape(-1, 1)\n\n# Validation data\ny_test = ynoise_free[n_train:].reshape(-1, 1)\nx_test = x[n_train:].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#polynomial-basis-function","title":"Polynomial Basis Function","text":"<p>As you can see bellow, using only the polynomial basis function with the following parameters do not result in a bad model. However, lets check how is the performance using the Fourier Basis Function.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    estimator=estimator,\n    err_tol=None,\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</code></pre> <pre><code>0.6768251106751224\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#ensembling-a-fourier-basis-function","title":"Ensembling a Fourier Basis Function","text":"<p>In this case, adding the Fourier Basis Function solves the problem and returns a model capable to predict the defined system</p> <pre><code>basis_function = Fourier(degree=2, n=2, p=2 * np.pi, ensemble=True)\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=70,\n    xlag=2,\n    ylag=2,  # the lags for all models will be 13\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    err_tol=None,\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</code></pre> <pre><code>0.3742244715879492\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#important","title":"Important","text":"<p>Currently you can't get the model representation using <code>sysidentpy.regressor_code</code> for Fourier NARX models. Actually, you can use the method, but the representation is not accurate because we don't make clear what are the regressors related to the polynomial or related to the fourier basis function. This is a improvement to be done in future updates!</p>"},{"location":"user-guide/tutorials/general-NARX-models/","title":"General NARX Models","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>In this example we will create NARX models using different estimator like GradientBoostingRegressor, Bayesian Regression, Automatic Relevance Determination (ARD) Regression and Catboost</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.general_estimators import NARX\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code># simulated dataset\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=10000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#importance-of-the-narx-architecture","title":"Importance of the NARX architecture","text":"<p>To get an idea of the importance of the NARX architecture, lets take a look in the performance of the models without the NARX configuration.</p> <pre><code>catboost = CatBoostRegressor(iterations=300, learning_rate=0.1, depth=6)\n</code></pre> <pre><code>gb = GradientBoostingRegressor(\n    loss=\"quantile\",\n    alpha=0.90,\n    n_estimators=250,\n    max_depth=10,\n    learning_rate=0.1,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\n</code></pre> <pre><code>def plot_results_tmp(y_valid, yhat):\n    _, ax = plt.subplots(figsize=(14, 8))\n    ax.plot(y_valid[:200], label=\"Data\", marker=\"o\")\n    ax.plot(yhat[:200], label=\"Prediction\", marker=\"*\")\n    ax.set_xlabel(\"$n$\", fontsize=18)\n    ax.set_ylabel(\"$y[n]$\", fontsize=18)\n    ax.grid()\n    ax.legend(fontsize=18)\n    plt.show()\n</code></pre> <pre><code>catboost.fit(x_train, y_train, verbose=False)\nplot_results_tmp(y_valid, catboost.predict(x_valid))\n</code></pre> <p></p> <pre><code>gb.fit(x_train, y_train.ravel())\nplot_results_tmp(y_valid, gb.predict(x_valid))\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/general-NARX-models/#introducing-the-narx-configuration-using-sysidentpy","title":"Introducing the NARX configuration using SysIdentPy","text":"<p>As you can see, you just need to pass the base estimator you want to the NARX class from SysIdentPy do build the NARX model! You can choose the lags of the input and output variables to build the regressor matrix.</p> <p>We keep the fit/predict method to make the process straightforward.</p>"},{"location":"user-guide/tutorials/general-NARX-models/#narx-with-catboost","title":"NARX with Catboost","text":"<pre><code>basis_function = Fourier(degree=1)\n\ncatboost_narx = NARX(\n    base_estimator=CatBoostRegressor(iterations=300, learning_rate=0.1, depth=8),\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    fit_params={\"verbose\": False},\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid, steps_ahead=1)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>MSE:  0.00024145290395678653\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#narx-with-gradient-boosting","title":"NARX with Gradient Boosting","text":"<pre><code>basis_function = Fourier(degree=1)\n\ngb_narx = NARX(\n    base_estimator=GradientBoostingRegressor(\n        loss=\"quantile\",\n        alpha=0.90,\n        n_estimators=250,\n        max_depth=10,\n        learning_rate=0.1,\n        min_samples_leaf=9,\n        min_samples_split=9,\n    ),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\ngb_narx.fit(X=x_train, y=y_train)\nyhat = gb_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.0011824693986863938\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#narx-with-ard","title":"NARX with ARD","text":"<pre><code>from sysidentpy.general_estimators import NARX\n\nARD_narx = NARX(\n    base_estimator=ARDRegression(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nARD_narx.fit(X=x_train, y=y_train)\nyhat = ARD_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.0011058934497373794\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#narx-with-bayesian-ridge","title":"NARX with Bayesian Ridge","text":"<pre><code>from sysidentpy.general_estimators import NARX\n\nBayesianRidge_narx = NARX(\n    base_estimator=BayesianRidge(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nBayesianRidge_narx.fit(X=x_train, y=y_train)\nyhat = BayesianRidge_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.0011077874945734536\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#note","title":"Note","text":"<p>Remember you can use n-steps-ahead prediction and NAR and NFIR models now. Check how to use it in their respective examples. </p>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/","title":"Importance of Extended Least Squares","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n</code></pre>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data","text":"<p>The data is generated by simulating the following model: \\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma\\) is defined by the user.</p> <p>In the next example we will generate a data with 3000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n</code></pre>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#build-the-model","title":"Build the model","text":"<p>First we will train a model without the Extended Least Squares Algorithm for comparison purpose.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=False)\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.5499799245432233\n</code></pre> <p>Clearly we have something wrong with the obtained model. See the basic_steps notebook to compare the results obtained using the same data but without colored noise. But let take a look in whats is wrong.</p> <pre><code>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0        x1(k-2)  8.9976E-01  7.41682256E-01\n1         y(k-1)  2.8734E-01  8.33321202E-02\n2  x1(k-1)y(k-1)  1.2348E-01  5.10334067E-03\n</code></pre>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#biased-parameter-estimation","title":"Biased parameter estimation","text":"<p>As we can observe above, the model structure is exact the same the one that generate the data. You can se that the ERR ordered the terms in the correct way. And this is an important note regarding the Error Reduction Ratio algorithm used here: it is very robust to colored noise!! </p> <p>That is a great feature! However, although the structure is correct, the model parameters are not ok! Here we have a biased estimation! The real parameter for \\(y_{k-1}\\) is \\(0.2\\), not \\(0.3\\).</p> <p>In this case, we are actually modeling using a NARX model, not a NARMAX. The MA part exists to allow a unbiased estimation of the parameters. To achieve a unbiased estimation of the parameters we have the Extend Least Squares algorithm. Remember, if the data have only white noise, NARX is fine. </p> <p>Before applying the Extended Least Squares Algorithm we will run several NARX models to check how different the estimated parameters are from the real ones.</p> <pre><code>parameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\n# Set the theme for seaborn (optional)\nsns.set_theme()\n\nplt.figure(figsize=(14, 4))\n\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\")\nsns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\")\nsns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")\n\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#using-the-extended-least-squares-algorithm","title":"Using the Extended Least Squares algorithm","text":"<p>As shown in figure above, we have a problem to estimate the parameter for \\(y_{k-1}\\). Now we will use the Extended Least Squares Algorithm.</p> <p>In SysIdentPy, just set extended_least_squares to True and the algorithm will be applied.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model = FROLS(\n        order_selection=False,\n        n_terms=3,\n        ylag=2,\n        xlag=2,\n        elag=2,\n        info_criteria=\"aic\",\n        estimator=estimator,\n        basis_function=basis_function,\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\n\nplt.figure(figsize=(14, 4))\n\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\")\nsns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\")\nsns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")\n\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Great! Now we have an unbiased estimation of the parameters!</p>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#note","title":"Note","text":"<p>Note: The Extended Least Squares is an iterative algorithm. In SysIdentpy the default is 30 iterations (<code>uiter=30</code>) because it is known from literature that the algorithm converges quickly (about 10 or 20 iterations).</p>"},{"location":"user-guide/tutorials/information-criteria-overview/","title":"Information Criteria - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#comparing-different-information-criteria-methods","title":"Comparing different information criteria methods","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n</code></pre>"},{"location":"user-guide/tutorials/information-criteria-overview/#generating-sample-data","title":"Generating sample data","text":"<p>The data is generated by simulating the following model: \\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <p>In the next example we will generate a data with 3000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_test, y_train, y_test = get_siso_data(\n    n=100, colored_noise=False, sigma=0.1, train_percentage=70\n)\n</code></pre> <p>The idea is to show the impact of the information criteria to select the number of terms to compose the final model. You will se why it is an auxiliary tool and let the algorithm select the number of terms based on the minimum value is not a good idea when dealing with data highly corrupted by noise (even white noise) </p> <p>Note: You may find different results when running the examples. This is due the fact we are not setting a fixed random generator for the sample data. However, the main analysis remain.</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#aic","title":"AIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    # estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\nmodel.fit(X=x_train, y=y_train)\n\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.1681621129389993\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2076E-01  9.41001395E-01\n1          y(k-1)   1.7063E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7342E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -9.7902E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   4.9319E-02  1.85339089E-03\n5        y(k-2)^2  -5.6743E-02  1.02439383E-03\n6         x1(k-1)  -2.0179E-02  6.78305323E-04\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-273.81858224, -311.60797635, -331.34011486, -338.49936124,\n       -342.10339048, -342.27073244, -342.82764626, -342.16492383,\n       -341.04704839, -339.58437034, -337.79642875, -336.20531349,\n       -333.72427584, -331.48645717, -329.53042523])\n</code></pre> <p>As can be seen above, the minimum value make the algorithm choose a model with 4 terms. However, if you check the plot, 3 terms is the best choice. Increasing the number of terms from 3 upwards do not lead to a better model since the difference is very small.</p> <p>In this case, you should run the model again with the parameters n_terms=3! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again!</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#aicc","title":"AICc","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aicc\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.1697650652880654\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2659E-01  9.41001395E-01\n1          y(k-1)   1.7219E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03\n4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03\n5        y(k-2)^2  -4.8117E-02  1.02439383E-03\n6         x1(k-1)  -2.4728E-02  6.78305323E-04\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-273.99834706, -311.49708248, -331.28179881, -338.08184328,\n       -341.32119362, -341.3373076 , -341.51818541, -340.50119461,\n       -339.16231408, -336.96924137, -334.34018578, -331.63849434,\n       -328.92685178, -325.8181513 , -322.55039655])\n</code></pre> <p>As can be seen above, the minimum value make the algorithm choose a model with 4 terms. AICc, however, have major differences compared with AIC when the number of samples is small.</p> <p>In this case, you should run the model again with the parameters n_terms=3! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again! </p>"},{"location":"user-guide/tutorials/information-criteria-overview/#bic","title":"BIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.16868631871856155\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.3050E-01  9.41001395E-01\n1          y(k-1)   1.7980E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-271.83944541, -307.24268246, -324.99827569, -329.8387331 ,\n       -331.19139703, -329.39731055, -327.84829815, -325.18581094,\n       -322.29019301, -318.63381344, -314.63988674, -310.67712915,\n       -306.81399235, -302.66957173, -298.4885502 ])\n</code></pre> <p>BIC did a better job in this case! The way it penalizes the model regarding the number of terms ensure that the minimum value here was exact the number of expected terms to compose the model. Good, but not always the best method!</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#lilc","title":"LILC","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"lilc\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.16868631871856155\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.3050E-01  9.41001395E-01\n1          y(k-1)   1.7980E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-273.17951619, -309.92282401, -329.01848803, -335.19901621,\n       -337.89175092, -337.43773522, -337.22879359, -335.90637716,\n       -334.35083001, -332.03452122, -329.3806653 , -326.75797849,\n       -324.23491246, -321.43056262, -318.58961187])\n</code></pre> <p>LILC also includes spurious terms. Like AIC, it fails to automatically select the correct terms but you could select the right number based on the plot above!</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#fpe","title":"FPE","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"fpe\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.1697650652880654\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2659E-01  9.41001395E-01\n1          y(k-1)   1.7219E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03\n4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03\n5        y(k-2)^2  -4.8117E-02  1.02439383E-03\n6         x1(k-1)  -2.4728E-02  6.78305323E-04\n\n\n\n\n\nText(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-274.05880892, -311.68054386, -331.65290152, -338.70751749,\n       -342.27085495, -342.68306863, -343.33508312, -342.86743567,\n       -342.15953986, -340.68281499, -338.85950374, -337.05732543,\n       -335.3437066 , -333.33668596, -331.27985457])\n</code></pre> <p>FPE also failed to automatically select the right number of terms! But, as we pointed out before, Information Criteria is an auxiliary tool! If you look at the plots, all the methods allows you to choose the right numbers of terms!</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#important-note","title":"Important Note","text":"<p>Here we are dealing with a known model structure! Concerning real data, we do not know the right number of terms so the methods above stands as excellent tools to help you out!</p> <p>If you check the metrics above, even with the models with more terms, you will see excellent metrics! But System Identification always search for the best model structure! Model Structure Selection is the core of NARMAX methods! In this respect, the examples are to show basic concepts and how the algorithms work!</p>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/","title":"Load Forecasting Benchmark","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#note","title":"Note","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained against neural prophet library.</p> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS and FROLS (with polynomial base function) methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p> <p>We will compare a 1-step ahead forecaster on electricity consumption of a building. The config of the neuralprophet model was taken from the neuralprophet documentation (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>The training will occur on 80% of the data, reserving the last 20% for the validation.</p> <p>Note: the data used in this example can be found in neuralprophet github.</p>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#benchmark-results","title":"Benchmark results:","text":"No. Package Mean Squared Error 1 SysIdentPy (FROLS) 4183 2 SysIdentPy (MetaMSS) 5264 3 SysIdentPy (AOLS) 5264 4 NeuralProphet 11471 <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom sktime.datasets import load_airline\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</code></pre>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#frols","title":"FROLS","text":"<pre><code>raw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_hospital/SanFrancisco_Hospital.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    info_criteria=\"bic\",\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n)\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(sysidentpy_loss)\n\n\nplot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8))\n</code></pre> <pre><code>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\ofr_base.py:537: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 5\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n4183.359498155755\n</code></pre>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#metamss","title":"MetaMSS","text":"<pre><code>raw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_hospital/SanFrancisco_Hospital.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy_metamss = MetaMSS(\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    steps_ahead=1,\n    n_agents=15,\n    random_state=42,\n)\nsysidentpy_metamss.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\n\nyhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1)\nmetamss_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\n\nplot_results(y=y_test[:700], yhat=yhat[:700], n=504, figsize=(18, 8))\n</code></pre> <pre><code>5264.428783519863\n</code></pre>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#aols","title":"AOLS","text":"<pre><code>set_random_seed(42)\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_hospital/SanFrancisco_Hospital.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(xlag=2, ylag=2, basis_function=basis_function)\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\n\nplot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8))\n</code></pre> <pre><code>5264.42917196841\n</code></pre>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\n\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_hospital/SanFrancisco_Hospital.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n    n_lags=24, ar_sparsity=0.5, num_hidden_layers=2, d_hidden=20, learning_rate=0.001\n)\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\n\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n\nneuralprophet_loss = loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1])\n</code></pre> <pre><code>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n\n\nINFO: nprophet.utils - set_auto_seasonalities: Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 32\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 7\nEpoch[7/7]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:04&lt;00:00,  1.74it/s, SmoothL1Loss=0.0102, MAE=81.6, RegLoss=0.011] \nINFO: nprophet - _evaluate: Validation metrics:    SmoothL1Loss    MAE\n1         0.011 84.733\n\n\n11397.103026422525\n</code></pre> <pre><code>plt.figure(figsize=(18, 8))\nplt.plot(forecast[\"y\"][-504:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-504:], \"k*-\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x237847417f0&gt;]\n</code></pre> <pre><code>results = {\n    \"SysIdentPy - FROLS\": sysidentpy_loss,\n    \"SysIdentPy (AOLS)\": aols_loss,\n    \"SysIdentPy (MetaMSS)\": metamss_loss,\n    \"NeuralProphet\": neuralprophet_loss,\n}\n\nsorted(results.items(), key=lambda result: result[1])\n</code></pre> <pre><code>[('SysIdentPy - FROLS', 4183.359498155755),\n ('SysIdentPy (MetaMSS)', 5264.429171346123),\n ('SysIdentPy (AOLS)', 5264.42917196841),\n ('NeuralProphet', 11397.103026422525)]\n</code></pre>"},{"location":"user-guide/tutorials/m4-benchmark/","title":"M4 Benchmark","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The M4 dataset is a well known resource for time series forecasting, offering a wide range of data series used to test and improve forecasting methods. Created for the M4 competition organized by Spyros Makridakis, this dataset has driven many advancements in forecasting techniques.</p> <p>The M4 dataset includes 100,000 time series from various fields such as demographics, finance, industry, macroeconomics, and microeconomics, which were selected randomly from the ForeDeCk database. The series come in different frequencies (yearly, quarterly, monthly, weekly, daily, and hourly), making it a comprehensive collection for testing forecasting methods.</p> <p>In this case study, we will focus on the hourly subset of the M4 dataset. This subset consists of time series data recorded hourly, providing a detailed and high-frequency look at changes over time. Hourly data presents unique challenges due to its granularity and the potential for capturing short-term fluctuations and patterns.</p> <p>The M4 dataset provides a standard benchmark to compare different forecasting methods, allowing researchers and practitioners to evaluate their models consistently. With series from various domains and frequencies, the M4 dataset represents real-world forecasting challenges, making it valuable for developing robust forecasting techniques. The competition and the dataset itself have led to the creation of new algorithms and methods, significantly improving forecasting accuracy and reliability.</p> <p>We will present a end to end walkthrough using the M4 hourly dataset to demonstrate the capabilities of SysIdentPy. SysIdentPy offers a range of tools and techniques designed to effectively handle the complexities of time series data, but we will focus on fast and easy setup for this case. We will cover model selection and evaluation metrics specific to the hourly dataset.</p> <p>By the end of this case study, you will have a solid understanding of how to use SysIdentPy for forecasting with the M4 hourly dataset, preparing you to tackle similar forecasting challenges in real-world scenarios.</p>"},{"location":"user-guide/tutorials/m4-benchmark/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\ndatasetsforecast==0.0.8\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\ns3fs==2024.6.1\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/m4-benchmark/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import SettingWithCopyWarning\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import (\n    root_relative_squared_error,\n    symmetric_mean_absolute_percentage_error,\n)\nfrom sysidentpy.utils.plotting import plot_results\n\nfrom datasetsforecast.m4 import M4, M4Evaluation\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=UserWarning)\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\ntrain = pd.read_csv(\"https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\")\ntest = pd.read_csv(\n    \"https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\"\n).rename(columns={\"y\": \"y_test\"})\n</code></pre> <p>The following plots provide a visualization of the training data for a small subset of the time series. The plot shows the raw data, giving you an insight into the patterns and behaviors inherent in each series.</p> <p>By observing the data, you can get a sense of the variety and complexity of the time series we are working with. The plots can reveal important characteristics such as trends, seasonal patterns, and potential anomalies within the time series. Understanding these elements is crucial for the development of accurate forecasting models.</p> <p>However, when dealing with a large number of different time series, it is common to start with broad assumptions rather than detailed individual analysis. In this context, we will adopt a similar approach. Instead of going into the specifics of each dataset, we will make some general assumptions and see how SysIdentPy handles them.</p> <p>This approach provides a practical starting point, demonstrating how SysIdentPy can manage different types of time series data without too much work. As you become more familiar with the tool, you can refine your models with more detailed insights. For now, let's focus on using SysIdentPy to create the forecasts based on these initial assumptions.</p> <p>Our first assumption is that there is a 24-hour seasonal pattern in the series. By examining the plots below, this seems reasonable. Therefore, we'll begin building our models with <code>ylag=24</code>.</p> <pre><code>ax = (\n    train[train[\"unique_id\"] == \"H10\"]\n    .reset_index(drop=True)[\"y\"]\n    .plot(figsize=(15, 2), title=\"H10\")\n)\nxcoords = [a for a in range(24, 24 * 30, 24)]\n\nfor xc in xcoords:\n    plt.axvline(x=xc, color=\"red\", linestyle=\"--\", alpha=0.5)\n</code></pre> <p></p> <p>Lets check build a model for the <code>H20</code> group before we extrapolate the settings for every group. Because there are no input features, we will be using a <code>NAR</code> model type in SysIdentPy. To keep things simple and fast, we will start with Polynomial basis function with degree \\(1\\).</p> <pre><code>unique_id = \"H20\"\ny_id = train[train[\"unique_id\"] == unique_id][\"y\"].values.reshape(-1, 1)\ny_val = test[test[\"unique_id\"] == unique_id][\"y_test\"].values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    order_selection=True,\n    ylag=24,\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\n\nmodel.fit(y=y_id)\ny_val = np.concatenate([y_id[-model.max_lag :], y_val])\ny_hat = model.predict(y=y_val, forecast_horizon=48)\nsmape = symmetric_mean_absolute_percentage_error(\n    y_val[model.max_lag : :], y_hat[model.max_lag : :]\n)\n\nplot_results(\n    y=y_val[model.max_lag :],\n    yhat=y_hat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Group: {unique_id} - SMAPE {round(smape, 4)}\",\n)\n</code></pre> <p></p> <p>Probably, the result are not optimal and will not work for every group. However, let's check how this setting performs against the winner model \u00a0M4 time series competition: the Exponential Smoothing with Recurrent Neural Networks (ESRNN).</p> <pre><code>esrnn_url = (\n    \"https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-118.zip\"\n)\nesrnn_forecasts = M4Evaluation.load_benchmark(\"data\", \"Hourly\", esrnn_url)\nesrnn_evaluation = M4Evaluation.evaluate(\"data\", \"Hourly\", esrnn_forecasts)\n\nesrnn_evaluation\n</code></pre> SMAPE MASE OWA Hourly 9.328443 0.893046 0.440163 <p>The following code took only 49 seconds to run on my machine (AMD Ryzen 5 5600x processor, 32GB RAM at 3600MHz). Because of its efficiency, I didn't create a parallel version. By the end of this use case, you will see how SysIdentPy can be both fast and effective, delivering good results without too much optimization.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n    y_id = data[\"y\"].values.reshape(-1, 1)\n    basis_function = Polynomial(degree=1)\n    model = FROLS(\n        ylag=24,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        n_info_values=25,\n    )\n    try:\n        model.fit(y=y_id)\n        y_val = y_id[-model.max_lag :].reshape(-1, 1)\n        y_hat = model.predict(y=y_val, forecast_horizon=48)\n        r.append(\n            [\n                u_id * len(y_hat[model.max_lag : :]),\n                ds_test,\n                y_hat[model.max_lag : :].ravel(),\n            ]\n        )\n    except Exception:\n        print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(\n    [\"unique_id\", \"ds\", \"NARMAX_1\"]\n)\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)  # .clip(lower=10)\npivot_df = results_1.pivot(index=\"unique_id\", columns=\"ds\", values=\"NARMAX_1\")\nresults = pivot_df.to_numpy()\n\nM4Evaluation.evaluate(\"data\", \"Hourly\", results)\n</code></pre> SMAPE MASE OWA Hourly 16.034196 0.958083 0.636132 <p>The initial results are reasonable, but they don't quite match the performance of <code>ESRNN</code>. These results are based solely on our first assumption. To better understand the performance, let\u2019s examine the groups with the worst results.</p> <p></p> <p>The following plot illustrates two such groups, <code>H147</code> and <code>H136</code>. Both exhibit a 24-hour seasonal pattern.</p> <p></p> <p></p> <p>However, a closer look reveals an additional insight: in addition to the daily pattern, these series also show a weekly pattern. Observe how the data looks like when we split the series into weekly segments.</p> <p></p> <pre><code>xcoords = list(range(0, 168 * 5, 168))\nfiltered_train = train[train[\"unique_id\"] == \"H147\"].reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(10, 1.5 * len(xcoords[1:])))\nfor i, start in enumerate(xcoords[:-1]):\n    end = xcoords[i + 1]\n    ax = fig.add_subplot(len(xcoords[1:]), 1, i + 1)\n    filtered_train[\"y\"].iloc[start:end].plot(ax=ax)\n    ax.set_title(f\"H147 -&gt; Slice {i+1}: Hour {start} to {end-1}\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Therefore, we will build models setting <code>ylag=168</code>.</p> <p>Note that this is a very high number for lags, so be careful if you want to try it with higher polynomial degrees because the time to run the models can increase significantly. I tried some configurations with polynomial degree equal to 2 and only took \\(6\\) minutes to run (even less, using <code>AOLS</code>), without making the code run in parallel. As you can see, SysIdentPy can be very fast and you can make it faster by applying parallelization.</p> <pre><code># this took 2min to run on my computer.\nr = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n    y_id = data[\"y\"].values.reshape(-1, 1)\n    basis_function = Polynomial(degree=1)\n    model = FROLS(\n        ylag=168,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n    )\n    try:\n        model.fit(y=y_id)\n        y_val = y_id[-model.max_lag :].reshape(-1, 1)\n        y_hat = model.predict(y=y_val, forecast_horizon=48)\n        r.append(\n            [\n                u_id * len(y_hat[model.max_lag : :]),\n                ds_test,\n                y_hat[model.max_lag : :].ravel(),\n            ]\n        )\n    except Exception:\n        print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(\n    [\"unique_id\", \"ds\", \"NARMAX_1\"]\n)\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)  # .clip(lower=10)\npivot_df = results_1.pivot(index=\"unique_id\", columns=\"ds\", values=\"NARMAX_1\")\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate(\"data\", \"Hourly\", results)\n</code></pre> SMAPE MASE OWA Hourly 10.475998 0.773749 0.446471 <p>Now, the results are much closer to those of the <code>ESRNN</code> model! While the Symmetric Mean Absolute Percentage Error (<code>SMAPE</code>) is slightly worse, the Mean Absolute Scaled Error (<code>MASE</code>) is better when comparing against <code>ESRNN</code>, leading to a very similar Overall Weighted Average (<code>OWA</code>) metric. Remarkably, these results are achieved using only simple <code>AR</code> models. Next, let's see if the <code>AOLS</code> method can provide even better results.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n    y_id = data[\"y\"].values.reshape(-1, 1)\n    basis_function = Polynomial(degree=1)\n    model = AOLS(\n        ylag=168,\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        # due to high lag settings, k was increased to 6 as an initial guess\n        k=6,\n    )\n    try:\n        model.fit(y=y_id)\n        y_val = y_id[-model.max_lag :].reshape(-1, 1)\n        y_hat = model.predict(y=y_val, forecast_horizon=48)\n        r.append(\n            [\n                u_id * len(y_hat[model.max_lag : :]),\n                ds_test,\n                y_hat[model.max_lag : :].ravel(),\n            ]\n        )\n    except Exception:\n        print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(\n    [\"unique_id\", \"ds\", \"NARMAX_1\"]\n)\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)  # .clip(lower=10)\npivot_df = results_1.pivot(index=\"unique_id\", columns=\"ds\", values=\"NARMAX_1\")\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate(\"data\", \"Hourly\", results)\n</code></pre> SMAPE MASE OWA Hourly 9.951141 0.809965 0.439755 <p>The Overall Weighted Average (<code>OWA</code>) is even better than that of the <code>ESRNN</code> model! Additionally, the <code>AOLS</code> method was incredibly efficient, taking only 6 seconds to run. This combination of high performance and rapid execution makes <code>AOLS</code> a compelling alternative for time series forecasting in cases with multiple series.</p> <p>Before we finish, let's verify how the performance of the <code>H147</code> model has improved with the <code>ylag=168</code> setting.</p> <p></p> <p>Based on the M4 benchmark paper, we could also clip the predictions lower than 10 to 10 and the results would be slightly better. But this is left to the user.</p> <p>We could achieve even better performance with some fine-tuning of the model configuration. However, I\u2019ll leave exploring these alternative adjustments as an exercise for the user. However, keep in mind that experimenting with different settings does not always guarantee improved results. A deeper theoretical knowledge can often lead you to better configurations and, hence, better results.</p>"},{"location":"user-guide/tutorials/model-with-multiple-inputs/","title":"Model With Multiple Inputs","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/model-with-multiple-inputs/#generating-2-input-1-output-sample-data","title":"Generating 2 input 1 output sample data","text":"<p>The data is generated by simulating the following model:</p> <p>\\(y_k = 0.4y_{k-1}^2 + 0.1y_{k-1}x1_{k-1} + 0.6x2_{k-1} -0.3x1_{k-1}x2_{k-2} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.001\\)</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.generate_data import get_miso_data\n</code></pre> <pre><code>x_train, x_valid, y_train, y_valid = get_miso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre> <p>There is a specific difference for multiple input data.</p> <ul> <li>You have to pass the lags for each input in a nested list (e.g., [[1, 2], [1, 2]])</li> </ul> <p>The remainder settings remains the same.</p>"},{"location":"user-guide/tutorials/model-with-multiple-inputs/#build-the-model","title":"Build the model","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_terms=4,\n    ylag=2,\n    xlag=[[1, 2], [1, 2]],\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1a88cc17350&gt;\n</code></pre>"},{"location":"user-guide/tutorials/model-with-multiple-inputs/#model-evaluation","title":"Model evaluation","text":"<pre><code>yhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.00314141814133057\n       Regressors   Parameters             ERR\n0         x2(k-1)   5.9999E-01  9.15006949E-01\n1  x2(k-2)x1(k-1)  -3.0010E-01  4.31748224E-02\n2        y(k-1)^2   3.9976E-01  4.15131661E-02\n3   x1(k-1)y(k-1)   1.0028E-01  2.96827987E-04\n</code></pre> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre>"},{"location":"user-guide/tutorials/modeling-a-magneto-rheological-damper-device/","title":"Modeling a Magneto Rheological Damper Device","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The memory effects between quasi-static input and output make the modeling of hysteretic systems very difficult. Physics-based models are often used to describe the hysteresis loops, but these models usually lack the simplicity and efficiency required in practical applications involving system characterization, identification, and control. As detailed in Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models, NARX models have proven to be a feasible choice to describe the hysteresis loops. See Chapter 8 for a detailed background. However, even considering the sufficient conditions for rate independent hysteresis representation, classical structure selection algorithms fails to return a model with decent performance and the user needs to set a multi-valued function to ensure the occurrence of the bounding structure \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models).</p> <p>Even though some progress has been made, previous work has been limited to models with a single equilibrium point. The present case study aims to present new prospects in the model structure selection of hysteretic systems regarding the cases where the models have multiple inputs and it is not restricted concerning the number of equilibrium points. For that, the MetaMSS algorithm will be used to build a model for a magneto-rheological damper (MRD) considering the mentioned sufficient conditions.</p>"},{"location":"user-guide/tutorials/modeling-a-magneto-rheological-damper-device/#a-brief-description-of-the-bouc-wen-model-of-magneto-rheological-damper-device","title":"A Brief description of the Bouc-Wen model of magneto-rheological damper device","text":"<p>The data used in this study-case is the Bouc-Wen model (Bouc, R - Forced Vibrations of a Mechanical System with Hysteresis), (Wen, Y. X. - Method for Random Vibration of Hysteretic Systems) of an MRD whose schematic diagram is shown in the figure below.</p> <p></p> <p>The model for a magneto-rheological damper proposed by Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback.</p> <p>The general form of the Bouc-Wen model can be described as (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback):</p> \\[ \\begin{equation} \\dfrac{dz}{dt} = g\\left[x,z,sign\\left(\\dfrac{dx}{dt}\\right)\\right]\\dfrac{dx}{dt}, \\end{equation} \\] <p>where \\(z\\) is the hysteretic model output, \\(x\\) the input and \\(g[\\cdot]\\) a nonlinear function of \\(x\\), \\(z\\) and \\(sign (dx/dt)\\). (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback) proposed the following phenomenological model for the aforementioned device:</p> \\[ \\begin{align} f&amp;= c_1\\dot{\\rho}+k_1(x-x_0),\\nonumber\\\\ \\dot{\\rho}&amp;=\\dfrac{1}{c_0+c_1}[\\alpha z+c_0\\dot{x}+k_0(x-\\rho)],\\nonumber\\\\ \\dot{z}&amp;=-\\gamma|\\dot{x}-\\dot{\\rho}|z|z|^{n-1}-\\beta(\\dot{x}-\\dot{\\rho})|z|^n+A(\\dot{x}-\\dot{\\rho}),\\nonumber\\\\ \\alpha&amp;=\\alpha_a+\\alpha_bu_{bw},\\nonumber\\\\ c_1&amp;=c_{1a}+c_{1b}u_{bw},\\nonumber\\\\ c_0&amp;=c_{0a}+c_{0b}u_{bw},\\nonumber\\\\ \\dot{u}_{bw}&amp;=-\\eta(u_{bw}-E). \\end{align} \\] <p>where \\(f\\) is the damping force, \\(c_1\\) and \\(c_0\\) represent the viscous coefficients, \\(E\\) is the input voltage, \\(x\\) is the displacement and \\(\\dot{x}\\) is the velocity of the model. The parameters of the system (see table below) were taken from Leva, A. and Piroddi, L. - NARX-based technique for the modelling of magneto-rheological damping devices.</p> Parameter Value Parameter Value \\(c_{0_a}\\) \\(20.2 \\, N \\, s/cm\\) \\(\\alpha_{a}\\) \\(44.9 \\, N/cm\\) \\(c_{0_b}\\) \\(2.68 \\, N \\, s/cm \\, V\\) \\(\\alpha_{b}\\) \\(638 \\, N/cm\\) \\(c_{1_a}\\) \\(350 \\, N \\, s/cm\\) \\(\\gamma\\) \\(39.3 \\, cm^{-2}\\) \\(c_{1_b}\\) \\(70.7 \\, N \\, s/cm \\, V\\) \\(\\beta\\) \\(39.3 \\, cm^{-2}\\) \\(k_{0}\\) \\(15 \\, N/cm\\) \\(n\\) \\(2\\) \\(k_{1}\\) \\(5.37 \\, N/cm\\) \\(\\eta\\) \\(251 \\, s^{-1}\\) \\(x_{0}\\) \\(0 \\, cm\\) \\(A\\) \\(47.2\\) <p>For this particular study, both displacement and voltage inputs, \\(x\\) and \\(E\\), respectively, were generated by filtering a white Gaussian noise sequence using a Blackman-Harris FIR filter with \\(6\\)Hz cutoff frequency. The integration step-size was set to \\(h = 0.002\\), following the procedures described in Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models. These procedures are for identification purposes only since the inputs of a MRD could have several different characteristics.</p> <p>The data used in this example is provided by the Professor Samir Angelo Milani Martins.</p> <p>The challenges are:</p> <ul> <li>it possesses a nonlinearity featuring memory, i.e. a dynamic nonlinearity;</li> <li>the nonlinearity is governed by an internal variable z(t), which is not measurable;</li> <li>the nonlinear functional form in the Bouc Wen equation is nonlinear in the parameter;</li> <li>the nonlinear functional form in the Bouc Wen equation does not admit a finite Taylor series expansion because of the presence of absolute values</li> </ul>"},{"location":"user-guide/tutorials/modeling-a-magneto-rheological-damper-device/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nscikit-learn==1.4.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/modeling-a-magneto-rheological-damper-device/#sysidentpy-configuration","title":"SysIdentPy Configuration","text":"<pre><code>import numpy as np\nfrom sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/bouc_wen/boucwen_histeretic_system.csv\"\n)\nscaler_x = MaxAbsScaler()\nscaler_y = MaxAbsScaler()\n\ninit = 400\nx_train = df[[\"E\", \"v\"]].iloc[init : df.shape[0] // 2, :]\nx_train[\"sign_v\"] = np.sign(df[\"v\"])\nx_train = scaler_x.fit_transform(x_train)\n\nx_test = df[[\"E\", \"v\"]].iloc[df.shape[0] // 2 + 1 : df.shape[0] - init, :]\nx_test[\"sign_v\"] = np.sign(df[\"v\"])\nx_test = scaler_x.transform(x_test)\n\ny_train = df[[\"f\"]].iloc[init : df.shape[0] // 2, :].values.reshape(-1, 1)\ny_train = scaler_y.fit_transform(y_train)\n\ny_test = (\n    df[[\"f\"]].iloc[df.shape[0] // 2 + 1 : df.shape[0] - init, :].values.reshape(-1, 1)\n)\ny_test = scaler_y.transform(y_test)\n\n# Plotting the data\nplt.figure(figsize=(10, 8))\nplt.suptitle(\"Identification (training) data\", fontsize=16)\n\nplt.subplot(221)\nplt.plot(y_train, \"k\")\nplt.ylabel(\"Force - Output\")\nplt.xlabel(\"Samples\")\nplt.title(\"y\")\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(222)\nplt.plot(x_train[:, 0], \"k\")\nplt.ylabel(\"Control Voltage\")\nplt.xlabel(\"Samples\")\nplt.title(\"x_1\")\nplt.grid()\nplt.axis([0, 1500, 0, 1])\n\nplt.subplot(223)\nplt.plot(x_train[:, 1], \"k\")\nplt.ylabel(\"Velocity\")\nplt.xlabel(\"Samples\")\nplt.title(\"x_2\")\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(224)\nplt.plot(x_train[:, 2], \"k\")\nplt.ylabel(\"sign(Velocity)\")\nplt.xlabel(\"Samples\")\nplt.title(\"x_3\")\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n</code></pre> <p>Let's check how is the histeretic behavior considering each input:</p> <pre><code>plt.figure()\nplt.plot(x_train[:, 0], y_train)\nplt.xlabel(\"x1 - Voltage\")\nplt.ylabel(\"y - Force\")\n\nplt.figure()\nplt.plot(x_train[:, 1], y_train)\nplt.xlabel(\"x2 - Velocity\")\nplt.ylabel(\"y - Force\")\n\nplt.figure()\nplt.plot(x_train[:, 2], y_train)\nplt.xlabel(\"u3 - sign(Velocity)\")\nplt.ylabel(\"y - Force\")\n</code></pre> <pre><code>Text(0, 0.5, 'y - Force')\n</code></pre> <p></p> <p></p> <p></p> <p>Now, we can just build a NARX model:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=[[1], [1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"FROLS: sign(v) and MaxAbsScaler\",\n)\n</code></pre> <pre><code>0.04510435472905795\n</code></pre> <p></p> <p>If we remove the <code>sign(v)</code> input and try to build a NARX model using the same configuration, the model diverge, as can be seen in the following figure:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=[[1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"FROLS: MaxAbsScaler, discarding sign(v)\",\n)\n</code></pre> <pre><code>nan\n\n\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\n</code></pre> <p></p> <p>If we use the <code>MetaMSS</code> algorithm instead, the results are better.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\nbasis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n    xlag=[[1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    random_state=42,\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"MetaMSS: MaxAbsScaler, discarding sign(v)\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:453: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n\n\n0.24685651932553157\n</code></pre> <p></p> <p>However, when the output of the system reach its minimum value, the model oscillate</p> <pre><code>plot_results(\n    y=y_test[1100:1200], yhat=yhat[1100:1200], n=10000, title=\"Unstable region\"\n)\n</code></pre> <p></p> <p>If we add the <code>sign(v)</code> input again and use <code>MetaMSS</code>, the results are very close to the <code>FROLS</code> algorithm with all inputs</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n    xlag=[[1], [1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"MetaMSS: sign(v) and MaxAbsScaler\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\parameter_estimation\\estimators.py:75: UserWarning: Psi matrix might have linearly dependent rows.Be careful and check your data\n  self._check_linear_dependence_rows(psi)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:453: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n\n\n0.055422497807759194\n</code></pre> <p></p> <p>This case will also highlight the significance of data scaling. Previously, we used the <code>MaxAbsScaler</code> method, which resulted in great models when using the <code>sign(v)</code> inputs, but also resulted in unstable models when removing that input feature. When scaling is applied using <code>MinMaxScaler</code>, however, the overall stability of the results improves, and the model does not diverge, even when the <code>sign(v)</code> input is removed, using the <code>FROLS</code> algorithm.</p> <p>The user can get the results bellow by just changing the data scaling method using</p> <pre><code>scaler_x = MinMaxScaler()\nscaler_y = MinMaxScaler()\n</code></pre> <p>and running the each model again. That is the only change to improve the results.</p> <p></p> <p>FROLS: with <code>sign(v)</code> and <code>MinMaxScaler</code>. RMSE: 0.1159</p> <p> FROLS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1639</p> <p></p> <p>MetaMSS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1762</p> <p></p> <p>MetaMSS: including <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.0694</p> <p>In contrast, the MetaMSS method returned the best model overall, but not better than the best <code>FROLS</code> method using <code>MaxAbsScaler</code>.</p> <p>Here is the predicted histeretic loop:</p> <pre><code>plt.plot(x_test[:, 1], yhat)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x225ff4f8b00&gt;]\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/","title":"Multiobjective Parameter Estimation - An Overview","text":"<p>Example created by Gabriel Bueno Leandro, Samir Milani Martins and Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Multiobjective parameter estimation represents a fundamental paradigm shift in the way we approach the parameter tuning problem for NARMAX models. Instead of seeking a single set of parameter values that optimally fits the model to the data, multiobjective approaches aim to identify a set of parameter solutions, known as the Pareto front, that provide a trade-off between competing objectives. These objectives often encompass a spectrum of model performance criteria, such as goodness-of-fit, model complexity, and robustness.</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#reference","title":"Reference","text":"<p>For further information, check this reference: https://doi.org/10.1080/00207170601185053.</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#use-case-buck-converter","title":"Use case: Buck converter","text":"A buck converter is a type of DC/DC converter that decreases the voltage (while increasing the current) from its input (power supply) to its output (load). It is similar to a boost converter (elevator) and is a type of switched-mode power supply (SMPS) that typically contains at least two semiconductors (a diode and a transistor, although modern buck converters replace the diode with a second transistor used for synchronous rectification) and at least one energy storage element, a capacitor, inductor or both combined.  <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.multiobjective_parameter_estimation import AILS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.narmax_tools import set_weights\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#dynamic-behavior","title":"Dynamic Behavior","text":"<pre><code>df_train = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/buck/buck_id.csv\"\n)\ndf_valid = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/buck/buck_valid.csv\"\n)\n\n# Plotting the measured output (identification and validation data)\nplt.figure(1)\nplt.title(\"Output\")\nplt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5)\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Voltage\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Plotting the measured input (identification and validation data)\nplt.figure(2)\nplt.title(\"Input\")\nplt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5)\nplt.ylim(2.1, 2.6)\nplt.ylabel(\"u\")\nplt.xlabel(\"Samples\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#buck-converter-static-function","title":"Buck Converter Static Function","text":"<p>The duty cycle, represented by the symbol \\(D\\), is defined as the ratio of the time the system is on (\\(T_{on}\\)\u200b) to the total operation cycle time (\\(T\\)). Mathematically, this can be expressed as \\(D=\\frac{T_{on}}{T}\\). The complement of the duty cycle, represented by \\(D'\\), is defined as the ratio of the time the system is off (\\(T_{off}\\)) to the total operation cycle time (\\(T\\)) and can be expressed as \\(D'=\\frac{T_{off}}{T}\\).</p> <p>The load voltage (\\(V_o\\)) is related to the source voltage (\\(V_d\\)) by the equation \\(V_o\u200b=D\u22c5V_d\u200b=(1\u2212D\u2019)\u22c5V_d\\).  For this particular converter, it is known that \\(D\u2032=\\frac{\\bar{u}-1}{3}\u200b\\),\u200b which means that the static function of this system can be derived from theory to be:</p> <p>\\(V_o = \\frac{4V_d}{3} - \\frac{V_d}{3}\\cdot \\bar{u}\\)</p> <p>If we assume that the source voltage \\(V_d\\)\u200b is equal to 24 V, then we can rewrite the above expression as follows:</p> <p>\\(V_o = (4 - \\bar{u})\\cdot 8\\)</p> <pre><code># Static data\nVd = 24\nUo = np.linspace(0, 4, 50)\nYo = (4 - Uo) * Vd / 3\nUo = Uo.reshape(-1, 1)\nYo = Yo.reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Curve\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{y}$\")\nplt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\")\nplt.show()\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#buck-converter-static-gain","title":"Buck converter Static Gain","text":"<p>The gain of a Buck converter is a measure of how its output voltage changes in response to changes in its input voltage. Mathematically, the gain can be calculated as the derivative of the converter\u2019s static function, which describes the relationship between its input and output voltages. In this case, the static function of the Buck converter is given by the equation:</p> <p>\\(V_o = (4 - \\bar{u})\\cdot 8\\)</p> <p>Taking the derivative of this equation with respect to \\(\\hat{u}\\), we find that the gain of the Buck converter is equal to \u22128. In other words, for every unit increase in the input voltage \\(\\hat{u}\\), the output voltage Vo\u200b will decrease by 8 units.</p> <p>so \\(gain=V_o'=-8\\)</p> <pre><code># Defining the gain\ngain = -8 * np.ones(len(Uo)).reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Gain\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{gain}$\")\nplt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#building-a-dynamic-model-using-the-mono-objective-approach","title":"Building a dynamic model using the mono-objective approach","text":"<pre><code>x_train = df_train.input.values.reshape(-1, 1)\ny_train = df_train.y.values.reshape(-1, 1)\nx_valid = df_valid.input.values.reshape(-1, 1)\ny_valid = df_valid.y.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=8,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1ee748da650&gt;\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#affine-information-least-squares-algorithm-ails","title":"Affine Information Least Squares Algorithm (AILS)","text":"<p>AILS is a multiobjective parameter estimation algorithm, based on a set of affine information pairs. The multiobjective approach proposed in the mentioned paper and implemented in SysIdentPy leads to a convex multiobjective optimization problem, which can be solved by AILS. AILS is a LeastSquares-type non-iterative scheme for finding the Pareto-set solutions for the multiobjective problem.</p> <p>So, with the model structure defined (we will be using the one built using the dynamic data above), one can estimate the parameters using the multiobjective approach.</p> <p>The information about static function and static gain, besides the usual dynamic input/output data, can be used to build the pair of affine information to estimate the parameters of the model. We can model the cost function as:</p> <p>$ \\gamma(\\hat\\theta) = w_1\\cdot J_{LS}(\\hat{\\theta})+w_2\\cdot J_{SF}(\\hat{\\theta})+w_3\\cdot J_{SG}(\\hat{\\theta}) $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#multiobjective-parameter-estimation-considering-3-different-objectives-the-prediction-error-the-static-function-and-the-static-gain","title":"Multiobjective parameter estimation considering 3 different objectives: the prediction error, the static function and the static gain","text":"<pre><code># you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work\nmo_estimator = AILS(final_model=model.final_model)\n\n# setting the log-spaced weights of each objective function\nw = set_weights(static_function=True, static_gain=True)\n\n# you can also use something like\n\n# w = np.array(\n#     [\n#         [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01],\n#         [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01],\n#         [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98],\n#     ]\n# )\n\n# to set the weights. Each row correspond to each objective\n</code></pre> <p>AILS has an <code>estimate</code> method that returns the cost functions (J), the Euclidean norm of the cost functions (E), the estimated parameters referring to each weight (theta), the regressor matrix of the gain and static_function affine information HR and QR, respectively.</p> <pre><code>J, E, theta, HR, QR, position = mo_estimator.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.006842 0.003078 0.990080 0.999970 1.095020e-05 0.000013 0.245244 1 0.007573 0.002347 0.990080 0.999938 2.294665e-05 0.000016 0.245236 2 0.008382 0.001538 0.990080 0.999885 6.504913e-05 0.000018 0.245223 3 0.009277 0.000642 0.990080 0.999717 4.505541e-04 0.000021 0.245182 4 0.006842 0.098663 0.894495 1.000000 7.393246e-08 0.000015 0.245251 ... ... ... ... ... ... ... ... 2290 0.659632 0.333527 0.006842 0.995896 3.965699e-04 1.000000 0.244489 2291 0.730119 0.263039 0.006842 0.995632 5.602981e-04 0.972842 0.244412 2292 0.808139 0.185020 0.006842 0.995364 8.321071e-04 0.868299 0.244300 2293 0.894495 0.098663 0.006842 0.995100 1.364999e-03 0.660486 0.244160 2294 0.990080 0.003078 0.006842 0.992584 9.825987e-02 0.305492 0.261455 <p>2295 rows \u00d7 7 columns</p> <p>Now we can set theta related to any weight results</p> <pre><code>model.theta = theta[-1, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the last combination of the weights\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 2.2930E+00 9.999E-01 1 y(k-1) 2.3307E-01 2.042E-05 2 y(k-2) 6.3209E-01 1.108E-06 3 x1(k-1) -5.9333E-01 4.688E-06 4 y(k-1)^2 2.7673E-01 3.922E-07 5 y(k-2)y(k-1) -5.3228E-01 8.389E-07 6 x1(k-1)y(k-1) 1.6667E-02 5.690E-07 7 y(k-2)^2 2.5766E-01 3.827E-06"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#the-dynamic-results-for-that-chosen-theta-is","title":"The dynamic results for that chosen theta is","text":"<pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#the-static-gain-result-is","title":"The static gain result is","text":"<pre><code>plt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#the-static-function-result-is","title":"The static function result is","text":"<pre><code>plt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#getting-the-best-weight-combination-based-on-the-norm-of-the-cost-function","title":"Getting the best weight combination based on the norm of the cost function","text":"<pre><code># the variable `position` returned in `estimate` method give the position of the best weight combination\nmodel.theta = theta[position, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the best combination of the weights\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\n# The dynamic results for that chosen theta is\nplot_results(y=y_valid, yhat=yhat, n=1000)\n# The static gain result is\nplt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n# The static function result is\nplt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>      Regressors   Parameters        ERR\n0              1   1.5405E+00  9.999E-01\n1         y(k-1)   2.9687E-01  2.042E-05\n2         y(k-2)   6.4693E-01  1.108E-06\n3        x1(k-1)  -4.1302E-01  4.688E-06\n4       y(k-1)^2   2.7671E-01  3.922E-07\n5   y(k-2)y(k-1)  -5.3474E-01  8.389E-07\n6  x1(k-1)y(k-1)   4.0624E-03  5.690E-07\n7       y(k-2)^2   2.5832E-01  3.827E-06\n</code></pre> <p>You can also plot the pareto-set solutions</p> <pre><code>plt.figure(6)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Pareto-set solutions\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#detailing-ails","title":"Detailing AILS","text":"<p>The polynomial NARX model built using the mono-objective approach has the following structure:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 $</p> <p>The, the goal when using the static function and static gain information in the multiobjective scenario is to estimate the vector \\(\\hat{\\theta}\\) based on:</p> <p>$ \\theta = [w_1\\Psi^T\\Psi + w_2(HR)^T(HR) + w_3(QR)(QR)<sup>T]</sup> [w_1\\Psi^T y + w_2(HR)<sup>T\\overline{g}+w_3(QR)</sup>T\\overline{y}] $</p> <p>The \\(\\Psi\\) matrix is built using the usual mono-objective dynamic modeling approach in SysIdentPy. However, it is still necessary to find the Q, H and R matrices. AILS have the methods to compute all of those matrices. Basically, to do that, \\(q_i^T\\) is first estimated:</p> <p>$ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_1} &amp; \\overline{y_i}^2 &amp; \\cdots &amp; \\overline{y_i}^l &amp;  F_{yu} &amp; \\overline{u_i}^2 &amp; \\cdots &amp; \\overline{u_i}^l \\end{bmatrix} $</p> <p>where \\(F_{yu}\\) stands for all non-linear monomials in the model that are related to \\(y(k)\\) and \\(u(k)\\), \\(l\\) is the largest non-linearity in the model for input and output terms. For a model with a degree of nonlinearity equal to 2, we can obtain:</p> <p>$ q_i^T =  \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_i} &amp; \\overline{y_i}^2 &amp; \\overline{u_i}:\\overline{y_i} &amp; \\overline{u_i}^2  \\end{bmatrix} $</p> <p>It is possible to encode the \\(q_i^T\\) matrix so that it follows the model encoding defined in SysIdentPy. To do this, 0 is considered as a constant, \\(y_i\\) equal to 1 and \\(u_i\\) equal to 2. The number of columns indicates the degree of nonlinearity of the system and the number of rows reflects the number of terms:</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\ 1 &amp; 0\\ 2 &amp; 0\\ 1 &amp; 1\\ 2 &amp; 1\\ 2 &amp; 2\\ \\end{bmatrix} $ $ = $ $  \\begin{bmatrix} 1 \\ \\overline{y_i}\\ \\overline{u_i}\\ \\overline{y_i}^2\\ \\overline{u_i}:\\overline{y_i}\\ \\overline{u_i}^2\\ \\end{bmatrix} $</p> <p>Finally, the result can be easily obtained using the \u2018regressor_space\u2019 method of SysIdentPy</p> <pre><code>from sysidentpy.narmax_base import RegressorDictionary\n\nobject_qit = RegressorDictionary(xlag=1, ylag=1)\nR_example = object_qit.regressor_space(n_inputs=1) // 1000\nprint(f\"R = {R_example}\")\n</code></pre> <pre><code>R = [[0 0]\n [1 0]\n [2 0]\n [1 1]\n [2 1]\n [2 2]]\n</code></pre> <p>such that:</p> <p>$ \\overline{y_i} = q_i^T R\\theta $</p> <p>and:</p> <p>$ \\overline{g_i} = H R\\theta $</p> <p>where \\(R\\) is the linear mapping of the static regressors represented by \\(q_i^T\\). In addition, the \\(H\\) matrix holds affine information regarding \\(\\overline{g_i}\\), which is equal to \\(\\overline{g_i} = \\frac{d\\overline{y}}{d\\overline{u}}{\\big |}_{(\\overline{u_i}\\:\\overline{y_i})}\\).</p> <p>From now on, we will begin to apply the parameter estimation in a multiobjective manner. This will be done with the NARX polynomial model of the BUCK converter in mind. In this context, \\(q_i^T\\) will be generic and will assume a specific format for the problem at hand. For this task, the \\(R_qit\\) method will be used, whose objective is to return the \\(q_i^T\\) related to the model and the matrix of the linear mapping \\(R\\):</p> <pre><code>R, qit = mo_estimator.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> <pre><code>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 0 0 0 0]\n [0 0 0 0 1 1 0 1]\n [0 0 0 0 0 0 1 0]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [2 0]\n [1 1]]\n</code></pre> <p>So</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\ 1 &amp; 0\\ 2 &amp; 0\\ 1 &amp; 1\\ 2 &amp; 1\\  \\end{bmatrix} $ $ =$ $ \\begin{bmatrix} 1\\ \\overline{y}\\ \\overline{u}\\ \\overline{y^2}\\ \\overline{u}:\\overline{y}\\  \\end{bmatrix} $</p> <p>You can notice that the method produces outputs consistent with what is expected:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 $</p> <p>and:</p> <p>$ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ \\overline{y^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\ \\overline{y}:\\overline{u} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\end{bmatrix} $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#validation","title":"Validation","text":"<p>The following model structure will be used to validate the approach:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 $</p> <p>\\(\\therefore\\)</p> <p>$ final_model =  \\begin{bmatrix} 1001 &amp; 0\\ 1002 &amp; 0\\ 0 &amp; 0\\ 2001 &amp; 0\\ 2001 &amp; 2001\\ 2002 &amp; 2001\\ 2002 &amp; 0\\ 2002 &amp; 2002 \\end{bmatrix} $</p> <p>defining in code:</p> <pre><code>final_model = np.array(\n    [\n        [1001, 0],\n        [1002, 0],\n        [0, 0],\n        [2001, 0],\n        [2001, 2001],\n        [2002, 2001],\n        [2002, 0],\n        [2002, 2002],\n    ]\n)\nfinal_model\n</code></pre> <pre><code>array([[1001,    0],\n       [1002,    0],\n       [   0,    0],\n       [2001,    0],\n       [2001, 2001],\n       [2002, 2001],\n       [2002,    0],\n       [2002, 2002]])\n</code></pre> <pre><code>mult2 = AILS(final_model=final_model)\n</code></pre> <pre><code>def psi(X, Y):\n    PSI = np.zeros((len(X), 8))\n    for k in range(2, len(Y)):\n        PSI[k, 0] = Y[k - 1]\n        PSI[k, 1] = Y[k - 2]\n        PSI[k, 2] = 1\n        PSI[k, 3] = X[k - 1]\n        PSI[k, 4] = X[k - 1] ** 2\n        PSI[k, 5] = X[k - 2] * X[k - 1]\n        PSI[k, 6] = X[k - 2]\n        PSI[k, 7] = X[k - 2] ** 2\n    return np.delete(PSI, [0, 1], axis=0)\n</code></pre> <p>The value of theta with the lowest mean squared error obtained with the same code implemented in Scilab was:</p> <p>$ W_{LS} = 0.3612343 $</p> <p>and:</p> <p>$ W_{SG} = 0.3548699 $</p> <p>and:</p> <p>$ W_{SF} = 0.3548699 $</p> <pre><code>PSI = psi(x_train, y_train)\nw = np.array([[0.3612343], [0.2838959], [0.3548699]])\n</code></pre> <pre><code>C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:4: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 0] = Y[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 1] = Y[k - 2]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:7: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 3] = X[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:8: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 4] = X[k - 1] ** 2\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:9: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 5] = X[k - 2] * X[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:10: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 6] = X[k - 2]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:11: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 7] = X[k - 2] ** 2\n</code></pre> <pre><code>J, E, theta, HR, QR, position = mult2.estimate(\n    y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\n# the order of the weights is different because the way we implemented in Python, but the results are very close as expected\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.361234 0.35487 0.283896 1.0 1.0 1.0 1.0 <p>Dynamic results</p> <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = mult2.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4287E+00 9.999E-01 1 y(k-1) 5.5147E-01 2.042E-05 2 y(k-2) 4.0449E-01 1.108E-06 3 x1(k-1) -1.2605E+01 4.688E-06 4 x1(k-2) 1.2257E+01 3.922E-07 5 x1(k-1)^2 8.3274E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1416E+01 5.690E-07 7 x1(k-2)^2 3.0846E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <p>Static gain</p> <pre><code>plt.figure(7)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Static function</p> <pre><code>plt.figure(8)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Pareto-set solutions</p> <pre><code>plt.figure(9)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Optimum pareto-curve\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p> <pre><code>theta[position, :]\n</code></pre> <pre><code>array([  1.42867821,   0.55147249,   0.40449005, -12.60549001,\n        12.25730092,   8.32739876, -11.41573694,   3.08460955])\n</code></pre> <p>The following table show the results reported in \u2018IniciacaoCientifica2007\u2019 and the ones obtained with SysIdentPy implementation</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.5514725 0.549144 \\(\\theta_2\\) 0.40449005 0.408028 \\(\\theta_3\\) 1.42867821 1.45097 \\(\\theta_4\\) -12.60548863 -12.55788 \\(\\theta_5\\) 8.32740057 8.1516315 \\(\\theta_6\\) -11.41574116 -11.09728 \\(\\theta_7\\) 12.25729955 12.215782 \\(\\theta_8\\) 3.08461195 2.9319577 <p>where:</p> <p>$ E_{Scilab} =    17.426613 $</p> <p>and:</p> <p>$ E_{Python} = 17.474865 $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#note-as-mentioned-before-the-order-of-the-regressors-in-the-model-change-but-it-is-the-same-structure-the-tables-shows-the-respective-regressor-parameter-concerning-sysidentpy-and-iniciacaocientifica2007-but-the-order-1-2-and-so-on-are-not-the-same-of-the-ones-in-modelfinal_model","title":"Note: as mentioned before, the order of the regressors in the model change, but it is the same structure. The tables shows the respective regressor parameter concerning <code>SysIdentPy</code> and <code>IniciacaoCientifica2007</code>,  but the order <code>\u03b8\u2081</code>, <code>\u03b8\u2082</code> and so on are not the same of the ones in model.final_model","text":"<pre><code>R, qit = mult2.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> <pre><code>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 1 0 0 0]\n [0 0 0 0 0 1 1 1]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [0 2]]\n</code></pre> <p>model's structure that will be utilized (\u2018IniciacaoCientifica2007\u2019):</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 $</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\ 1 &amp; 0\\ 2 &amp; 0\\ 2 &amp; 2\\  \\end{bmatrix} $ $ = $ $ \\begin{bmatrix} 1\\ \\overline{y}\\ \\overline{u}\\ \\overline{u^2} \\end{bmatrix} $</p> <p>and:</p> <p>$ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\ \\overline{u^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} $</p> <p>consistent with matrix R:</p> <p>R = [0 0 1 0 0 0 0 0;1 1 0 0 0 0 0 0;0 0 0 1 0 0 1 0;0 0 0 0 1 1 0 1]; // R </p> <p>or:</p> <p>$  R =  \\begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#biobjective-optimization","title":"Biobjective optimization","text":""},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#an-use-case-applied-to-buck-converter-cc-cc-using-as-objectives-the-static-curve-information-and-the-prediction-error-dynamic","title":"An use case applied to Buck converter CC-CC using as objectives the static curve information and the prediction error (dynamic)","text":"<pre><code>bi_objective = AILS(\n    static_function=True, static_gain=False, final_model=final_model, normalize=True\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> <p>$ W_{LS} = 0.9931126 $</p> <p>and:</p> <p>$ W_{SF} = 0.0068874 $</p> <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective.estimate(\n    y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg ||J||: 0 0.990080 0.009920 0.990863 1.000000 0.990939 1 0.987127 0.012873 0.990865 0.987032 0.990939 2 0.984182 0.015818 0.990867 0.974307 0.990939 3 0.981247 0.018753 0.990870 0.961803 0.990940 4 0.978320 0.021680 0.990873 0.949509 0.990941 ... ... ... ... ... ... 1995 0.002555 0.997445 0.999993 0.000072 0.999993 1996 0.002547 0.997453 0.999994 0.000072 0.999994 1997 0.002540 0.997460 0.999996 0.000071 0.999996 1998 0.002532 0.997468 0.999998 0.000071 0.999998 1999 0.002525 0.997475 1.000000 0.000070 1.000000 <p>2000 rows \u00d7 5 columns</p> <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 1.3873E+00 9.999E-01 1 y(k-1) 5.4941E-01 2.042E-05 2 y(k-2) 4.0804E-01 1.108E-06 3 x1(k-1) -1.2515E+01 4.688E-06 4 x1(k-2) 1.2227E+01 3.922E-07 5 x1(k-1)^2 8.1171E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1047E+01 5.690E-07 7 x1(k-2)^2 2.9043E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(10)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Static Curve Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>where the best estimated \\(\\theta\\) is</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54940883 0.5494135 \\(\\theta_2\\) 0.40803995 0.4080312 \\(\\theta_3\\) 1.38725684 3.3857601 \\(\\theta_4\\) -12.51466378 -12.513688 \\(\\theta_5\\) 8.11712897 8.116575 \\(\\theta_6\\) -11.04664789 -11.04592 \\(\\theta_7\\) 12.22693907 12.227184 \\(\\theta_8\\) 2.90425844 2.9038468 <p>where:</p> <p>$ E_{Scilab} = 17.408934 $</p> <p>and:</p> <p>$ E_{Python} = 17.408947 $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#multiobjective-parameter-estimation","title":"Multiobjective parameter estimation","text":""},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#use-case-considering-2-different-objectives-the-prediction-error-and-the-static-gain","title":"Use case considering 2 different objectives: the prediction error and the static gain","text":"<pre><code>bi_objective_gain = AILS(\n    static_function=False, static_gain=True, final_model=final_model, normalize=False\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> <p>$ W_{LS} = 0.9931126 $</p> <p>and:</p> <p>$ W_{SF} = 0.0068874 $</p> <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(0, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\n# W = np.array([[0.9931126],\n# [0.0068874]])\nJ, E, theta, HR, QR, position = bi_objective_gain.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg ||J||: 0 1.000000 0.000000 17.407256 3.579461e+01 39.802849 1 0.997012 0.002988 17.407528 2.109260e-01 17.408806 2 0.994033 0.005967 17.407540 2.082067e-01 17.408785 3 0.991063 0.008937 17.407559 2.056636e-01 17.408774 4 0.988102 0.011898 17.407585 2.031788e-01 17.408771 ... ... ... ... ... ... 1995 0.002555 0.997445 17.511596 3.340081e-07 17.511596 1996 0.002547 0.997453 17.511596 3.320125e-07 17.511596 1997 0.002540 0.997460 17.511597 3.300289e-07 17.511597 1998 0.002532 0.997468 17.511598 3.280571e-07 17.511598 1999 0.002525 0.997475 17.511599 3.260972e-07 17.511599 <p>2000 rows \u00d7 5 columns</p> <pre><code># Writing the results\nmodel.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective_gain.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4853E+00 9.999E-01 1 y(k-1) 5.4940E-01 2.042E-05 2 y(k-2) 4.0806E-01 1.108E-06 3 x1(k-1) -1.2581E+01 4.688E-06 4 x1(k-2) 1.2210E+01 3.922E-07 5 x1(k-1)^2 8.1686E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1122E+01 5.690E-07 7 x1(k-2)^2 2.9455E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(12)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Gain Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>being the selected \\(\\theta\\):</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54939785 0.54937289 \\(\\theta_2\\) 0.40805603 0.40810168 \\(\\theta_3\\) 1.48525190 1.48663719 \\(\\theta_4\\) -12.58066084 -12.58127183 \\(\\theta_5\\) 8.16862622 8.16780294 \\(\\theta_6\\) -11.12171897 -11.11998621 \\(\\theta_7\\) 12.20954849 12.20927355 \\(\\theta_8\\) 2.94548501 2.9446532 <p>where:</p> <p>$ E_{Scilab} =  17.408997 $</p> <p>and:</p> <p>$ E_{Python} = 17.408781 $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#additional-information","title":"Additional Information","text":"<p>You can also access the matrix Q and H using the following methods</p> <p>Matrix Q:</p> <pre><code>bi_objective_gain.build_static_function_information(Uo, Yo)[1]\n</code></pre> <pre><code>array([[   50.        ,   800.        ,   800.        ,   100.        ,\n          100.        ,   269.3877551 ,   269.3877551 ,   269.3877551 ],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407]])\n</code></pre> <p>Matrix H+R:</p> <pre><code>bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1]\n</code></pre> <pre><code>array([[    0.        ,     0.        ,     0.        ,     0.        ,\n            0.        ,     0.        ,     0.        ,     0.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041]])\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/","title":"Parameter Estimation - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> <pre><code>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import (\n    TotalLeastSquares,\n    RecursiveLeastSquares,\n    NonNegativeLeastSquares,\n    LeastMeanSquares,\n    AffineLeastMeanSquares,\n)\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data","text":"<p>The data is generated by simulating the following model:</p> <p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#there-are-several-method-to-be-used-for-parameter-estimation","title":"There are several method to be used for parameter estimation.","text":"<ul> <li>Least Squares;</li> <li>Total Least Squares;</li> <li>Recursive Least Squares</li> <li>Ridge Regression</li> <li>NonNegative Least Squares</li> <li>Least Squares Minimal Residues</li> <li>Bounded Variable Least Squares</li> <li>Least Mean Squares</li> <li>Affine Least Mean Squares</li> <li>Least Mean Squares Sign Error</li> <li>Normalized Least Mean Squares</li> <li>Least Mean Squares Normalized Sign Error</li> <li>Least Mean Squares Sign Regressor</li> <li>Least Mean Squares Normalized Sign Regressor</li> <li>Least Mean Squares Sign Sign</li> <li>Least Mean Squares Normalized Sign Sign</li> <li>Least Mean Squares Normalized Leaky</li> <li>Least Mean Squares Leaky</li> <li>Least Mean Squares Fourth</li> <li>Least Mean Squares Mixed Norm</li> </ul> <p>Polynomial NARMAX models are linear-in-the-parameter, so Least Squares based methods works well for most cases (using with extended least squares algorithm when dealing with colered noise).</p> <p>However, the user can choose some recursive and stochastic gradient descent methods (in this case, the least mean squares algorithm and its variants) to that task too.</p> <p>Choosing the method is straightforward: pass any of the methods mentioned above on estimator parameters.</p> <ul> <li>Note: Each algorithm have specifc parameter that need to be tunned. In the following examples we will use the default ones. More examples regarding tunned parameter will be available soon. For now, the user can read the method documentation for more information.</li> </ul>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#total-least-squares","title":"Total Least Squares","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = TotalLeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0021167167052431584\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56200123E-01\n1         y(k-1)  1.9995E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  1.0004E-01  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#recursive-least-squares","title":"Recursive Least Squares","text":"<pre><code># recursive least squares\nbasis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0020703083403116164\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0012E-01  9.56200123E-01\n1         y(k-1)  2.0021E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  9.9550E-02  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#least-mean-squares","title":"Least Mean Squares","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastMeanSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.015488793944313425\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9775E-01  9.56200123E-01\n1         y(k-1)  2.0085E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  7.5708E-02  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#affine-least-mean-squares","title":"Affine Least Mean Squares","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = AffineLeastMeanSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0021441596280611167\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9989E-01  9.56200123E-01\n1         y(k-1)  1.9992E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  1.0003E-01  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#nonnegative-least-squares","title":"NonNegative Least Squares","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = NonNegativeLeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0021170157359329173\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56200123E-01\n1         y(k-1)  1.9995E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  1.0004E-01  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/silver-box-system/","title":"Silver Box System","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The description content mainly derives (copy and paste) from the associated paper - Three free data sets for development and benchmarking in nonlinear system identification. For a detailed description, readers are referred to the linked reference.</p> <p>The Silverbox system can be seen as an electronic implementation of the Duffing oscillator. It is build as a 2<sup>nd</sup> order linear time-invariant system with a 3<sup>rd</sup> degree polynomial static nonlinearity around it in feedback. This type of dynamics are, for instance, often encountered in mechanical systems Nonlinear Benchmark - Silverbox.</p> <p>In this case study, we will create a NARX model for the Silver box benchmark. The Silver box represents a simplified version of mechanical oscillating processes, which are a critical category of nonlinear dynamic systems. Examples include vehicle suspensions, where shock absorbers and progressive springs play vital roles. The data generated by the Silver box provides a simplified representation of such combined components. The electrical circuit generating this data closely approximates, but does not perfectly match, the idealized models described below.</p> <p>As described in the original paper, the system was excited using a general waveform generator (HPE1445A). The input signal begins as a discrete-time signal \\(r(k)\\), which is converted to an analog signal \\(r_c(t)\\) using zero-order-hold reconstruction. The actual excitation signal \\(u_0(t)\\) is then obtained by passing \\(r_c(t)\\) through an analog low-pass filter \\(G(p)\\) to eliminate high-frequency content around multiples of the sampling frequency. Here, \\(p\\) denotes the differentiation operator. Thus, the input is given by:</p> \\[ u_0(t) = G(p) r_c(t). \\] <p>The input and output signals were measured using HP1430A data acquisition cards, with synchronized clocks for the acquisition and generator cards. The sampling frequency was:</p> \\[ f_s = \\frac{10^7}{2^{14}} = 610.35 \\, \\text{Hz}. \\] <p>The silver box uses analog electrical circuitry to generate data representing a nonlinear mechanical resonating system with a moving mass \\(m\\), viscous damping \\(d\\), and a nonlinear spring \\(k(y)\\). The electrical circuit is designed to relate the displacement \\(y(t)\\) (the output) to the force \\(u(t)\\) (the input) by the following differential equation:</p> \\[ m \\frac{d^2 y(t)}{dt^2} + d \\frac{d y(t)}{dt} + k(y(t)) y(t) = u(t). \\] <p>The nonlinear progressive spring is described by a static, position-dependent stiffness:</p> \\[ k(y(t)) = a + b y^2(t). \\] <p>The signal-to-noise ratio is sufficiently high to model the system without accounting for measurement noise. However, measurement noise can be included by replacing \\(y(t)\\) with the artificial variable \\(x(t)\\) in the equation above, and introducing disturbances \\(w(t)\\) and \\(e(t)\\) as follows:</p> \\[ \\begin{align} &amp; m \\frac{d^2 x(t)}{dt^2} + d \\frac{d x(t)}{dt} + k(x(t)) x(t) = u(t) + w(t), \\\\ &amp; k(x(t)) = a + b x^2(t), \\\\ &amp; y(t) = x(t) + e(t). \\end{align} \\]"},{"location":"user-guide/tutorials/silver-box-system/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/silver-box-system/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.Silverbox(atleast_2d=True)\n\nx_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_multisine.u, test_multisine.y\n\nn = test_multisine.state_initialization_window_length\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation - GerbenBeintema/nonlinear_benchmarks: The official dataload for http://www.nonlinearbenchmark.org/ (github.com) to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plt.plot(x_train)\nplt.plot(y_train, alpha=0.3)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(x_test)\nplt.plot(y_test, alpha=0.3)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(test_arrow_full.u)\nplt.plot(test_arrow_full.y, alpha=0.3)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(test_arrow_no_extrapolation.u)\nplt.plot(test_arrow_no_extrapolation.y, alpha=0.2)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Important Note</p> <p>The goal of this benchmark is to develop a model that outperforms the state-of-the-art (SOTA) model presented in the benchmarking paper. However, the results in the paper differ from those provided in the  GitHub repository.</p> nx Set NRMS RMS (mV) 2 Train 0.10653 5.8103295 2 Validation 0.11411 6.1938068 2 Test 0.19151 10.2358533 2 Test (no extra) 0.12284 5.2789727 4 Train 0.03571 1.9478290 4 Validation 0.03922 2.1286373 4 Test 0.12712 6.7943448 4 Test (no extra) 0.05204 2.2365904 8 Train 0.03430 1.8707026 8 Validation 0.03732 2.0254112 8 Test 0.10826 5.7865255 8 Test (no extra) 0.04743 2.0382715 &gt; Table: results presented in the github. <p>It appears that the values shown in the paper actually represent the training time, not the error metrics. I will contact the authors to confirm this information. According to the Nonlinear Benchmark website, the information is as follows:</p> <p></p> <p>where the values in the \"Training time\" column matches the ones presented as error metrics in the paper.</p> <p>While we await confirmation of the correct values for this benchmark, we will demonstrate the performance of SysIdentPy. However, we will refrain from making any comparisons or attempting to improve the model at this stage.</p>"},{"location":"user-guide/tutorials/silver-box-system/#results","title":"Results","text":"<p>We will start (as we did in every other case study) with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=40</code>. Because we dealing with a large training dataset, we will use the <code>err_tol</code> instead of information criteria to have a faster performance. We will also set <code>n_terms=40</code>, which means that the search will stop if the <code>err_tol</code> is reached or 40 regressors is tested in the <code>ERR</code> algorithm. While this approach might result in a sub-optimal model, it is a reasonable starting point for our first attempt. There are three different experiments: multisine, arrow (full), and arrow (no extrapolation).</p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.999,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nnrmse = rmse / y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=300,\n    figsize=(15, 4),\n    title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n</code></pre> <pre><code>0.1423804033714937 7.727682109791501\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_full.u, test_arrow_full.y\n\nn = test_arrow_full.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9999,\n    n_terms=80,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\n# we will not concatente the last values from train data to use as initial condition here because\n# this test data have a very different behavior.\n# However, if you want you can do that and you will see that the model will still perform\n# great after a few iterations\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nnrmse = rmse / y_test.std()\nrmse_mv = 1000 * rmse\n\nprint(nrmse, rmse_mv)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=300,\n    figsize=(15, 4),\n    title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n</code></pre> <pre><code>0.07762658947015803 4.14903534238172\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_no_extrapolation.u, test_arrow_no_extrapolation.y\n\nn = test_arrow_no_extrapolation.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9999,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nnrmse = rmse / y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Arrow (no extrapolation). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=300,\n    figsize=(15, 4),\n    title=f\"Free Run simulation. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n</code></pre> <pre><code>0.05187400789723806 2.2293393254015776\n</code></pre> <p></p> <p></p>"},{"location":"user-guide/tutorials/wiener-hammerstein-system/","title":"Wiener Hammerstein System","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The description content primarily derives from the benchmark website - Nonlinear Benchmark and associated paper - Wiener-Hammerstein benchmark with process noise. For a detailed description, readers are referred to the linked references.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p> <p>This benchmark focuses on a Wiener-Hammerstein electronic circuit where process noise plays a significant role in distorting the output signal.</p> <p>The Wiener-Hammerstein structure is a well-known block-oriented system which contains a static nonlinearity sandwiched between two Linear Time-Invariant (LTI) blocks (Figure 2). This arrangement presents a challenging identification problem due to the presence of these LTI blocks.</p> <p></p> <p>Figure 2: the Wiener-Hammerstein system</p> <p>In Figure 2, the Wiener-Hammerstein system is illustrated with process noise \\(e_x(t)\\) entering before the static nonlinearity \\(f(x)\\), sandwiched between LTI blocks represented by \\(R(s)\\) and \\(S(s)\\) at the input and output, respectively. Additionally, small, negligible noise sources \\(e_u(t)\\) and \\(e_y(t)\\) affect the measurement channels. The measured input and output signals are denoted as \\(u_m(t)\\) and \\(y_m(t)\\).</p> <p>The first LTI block \\(R(s)\\) is effectively modeled as a third-order lowpass filter. The second LTI subsystem \\(S(s)\\) is configured as an inverse Chebyshev filter with a stop-band attenuation of \\(40 dB\\) and a cutoff frequency of \\(5 kHz\\). Notably, \\(S(s)\\) includes a transmission zero within the operational frequency range, complicating its inversion.</p> <p>The static nonlinearity \\(f(x)\\) is implemented using a diode-resistor network, resulting in saturation nonlinearity. Process noise \\(e_x(t)\\) is introduced as filtered white Gaussian noise, generated from a discrete-time third-order lowpass Butterworth filter followed by zero-order hold and analog low-pass reconstruction filtering with a cutoff of \\(20 kHz\\).</p> <p>Measurement noise sources \\(e_u(t)\\) and \\(e_y(t)\\) are minimal compared to \\(e_x(t)\\). The system's inputs and process noise are generated using an Arbitrary Waveform Generator (AWG), specifically the Agilent/HP E1445A, sampling at \\(78125 Hz\\), synchronized with an acquisition system (Agilent/HP E1430A) to ensure phase coherence and prevent leakage errors. Buffering between the acquisition cards and the system's inputs and outputs minimizes measurement equipment distortion.</p> <p>The benchmark provides two standard test signals through the benchmarking website: a random phase multisine and a sine-sweep signal. Both signals have an \\(rms\\) value of \\(0.71 Vrms\\) and cover frequencies from DC to \\(15 kHz\\) (excluding DC). The sine-sweep spans this frequency range at a rate of \\(4.29 MHz/min\\). These test sets serve as targets for evaluating the model's performance, emphasizing accurate representation under varied conditions.</p> <p>The Wiener-Hammerstein benchmark highlights three primary nonlinear system identification challenges:</p> <ol> <li>Process Noise: Significant in the system, influencing output fidelity.</li> <li>Static Nonlinearity: Indirectly accessible from measured data, posing identification challenges.</li> <li>Output Dynamics: Complex inversion due to transmission zero presence in \\(S(s)\\).</li> </ol> <p>The goal of this benchmark is to develop and validate robust models using separate estimation data, ensuring accurate characterization of the Wiener-Hammerstein system's behavior.</p>"},{"location":"user-guide/tutorials/wiener-hammerstein-system/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/wiener-hammerstein-system/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Wiener-Hammerstein system dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for Wiener-Hammerstein system.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import (\n    LeastSquares,\n    BoundedVariableLeastSquares,\n    NonNegativeLeastSquares,\n    LeastSquaresMinimalResidual,\n)\n\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.WienerHammerBenchMark(atleast_2d=True)\nx_train, y_train = train_val\nx_test, y_test = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plot_n = 800\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_train[:plot_n])\nplt.plot(y_train[:plot_n])\nplt.title(\"Experiment: training data\")\nplt.legend([\"x_train\", \"y_train\"])\nplt.show()\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_test[:plot_n])\nplt.plot(y_test[:plot_n])\nplt.title(\"Experiment: testing data\")\nplt.legend([\"x_test\", \"y_test\"])\nplt.show()\n</code></pre> <p></p> <p></p> <p>The goal of this benchmark it to get a model that have a better performance than the SOTA model provided in the benchmarking paper.</p> <p></p> <p>State of the art results presented in the benchmarking paper. In this section we are only working with the Wiener-Hammerstein results, which are presented in the \\(W-H\\)  column.</p>"},{"location":"user-guide/tutorials/wiener-hammerstein-system/#results","title":"Results","text":"<p>We will start with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=50</code>. This means the FROLS algorithm will not include all regressors when calculating the information criteria used to determine the model order. While this approach might result in a sub-optimal model, it is a reasonable starting point for our first attempt.</p> <pre><code># 3min to run in my machine (amd 5600x, 32gb ram)\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nrmse_sota = rmse / y_test.std()\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=1000,\n    title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\",\n)\n</code></pre> <p></p> <p>The first configuration is already better than the SOTA models shown in the benchmark table! We started using <code>xlag=ylag=7</code> to have an idea of how well SysIdentPy would handle this dataset, but the results are pretty good already! However, the benchmarking paper indicates  that they used higher lags for their models. Let's check what happens if we set <code>xlag=ylag=10</code>.</p> <pre><code># 7min to run in my machine (amd 5600x, 32gb ram)\n\nx_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nrmse_sota = rmse / y_test.std()\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=1000,\n    title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\",\n)\n</code></pre> <p></p> <p>The performance is even better now! For now, we are not worried about the model complexity (even in this case where we are comparing to a deep state neural network...). However, if we check the model order and the <code>AIC</code> plot, we see that the model have 50 regressors , but the <code>AIC</code> values do not change much after each added regression.</p> <pre><code>plt.plot(model.info_values)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x28c0058a450&gt;]\n</code></pre> <p></p> <p>So, what happens if we set a model with half of the regressors?</p> <pre><code># 14 seconds to run\n\nx_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    n_info_values=50,\n    n_terms=25,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nrmse_sota = rmse / y_test.std()\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=1000,\n    title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\",\n)\n</code></pre> <p></p> <p>As shown in the figure above, the results still outperform the SOTA models presented in the benchmarking paper. The SOTA results from the paper could likely be improved as well. Users are encouraged to explore the deepsysid package, which can be used to build deep state neural networks.</p> <p>This basic configuration can serve as a starting point for users to develop even better models using SysIdentPy. Give it a try!</p>"},{"location":"user-guide/tutorials/your-first-model/","title":"Your First Model","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data","text":"<p>The data is generated by simulating the following model:</p> <p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n</code></pre> <p>To obtain a NARMAX model we have to choose some values, e.g, the nonlinearity degree (degree), the maximum lag for the inputs and output (xlag and ylag).</p> <p>In addition, you can select the information criteria to be used with the Error Reduction Ratio to select the model order and the method to estimate the model parameters:</p> <ul> <li>Information Criteria: aic, aicc, bic, lilc, fpe</li> <li>Parameter Estimation: LeastSquares, TotalLeastSquares, RecursiveLeastSquares, NonNegativeLeastSquares, LeastMeanSquares and many more (see the docs)</li> </ul> <p>The n_terms values is optional. It refer to the number of terms to include in the final model. You can set this value based on the information criteria (see below) or based on priori information about the model structure. The default value is n_terms=None, so the algorithm will choose the minimum value reached by the information criteria.</p> <p>To use information criteria you have to set order_selection=True. You can also select n_info_values (default = 15).</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    err_tol=None,\n    basis_function=basis_function,\n)\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#model-structure-selection","title":"Model Structure Selection","text":"<p>The fit method executes the Error Reduction Ratio algorithm using Househoulder reflection to select the model structure.</p> <p>Enforcing keyword-only arguments in fit and predict methods as well. This is an effort to promote clear and non-ambiguous use of the library.</p> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1db932f5090&gt;\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#free-run-simulation","title":"Free run simulation","text":"<p>The predict method is use to generate the predictions. For now we only support free run simulation (also known as infinity steps ahead). Soon will let the user define a one-step ahead or k-step ahead prediction.</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#evaluate-the-model","title":"Evaluate the model","text":"<p>In this example we use the root_relative_squared_error metric because it is often used in System Identification. More metrics and information about it can be found on documentation.</p> <pre><code>rrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00017649882109753117\n</code></pre> <p>model_object.results return the selected model regressors, the estimated parameters and the ERR values. As shown below, the algorithm detect the exact model that was used for simulate the data.</p> <pre><code>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.57604864E-01\n1         y(k-1)  2.0000E-01  3.88976063E-02\n2  x1(k-1)y(k-1)  9.9992E-02  3.49749526E-03\n</code></pre> <p>In addition, you can access the residuals and plot_result methods to take a look at the prediction and two residual analysis. The extras and lam values below contain another residues analysis so you can plot it manually. This method will be improved soon. </p> <pre><code>plt.style.available\n</code></pre> <pre><code>['Solarize_Light2',\n '_classic_test_patch',\n '_mpl-gallery',\n '_mpl-gallery-nogrid',\n 'bmh',\n 'classic',\n 'dark_background',\n 'fast',\n 'fivethirtyeight',\n 'ggplot',\n 'grayscale',\n 'seaborn-v0_8',\n 'seaborn-v0_8-bright',\n 'seaborn-v0_8-colorblind',\n 'seaborn-v0_8-dark',\n 'seaborn-v0_8-dark-palette',\n 'seaborn-v0_8-darkgrid',\n 'seaborn-v0_8-deep',\n 'seaborn-v0_8-muted',\n 'seaborn-v0_8-notebook',\n 'seaborn-v0_8-paper',\n 'seaborn-v0_8-pastel',\n 'seaborn-v0_8-poster',\n 'seaborn-v0_8-talk',\n 'seaborn-v0_8-ticks',\n 'seaborn-v0_8-white',\n 'seaborn-v0_8-whitegrid',\n 'tableau-colorblind10']\n</code></pre> <pre><code>plot_results(\n    y=y_valid,\n    yhat=yhat,\n    n=1000,\n    title=\"test\",\n    xlabel=\"Samples\",\n    ylabel=r\"y, $\\hat{y}$\",\n    data_color=\"#1f77b4\",\n    model_color=\"#ff7f0e\",\n    marker=\"o\",\n    model_marker=\"*\",\n    linewidth=1.5,\n    figsize=(10, 6),\n    style=\"seaborn-v0_8-notebook\",\n    facecolor=\"white\",\n)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(\n    data=ee, title=\"Residues\", ylabel=\"$e^2$\", style=\"seaborn-v0_8-notebook\"\n)\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(\n    data=x1e, title=\"Residues\", ylabel=\"$x_1e$\", style=\"seaborn-v0_8-notebook\"\n)\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/tutorials/your-first-model/#setting-the-n_terms-parameter","title":"Setting the n_terms parameter","text":"<p>In the example above we let the number of terms to compose the final model to be defined as the minimum value of the information criteria. Once you ran the algorithm and choose the best number of parameters, you can turn order_selection to False and set the n_terms value (3 in this example). Here we have a small dataset, but in bigger data this can be critical because running information criteria algorithm is more computational expensive. Since we already know the best number of regressor, we set n_terms and we get the same result.</p> <p>However, this is not only critical because computational efficiency. In many situation, the minimum value of the information criteria can lead to overfitting. In some cases, the difference between choosing a model with 30 regressors or 10 is minimal, so you can take the model with 10 terms without loosing accuracy.</p> <p>In the following we use info_values to plot the information criteria values. As you can see, the minimum value relies where \\(xaxis = 5\\) </p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <p></p> <pre><code> Here we are creating random samples with white noise and letting the algorithm choose\n the number of terms based on the minimum value of information criteria.\n This is not the best approach in System Identification, but serves as a simple example.\n The information criteria must be used as an __auxiliary tool__ to select *n_terms*.\n Plot the information values to help you on that!\n\n If you run the example above several times you might find some cases where the\n algorithm choose only the first two regressors, or four (depending on the information\n criteria method selected). This is because the minimum value of information criteria\n depends on residual variance (affected by noise) and have some limitations in nonlinear\n scenarios. However, if you check the ERR values (robust to noise) you will see that the\n ERR is ordering the regressors in the correct way!\n\n We have some examples on *information_criteria* notebook!\n</code></pre> <p>The n_info_values limits the number of regressors to apply the information criteria. We choose \\(n_y = n_x = \\ell = 2\\), so the candidate regressor is a list of 15 regressors. We can set n_info_values = 15 and see the information values for all regressors. This option can save some amount of computational resources when dealing with multiples inputs and large datasets.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <p></p> <p>Now running without executing information criteria methods (setting the n_terms) because we already know the optimal number of regressors</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_info_values=15,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.00017649882109753117\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.57604864E-01\n1         y(k-1)  2.0000E-01  3.88976063E-02\n2  x1(k-1)y(k-1)  9.9992E-02  3.49749526E-03\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#predict-method","title":"Predict method","text":"<p>One could ask why it is necessary to pass the test data on the predict method. The answers is: you don't need to pass the test data when you are running a infinity-steps ahead prediction, you just need to pass the initial conditions. However, if you wants to check how your model performs in a 1-step ahead prediction or n-step ahead prediction, you should provide the test data.</p> <p>To show you only need the initial condition, consider the following example using the previous trained model:</p> <pre><code>model.max_lag  # the number of initial conditions you should provide\n</code></pre> <pre><code>2\n</code></pre> <pre><code>yhat = model.predict(\n    X=x_valid, y=y_valid[: model.max_lag]\n)  # passing only the 2 initial values which will be used as initial conditions\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00017649882109753117\n</code></pre> <p>As you can see, the rrse obtained is the same as the one obtained when we input the full test data. That is due the fact that even in the cases you provide the full test data, the prediction method only uses the first values as initial conditions and drop the other values in the backend.</p> <p>In the 1-step ahead prediction or n-steps ahead prediction you should provide the full test data because we handle all the computations in the background so the users don't need to care to implement the loops themselves.</p> <p>If you wants the check how your model performs in a 3-steps ahead scenario using 200 samples in the test data, that means that at each 3 iterations the model feedback will use the real data as initial conditions. Thats why the full data test is necessary, because otherwise the model won't find the real value to use as feedback in the n-iteration.</p> <p>This is the case where you have access the historical data so you can check how your model performs in n-steps ahead prediction. If you choose to use a 3-steps ahead model prediction in real life, you have to predict the next 3 samples, wait the 3 iterations, collect the real data and use the new data as initial condition to predict the next 3 values and so on.</p> <p>In the infinity-steps ahead prediction scenario, if your model has a input it will be able to make predictions for all inputs by only providing the initial conditions. If your model has no input (a NAR model, for example), you can set the forecast horizon and the model will make predictions by using only the initial conditions. All the feedback will be the predicted values (this is, by the way, one of the reasons that usually n-steps ahead models are better than infinity-ahead models).</p> <p>It is worth to mention that changing the initial condition doesn't mean you are changing your model. The only thing changing is the initial condition and that could make a real difference in many cases.</p>"},{"location":"user-guide/tutorials/your-first-model/#extra-information","title":"Extra information","text":"<p>You can access some extra information like the list of all candidate regressors</p> <pre><code># for now the list is returned as a codification. Here, $0$ is the constant term, $[1001]=y{k-1}, [100n]=y_{k-n}, [200n] = x1_{k-n}, [300n]=x2_{k-n}$ and so on\nmodel.regressor_code  # list of all possible regressors given non_degree, n_y and n_x values\n</code></pre> <pre><code>array([[   0,    0],\n       [1001,    0],\n       [1002,    0],\n       [2001,    0],\n       [2002,    0],\n       [1001, 1001],\n       [1002, 1001],\n       [2001, 1001],\n       [2002, 1001],\n       [1002, 1002],\n       [2001, 1002],\n       [2002, 1002],\n       [2001, 2001],\n       [2002, 2001],\n       [2002, 2002]])\n</code></pre> <pre><code>print(model.err, \"\\n\\n\")  # err values for the selected terms\nprint(model.theta)  # estimated parameters for the final model structure\n</code></pre> <pre><code>[9.57604864e-01 3.88976063e-02 3.49749526e-03 1.43420284e-10\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n\n\n[[0.90000582]\n [0.20000142]\n [0.0999919 ]]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/","title":"Logistic Map","text":"<p>Tutorial created by Wilson Rocha</p>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/#the-logistic-map","title":"The Logistic Map","text":"<p>Chaotic systems are deterministic processes that exhibit unpredictable, seemingly random behavior due to their extreme sensitivity to initial conditions. Though governed by simple rules, these systems are notoriously difficult to model over long timescales, making them a fascinating challenge for system identification. The Logistic Map is a classic example of chaos, first popularized in ecology to model population growth. Its simple equation,</p> \\[ x_{n+1} = r x_n (1 - x_n) \\] <p>captures a rich variety of behaviors, from stable equilibria to periodic oscillations and full chaos, depending on the growth parameter \\(r\\). By studying this system, we gain insights into universal phenomena like the period-doubling route to chaos and the emergence of fractal structures in bifurcation diagrams. In this tutorial, we\u2019ll use SysIdentPy to model the Logistic Map and reconstruct its bifurcation diagram, showcasing how data-driven approaches can capture chaotic dynamics.</p>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/#visualizing-the-logistic-map","title":"Visualizing the Logistic Map","text":"<p>To understand the Logistic Map, let\u2019s first visualize its behavior across different regimes of \\(r\\):</p> <ul> <li>Stable Fixed Points \\(r &lt; 3\\): For low \\(r\\), the population converges to a steady value.</li> <li>Periodic Regimes \\(3 &lt; r &lt; 3.57\\): As \\(r\\) increases, the system undergoes period-doubling bifurcations, oscillating between 2, 4, 8, \u2026 states.</li> <li>Chaos \\(r \\geq 3.57\\): Beyond the accumulation point, the system behaves chaotically, with trajectories never repeating.</li> </ul> <p>A cobweb plot helps visualize the iteration process: starting from an initial \\(x_0\\), we alternate between evaluating the Logistic Map (vertical jumps) and updating \\(x_n\\) (horizontal moves to the diagonal). Chaotic regimes show erratic, non-repeating cobweb patterns, while periodic regimes trace stable cycles.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.display_results import results\n\n\ndef logistic_map(x, r):\n    \"\"\"Compute one iteration of the logistic map.\"\"\"\n    return r * x * (1 - x)\n\n\n# Parameters\nr = 3.7  # Growth rate (try 2.8, 3.2, 3.5, 3.9)\nn_iter = 50  # Number of iterations\nx0 = 0.2  # Initial condition\n\n# Create cobweb plot\nx = np.linspace(0, 1, 1000)\nf = logistic_map(x, r)\n\nplt.figure(figsize=(8, 6))\nplt.plot(x, f, \"b-\", label=f\"Logistic Map ($r={r}$)\")\nplt.plot(x, x, \"k--\", label=\"$y = x$\")\n\n# Simulate iterations\nxt = np.zeros(n_iter)\nxt[0] = x0\nfor i in range(n_iter - 1):\n    y = logistic_map(xt[i], r)\n    plt.plot([xt[i], xt[i]], [xt[i], y], \"r\", lw=0.5)  # Vertical line\n    plt.plot([xt[i], y], [y, y], \"r\", lw=0.5)  # Horizontal line\n    xt[i + 1] = y\n\nplt.xlabel(\"$x_n$\", fontsize=12)\nplt.ylabel(\"$x_{n+1}$\", fontsize=12)\nplt.title(\"Cobweb Plot for the Logistic Map\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.show()\n</code></pre> <p></p> <p>The plot above shows the Logistic Curve (Blue). The diagonal line (Black Dashed) represents \\(x_{n+1} = x_n\\) and shows the intersections with the logistic curve indicate fixed points. Finally, the vertical lines of the Cobweb trajectory (in red) apply the map \\(x_n \\to x_{n+1}\\) and the horizontal lines reset \\(x_n = x_{n+1}\\) for the next iteration.</p> <p>The interpretation by regime can be taken as follows:</p> <ul> <li> <p>Stable Fixed Point (e.g., \\(r = 2.8\\)):   The cobweb spirals inward to a single point.</p> </li> <li> <p>Periodic (e.g., \\(r = 3.5\\)):   The trajectory cycles between 4 points (period-4).</p> </li> <li> <p>Chaotic (e.g., \\(r = 3.9\\)):   The red lines never repeat, filling the space erratically.</p> </li> </ul>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/#3-generating-the-bifurcation-diagram","title":"3. Generating the Bifurcation Diagram","text":"<p>The bifurcation diagram summarizes how the Logistic Map\u2019s long-term behavior changes with \\(r\\). We will create it considering: 1. Sweeping \\(r\\) across a range (\\(3.5 \\leq r \\leq 4.0\\)). 2. Iterate the map: For each \\(r\\), discard transient iterations (first 500 steps) to focus on asymptotic behavior. 3. Plot \\(x\\) for the remaining iterations.</p> <p>The bifurcation diagram reveals several key features. Period-doubling cascades occur as successive bifurcations split stable points into pairs, leading to progressively longer periods. Chaotic regions emerge as dense bands of points, indicating a high sensitivity to initial conditions. Within these chaotic regimes, windows of order appear, such as near \\(r \\approx 3.83\\), where periodic behavior temporarily reemerges, highlighting the subtle structure underlying chaos.</p> <p>This diagram serves as a \u201cfingerprint\u201d of the system, which we\u2019ll later reconstruct using SysIdentPy models.</p> <pre><code>num = 1000\nN = 1000\nN_drop = 500\nr0 = 3.5\n\nrs = r0 + np.arange(num) / num * (4 - r0)\nxss = []\n\n# Generate bifurcation data\nfor r in rs:\n    x = 0.5  # Initial condition\n    xs = []\n\n    # Warmup iterations (discard transient)\n    for _ in range(N_drop):\n        x = logistic_map(x, r)\n\n    # Store stable iterations\n    for _ in range(N):\n        x = logistic_map(x, r)\n        xs.append(x)\n\n    xss.append(xs)\n\nplt.figure(figsize=(4, 4), dpi=100)\nfor r, xs in zip(rs, xss):\n    plt.plot([r] * len(xs), xs, \",\", alpha=0.1, c=\"black\", rasterized=True)\n\nplt.xlabel(\"$r$\")\nplt.ylabel(\"$x_n$\")\nplt.title(\"Logistic Map Bifurcation Diagram\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Let's start by building a model for the logistic map with \\(r = 3.5\\) to evaluate how SysIdentPy performs in this periodic scenario.</p> <pre><code>y = np.array(xss[0]).reshape(-1, 1)\ny_train = y[:800, :].copy()\ny_test = y[200:, :].copy()\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    ylag=1,\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\nmodel.fit(y=y_train)\nyhat = model.predict(y=y_test[:2].reshape(-1, 1), forecast_horizon=len(y_test))\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :])\n</code></pre> <pre><code>  Regressors   Parameters             ERR\n0          1  -2.4433E-14  9.05069788E-01\n1   y(k-1)^3   3.6415E-14  9.17450154E-02\n2     y(k-1)   3.5000E+00  3.13396079E-03\n3   y(k-1)^2  -3.5000E+00  5.12359326E-05\n\n\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 4\n  self.info_values = self.information_criterion(reg_matrix, y)\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/#5-reconstructing-the-bifurcation-diagram-with-models","title":"5. Reconstructing the Bifurcation Diagram with Models","text":"<p>Can a data-driven model like NARMAX replicate the Logistic Map\u2019s bifurcation behavior? Let\u2019s find out:</p> <p>The following steps will be executed:</p> <ol> <li>Loop over \\(r\\): For each value in the range (e.g., 2.5 to 4.0 in steps of 0.01).</li> <li>Train a model: Generate synthetic data for the current \\(r\\), split into training/testing sets, and fit a NARMAX model using SysIdentPy\u2019s <code>FROLS</code> algorithm.</li> <li>Prediction: Use the trained model to predict future states, starting from an initial condition.</li> <li>Plot: Collect the asymptotic states and overlay them on a bifurcation diagram.</li> </ol> <pre><code>def fit_model(y_train, y_test, degree=3, ylag=1, n_info_values=50):\n    \"\"\"Fit a NARX model and return predictions.\"\"\"\n    basis_function = Polynomial(degree=degree)\n    model = FROLS(\n        ylag=ylag,\n        n_info_values=n_info_values,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n    )\n    model.fit(y=y_train)\n    return model.predict(\n        y=y_test[: model.max_lag].reshape(-1, 1), forecast_horizon=len(y_test)\n    )\n\n\ndef generate_bifurcation_data(rs, N, N_drop):\n    \"\"\"Generate logistic map data and model predictions for each r.\"\"\"\n    xss, yhat_bifurcation = [], []\n    for r in rs:\n        x = 0.5\n        # Warm-up iterations\n        for _ in range(N_drop):\n            x = logistic_map(x, r)\n\n        # Stable iterations (corrected loop)\n        xs = []\n        for _ in range(N):\n            x = logistic_map(x, r)\n            xs.append(x)\n\n        xss.append(xs)\n\n        # Prepare data for modeling\n        y = np.array(xs).reshape(-1, 1)\n        y_train, y_test = y[:800, :], y[200:, :]\n        yhat = fit_model(y_train, y_test)\n        yhat_bifurcation.append(np.array(yhat))\n\n    return xss, yhat_bifurcation\n\n\n# Parameters\nnum = 1000\nN, N_drop = 1000, 500\nr0 = 3.5\nrs = r0 + np.arange(num) / num * (4 - r0)\n\nxss, yhat_bifurcation = generate_bifurcation_data(rs, N, N_drop)\n\n# Plot predicted bifurcation diagram\nplt.figure(figsize=(4, 4), dpi=100)\nfor ind, r in enumerate(rs):\n    plt.plot(\n        [r] * len(yhat_bifurcation[ind]),\n        yhat_bifurcation[ind],\n        \",\",\n        alpha=0.1,\n        c=\"black\",\n        rasterized=True,\n    )\n\nplt.title(\"Bifurcation Diagram with FROLS Predictions\", fontsize=14)\nplt.xlabel(\"$r$\", fontsize=12)\nplt.ylabel(\"$x_n$\", fontsize=12)\nplt.grid(alpha=0.2)\nplt.show()\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 4\n  self.info_values = self.information_criterion(reg_matrix, y)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\parameter_estimation\\estimators.py:75: UserWarning: Psi matrix might have linearly dependent rows.Be careful and check your data\n  self._check_linear_dependence_rows(psi)\n</code></pre> <p></p> <p>As shown in the plots above, the models accurately reproduce stable cycles in periodic regimes, with minimal prediction errors. In chaotic regimes, the models provide accurate short-term predictions, but long-term divergence occurs due to the sensitivity to initial conditions, highlighting the inherent limitations of chaotic system modeling. The generated bifurcation diagram retains the key features, such as period-doubling and chaotic transitions, although finer details may require a model with higher complexity. This experiment demonstrates SysIdentPy\u2019s ability to capture essential nonlinear dynamics, even in chaotic scenarios.</p>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/","title":"Lorenz System","text":"<p>Example created by Wilson Rocha</p> <p>Note: This example is based on the Lorenz system simulation reference from UF|Physics: Introduction to Biological Physics. Some of the code used for the Lorenz simulations and the bifurcation diagram is adapted from this resource.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.integrate import odeint\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\n</code></pre>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/#the-lorenz-system","title":"The Lorenz System","text":"<p>The Lorenz system is a well-known example in chaos theory, consisting of three ordinary differential equations that describe the behavior of convection currents in the atmosphere. First introduced by Edward Lorenz in 1963, this system is famous for exhibiting chaotic dynamics, where small changes in initial conditions lead to drastically different outcomes. Despite being deterministic, the system\u2019s behavior is highly unpredictable, making it a key example of chaos in mathematics and science.</p> <p>The equations that define the Lorenz system are:</p> \\[ \\begin{aligned} \\frac{dx}{dt} &amp;= \\sigma (y - x), \\\\ \\frac{dy}{dt} &amp;= x (\\rho - z) - y, \\\\ \\frac{dz}{dt} &amp;= xy - \\beta z, \\end{aligned} \\] <p>Where: - \\(x\\), \\(y\\), and \\(z\\) represent variables such as the rate of convection, the temperature difference between the rising and descending air, and the deviation of the temperature from average, - \\(\\sigma\\) is the Prandtl number (fluid viscosity/thermal diffusivity), \\(\\rho\\) is the Rayleigh number (drives convection), and \\(\\beta\\) is a system parameter related to the geometry of the system. Chaos emerges at \\(\\rho = 28\\), \\(\\sigma = 10\\), \\(\\beta = 8/3\\).</p> <p>In this tutorial, we will use SysIdentPy to create a model of the Lorenz system. SysIdentPy allows us to identify the system's parameters from data and explore its chaotic behavior in a structured way. We will first generate data by solving the Lorenz equations numerically and then apply SysIdentPy to model the system, capturing the dynamics of the attractor and estimating key system parameters.</p>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/#visualizing-the-lorenz-attractor","title":"Visualizing the Lorenz Attractor","text":"<p>The Lorenz attractor, a three-dimensional plot of the system\u2019s solutions, visually illustrates the chaotic behavior of the system, often resembling a \"butterfly\" shape. This chaotic structure reveals the system's sensitive dependence on initial conditions, meaning that even small differences in starting points can lead to vastly different trajectories. This is a striking example of non-periodic motion in dynamical systems.</p> <p>The parameter \\(\\rho\\) plays a critical role in shaping the behavior of the system. By varying \\(\\rho\\), it is possible to observe transitions from steady states to chaos. For lower values of \\(\\rho\\), the system remains stable, often exhibiting periodic behavior. As \\(\\rho\\) increases, the system becomes increasingly sensitive and begins to exhibit complex, chaotic dynamics. Exploring these transitions allows for a deeper understanding of how deterministic systems can evolve into chaotic systems and highlights the concept of bifurcation within nonlinear dynamics.</p> <pre><code>def lorenz(xyz, t, sigma, rho, beta):\n    x, y, z = xyz  # parse variables\n    dxdt = sigma * (y - x)\n    dydt = x * (rho - z) - y\n    dzdt = x * y - beta * z\n    return [dxdt, dydt, dzdt]\n\n\nsigma = 10.0\nbeta = 8 / 3\nrho = 28.0\n\nT = 50.0  # total time to run\ndt = 0.01  # time step\ntime_points = np.arange(0.0, T, dt)\n\n# Nontrivial steady state values for plotting\nx2 = y2 = np.sqrt(beta * (rho - 1))  # nontrivial steady state\nz2 = rho - 1\n\ninit = np.random.rand(3) * 2 - 1  # initial conditions between -1 and 1\nsol2 = odeint(lorenz, init, time_points, args=(sigma, rho, beta))\n\nplt.figure(figsize=(10, 6))\n# Plot nontrivial steady states as horizontal lines\nplt.axhline(x2, color=\"k\", lw=1, linestyle=\"--\", label=f\"Steady state: x={x2:.2f}\")\nplt.axhline(-x2, color=\"k\", lw=1, linestyle=\"--\", label=f\"Steady state: x={-x2:.2f}\")\n\n# Plot the single trajectory\nplt.plot(time_points, sol2[:, 0], lw=2, label=f\"Trajectory starting at {init}\")\n\nplt.xlabel(r\"$t$\")\nplt.ylabel(r\"$x$\")\nplt.title(\"Lorenz Attractor: Single Trajectory\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure()\nplt.plot(sol2[:, 0], sol2[:, 2])  # plot trajectories projected onto the (x,z) plane\nplt.plot(\n    [sol2[0, 0]], [sol2[0, 2]], \"b^\"\n)  # blue trianble labels starting point of each trajectory\nplt.plot([x2], [z2], \"rx\")  # steady state\nplt.plot([-x2], [z2], \"rx\")  # steady state\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$z$\")\nplt.show()\n</code></pre> <p></p> <pre><code># Let us now visualize all the solutions as trajectories in a 3-d plot\n\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\nax.plot(sol2[:, 0], sol2[:, 1], sol2[:, 2])\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$y$\")\nax.set_zlabel(r\"$z$\")\nplt.show()\n</code></pre> <p></p> <p>The bifurcation diagram shows the possible steady states or periodic orbits of the system for different values of a system parameter (like \\(\\rho\\)). This is a compact and powerful way to visualize how small changes in a parameter can drastically alter the system's behavior.</p> <pre><code>sigma = 10.0\nbeta = 8 / 3.0\n\nt1 = 20.0  # time to add perturbation\nT = 100.0\ndt = 0.01\ntime_points = np.arange(0.0, 50.0, 0.01)\n\nrho_list = np.geomspace(0.1, 1000.0, 401)  # list of rho values geometrically spaced\nsol_list = []  # list of solutions of Lorenz equations\n\nfor rho in rho_list:\n    init = np.random.rand(3) * 2 - 1  # random initial values\n    sol = odeint(lorenz, init, time_points, args=(sigma, rho, beta))\n    sol_list.append(sol)\n\n\nrP = 1  # onset of instability at the origin, pitchfork bifurcation\nrH = sigma * (sigma + beta + 3) / (sigma - beta - 1)  # onset of chaos, Hopf bifurcation\n\nplt.figure(figsize=(8, 6))\nplt.axvline(rP, lw=1, ls=\":\", label=r\"$\\rho = 1$\")\nplt.axvline(rH, lw=1, ls=\"--\", label=r\"$\\rho = %.3f$\" % rH)\nplt.axvline(28, lw=1, ls=\"-\", label=r\"$\\rho = 28$\")\nfor i in range(len(rho_list)):\n    rho = rho_list[i]\n    sol = sol_list[i]\n    y = sol[int(t1 / dt) :, 0]\n    x = [rho] * len(y)\n    plt.scatter(x, y, s=0.1, c=\"k\", marker=\".\", edgecolor=\"none\")\n\nplt.xscale(\"log\")\nplt.xlabel(r\"$\\rho$\")\nplt.ylabel(r\"$x$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>The stable steady state at the origin undergoes a pitchfork bifurcation, splitting into two distinct steady states. As the parameter increases further, these two steady states experience a Hopf bifurcation, where one of the fixed points becomes a saddle point, marking the onset of chaotic behavior. For \\(\\rho &lt; 24.74\\), the system exhibits steady states or periodic orbits, while for \\(\\rho \\geq 24.74\\), chaos emerges, revealing a fractal structure.</p>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/#modeling-with-sysidentpy","title":"Modeling with SysIdentPy","text":"<p>Let's visualize the data that we need to model</p> <pre><code>plt.plot(sol2)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x27c7c2b5bb0&gt;,\n &lt;matplotlib.lines.Line2D at 0x27c7c177ce0&gt;,\n &lt;matplotlib.lines.Line2D at 0x27c75eb8da0&gt;]\n</code></pre> <p></p> <pre><code>def prepare_data(data, y_col, x_cols, train_size=4000):\n    \"\"\"Split and reshape the data for each model.\"\"\"\n    train_data, test_data = data[:train_size], data[train_size:]\n\n    # Extract y (target) and x (predictors)\n    y_train, y_test = train_data[:, y_col].reshape(-1, 1), test_data[:, y_col].reshape(\n        -1, 1\n    )\n    x_train, x_test = train_data[:, x_cols], test_data[:, x_cols]\n\n    return x_train, y_train, x_test, y_test\n\n\ndef train_and_predict(x_train, y_train, x_test, y_test):\n    \"\"\"Train the model and make predictions.\"\"\"\n    basis_function = Polynomial(degree=2)\n    model = FROLS(\n        ylag=1,\n        xlag=[[1], [1]],\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n    )\n    model.fit(X=x_train, y=y_train)\n    yhat = model.predict(\n        X=x_test,\n        y=y_test[: model.max_lag].reshape(-1, 1),\n    )\n\n    r = pd.DataFrame(\n        results(\n            model.final_model,\n            model.theta,\n            model.err,\n            model.n_terms,\n            err_precision=8,\n            dtype=\"sci\",\n        ),\n        columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n    )\n    print(r)\n    return yhat\n\n\n# First model: Using sol2[:, 0] for y and sol2[:, [1, 2]] for x\nx_train, y_train, x_test, y_test = prepare_data(\n    sol2, y_col=0, x_cols=[1, 2], train_size=4000\n)\nyhat_1 = train_and_predict(x_train, y_train, x_test, y_test)\n\n# set 1 because is the maximum lag. Can set as model.max_lag as well\nplot_results(y=y_test[1:], yhat=yhat_1[1:], n=1000)\n\n# Second model: Using sol2[:, 1] for y and sol2[:, [0, 2]] for x\nx_train, y_train, x_test, y_test = prepare_data(\n    sol2, y_col=1, x_cols=[0, 2], train_size=4000\n)\nyhat_2 = train_and_predict(x_train, y_train, x_test, y_test)\n\nplot_results(y=y_test[1:], yhat=yhat_2[1:], n=1000)\n\n# Third model: Using sol2[:, 2] for y and sol2[:, [0, 1]] for x\nx_train, y_train, x_test, y_test = prepare_data(\n    sol2, y_col=2, x_cols=[0, 1], train_size=4000\n)\nyhat_3 = train_and_predict(x_train, y_train, x_test, y_test)\n\nplot_results(y=y_test[1:], yhat=yhat_3[1:], n=1000)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 10\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n       Regressors   Parameters             ERR\n0          y(k-1)   9.1832E-01  9.98019711E-01\n1         x1(k-1)   9.4946E-02  1.97448493E-03\n2   x2(k-1)y(k-1)  -4.7263E-04  5.79130765E-06\n3  x2(k-1)x1(k-1)  -2.1239E-05  4.72613968E-09\n4         x2(k-1)   6.9084E-06  4.82674757E-10\n5       x1(k-1)^2  -1.1524E-06  4.27141670E-11\n6        y(k-1)^2  -8.8057E-06  3.31043247E-11\n7               1  -1.8509E-04  3.67821804E-11\n8   x1(k-1)y(k-1)   7.9252E-06  5.31217490E-11\n9       x2(k-1)^2   5.5901E-07  5.27580418E-12\n</code></pre> <p></p> <pre><code>       Regressors   Parameters             ERR\n0          y(k-1)   9.9829E-01  9.96084591E-01\n1   x2(k-1)y(k-1)  -6.5230E-04  3.09653629E-03\n2  x2(k-1)x1(k-1)  -9.6452E-03  4.62214305E-04\n3         x1(k-1)   2.7760E-01  3.50549821E-04\n4         x2(k-1)   2.8138E-04  3.86911341E-07\n5        y(k-1)^2  -3.2352E-05  3.41517241E-08\n6       x1(k-1)^2  -2.6343E-04  2.79943269E-08\n7               1  -6.2559E-03  3.02963912E-08\n8   x1(k-1)y(k-1)   2.3472E-04  4.11556688E-08\n9       x2(k-1)^2   1.5355E-05  3.32074748E-09\n\n\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 10\n  self.info_values = self.information_criterion(reg_matrix, y)\n</code></pre> <p></p> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 10\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n       Regressors   Parameters             ERR\n0          y(k-1)   9.7932E-01  9.99326652E-01\n1  x2(k-1)x1(k-1)   8.7471E-03  6.70396643E-04\n2       x2(k-1)^2   9.5637E-04  2.13515631E-06\n3       x1(k-1)^2  -1.1022E-04  4.45532422E-07\n4         x1(k-1)   4.1767E-03  3.37917913E-08\n5        y(k-1)^2  -1.5200E-04  1.35172660E-08\n6               1  -3.4932E-02  2.45525697E-08\n7         x2(k-1)  -2.9565E-03  1.41491234E-09\n8   x2(k-1)y(k-1)   8.8849E-05  3.12544165E-09\n9   x1(k-1)y(k-1)  -1.0276E-04  6.02297621E-09\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/#results","title":"Results","text":"<p>Now we can visualize the results of each predicted trajectory</p> <pre><code># First model: Using predictions from yhat_1 (predicted x vs predicted z)\nplt.figure()\nplt.plot(yhat_1, yhat_3)  # plot predictions projected onto the (x, z) plane\nplt.plot(\n    [sol2[0, 0]], [sol2[0, 2]], \"b^\"\n)  # blue triangle labels starting point of each trajectory\nplt.plot([x2], [z2], \"rx\")  # steady state\nplt.plot([-x2], [z2], \"rx\")  # steady state\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$z$\")\nplt.title(\"Butterfly Plot - Model 1 Predictions\")\nplt.show()\n\n# Second model: Using predictions from yhat_2 (predicted y vs predicted z)\nplt.figure()\nplt.plot(yhat_2, yhat_3)  # plot predictions projected onto the (y, z) plane\nplt.plot(\n    [sol2[0, 1]], [sol2[0, 2]], \"b^\"\n)  # blue triangle labels starting point of each trajectory\nplt.plot([x2], [z2], \"rx\")  # steady state\nplt.plot([-x2], [z2], \"rx\")  # steady state\nplt.xlabel(r\"$y$\")\nplt.ylabel(r\"$z$\")\nplt.title(\"Butterfly Plot - Model 2 Predictions\")\nplt.show()\n\n# Third model: Using predictions from yhat_3 (predicted z vs predicted x)\nplt.figure()\nplt.plot(yhat_3, yhat_1)  # plot predictions projected onto the (z, x) plane\nplt.plot(\n    [sol2[0, 2]], [sol2[0, 0]], \"b^\"\n)  # blue triangle labels starting point of each trajectory\nplt.plot([z2], [x2], \"rx\")  # steady state\nplt.plot([z2], [-x2], \"rx\")  # steady state\nplt.xlabel(r\"$z$\")\nplt.ylabel(r\"$x$\")\nplt.title(\"Butterfly Plot - Model 3 Predictions\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <pre><code>fig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\nax.plot(yhat_1, yhat_2, yhat_3)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$y$\")\nax.set_zlabel(r\"$z$\")\nax.set_title(\"3D Butterfly Plot with Predicted Data\")\nplt.show()\n</code></pre> <p></p> <p>As shown in the plots above, the models effectively capture the dynamics of the Lorenz system, reproducing the stable cycles in periodic regimes with minimal prediction errors. In chaotic regimes, the models maintain accurate short-term predictions but can exhibit long-term divergence, which is expected due to the sensitivity to initial conditions intrinsic to chaotic systems. Although finer details may require more sophisticated model configurations, SysIdentPy demonstrates its capability to model the nonlinear dynamics of the Lorenz system, even in the chaotic regime. This experiment highlights SysIdentPy's versatility in capturing both stable and chaotic behaviors in complex systems.</p>"},{"location":"pt/book/0-Preface/","title":"Pref\u00e1cio","text":"<p>All the world is a nonlinear system</p> <p>He linearised to the right</p> <p>He linearised to the left</p> <p>Till nothing was right</p> <p>And nothing was left</p> <p>Stephen A. Billings - Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains</p>"},{"location":"pt/book/0-Preface/#identificacao-de-sistemas-nao-lineares-e-previsao-teoria-e-pratica-com-sysidentpy","title":"Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares e Previs\u00e3o: Teoria e Pr\u00e1tica Com SysIdentPy","text":"<p>Bem-vindo ao nosso livro sobre Identifica\u00e7\u00e3o de Sistemas! Este livro \u00e9 uma abordagem abrangente para aprender sobre modelos din\u00e2micos e previs\u00e3o. O principal objetivo deste livro \u00e9 descrever um conjunto completo de algoritmos para identifica\u00e7\u00e3o, previs\u00e3o e an\u00e1lise de sistemas n\u00e3o lineares.</p> <p>Nosso livro foi especificamente desenvolvido para aqueles que t\u00eam interesse em aprender identifica\u00e7\u00e3o de sistemas e previs\u00e3o. Vamos gui\u00e1-lo atrav\u00e9s do processo passo a passo usando Python e o pacote SysIdentPy. Com o SysIdentPy, voc\u00ea ser\u00e1 capaz de aplicar uma variedade de t\u00e9cnicas para modelagem de sistemas din\u00e2micos, fazer previs\u00f5es e explorar diferentes esquemas de projeto para modelos din\u00e2micos, desde polinomiais at\u00e9 redes neurais. Este livro \u00e9 destinado a graduandos, p\u00f3s-graduandos, pesquisadores e todas as pessoas de diferentes \u00e1reas de pesquisa que possuem dados e desejam encontrar modelos para entender melhor seus sistemas.</p> <p>A literatura de pesquisa est\u00e1 repleta de livros e artigos cobrindo v\u00e1rios aspectos da identifica\u00e7\u00e3o de sistemas n\u00e3o lineares, incluindo m\u00e9todos NARMAX. Neste livro, nosso objetivo n\u00e3o \u00e9 replicar todas as numerosas varia\u00e7\u00f5es de algoritmos dispon\u00edveis. Em vez disso, queremos mostrar como modelar seus dados usando esses algoritmos com o SysIdentPy. Mencionaremos todos os detalhes espec\u00edficos e diferentes vers\u00f5es dos algoritmos no livro, ent\u00e3o se voc\u00ea estiver mais interessado nos aspectos te\u00f3ricos, poder\u00e1 explorar essas ideias mais a fundo. Nosso objetivo \u00e9 focar nas t\u00e9cnicas fundamentais, explicando-as em linguagem direta e mostrando como us\u00e1-las em situa\u00e7\u00f5es do mundo real. Embora haja alguma matem\u00e1tica e detalhes t\u00e9cnicos envolvidos, o objetivo \u00e9 manter tudo o mais f\u00e1cil de entender poss\u00edvel. Em ess\u00eancia, este livro visa ser um recurso que leitores de diversas \u00e1reas podem usar para aprender como modelar sistemas din\u00e2micos n\u00e3o lineares.</p> <p>A melhor parte do nosso livro \u00e9 que ele \u00e9 um material open source, o que significa que est\u00e1 dispon\u00edvel gratuitamente para qualquer pessoa usar e contribuir. Esperamos que isso re\u00fana pessoas que compartilham interesse por t\u00e9cnicas de identifica\u00e7\u00e3o de sistemas e previs\u00e3o, desde modelos lineares at\u00e9 n\u00e3o lineares.</p> <p>Ent\u00e3o, seja voc\u00ea um estudante, pesquisador, cientista de dados ou profissional, convidamos voc\u00ea a compartilhar seu conhecimento e contribuir conosco. Vamos explorar identifica\u00e7\u00e3o de sistemas e previs\u00e3o com o SysIdentPy!</p> <p>Para acompanhar os exemplos em Python no livro, voc\u00ea precisar\u00e1 ter alguns pacotes instalados. Vamos cobrir os principais aqui e inform\u00e1-lo se algum pacote adicional for necess\u00e1rio conforme avan\u00e7amos.</p> <pre><code>import sysidentpy\nimport pandas as pd\nimport numpy as np\nimport torch\nimport matplotlib\nimport scipy\n</code></pre>"},{"location":"pt/book/0-Preface/#sobre-o-autor","title":"Sobre o Autor","text":"<p>Wilson Rocha \u00e9 Head de Data Science na RD Sa\u00fade e criador da biblioteca SysIdentPy. Ele possui gradua\u00e7\u00e3o em Engenharia El\u00e9trica e Mestrado em Modelagem de Sistemas e Controle, ambos pela Universidade Federal de S\u00e3o Jo\u00e3o del-Rei (UFSJ), Brasil. Wilson come\u00e7ou sua jornada em Machine Learning desenvolvendo rob\u00f4s jogadores de futebol e continua avan\u00e7ando sua pesquisa nas \u00e1reas de Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares Multiobjetivo e Previs\u00e3o de S\u00e9ries Temporais.</p> <p>Conecte-se com Wilson Rocha atrav\u00e9s das seguintes redes sociais:</p> <ul> <li>LinkedIn</li> <li>ResearchGate</li> <li>Discord</li> </ul>"},{"location":"pt/book/0-Preface/#referenciando-este-livro","title":"Referenciando Este Livro","text":"<p>Se voc\u00ea achar este livro \u00fatil, por favor cite-o da seguinte forma:</p> <pre><code>Lacerda Junior, W.R. (2024). *Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy*. Web version. https://sysidentpy.org\n</code></pre> <p>Se voc\u00ea usar o SysIdentPy em seu projeto, por favor entre em contato.</p> <p>Se voc\u00ea usar o SysIdentPy em sua publica\u00e7\u00e3o cient\u00edfica, agradecer\u00edamos cita\u00e7\u00f5es ao seguinte artigo: - Lacerda et al., (2020). SysIdentPy: A Python package for System Identification using NARMAX models. Journal of Open Source Software, 5(54), 2384, https://doi.org/10.21105/joss.02384</p> <pre><code>@article{Lacerda2020,\n  doi = {10.21105/joss.02384},\n  url = {https://doi.org/10.21105/joss.02384},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {54},\n  pages = {2384},\n  author = {Wilson Rocha Lacerda Junior and Luan Pascoal Costa da Andrade and Samuel Carlos Pessoa Oliveira and Samir Angelo Milani Martins},\n  title = {SysIdentPy: A Python package for System Identification using NARMAX models},\n  journal = {Journal of Open Source Software}\n}\n</code></pre>"},{"location":"pt/book/0-Preface/#versoes-em-pdf-epub-e-mobi","title":"Vers\u00f5es em PDF, Epub e Mobi","text":"<p>Baixe a vers\u00e3o em pdf do livro: vers\u00e3o pdf</p> <p>Baixe a vers\u00e3o em epub do livro: vers\u00e3o epub</p> <p>Baixe a vers\u00e3o em mobi do livro: vers\u00e3o mobi</p>"},{"location":"pt/book/0-Preface/#agradecimentos","title":"Agradecimentos","text":"<p>A disciplina de Identifica\u00e7\u00e3o de Sistemas ministrada por Samir Martins (em Portugu\u00eas) foi uma grande fonte de inspira\u00e7\u00e3o para esta s\u00e9rie. Neste livro, exploraremos Sistemas Din\u00e2micos e aprenderemos como dominar modelos NARMAX usando Python e o pacote SysIdentPy. O livro de Stephen A. Billings, Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains, foi fundamental para nos mostrar o qu\u00e3o poderosa a Identifica\u00e7\u00e3o de Sistemas pode ser.</p> <p>Al\u00e9m desses recursos, tamb\u00e9m faremos refer\u00eancia ao livro de Luis Ant\u00f4nio Aguirre, Introdu\u00e7\u00e3o \u00e0 Identifica\u00e7\u00e3o de Sistemas. T\u00e9cnicas Lineares e n\u00e3o Lineares Aplicadas a Sistemas. Teoria e Aplica\u00e7\u00e3o (em Portugu\u00eas), que provou ser uma ferramenta inestim\u00e1vel na introdu\u00e7\u00e3o de conceitos complexos de modelagem din\u00e2mica de forma direta. Como um material open source sobre Identifica\u00e7\u00e3o de Sistemas e Previs\u00e3o, este livro visa fornecer uma abordagem acess\u00edvel, por\u00e9m rigorosa, para aprender modelos din\u00e2micos e previs\u00e3o.</p>"},{"location":"pt/book/0-Preface/#apoie-o-projeto","title":"Apoie o Projeto","text":"<p>O Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy \u00e9 um extenso recurso open source dedicado \u00e0 ci\u00eancia da Identifica\u00e7\u00e3o de Sistemas. Nosso objetivo \u00e9 tornar este conhecimento acess\u00edvel a todos, tanto financeiramente quanto intelectualmente.</p> <p>Se este livro foi valioso para voc\u00ea e voc\u00ea gostaria de apoiar nossos esfor\u00e7os, aceitamos contribui\u00e7\u00f5es financeiras atrav\u00e9s da nossa p\u00e1gina de Sponsor.</p> <p>Se voc\u00ea n\u00e3o est\u00e1 em posi\u00e7\u00e3o de contribuir financeiramente, ainda pode apoiar ajudando-nos a melhorar o livro. Encorajamos voc\u00ea a reportar quaisquer erros de digita\u00e7\u00e3o, sugerir edi\u00e7\u00f5es ou fornecer feedback sobre se\u00e7\u00f5es que voc\u00ea achou desafiadoras. Voc\u00ea pode fazer isso visitando o reposit\u00f3rio do livro e abrindo uma issue. Al\u00e9m disso, se voc\u00ea gostou do conte\u00fado, por favor considere compartilh\u00e1-lo com outras pessoas que possam se beneficiar dele, e nos d\u00ea uma estrela no GitHub.</p> <p>Seu apoio, em qualquer forma, nos ajuda a continuar aprimorando este projeto e manter um recurso de alta qualidade para a comunidade. Obrigado pela sua contribui\u00e7\u00e3o!</p>"},{"location":"pt/book/0.1-Contents/","title":"Sum\u00e1rio","text":"<p>Pref\u00e1cio</p> <ol> <li>Introdu\u00e7\u00e3o<ol> <li>Modelos</li> <li>Identifica\u00e7\u00e3o de Sistemas</li> <li>Identifica\u00e7\u00e3o de Sistemas Lineares ou N\u00e3o Lineares<ol> <li>Modelos Lineares</li> <li>Modelos N\u00e3o Lineares</li> </ol> </li> <li>M\u00e9todos NARMAX</li> <li>Qual \u00e9 o Prop\u00f3sito da Identifica\u00e7\u00e3o de Sistemas?</li> <li>Identifica\u00e7\u00e3o de Sistemas \u00e9 Machine Learning?</li> <li>Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares e Aplica\u00e7\u00f5es de Previs\u00e3o: Estudos de Caso</li> <li>Abrevia\u00e7\u00f5es</li> <li>Vari\u00e1veis</li> <li>Organiza\u00e7\u00e3o do Livro</li> </ol> </li> <li>Representa\u00e7\u00e3o do Modelo NARMAX<ol> <li>Fun\u00e7\u00f5es Base</li> <li>Modelos Lineares<ol> <li>ARMAX</li> <li>ARX</li> <li>ARMA</li> <li>AR</li> <li>FIR</li> <li>Outras Variantes</li> </ol> </li> <li>Modelos N\u00e3o Lineares<ol> <li>NARMAX</li> <li>NARMA</li> <li>NAR</li> <li>NFIR</li> <li>Modelos NARMAX Mistos</li> <li>Rede Neural NARX</li> <li>Representa\u00e7\u00e3o Geral do Conjunto de Modelos</li> <li>Modelos MIMO</li> </ol> </li> </ol> </li> <li>Estima\u00e7\u00e3o de Par\u00e2metros<ol> <li>M\u00ednimos Quadrados</li> <li>M\u00ednimos Quadrados Totais</li> <li>M\u00ednimos Quadrados Recursivo</li> <li>M\u00ednimos Quadrados M\u00e9dios</li> <li>Algoritmos Extended Least Squares</li> </ol> </li> <li>Sele\u00e7\u00e3o de Estrutura de Modelo<ol> <li>Introdu\u00e7\u00e3o</li> <li>Forward Regression Orthogonal Least Squares<ol> <li>Estudo de Caso</li> </ol> </li> <li>Crit\u00e9rios de Informa\u00e7\u00e3o<ol> <li>Vis\u00e3o Geral dos M\u00e9todos de Crit\u00e9rios de Informa\u00e7\u00e3o<ol> <li>AIC</li> <li>AICc</li> <li>BIC</li> <li>LILC</li> <li>FPE</li> </ol> </li> </ol> </li> <li>Meta Model Structure Selection (MetaMSS)<ol> <li>Meta-heur\u00edsticas</li> <li>Particle Swarm Optimization (PSO) Padr\u00e3o</li> <li>Gravitational Search Algorithm (GSA) Padr\u00e3o</li> <li>Algoritmo de Otimiza\u00e7\u00e3o H\u00edbrido Bin\u00e1rio</li> <li>Meta-Model Structure Selection (MetaMSS): Construindo NARX para Regress\u00e3o</li> <li>Estudos de Caso: Resultados de Simula\u00e7\u00e3o</li> <li>MetaMSS vs FROLS</li> <li>Meta-MSS vs RJMCMC</li> <li>Algoritmo MetaMSS usando SysIdentPy</li> </ol> </li> <li>Accelerated Orthogonal Least Squares</li> <li>Entropic Regression</li> </ol> </li> <li>Estima\u00e7\u00e3o de Par\u00e2metros Multiobjetivo<ol> <li>Introdu\u00e7\u00e3o</li> <li>Problema de Otimiza\u00e7\u00e3o Multiobjetivo</li> <li>Defini\u00e7\u00e3o de Pareto \u00d3timo e Domin\u00e2ncia de Pareto</li> <li>Algoritmo Affine Information Least Squares</li> <li>Estudo de Caso - Conversor Buck</li> </ol> </li> <li>Sele\u00e7\u00e3o de Estrutura de Modelo Multiobjetivo<ol> <li>Introdu\u00e7\u00e3o</li> <li>Multiobjective Error Reduction Ratio</li> <li>Multiobjective Meta Model Structure Selection</li> <li>Estudos de Caso</li> <li>Refer\u00eancias</li> </ol> </li> <li>Rede Neural NARX<ol> <li>Introdu\u00e7\u00e3o</li> <li>Rede Neural NARX</li> <li>Rede Neural NARX vs. Rede Neural Recorrente</li> <li>Estudos de Caso</li> <li>Refer\u00eancias</li> </ol> </li> <li>Sistemas Severamente N\u00e3o Lineares<ol> <li>Introdu\u00e7\u00e3o</li> <li>Modelagem de Histerese com Modelo NARX Polinomial</li> <li>Sinal quase-est\u00e1tico de carregamento-descarregamento em tempo cont\u00ednuo</li> <li>La\u00e7os de histerese em tempo cont\u00ednuo \\(\\mathcal{H}_t(\\omega)\\)</li> <li>Histerese independente da taxa no modelo NARX polinomial</li> </ol> </li> <li>Valida\u00e7\u00e3o<ol> <li>O M\u00e9todo <code>predict</code> no SysIdentPy</li> <li>Predi\u00e7\u00e3o Infinitos Passos \u00e0 Frente</li> <li>Predi\u00e7\u00e3o Um Passo \u00e0 Frente</li> <li>Predi\u00e7\u00e3o n Passos \u00e0 Frente</li> <li>Desempenho do Modelo</li> <li>M\u00e9tricas Dispon\u00edveis no SysIdentPy</li> <li>Estudo de Caso</li> </ol> </li> <li>Estudos de Caso: Identifica\u00e7\u00e3o de Sistemas e Previs\u00e3o<ol> <li>Dataset M4</li> <li>Dispositivo El\u00e9trico Acoplado</li> <li>Wiener-Hammerstein</li> <li>Previs\u00e3o de Demanda de Passageiros A\u00e9reos</li> <li>Sistema com Histerese - Modelagem de um Dispositivo Amortecedor Magneto-reol\u00f3gico</li> <li>Silver box</li> <li>F-16 Ground Vibration Test Benchmark</li> <li>Previs\u00e3o Fotovoltaica</li> <li>Industrial Robot Identification Benchmark (em breve)</li> <li>Two-Story Frame with Hysteretic Links (em breve)</li> <li>Cortical Responses Evoked by Wrist Joint Manipulation (em breve)</li> <li>Produ\u00e7\u00e3o trimestral total de cerveja na Austr\u00e1lia (em breve)</li> <li>Demanda de Turismo Dom\u00e9stico Australiano (em breve)</li> <li>Consumo de Energia El\u00e9trica (em breve)</li> <li>Taxa de G\u00e1s CO2 (em breve)</li> <li>N\u00famero de Pacientes Atendidos com Doen\u00e7a Semelhante \u00e0 Gripe (em breve)</li> <li>Vendas Mensais de Aquecedores e Sorvetes (em breve)</li> <li>Produ\u00e7\u00e3o Mensal de Leite (em breve)</li> <li>Demanda de Eletricidade a Cada Meia Hora na Inglaterra e Pa\u00eds de Gales (em breve)</li> <li>Temperatura Di\u00e1ria em Melbourne (em breve)</li> <li>Oferta Semanal de Gasolina Acabada nos EUA (em breve)</li> <li>Vendas Totais de Vinho na Austr\u00e1lia (em breve)</li> <li>Produ\u00e7\u00e3o Trimestral de Fio de L\u00e3 na Austr\u00e1lia (em breve)</li> <li>Gera\u00e7\u00e3o Hor\u00e1ria de Energia Nuclear (em breve)</li> </ol> </li> </ol>"},{"location":"pt/book/1-Introduction/","title":"Introdu\u00e7\u00e3o","text":"<p>O conceito de modelo matem\u00e1tico \u00e9 fundamental em muitas \u00e1reas da ci\u00eancia. Da engenharia \u00e0 sociologia, modelos desempenham um papel central no estudo de sistemas complexos, pois permitem simular o que acontecer\u00e1 em diferentes cen\u00e1rios e condi\u00e7\u00f5es, prever a sa\u00edda para uma determinada entrada, analisar suas propriedades e explorar diferentes esquemas de projeto. Para alcan\u00e7ar esses objetivos, no entanto, \u00e9 crucial que o modelo seja uma representa\u00e7\u00e3o adequada do sistema em estudo. A modelagem de comportamentos din\u00e2micos e de regime permanente \u00e9, portanto, fundamental para esse tipo de an\u00e1lise e depende de procedimentos de Identifica\u00e7\u00e3o de Sistemas (SI).</p>"},{"location":"pt/book/1-Introduction/#modelos","title":"Modelos","text":"<p>A modelagem matem\u00e1tica \u00e9 uma excelente maneira de entender e analisar diferentes partes do nosso mundo. Ela nos fornece uma estrutura clara para compreender sistemas complexos e seu comportamento. Seja para tarefas cotidianas ou quest\u00f5es de grande escala como controle de doen\u00e7as, modelos s\u00e3o uma parte essencial de como lidamos com diversos desafios.</p> <p>Digitar eficientemente em um layout de teclado QWERTY convencional \u00e9 resultado de um modelo bem aprendido do teclado QWERTY incorporado nos processos cognitivos individuais. No entanto, se voc\u00ea se deparar com um layout de teclado diferente, como Dvorak ou AZERTY, provavelmente ter\u00e1 dificuldades para se adaptar ao novo modelo. O sistema mudou, ent\u00e3o voc\u00ea ter\u00e1 que atualizar seu modelo.</p> <p></p> <p>QWERTY - Wikipedia - Layout de teclado ANSI QWERTY (US)</p> <p></p> <p>Layout AZERTY usado em um teclado</p> <p>A modelagem matem\u00e1tica est\u00e1 presente em muitas partes de nossas vidas. Seja analisando tend\u00eancias econ\u00f4micas, rastreando como doen\u00e7as se propagam, ou compreendendo o comportamento do consumidor, modelos s\u00e3o ferramentas essenciais para adquirir conhecimento, tomar decis\u00f5es informadas e assumir controle sobre sistemas complexos.</p> <p>Em ess\u00eancia, modelos matem\u00e1ticos nos ajudam a dar sentido ao mundo. Eles nos permitem entender o comportamento humano e os sistemas com os quais lidamos todos os dias. Ao usar esses modelos, podemos aprender, adaptar e ajustar nossas estrat\u00e9gias para acompanhar as mudan\u00e7as ao nosso redor.</p>"},{"location":"pt/book/1-Introduction/#identificacao-de-sistemas","title":"Identifica\u00e7\u00e3o de Sistemas","text":"<p>Identifica\u00e7\u00e3o de sistemas \u00e9 uma estrutura orientada a dados para modelar sistemas din\u00e2micos. Inicialmente, cientistas focavam na identifica\u00e7\u00e3o de sistemas lineares, mas isso tem mudado nas \u00faltimas d\u00e9cadas com maior \u00eanfase em sistemas n\u00e3o lineares. A identifica\u00e7\u00e3o de sistemas n\u00e3o lineares \u00e9 amplamente considerada um dos t\u00f3picos mais importantes relacionados \u00e0 modelagem de diversos sistemas din\u00e2micos, desde s\u00e9ries temporais at\u00e9 comportamentos din\u00e2micos severamente n\u00e3o lineares.</p> <p>Recursos extensivos, incluindo excelentes livros-texto cobrindo identifica\u00e7\u00e3o de sistemas lineares e previs\u00e3o de s\u00e9ries temporais, est\u00e3o prontamente dispon\u00edveis. Neste livro, revisitamos alguns t\u00f3picos conhecidos, mas tamb\u00e9m tentamos abordar tais assuntos de uma forma diferente e complementar. Exploraremos a modelagem de sistemas din\u00e2micos n\u00e3o lineares usando m\u00e9todos NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs), que foram introduzidos por [Stephen A. Billings] em 1981.</p>"},{"location":"pt/book/1-Introduction/#identificacao-de-sistemas-lineares-ou-nao-lineares","title":"Identifica\u00e7\u00e3o de Sistemas Lineares ou N\u00e3o Lineares","text":""},{"location":"pt/book/1-Introduction/#modelos-lineares","title":"Modelos Lineares","text":"<p>Embora a maioria dos sistemas do mundo real seja n\u00e3o linear, voc\u00ea provavelmente deveria tentar modelos lineares primeiro. Modelos lineares geralmente servem como uma forte baseline e podem ser suficientes para o seu caso, proporcionando desempenho satisfat\u00f3rio. Astron e Murray e Glad e Ljung mostraram que muitos sistemas n\u00e3o lineares podem ser bem descritos por modelos localmente lineares. Al\u00e9m disso, modelos lineares s\u00e3o f\u00e1ceis de ajustar, f\u00e1ceis de interpretar e requerem menos recursos computacionais do que modelos n\u00e3o lineares, permitindo que voc\u00ea experimente rapidamente e obtenha insights antes de pensar em modelos gray box ou modelos n\u00e3o lineares complexos.</p> <p>Modelos lineares podem ser muito \u00fateis, mesmo na presen\u00e7a de fortes n\u00e3o linearidades, porque \u00e9 muito mais f\u00e1cil lidar com eles. Al\u00e9m disso, o desenvolvimento de algoritmos de identifica\u00e7\u00e3o linear ainda \u00e9 um campo de pesquisa muito ativo e saud\u00e1vel, com muitos artigos sendo lan\u00e7ados todos os anos Sai Li, Linjun Zhang, T. Tony Cai &amp; Hongzhe Li, Maria Jaenada, Leandro Pardo, Xing Liu; Lin Qiu, Youtong Fang; Kui Wang; Yongdong Li, Jose Rodr\u00edguez, Alessandro D'Innocenzo and Francesco Smarra. Modelos lineares funcionam bem na maioria das vezes e devem ser a primeira escolha para muitas aplica\u00e7\u00f5es. No entanto, ao lidar com sistemas complexos onde as suposi\u00e7\u00f5es lineares n\u00e3o se aplicam, modelos n\u00e3o lineares tornam-se essenciais.</p>"},{"location":"pt/book/1-Introduction/#modelos-nao-lineares","title":"Modelos N\u00e3o Lineares","text":"<p>Quando modelos lineares n\u00e3o apresentam desempenho suficientemente bom, voc\u00ea deve considerar modelos n\u00e3o lineares. \u00c9 importante notar, no entanto, que mudar de um modelo linear para um n\u00e3o linear nem sempre \u00e9 uma tarefa simples. Para usu\u00e1rios inexperientes, \u00e9 comum construir modelos n\u00e3o lineares que apresentam desempenho pior do que os lineares. Para trabalhar com modelos n\u00e3o lineares, voc\u00ea deve considerar que caracter\u00edsticas como erros estruturais, ru\u00eddo, ponto de opera\u00e7\u00e3o, sinais de excita\u00e7\u00e3o e muitos outros aspectos do seu sistema em estudo impactam sua abordagem e estrat\u00e9gia de modelagem.</p> <p>Como sugerido por Johan Schoukens e Lennart Ljung em \"Nonlinear System Identi\ufb01cation - A User-Oriented Roadmap\", s\u00f3 comece a trabalhar com modelos n\u00e3o lineares se houver evid\u00eancia suficiente de que modelos lineares n\u00e3o resolver\u00e3o o problema.</p> <p>Modelos n\u00e3o lineares s\u00e3o mais flex\u00edveis do que modelos lineares e podem ser constru\u00eddos usando muitas representa\u00e7\u00f5es matem\u00e1ticas diferentes, como polinomial, aditivo generalizado, redes neurais, wavelet e muitas outras (Billings, S. A.). Tal flexibilidade, no entanto, torna a identifica\u00e7\u00e3o de sistemas n\u00e3o lineares muito mais complexa do que a linear, desde o projeto do experimento at\u00e9 a sele\u00e7\u00e3o do modelo. O usu\u00e1rio deve considerar que, al\u00e9m da complexidade da modelagem, a transi\u00e7\u00e3o para modelos n\u00e3o lineares exigir\u00e1 uma revis\u00e3o no roteiro e nos recursos computacionais definidos ao lidar com modelos lineares. Nesse sentido, sempre se pergunte se os benef\u00edcios potenciais dos modelos n\u00e3o lineares valem o esfor\u00e7o.</p>"},{"location":"pt/book/1-Introduction/#metodos-narmax","title":"M\u00e9todos NARMAX","text":"<p>O modelo NARMAX \u00e9 uma das representa\u00e7\u00f5es de modelos n\u00e3o lineares mais frequentemente empregadas e \u00e9 amplamente utilizado para representar uma ampla classe de sistemas n\u00e3o lineares. Os m\u00e9todos NARMAX foram aplicados com sucesso em muitos cen\u00e1rios, que incluem processos industriais, sistemas de controle, sistemas estruturais, sistemas econ\u00f4micos e financeiros, biologia, medicina, sistemas sociais e muito mais. A representa\u00e7\u00e3o do modelo NARMAX e a classe de sistemas que podem ser representados por ele ser\u00e3o discutidas posteriormente no livro.</p> <p>Os principais passos envolvidos na constru\u00e7\u00e3o de modelos NARMAX s\u00e3o (Billings, 2013):</p> <ol> <li>Representa\u00e7\u00e3o do Modelo: definir a representa\u00e7\u00e3o matem\u00e1tica do modelo.</li> <li>Sele\u00e7\u00e3o de Estrutura do Modelo: definir quais termos est\u00e3o no modelo final.</li> <li>Estima\u00e7\u00e3o de Par\u00e2metros: estimar os coeficientes de cada termo do modelo selecionado no passo 1.</li> <li>Valida\u00e7\u00e3o do Modelo: garantir que o modelo seja n\u00e3o polarizado e preciso;</li> <li>Predi\u00e7\u00e3o/Simula\u00e7\u00e3o do Modelo: prever sa\u00eddas futuras ou simular o comportamento do sistema dadas diferentes entradas.</li> <li>An\u00e1lise: compreender as propriedades din\u00e2micas do sistema em estudo.</li> <li>Controle: desenvolver esquemas de projeto de controle baseados no modelo obtido.</li> </ol> <p>Model Structure Selection (MSS) \u00e9 o aspecto mais importante dos m\u00e9todos NARMAX e tamb\u00e9m o mais complexo. Selecionar os termos do modelo \u00e9 fundamental se o objetivo da identifica\u00e7\u00e3o \u00e9 obter modelos que possam reproduzir a din\u00e2mica do sistema original e impacta todos os outros aspectos do processo de identifica\u00e7\u00e3o. Problemas relacionados \u00e0 sobreparametriza\u00e7\u00e3o e mal-condicionamento num\u00e9rico s\u00e3o t\u00edpicos devido \u00e0s limita\u00e7\u00f5es dos algoritmos de identifica\u00e7\u00e3o em selecionar os termos apropriados que devem compor o modelo final (L. A. Aguirre e S. A. Billings, L. Piroddi e W. Spinelli).</p> <p>No SysIdentPy, voc\u00ea pode interagir diretamente com cada item descrito nos 7 passos, exceto o de controle. O SysIdentPy foca em modelagem, n\u00e3o em projeto de controle. Voc\u00ea ter\u00e1 que usar parte do c\u00f3digo abaixo em toda tarefa de modelagem usando SysIdentPy. Voc\u00ea aprender\u00e1 os detalhes ao longo do livro, ent\u00e3o n\u00e3o se preocupe se ainda n\u00e3o estiver familiarizado com esses m\u00e9todos.</p> <pre><code>from sysidentpy.basis_function import Polynomial\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.general_estimators import NARX\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.simulation import SimulateNARMAX\n</code></pre>"},{"location":"pt/book/1-Introduction/#qual-e-o-proposito-da-identificacao-de-sistemas","title":"Qual \u00e9 o Prop\u00f3sito da Identifica\u00e7\u00e3o de Sistemas?","text":"<p>Por causa do problema de Model Structure Selection, Billings, S. A. afirma que o objetivo da Identifica\u00e7\u00e3o de Sistemas usando m\u00e9todos NARMAX \u00e9 duplo: desempenho e parcim\u00f4nia.</p> <p>O primeiro objetivo geralmente \u00e9 sobre aproxima\u00e7\u00e3o. Aqui, o foco principal \u00e9 construir um modelo que fa\u00e7a previs\u00f5es com o menor erro poss\u00edvel. Essa abordagem \u00e9 comum em aplica\u00e7\u00f5es como previs\u00e3o do tempo, previs\u00e3o de demanda, previs\u00e3o de pre\u00e7os de a\u00e7\u00f5es, reconhecimento de fala, rastreamento de alvos e classifica\u00e7\u00e3o de padr\u00f5es. Nesses casos, a forma espec\u00edfica do modelo n\u00e3o \u00e9 t\u00e3o cr\u00edtica. Em outras palavras, como os termos interagem (em modelos param\u00e9tricos), a representa\u00e7\u00e3o matem\u00e1tica, o comportamento est\u00e1tico e assim por diante n\u00e3o s\u00e3o t\u00e3o importantes; o que mais importa \u00e9 encontrar uma maneira de minimizar os erros de previs\u00e3o.</p> <p>Mas a identifica\u00e7\u00e3o de sistemas n\u00e3o \u00e9 apenas sobre minimizar erros de previs\u00e3o. Um dos principais objetivos da Identifica\u00e7\u00e3o de Sistemas \u00e9 construir modelos que ajudem o usu\u00e1rio a entender e interpretar o sistema sendo modelado. Al\u00e9m de apenas fazer previs\u00f5es precisas, o objetivo \u00e9 desenvolver modelos que realmente capturem o comportamento din\u00e2mico do sistema em estudo, idealmente na forma mais simples poss\u00edvel. Ci\u00eancia e engenharia tratam de entender sistemas decompondo comportamentos complexos em outros mais simples que podemos entender e controlar. Por exemplo, se o comportamento do sistema pode ser descrito por um modelo din\u00e2mico simples de primeira ordem com um termo n\u00e3o linear c\u00fabico na entrada, a identifica\u00e7\u00e3o de sistemas deve ajudar a descobrir isso.</p>"},{"location":"pt/book/1-Introduction/#identificacao-de-sistemas-e-machine-learning","title":"Identifica\u00e7\u00e3o de Sistemas \u00e9 Machine Learning?","text":"<p>Primeiro, vamos ter uma vis\u00e3o geral de sistemas est\u00e1ticos e din\u00e2micos. Imagine que voc\u00ea tem uma guitarra el\u00e9trica conectada a um processador de efeitos que pode aplicar v\u00e1rios efeitos de \u00e1udio, como reverb ou distor\u00e7\u00e3o. O efeito \u00e9 controlado por um interruptor que alterna entre \"ligado\" e \"desligado\". Vamos considerar isso da perspectiva de sinais. O sinal de entrada representa o estado do interruptor de efeito: interruptor desligado (n\u00edvel baixo), interruptor ligado (n\u00edvel alto). Se representarmos o sinal da guitarra, temos uma condi\u00e7\u00e3o bin\u00e1ria: efeito desligado (som original da guitarra), efeito ligado (som modificado da guitarra). Este \u00e9 um exemplo de sistema est\u00e1tico: a sa\u00edda (som da guitarra) segue diretamente a entrada (estado do interruptor de efeito).</p> <p>Quando o interruptor de efeito est\u00e1 desligado, a sa\u00edda \u00e9 apenas o sinal limpo e inalterado da guitarra. Quando o interruptor de efeito est\u00e1 ligado, a sa\u00edda \u00e9 o sinal da guitarra com o efeito aplicado, como amplifica\u00e7\u00e3o ou distor\u00e7\u00e3o. Neste sistema, o efeito estar ligado ou desligado influencia diretamente o sinal da guitarra sem qualquer atraso ou processamento adicional.</p> <p>Este exemplo ilustra como um sistema est\u00e1tico opera com entradas de controle bin\u00e1rias, onde a sa\u00edda reflete diretamente o estado da entrada, fornecendo um mapeamento direto entre o sinal de controle e a resposta do sistema.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\nt = np.linspace(0, 30, 500, endpoint=False)\nu = signal.square(0.2*np.pi * t)\nu[u &lt; 0] = 0\n# In a static system, the output y directly follows the input u\ny = u\n\n# Plot the input and output\nplt.figure(figsize=(15, 3))\nplt.plot(t, u, label='Input (State of the Switch)', color=\"grey\", linewidth=10, alpha=0.5)\nplt.plot(t, y, label='Output (Static System Response)', color='k', linewidth=0.5)\nplt.title('Static System Response to the Input')\nplt.xlabel('Time [s]')\nplt.ylabel('y')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Representa\u00e7\u00e3o de resposta est\u00e1tica. O sinal de entrada representando o estado do interruptor (interruptor desligado (n\u00edvel baixo), interruptor ligado (n\u00edvel alto)), e a resposta est\u00e1tica: som original (n\u00edvel baixo), som processado (n\u00edvel alto).</p> <p>Agora, vamos considerar um sistema din\u00e2mico: usar um ar-condicionado para baixar a temperatura do ambiente. Este exemplo ilustra efetivamente os conceitos de sistemas din\u00e2micos e como sua sa\u00edda responde ao longo do tempo.</p> <p>Vamos imaginar isso da perspectiva de sinais. O sinal de entrada representa o estado do controle do ar-condicionado: ligar o ar-condicionado (n\u00edvel alto) ou deslig\u00e1-lo (n\u00edvel baixo). Quando o ar-condicionado \u00e9 ligado, ele come\u00e7a a resfriar o ambiente. No entanto, a temperatura do ambiente n\u00e3o cai instantaneamente para o n\u00edvel mais frio desejado. Leva tempo para o ar-condicionado afetar a temperatura, e a taxa na qual a temperatura diminui pode variar com base em fatores como o tamanho do ambiente e o isolamento.</p> <p>Por outro lado, quando o ar-condicionado \u00e9 desligado, a temperatura do ambiente n\u00e3o retorna imediatamente \u00e0 sua temperatura ambiente original. Em vez disso, ela gradualmente esquenta conforme o efeito de resfriamento diminui.</p> <p></p> <p>Usar um ar-condicionado para baixar a temperatura do ambiente como representa\u00e7\u00e3o de sistema din\u00e2mico.</p> <p>Neste sistema din\u00e2mico, a sa\u00edda (temperatura do ambiente) n\u00e3o segue instantaneamente a entrada (estado do ar-condicionado) porque h\u00e1 um atraso de tempo envolvido tanto nos processos de resfriamento quanto de aquecimento. O sistema tem mem\u00f3ria, o que significa que a temperatura atual do ambiente depende n\u00e3o apenas do estado atual do ar-condicionado, mas tamb\u00e9m de quanto tempo ele esteve ligado ou desligado, e quanto ele j\u00e1 resfriou ou permitiu que o ambiente esquentasse.</p> <p>Este exemplo destaca a natureza dos sistemas din\u00e2micos: a resposta a uma entrada \u00e9 gradual e afetada pela din\u00e2mica interna do sistema. O efeito do ar-condicionado na temperatura do ambiente exemplifica como sistemas din\u00e2micos t\u00eam uma resposta dependente do tempo, onde a sa\u00edda muda ao longo do tempo e n\u00e3o corresponde imediatamente ao sinal de entrada.</p> <p>Para sistemas est\u00e1ticos, a sa\u00edda \u00e9 uma fun\u00e7\u00e3o direta da entrada, representada por uma equa\u00e7\u00e3o alg\u00e9brica:</p> \\[ y(t) = G \\cdot u(t) \\] <p>Para sistemas din\u00e2micos, a sa\u00edda depende da entrada e da taxa de varia\u00e7\u00e3o da entrada, representada por uma equa\u00e7\u00e3o diferencial. Por exemplo, a sa\u00edda \\(y(t)\\) pode ser modelada como:</p> \\[ y(t) = G \\cdot u(t) - \\tau \\cdot \\frac{dy(t)}{dt} \\] <p>Aqui, \\(G\\) \u00e9 o ganho, e \\(\\tau\\) \u00e9 uma constante que incorpora a mem\u00f3ria do sistema. Para sistemas em tempo discreto, consideramos sinais em intervalos espec\u00edficos e espa\u00e7ados. A equa\u00e7\u00e3o diferencial \u00e9 discretizada, e a derivada \u00e9 aproximada por uma diferen\u00e7a finita:</p> \\[ y[k] = \\alpha y[k-1] + \\beta u[k] \\] <p>onde \\(\\alpha\\) e \\(\\beta\\) s\u00e3o constantes que determinam a resposta do sistema. A transformada z pode ser usada para obter a fun\u00e7\u00e3o de transfer\u00eancia no dom\u00ednio z.</p> <p>Em resumo, sistemas est\u00e1ticos s\u00e3o modelados por equa\u00e7\u00f5es alg\u00e9bricas, enquanto sistemas din\u00e2micos s\u00e3o modelados por equa\u00e7\u00f5es diferenciais.</p> <p>Como Luis Antonio Aguirre afirma em uma de suas aulas no YouTube, todos os sistemas f\u00edsicos s\u00e3o din\u00e2micos, mas dependendo da escala de tempo, podem ser modelados como est\u00e1ticos para simplifica\u00e7\u00e3o. Por exemplo, a transi\u00e7\u00e3o entre os efeitos no som da guitarra, se considerada em segundos (como fizemos no exemplo), poderia ser tratada como est\u00e1tica dependendo da sua an\u00e1lise. No entanto, a pedaleira tem componentes como capacitores, que s\u00e3o componentes el\u00e9tricos din\u00e2micos, tornando-a um sistema din\u00e2mico. A resposta, no entanto, \u00e9 t\u00e3o r\u00e1pida que a tratamos como um sistema est\u00e1tico. Portanto, representar um sistema como est\u00e1tico \u00e9 uma decis\u00e3o de modelagem.</p> <p>A Tabela 1 mostra como este campo pode ser categorizado em rela\u00e7\u00e3o a sistemas lineares/n\u00e3o lineares e est\u00e1ticos/din\u00e2micos.</p> Caracter\u00edsticas do Sistema Modelo Linear Modelo N\u00e3o Linear Est\u00e1tico Regress\u00e3o Linear Machine Learning Din\u00e2mico Identifica\u00e7\u00e3o de Sistemas Lineares Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares &gt; Tabela 1: Conven\u00e7\u00f5es de nomenclatura no campo de Identifica\u00e7\u00e3o de Sistemas. Adaptado de Oliver Nelles"},{"location":"pt/book/1-Introduction/#aplicacoes-de-identificacao-de-sistemas-nao-lineares-e-previsao-estudos-de-caso","title":"Aplica\u00e7\u00f5es de Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares e Previs\u00e3o: Estudos de Caso","text":"<p>H\u00e1 muita pesquisa sobre identifica\u00e7\u00e3o de sistemas n\u00e3o lineares, incluindo m\u00e9todos NARMAX. No entanto, h\u00e1 um n\u00famero relativamente pequeno de livros e artigos mostrando como aplicar esses m\u00e9todos a sistemas da vida real de uma forma f\u00e1cil de entender. Nosso objetivo com este livro \u00e9 mudar isso. Queremos tornar esses m\u00e9todos pr\u00e1ticos e acess\u00edveis. Embora cobriremos a matem\u00e1tica e os algoritmos necess\u00e1rios, manteremos as coisas o mais claras e simples poss\u00edvel, facilitando para leitores de todas as forma\u00e7\u00f5es aprenderem como modelar sistemas din\u00e2micos n\u00e3o lineares usando o SysIdentPy.</p> <p>Portanto, este livro visa preencher uma lacuna na literatura existente. No Cap\u00edtulo 10, apresentamos estudos de caso do mundo real para mostrar como os m\u00e9todos NARMAX podem ser aplicados a uma variedade de sistemas complexos. Seja modelando um sistema altamente n\u00e3o linear como o modelo de Bouc-Wen, modelando um comportamento din\u00e2mico em uma aeronave F-16 em escala real, ou trabalhando com o dataset M4 para benchmarking, guiaremos voc\u00ea na constru\u00e7\u00e3o de modelos NARMAX usando o SysIdentPy.</p> <p>Os estudos de caso que selecionamos v\u00eam de uma ampla gama de campos, n\u00e3o apenas os t\u00edpicos exemplos de s\u00e9ries temporais ou industriais que voc\u00ea poderia esperar de livros tradicionais de identifica\u00e7\u00e3o de sistemas ou s\u00e9ries temporais. Nosso objetivo \u00e9 mostrar a versatilidade dos algoritmos NARMAX e do SysIdentPy e ilustrar o tipo de an\u00e1lise aprofundada que voc\u00ea pode alcan\u00e7ar com essas ferramentas.</p>"},{"location":"pt/book/1-Introduction/#abreviacoes","title":"Abrevia\u00e7\u00f5es","text":"Abrevia\u00e7\u00e3o Nome Completo AIC Akaike Information Criterion AICC Corrected Akaike Information Criterion AOLS Accelerated Orthogonal Least Squares ANN Artificial Neural Network AR AutoRegressive ARMAX AutoRegressive Moving Average with eXogenous Input ARARX AutoRegressive AutoRegressive with eXogenous Input ARX AutoRegressive with eXogenous Input BIC Bayesian Information Criterion ELS Extended Least Squares ER Entropic Regression ERR Error Reduction Ratio FIR Finite Impulse Response FPE Final Prediction Error FROLS Forward Regression Orthogonal Least Squares GLS Generalized Least Squares LMS Least Mean Square LS Least Squares LSTM Long Short-Term Memory MA Moving Average MetaMSS Meta Model Structure Selection MIMO Multiple Input Multiple Output MISO Multiple Input Single Output MLP Multilayer Perceptron MSE Mean Squared Error MSS Model Structure Selection NARMAX Nonlinear AutoRegressive Moving Average with eXogenous Input NARX Nonlinear AutoRegressive with eXogenous Input NFIR Nonlinear Finite Impulse Response NIIR Nonlinear Infinite Impulse Response NLS Nonlinear Least Squares NN Neural Network OBF Orthonormal Basis Function OE Output Error OLS Orthogonal Least Squares RBF Radial Basis Function RELS Recursive Extended Least Squares RLS Recursive Least Squares RMSE Root Mean Squared Error SI System Identification SISO Single Input Single Output SVD Singular Value Decomposition WLS Weighted Least Squares"},{"location":"pt/book/1-Introduction/#variaveis","title":"Vari\u00e1veis","text":"Nome da Vari\u00e1vel Descri\u00e7\u00e3o \\(f(\\cdot)\\) fun\u00e7\u00e3o a ser aproximada \\(k\\) tempo discreto \\(m\\) ordem din\u00e2mica \\(x\\) entradas do sistema \\(y\\) sa\u00edda do sistema \\(\\hat{y}\\) sa\u00edda predita do modelo \\(\\lambda\\) for\u00e7a de regulariza\u00e7\u00e3o \\(\\sigma\\) desvio padr\u00e3o \\(\\theta\\) vetor de par\u00e2metros \\(N\\) n\u00famero de pontos de dados \\(\\Psi(\\cdot)\\) Matriz de Informa\u00e7\u00e3o \\(n_{m^r}\\) N\u00famero de regressores potenciais para modelos MIMO \\(\\mathcal{F}\\) Representa\u00e7\u00e3o matem\u00e1tica arbitr\u00e1ria \\(\\Omega_{y^p x^m}\\) Cluster de termos do NARX polinomial \\(\\ell\\) grau de n\u00e3o linearidade do modelo \\(\\hat{\\Theta}\\) Vetor de Par\u00e2metros Estimados \\(\\hat{y}_k\\) sa\u00edda predita do modelo no tempo discreto \\(k\\) \\(\\mathbf{X}_k\\) Vetor coluna de m\u00faltiplas entradas do sistema no tempo discreto \\(k\\) \\(\\mathbf{Y}_k\\) Vetor coluna de m\u00faltiplas sa\u00eddas do sistema no tempo discreto \\(k\\) \\(\\mathcal{H}_t(\\omega)\\) Loop de histerese do sistema em tempo cont\u00ednuo \\(\\mathcal{H}\\) Estrutura delimitadora que delimita o loop de histerese do sistema \\(\\rho\\) Valor de toler\u00e2ncia \\(\\sum_{y^p x^m}\\) Coeficientes de cluster do NARX polinomial \\(e_k\\) vetor de erro no tempo discreto \\(k\\) \\(n_r\\) N\u00famero de regressores potenciais para modelos SISO \\(n_x\\) lag m\u00e1ximo do regressor de entrada \\(n_y\\) lag m\u00e1ximo do regressor de sa\u00edda \\(n\\) n\u00famero de observa\u00e7\u00f5es em uma amostra \\(x_k\\) entrada do sistema no tempo discreto \\(k\\) \\(y_k\\) sa\u00edda do sistema no tempo discreto \\(k\\)"},{"location":"pt/book/1-Introduction/#organizacao-do-livro","title":"Organiza\u00e7\u00e3o do Livro","text":"<p>Este livro foca em tornar os conceitos f\u00e1ceis de entender, enfatizando explica\u00e7\u00f5es claras e conex\u00f5es pr\u00e1ticas entre diferentes m\u00e9todos. Evitamos formalismo excessivo e equa\u00e7\u00f5es complexas, optando em vez disso por ilustrar ideias centrais com muitos exemplos pr\u00e1ticos. Escrito com uma perspectiva de Identifica\u00e7\u00e3o de Sistemas, o livro oferece detalhes de implementa\u00e7\u00e3o pr\u00e1tica ao longo dos cap\u00edtulos.</p> <p>Os objetivos deste livro s\u00e3o ajud\u00e1-lo a:</p> <ul> <li>Entender as vantagens, desvantagens e \u00e1reas de aplica\u00e7\u00e3o de diferentes modelos e algoritmos NARMAX.</li> <li>Escolher a abordagem correta para o seu problema espec\u00edfico.</li> <li>Ajustar todos os hiperpar\u00e2metros adequadamente.</li> <li>Interpretar e compreender os resultados obtidos.</li> <li>Avaliar a confiabilidade e limita\u00e7\u00f5es dos seus modelos.</li> </ul> <p>Muitos cap\u00edtulos incluem exemplos e dados do mundo real, guiando voc\u00ea sobre como aplicar esses m\u00e9todos usando o SysIdentPy na pr\u00e1tica.</p>"},{"location":"pt/book/10-Case-Studies/","title":"10. Estudos de Caso","text":""},{"location":"pt/book/2-NARMAX-Model-Representation/","title":"2. Representa\u00e7\u00e3o do Modelo NARMAX","text":"<p>Existem diversas representa\u00e7\u00f5es de modelos NARMAX, incluindo polinomial, Fourier, aditivo generalizado, redes neurais e wavelet (Billings, S. A, Aguirra, L. A). Este livro foca nas representa\u00e7\u00f5es de modelos dispon\u00edveis no SysIdentPy, e manteremos o conte\u00fado atualizado \u00e0 medida que novos m\u00e9todos forem adicionados ao pacote. Se alguma representa\u00e7\u00e3o espec\u00edfica for mencionada, mas ainda n\u00e3o estiver dispon\u00edvel no SysIdentPy, isso ser\u00e1 explicitamente indicado.</p> <p>Para reproduzir os c\u00f3digos apresentados nesta se\u00e7\u00e3o, certifique-se de ter os seguintes pacotes instalados:</p> <pre><code>sysidentpy, scikit-learn, scipy, pytorch, matplotlib\n</code></pre>"},{"location":"pt/book/2-NARMAX-Model-Representation/#funcoes-base-basis-function","title":"Fun\u00e7\u00f5es Base (Basis Function)","text":"<p>Em Identifica\u00e7\u00e3o de Sistemas, entender o conceito de fun\u00e7\u00f5es base \u00e9 fundamental para modelar de forma eficaz sistemas complexos. Fun\u00e7\u00f5es base s\u00e3o fun\u00e7\u00f5es matem\u00e1ticas pr\u00e9-definidas usadas para transformar os dados de entrada em um novo espa\u00e7o, no qual as rela\u00e7\u00f5es nos dados podem ser mais facilmente modeladas. Ao expressar os dados originais em termos dessas fun\u00e7\u00f5es base, podemos construir modelos n\u00e3o lineares em rela\u00e7\u00e3o \u00e0 estrutura, mantendo-os lineares nos par\u00e2metros, o que permite o uso de m\u00e9todos diretos de estima\u00e7\u00e3o de par\u00e2metros.</p> <p>Fun\u00e7\u00f5es base comumente usadas em Identifica\u00e7\u00e3o de Sistemas:</p> <ol> <li> <p>Fun\u00e7\u00f5es Base Polinomiais: S\u00e3o pot\u00eancias das vari\u00e1veis de entrada. S\u00e3o \u00fateis para capturar rela\u00e7\u00f5es n\u00e3o lineares simples.</p> </li> <li> <p>Fun\u00e7\u00f5es Base de Fourier: S\u00e3o fun\u00e7\u00f5es senoidais (seno e cosseno), ideais para representar padr\u00f5es peri\u00f3dicos nos dados.</p> </li> <li> <p>Fun\u00e7\u00f5es Base Wavelet: S\u00e3o fun\u00e7\u00f5es localizadas no tempo e na frequ\u00eancia, adequadas para analisar dados com componentes de frequ\u00eancia vari\u00e1veis. Ainda n\u00e3o est\u00e3o dispon\u00edveis no SysIdentPy.</p> </li> </ol> <p>No SysIdentPy voc\u00ea pode definir a fun\u00e7\u00e3o base que deseja usar no seu modelo simplesmente importando-as:</p> <pre><code>from sysidentpy.basis_function import Polynomial, Fourier, Bernstein\n</code></pre> <p>Para manter as coisas simples por enquanto, vamos mostrar exemplos simples de como fun\u00e7\u00f5es base podem ser usadas em uma tarefa de modelagem. Apresentaremos uma fun\u00e7\u00e3o base polinomial simples, uma fun\u00e7\u00e3o base triangular, uma fun\u00e7\u00e3o base radial e uma fun\u00e7\u00e3o base retangular.</p> <p>O SysIdentPy atualmente n\u00e3o inclui Vandermonde nem nenhuma das outras fun\u00e7\u00f5es base definidas abaixo. Essas fun\u00e7\u00f5es s\u00e3o fornecidas apenas como exemplos para ilustrar a import\u00e2ncia das fun\u00e7\u00f5es base. Os exemplos s\u00e3o baseados na tese de doutorado de Fredrik Bagge Carlson, que \u00e9 altamente recomendada para quem tem interesse em Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares.</p> <p>Embora Vandermonde e Radial Basis Functions (RBF) estejam planejadas para serem inclu\u00eddas como fun\u00e7\u00f5es base nativas no SysIdentPy vers\u00e3o 1.0, os usu\u00e1rios j\u00e1 podem criar e usar suas pr\u00f3prias fun\u00e7\u00f5es base customizadas com o SysIdentPy. Um exemplo de como fazer isso est\u00e1 dispon\u00edvel na p\u00e1gina de documenta\u00e7\u00e3o do SysIdentPy.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#exemplo-matriz-de-vandermonde","title":"Exemplo: Matriz de Vandermonde","text":"<p>As fun\u00e7\u00f5es base polinomiais usadas neste exemplo s\u00e3o definidas como:</p> \\[ \\phi_i(x) = x^i \\tag{2.1} \\] <p>em que \\(i\\) \u00e9 o grau do polin\u00f4mio e \\(x\\) \u00e9 a vari\u00e1vel de entrada.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n\n# Polynomial basis function\ndef poly_basis(x, degree):\n    return np.vander(x, degree + 1, increasing=True)\n\n# Create polynomial features\ndegree = 2\nX_poly = poly_basis(x, degree)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\ny_pred = model.predict(X_poly)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the polynomial approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the polynomial basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(degree + 1):\n    plt.plot(x, poly_basis(x, degree)[:, i], linewidth=0.5, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figura 1. Aproxima\u00e7\u00e3o usando Matriz de Vandermonde. Os pontos amarelos representam os dados do sistema, a linha azul em negrito representa os valores preditos e as demais linhas representam as fun\u00e7\u00f5es base.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#exemplo-funcoes-base-retangulares","title":"Exemplo: Fun\u00e7\u00f5es Base Retangulares","text":"<p>As fun\u00e7\u00f5es base retangulares s\u00e3o definidas como:</p> \\[ \\phi_{i}(x) = \\begin{cases} 1 &amp; \\text{se } c_i - \\frac{w}{2} \\leq x &lt; c_i + \\frac{w}{2} \\\\ 0 &amp; \\text{caso contr\u00e1rio} \\end{cases} \\tag{2.2} \\] <p>em que \\(c_i\\) representa o centro da fun\u00e7\u00e3o base, \\(w\\) \u00e9 a largura e \\(x\\) \u00e9 a vari\u00e1vel de entrada.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n# Rectangular basis function\ndef rectangular_basis(x, centers, width):\n    return np.column_stack([(np.abs(x - c) &lt; width).astype(float) for c in centers])\n\n# Create rectangular features\ncenters = np.linspace(-3, 3, 6)\nwidth = 3\nX_rect = rectangular_basis(x, centers, width)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_rect, y)\ny_pred = model.predict(X_rect)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the rectangular approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the rectangular basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(len(centers)):\n    plt.plot(x, rectangular_basis(x, centers, width)[:, i], linewidth=1, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figura 2. Aproxima\u00e7\u00e3o usando Fun\u00e7\u00e3o Base Retangular. Os pontos amarelos representam os dados do sistema, a linha azul em negrito representa os valores preditos e as demais linhas representam as fun\u00e7\u00f5es base.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#exemplo-funcoes-base-triangulares","title":"Exemplo: Fun\u00e7\u00f5es Base Triangulares","text":"<p>As fun\u00e7\u00f5es base triangulares s\u00e3o definidas como:</p> \\[ \\phi_{i}(x) = \\max \\left(0, 1 - \\frac{|x - c_i|}{w} \\right) \\tag{2.3} \\] <p>em que \\(c_i\\) \u00e9 o centro da fun\u00e7\u00e3o base, \\(w\\) \u00e9 a largura e \\(x\\) \u00e9 a vari\u00e1vel de entrada.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n# Triangular basis function\ndef triangular_basis(x, centers, width):\n    return np.column_stack([np.maximum(0, 1 - np.abs((x - c) / width)) for c in centers])\n\n# Create triangular features\ncenters = np.linspace(-3, 3, 6)\nwidth = 1.5\nX_tri = triangular_basis(x, centers, width)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_tri, y)\ny_pred = model.predict(X_tri)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the triangular approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the triangular basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(len(centers)):\n    plt.plot(x, triangular_basis(x, centers, width)[:, i], linewidth=1, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figura 3. Aproxima\u00e7\u00e3o usando Fun\u00e7\u00e3o Base Triangular. Os pontos amarelos representam os dados do sistema, a linha azul em negrito representa os valores preditos e as demais linhas representam as fun\u00e7\u00f5es base.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#exemplo-radial-basis-function-rbf-gaussiana","title":"Exemplo: Radial Basis Function (RBF) - Gaussiana","text":"<p>A Radial Basis Function Gaussiana \u00e9 definida como:</p> \\[ \\phi(x; c, \\sigma) = \\exp\\left(- \\frac{(x - c)^2}{2 \\sigma^2}\\right) \\tag{2.4} \\] <p>em que: - \\(x\\) \u00e9 a vari\u00e1vel de entrada; - \\(c\\) \u00e9 o centro da RBF; - \\(\\sigma\\) \u00e9 a largura (ou escala) da RBF.</p> <p>Essa fun\u00e7\u00e3o mede a dist\u00e2ncia entre \\(x\\) e o centro \\(c\\), decaindo exponencialmente com base na largura \\(\\sigma\\). Quanto menor o \\(\\sigma\\), mais localizada \u00e9 a fun\u00e7\u00e3o base ao redor do centro \\(c\\).</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)  # More points for a smoother curve\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)  # Quadratic polynomial with noise\n# RBF centers and sigma\ncenters = np.linspace(-3, 3, 6)  # More centers for better coverage\nsigma = 0.5  # Spread of the RBF\n# RBF basis function\ndef rbf_basis(x, c, sigma):\n    return np.exp(- (x - c) ** 2 / (2 * sigma ** 2))\n\n# Create RBF features\nX_rbf = np.column_stack([rbf_basis(x, c, sigma) for c in centers])\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_rbf, y)\ny_pred = model.predict(X_rbf)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Basis function colors\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nn_colors = len(basis_colors)\n# Plot the basis functions\nfor i, c in enumerate(centers):\n    color = basis_colors[i % n_colors]\n    plt.plot(x, rbf_basis(x, c, sigma), linewidth=1, color=color, label=f'RBF Center {c:.2f}')\n\n# Plot the approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figura 4. Aproxima\u00e7\u00e3o usando Radial Basis Function. Os pontos amarelos representam os dados do sistema, a linha azul em negrito representa os valores preditos e as demais linhas representam as fun\u00e7\u00f5es base.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#modelos-lineares","title":"Modelos Lineares","text":""},{"location":"pt/book/2-NARMAX-Model-Representation/#armax","title":"ARMAX","text":"<p>Voc\u00ea provavelmente j\u00e1 notou a semelhan\u00e7a entre o acr\u00f4nimo NARMAX e os modelos bem conhecidos ARX, ARMAX etc., amplamente usados para previs\u00e3o de s\u00e9ries temporais. E essa semelhan\u00e7a n\u00e3o \u00e9 por acaso. Os modelos AutoRegressivos com M\u00e9dia M\u00f3vel e Entrada Ex\u00f3gena (ARMAX) e suas varia\u00e7\u00f5es AR, ARX, ARMA (para citar apenas algumas) est\u00e3o entre as representa\u00e7\u00f5es matem\u00e1ticas mais utilizadas para identifica\u00e7\u00e3o de sistemas lineares. O modelo ARMAX pode ser expresso como:</p> \\[ y_k= \\mathcal{\\phi}[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\tag{2.5} \\] <p>em que \\(n_y\\in \\mathbb{N}\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) s\u00e3o os m\u00e1ximos atrasos para os regressors de sa\u00edda, entrada e ru\u00eddo do sistema (representando a parte de m\u00e9dia m\u00f3vel), respectivamente; \\(x_k \\in \\mathbb{R}^{n_x}\\) \u00e9 a entrada do sistema e \\(y_k \\in \\mathbb{R}^{n_y}\\) \u00e9 a sa\u00edda do sistema no instante discreto \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) representa incertezas e poss\u00edveis ru\u00eddos no instante discreto \\(k\\). Neste caso, \\(\\mathcal{\\phi}\\) \u00e9 alguma fun\u00e7\u00e3o linear dos regressors de entrada e sa\u00edda e \\(d\\) \u00e9 um atraso de tempo, tipicamente definido como \\(d=1\\).</p> <p>Se \\(\\mathcal{F}\\) \u00e9 um polin\u00f4mio, obtemos um modelo ARMAX polinomial:</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.6} \\] <p>em que \\(\\sum\\nolimits_{0}\\), \\(\\Theta_{y}^{i}\\), \\(\\Theta_{e}^{j}\\) e \\(\\Theta_{x}^{m}\\) s\u00e3o par\u00e2metros constantes.</p> <p>O exemplo a seguir \u00e9 um modelo ARMAX polinomial:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139x_{k-1} -0.1691x_{k-1} + 0.2245e_{k-1} \\end{align} \\tag{2.7} \\] <p>Voc\u00ea pode construir facilmente um modelo ARMAX polinomial usando o SysIdentPy: <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True)\n)\n</code></pre></p> <p>No exemplo acima, definimos a fun\u00e7\u00e3o base polinomial linear importando a fun\u00e7\u00e3o base <code>Polynomial</code> e definindo o grau igual a 1 (isso garante que n\u00e3o tenhamos combina\u00e7\u00f5es n\u00e3o lineares dos regressors). N\u00e3o se preocupe ainda com <code>FROLS</code> e <code>LeastSquares</code>. Falaremos sobre eles nos cap\u00edtulos 3 e 4, respectivamente.</p> <p>Para a Figura 4, realizamos 10 simula\u00e7\u00f5es independentes para analisar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do sistema ARMAX. Cada simula\u00e7\u00e3o usa uma amostra distinta de ru\u00eddo para observar como varia\u00e7\u00f5es nesse componente aleat\u00f3rio influenciam a sa\u00edda do sistema. Para ilustrar isso, destacamos uma simula\u00e7\u00e3o espec\u00edfica enquanto as demais s\u00e3o exibidas com menor destaque.</p> <p>\u00c9 importante notar que todas as simula\u00e7\u00f5es, destacadas ou n\u00e3o, s\u00e3o governadas pelo mesmo modelo subjacente. A parte determin\u00edstica da equa\u00e7\u00e3o do modelo explica o comportamento de todos os sinais exibidos. As diferen\u00e7as observadas entre os sinais surgem apenas das diferentes amostras de ru\u00eddo usadas em cada simula\u00e7\u00e3o. Apesar dessas varia\u00e7\u00f5es, a din\u00e2mica central do sinal permanece consistente e \u00e9 descrita pelo componente determin\u00edstico do modelo.</p> <p>A maior parte do c\u00f3digo apresentado neste cap\u00edtulo tem o objetivo de ilustrar conceitos fundamentais, e n\u00e3o de mostrar especificamente como utilizar o SysIdentPy. Muitos exemplos s\u00e3o implementados usando Python \"puro\" para ajudar voc\u00ea a compreender melhor os conceitos subjacentes, reproduzir os exemplos e adapt\u00e1-los conforme necess\u00e1rio. O SysIdentPy em si ser\u00e1 introduzido e utilizado nos exemplos a partir do pr\u00f3ximo cap\u00edtulo.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, u, nu):\n    yk = 0.9*y[0] - 0.24*y[1] + 0.92*u[0] + 0.92*nu[0] + nu[1]\n    return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nu = np.random.normal(size=(random_samples,), scale=1)\nfor k in range(10):\n    nu = np.random.normal(size=(random_samples,), scale=0.9)\n    y = np.empty_like(nu)\n    # Initial Conditions\n    y0 = [0.5, -0.1]\n    y[0:2] = y0\n    for i in range(2, len(y)):\n        y[i] = system_equation([y[i - 1], y[i - 2]], [u[i - 1]], [nu[i - 1], nu[i]])\n\n    # Interpolate the data just to make the plot \"nicer\"\n    interpolation_function = interp1d(n, y, kind='quadratic')\n    n_fine = np.linspace(n.min(), n.max(), 10*len(n))  # More points for a smoother curve\n    y_interpolated = interpolation_function(n_fine)\n    # Plotting the interpolated data\n    if k == 0:\n        ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n    else:\n        ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARMAX model\")\nplt.show()\n</code></pre> <p></p> <p>Figura 5. Simula\u00e7\u00f5es para mostrar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do modelo ARMAX.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#arx","title":"ARX","text":"<p>Se n\u00e3o incluirmos os termos de ru\u00eddo \\(e_{k-n_e}\\) na Equa\u00e7\u00e3o (2.5), obtemos modelos ARX.</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.8} \\] <p>O exemplo a seguir \u00e9 um modelo ARX polinomial:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139x_{k-1} -0.1691x_{k-1} \\end{align} \\tag{2.9} \\] <p>A \u00fanica diferen\u00e7a no SysIdentPy \u00e9 definir <code>unbiased=False</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False)\n)\n</code></pre> <p>O exemplo a seguir mostra 10 simula\u00e7\u00f5es independentes para analisar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do sistema ARX.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, u, nu):\n    yk = 0.9*y[0] - 0.24*y[1] + 0.92*u[0] + nu[0]\n    return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nu = np.random.normal(size=(random_samples,), scale=1)\nfor k in range(10):\n    nu = np.random.normal(size=(random_samples,), scale=0.9)\n    y = np.empty_like(nu)\n    # Initial Conditions\n    y0 = [0.5, -0.1]\n    y[0:2] = y0\n    for i in range(2, len(y)):\n        y[i] = system_equation([y[i - 1], y[i - 2]], [u[i - 1]], [nu[i]])\n\n    # Interpolate the data just to make the plot easier to understand\n    interpolation_function = interp1d(n, y, kind='quadratic')\n    n_fine = np.linspace(n.min(), n.max(), 10*len(n))  # More points for a smoother curve\n    y_interpolated = interpolation_function(n_fine)\n    # Plotting the interpolated data\n    if k == 0:\n        ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n    else:\n        ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARX model\")\nplt.show()\n</code></pre> <p></p> <p>Figura 6. Simula\u00e7\u00f5es para mostrar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do modelo ARX.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#arma","title":"ARMA","text":"<p>Se n\u00e3o incluirmos termos de entrada na Equa\u00e7\u00e3o (2.5), obtemos o modelo ARMA:</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + e_k \\tag{2.10} \\] <p>O exemplo a seguir \u00e9 um modelo ARMA polinomial:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139y_{k-3} -0.1691y_{k-4} + 0.2245e_{k-1} \\end{align} \\tag{2.11} \\] <p>Como a representa\u00e7\u00e3o do modelo n\u00e3o possui entradas, precisamos definir o tipo de modelo como <code>NAR</code> e <code>unbiased=True</code> novamente em <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True),\n    model_type=\"NAR\"\n)\n</code></pre> <p>A figura abaixo mostra 10 simula\u00e7\u00f5es independentes para analisar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do sistema ARMA.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, nu):\n    yk = 0.5*y[0] - 0.4*y[1] + 0.8*nu[0] + nu[1]\n    return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n    nu = np.random.normal(size=(random_samples,), scale=0.9)\n    y = np.empty_like(nu)\n    # Initial Conditions\n    y0 = [0.5, -0.1]\n    y[0:2] = y0\n    for i in range(2, len(y)):\n        y[i] = system_equation([y[i - 1], y[i - 2]], [nu[i - 1], nu[i]])\n\n    # Interpolate the data just to make the plot easier to understand\n    interpolation_function = interp1d(n, y, kind='quadratic')\n    n_fine = np.linspace(n.min(), n.max(), 10*len(n))  # More points for a smoother curve\n    y_interpolated = interpolation_function(n_fine)\n    # Plotting the interpolated data\n    if k == 0:\n        ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n    else:\n        ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARMA model\")\nplt.show()\n</code></pre> <p></p> <p>Figura 7. Simula\u00e7\u00f5es para mostrar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do modelo ARMA.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#ar","title":"AR","text":"<p>Se n\u00e3o incluirmos termos de entrada e de ru\u00eddo na Equa\u00e7\u00e3o (2.5), obtemos o modelo AR:</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + e_k \\tag{2.12} \\] <p>O exemplo a seguir \u00e9 um modelo AR polinomial:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139y_{k-3} -0.1691y_{k-4} \\end{align} \\tag{2.13} \\] <p>Nesse caso, precisamos definir o tipo de modelo como <code>NAR</code> e <code>unbiased=False</code> em <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NAR\"\n)\n</code></pre> <p>A figura abaixo mostra 10 simula\u00e7\u00f5es independentes para analisar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do sistema AR.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, nu):\n    yk = 0.5*y[0] - 0.3*y[1] + nu[0]\n    return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n    nu = np.random.normal(size=(random_samples,), scale=0.9)\n    y = np.empty_like(nu)\n    # Initial Conditions\n    y0 = [0.5, -0.1]\n    y[0:2] = y0\n    for i in range(2, len(y)):\n        y[i] = system_equation([y[i - 1], y[i - 2]], [nu[i]])\n\n    # Interpolate the data just to make the plot easier to understand\n    interpolation_function = interp1d(n, y, kind='quadratic')\n    n_fine = np.linspace(n.min(), n.max(), 10*len(n))  # More points for a smoother curve\n    y_interpolated = interpolation_function(n_fine)\n    # Plotting the interpolated data\n    if k == 0:\n        ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n    else:\n        ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an AR model\")\nplt.show()\n</code></pre> <p></p> <p>Figura 8. Simula\u00e7\u00f5es para mostrar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do modelo AR.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#fir","title":"FIR","text":"<p>Se mantivermos apenas os termos de entrada na Equa\u00e7\u00e3o (2.5), obtemos o modelo NFIR:</p> \\[ y_k = \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.14} \\] <p>O exemplo a seguir \u00e9 um modelo FIR polinomial:</p> \\[ \\begin{align}   y_k =&amp; 0.7213x_{k-1}-0.5692x_{k-2}+0.1139x_{k-3} -0.1691x_{k-4} \\end{align} \\tag{2.15} \\] <p>Nesse caso, precisamos definir o tipo de modelo como <code>NFIR</code> e <code>unbiased=False</code> em <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NFIR\"\n)\n</code></pre> <p>A figura abaixo mostra 10 simula\u00e7\u00f5es independentes para analisar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do sistema FIR.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(u, nu):\n    yk = 0.28*u[0] - 0.34*u[1] + nu[0]\n    return yk\n\nu = np.random.normal(size=(random_samples,), scale=1)\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n    nu = np.random.normal(size=(random_samples,), scale=0.9)\n    y = np.empty_like(nu)\n    # Initial Conditions\n    y0 = [0.5, -0.1]\n    y[0:2] = y0\n    for i in range(2, len(y)):\n        y[i] = system_equation([0.1*u[i - 1], u[i - 2]], [nu[i]])\n\n    # Interpolate the data just to make the plot easier to understand\n    interpolation_function = interp1d(n, y, kind='quadratic')\n    n_fine = np.linspace(n.min(), n.max(), 10*len(n))  # More points for a smoother curve\n    y_interpolated = interpolation_function(n_fine)\n    # Plotting the interpolated data\n    if k == 0:\n        ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n    else:\n        ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an FIR model\")\nplt.show()\n</code></pre> <p></p> <p>Figura 9. Simula\u00e7\u00f5es para mostrar os efeitos de diferentes realiza\u00e7\u00f5es de processo de ru\u00eddo no comportamento do modelo FIR.</p> <p>N\u00e3o definimos o <code>model_type</code> para ARMAX e ARX porque o padr\u00e3o \u00e9 <code>NARMAX</code>. O SysIdentPy permite tr\u00eas tipos de modelo: <code>NARMAX</code>, <code>NAR</code> e <code>NFIR</code>. Como ARMAX, ARX e outras variantes lineares s\u00e3o subconjuntos de modelos NARMAX, n\u00e3o h\u00e1 necessidade de um tipo de modelo espec\u00edfico <code>ARMAX</code>. A ideia \u00e9 ter tipos de modelos para: modelos com regressors de entrada e sa\u00edda; modelos apenas com regressors de sa\u00edda; e modelos apenas com regressors de entrada.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#outras-variantes","title":"Outras Variantes","text":"<p>Por simplicidade, definimos a Equa\u00e7\u00e3o (2.5) e consideramos apenas as representa\u00e7\u00f5es polinomiais. No entanto, voc\u00ea pode estender essas representa\u00e7\u00f5es para outras fun\u00e7\u00f5es base, como Fourier. Se definirmos \\(\\mathcal{F}\\) como a extens\u00e3o de Fourier:</p> \\[ \\mathcal{F}(x) = [\\cos(\\pi x), \\sin(\\pi x), \\cos(2\\pi x), \\sin(2\\pi x), \\ldots, \\cos(N\\pi x), \\sin(N\\pi x)] \\tag{2.16} \\] <p>Nesse caso, a representa\u00e7\u00e3o ARX de Fourier ser\u00e1:</p> \\[ \\begin{aligned} y_k = &amp;\\Big[ \\cos(\\pi y_{k-1}), \\sin(\\pi y_{k-1}), \\cos(2\\pi y_{k-1}), \\sin(2\\pi y_{k-1}), \\ldots, \\cos(N\\pi y_{k-1}), \\sin(N\\pi y_{k-1}), \\\\ &amp;\\ \\ \\cos(\\pi y_{k-2}), \\sin(\\pi y_{k-2}), \\ldots, \\cos(N\\pi y_{k-n_y}), \\sin(N\\pi y_{k-n_y}), \\\\ &amp;\\ \\ \\cos(\\pi x_{k-1}), \\sin(\\pi x_{k-1}), \\cos(2\\pi x_{k-1}), \\sin(2\\pi x_{k-1}), \\ldots, \\cos(N\\pi x_{k-n_x}), \\sin(N\\pi x_{k-n_x}) \\Big] \\\\ &amp;\\ \\ + e_k \\end{aligned} \\tag{2.17} \\] <p>Para fazer isso no SysIdentPy, basta importar a fun\u00e7\u00e3o base de Fourier em vez da polinomial:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Fourier(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NARMAX\"\n)\n</code></pre>"},{"location":"pt/book/2-NARMAX-Model-Representation/#modelos-nao-lineares","title":"Modelos N\u00e3o Lineares","text":""},{"location":"pt/book/2-NARMAX-Model-Representation/#narmax","title":"NARMAX","text":"<p>O modelo NARMAX foi proposto por Stephen A. Billings e I. J. Leontaritis em 1981, (Billings, S. A. - Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains), e pode ser descrito como</p> \\[ \\begin{equation} y_k= \\mathcal{F}[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k, \\end{equation} \\tag{2.18} \\] <p>em que \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) s\u00e3o os m\u00e1ximos atrasos para sa\u00edda, entrada e ru\u00eddo, respectivamente; \\(x_k \\in \\mathbb{R}^{n_x}\\) \u00e9 a entrada do sistema e \\(y_k \\in \\mathbb{R}^{n_y}\\) \u00e9 a sa\u00edda do sistema no instante discreto \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) representa incertezas e poss\u00edveis ru\u00eddos no instante discreto \\(k\\). Nesse caso, \\(\\mathcal{F}\\) \u00e9 alguma fun\u00e7\u00e3o n\u00e3o linear dos regressors de entrada e sa\u00edda, e \\(d\\) \u00e9 um atraso de tempo tipicamente definido como \\(d=1\\).</p> <p>Voc\u00ea pode notar que a diferen\u00e7a entre as Equa\u00e7\u00f5es (2.5) e (2.18) est\u00e1 na fun\u00e7\u00e3o que representa o sistema. Para modelos NARMAX, \\(\\mathcal{F}\\) pode ser qualquer fun\u00e7\u00e3o n\u00e3o linear, enquanto na Equa\u00e7\u00e3o (2.5) apenas fun\u00e7\u00f5es lineares s\u00e3o permitidas. Embora existam muitas poss\u00edveis aproxima\u00e7\u00f5es para \\(\\mathcal{F}(\\cdot)\\) (por exemplo, redes neurais, fuzzy, wavelet, Radial Basis Function), o modelo NARMAX polinomial em forma de pot\u00eancia \u00e9 o mais amplamente utilizado (Billings, S. A.; Khandelwal, D. and Schoukens, M. and Toth, R.):</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j}\\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.19} \\] <p>em que \\(p\\) \u00e9 o n\u00famero de regressors, \\(\\Theta_i\\) s\u00e3o os par\u00e2metros do modelo e \\(a_i, m\\), \\(b_i, j\\) e \\(d_i, l \\in \\mathbb{N}\\) s\u00e3o os expoentes dos termos de sa\u00edda, entrada e ru\u00eddo, respectivamente.</p> <p>A Equa\u00e7\u00e3o (2.20) descreve um modelo NARMAX polinomial com grau de n\u00e3o linearidade igual a \\(2\\), identificado a partir de dados experimentais de um sistema motor/gerador CC, sem conhecimento pr\u00e9vio da forma do modelo, extra\u00eddo de Lacerda Junior, W. R., Almeida, V. M., &amp; Martins, S. A. M. (2017):</p> \\[ \\begin{align}   y_k =&amp; 1.7813y_{k-1}-0.7962y_{k-2}+0.0339x_{k-1} -0.1597x_{k-1} y_{k-1} +0.0338x_{k-2} + \\\\   &amp; + 0.1297x_{k-1}y_{k-2} - 0.1396x_{k-2}y_{k-1}+ 0.1086x_{k-2}y_{k-2}+0.0085y_{k-2}^2 + 0.0247e_{k-1}e_{k-2} \\end{align} \\tag{2.20} \\] <p>Os valores de \\(\\Theta\\) s\u00e3o os coeficientes de cada termo da equa\u00e7\u00e3o polinomial.</p> <p>Fun\u00e7\u00f5es base polinomiais s\u00e3o uma das representa\u00e7\u00f5es de NARMAX mais utilizadas devido a diversas caracter\u00edsticas interessantes (Billings, S. A.):</p> <ul> <li>Todas as fun\u00e7\u00f5es polinomiais s\u00e3o suaves em \\(\\mathbb{R}\\).</li> <li>O Teorema de Aproxima\u00e7\u00e3o de Weierstrass afirma que qualquer fun\u00e7\u00e3o cont\u00ednua real definida em um espa\u00e7o fechado e limitado \\([a,b]\\) pode ser aproximada uniformemente por um polin\u00f4mio nesse intervalo.</li> <li>Elas podem descrever diversos sistemas din\u00e2micos n\u00e3o lineares, incluindo processos industriais, sistemas de controle, sistemas estruturais, sistemas econ\u00f4micos e financeiros, biologia, medicina e sistemas sociais (alguns exemplos s\u00e3o detalhados em Lacerda Junior, W. R. and Martins, S. A. M. and Nepomuceno, E. G. and Lacerda, Marcio J.; Fung, E. H. K. and Wong, Y. K. and Ho, H. F. and Mignolet, M. P.; Kukreja, S. L. and Galiana, H. L. and Kearney, R. E.; Billings, S. A; Aguirre, L. A.; entre muitos outros).</li> <li>Diversos algoritmos foram desenvolvidos para sele\u00e7\u00e3o de estrutura e estima\u00e7\u00e3o de par\u00e2metros de modelos NARMAX polinomiais, e essa continua sendo uma \u00e1rea ativa de pesquisa.</li> <li>Modelos NARMAX polinomiais s\u00e3o vers\u00e1teis e podem ser usados tanto para predi\u00e7\u00e3o quanto para infer\u00eancia. A estrutura desses modelos \u00e9 relativamente f\u00e1cil de interpretar e pode ser relacionada ao sistema subjacente, o que \u00e9 muito mais dif\u00edcil de alcan\u00e7ar com redes neurais ou fun\u00e7\u00f5es wavelet, por exemplo.</li> </ul> <p>Voc\u00ea pode construir facilmente um modelo NARMAX polinomial usando o SysIdentPy. Note que, neste caso, a diferen\u00e7a em rela\u00e7\u00e3o ao ARMAX est\u00e1 no grau da fun\u00e7\u00e3o polinomial.</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True)\n)\n</code></pre> <p>Pode parecer uma mudan\u00e7a simples, mas em cen\u00e1rios n\u00e3o lineares a \"curse of dimensionality\" se torna um problema real. O n\u00famero de regressors candidatos, \\(n_r\\), de um modelo NARX polinomial pode ser definido como em Korenberg, M. L., Billings, S. A., Liu, Y. P., and McIlroy, P. J. - Orthogonal parameter estimation algorithm for non-linear stochastic systems:</p> \\[ \\begin{equation}     n_r = M+1, \\end{equation} \\tag{2.21} \\] <p>em que</p> \\[ \\begin{align}     M = &amp; \\sum_{i=1}^{\\ell}n_i \\\\     n_i = &amp; \\frac{n_{i-1}(n_y+n_x+i-1)}{i}, n_{0} = 1. \\end{align} \\tag{2.22} \\] <p>Como mencionamos na Introdu\u00e7\u00e3o do livro, os m\u00e9todos NARMAX buscam construir modelos o mais simples poss\u00edvel. A ideia \u00e9 reproduzir uma ampla gama de comportamentos usando um pequeno subconjunto de termos do vasto espa\u00e7o de busca formado pelos regressors candidatos.</p> <p>Vamos usar o SysIdentPy para ver como o espa\u00e7o de busca cresce no cen\u00e1rio linear versus o n\u00e3o linear. O m\u00e9todo <code>count_model_regressors</code> dispon\u00edvel em <code>narmax_tools</code> pode ser usado para verificar quantos regressors existem no espa\u00e7o de busca, dado o n\u00famero de entradas, os atrasos de <code>y</code> e <code>x</code> e a fun\u00e7\u00e3o base. Vamos usar <code>xlag=ylag=10</code> e a fun\u00e7\u00e3o base polinomial. O usu\u00e1rio pode simular diferentes cen\u00e1rios definindo par\u00e2metros distintos.</p> <pre><code>from sysidentpy.utils.information_matrix import count_model_regressors\nfrom sysidentpy.basis_function import Polynomial\nimport numpy as np\n</code></pre> <p>Para o caso linear com 1 entrada, temos 21 regressors: <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=1)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=10,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 21\n</code></pre></p> <p>Para o caso linear com 2 entradas, o n\u00famero de regressors salta para 31:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=1)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=xlag,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 31\n</code></pre> <p>Se considerarmos um caso n\u00e3o linear com 1 entrada apenas mudando o grau para 2, temos 231 regressors:</p> <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=2)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=10,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 231\n</code></pre> <p>Se definirmos o grau como 3, o n\u00famero de termos aumenta significativamente para 1771 regressors:</p> <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=3)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=10,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 1771\n</code></pre> <p>Se tivermos 2 entradas no cen\u00e1rio n\u00e3o linear com <code>degree=2</code>, o n\u00famero de regressors \u00e9 496:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=2)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=xlag,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 496\n</code></pre> <p>Se tivermos 2 entradas no cen\u00e1rio n\u00e3o linear com <code>degree=3</code>, o n\u00famero salta para 5456 regressors:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=3)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=xlag,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 5456\n</code></pre> <p>Como voc\u00ea pode notar, o n\u00famero de regressors aumenta significativamente \u00e0 medida que o grau do polin\u00f4mio e o n\u00famero de entradas crescem. Isso torna a sele\u00e7\u00e3o da estrutura do modelo muito mais complexa! No caso linear com 10 entradas, temos <code>2^31=2.15e+09</code> combina\u00e7\u00f5es poss\u00edveis de modelos. Quando <code>degree=2</code> com 2 entradas, temos <code>2^496=2.05e+149</code> combina\u00e7\u00f5es poss\u00edveis! Tente obter o n\u00famero de combina\u00e7\u00f5es poss\u00edveis quando <code>degree=3</code> com 2 entradas. Al\u00e9m disso, tente repetir o exerc\u00edcio com mais entradas e graus de n\u00e3o linearidade mais altos e veja como a \"curse of dimensionality\" \u00e9 um grande problema.</p> <p>Como se pode ver, obter um modelo simples em um espa\u00e7o de busca t\u00e3o grande \u00e9 uma tarefa complexa de sele\u00e7\u00e3o de estrutura. Selecionar os termos mais significativos a partir de um dicion\u00e1rio enorme de termos poss\u00edveis n\u00e3o \u00e9 uma tarefa trivial. E isso \u00e9 dif\u00edcil n\u00e3o apenas devido ao problema combinat\u00f3rio complexo e \u00e0 incerteza sobre a ordem do modelo. Identificar os termos mais relevantes em um cen\u00e1rio n\u00e3o linear \u00e9 muito desafiador porque depende do tipo de n\u00e3o linearidade (singularidade esparsa ou quase singular, efeitos de mem\u00f3ria ou amortecimento, entre outros), da resposta din\u00e2mica (sistemas espa\u00e7o-temporais, dependentes do tempo), da resposta em regime permanente, da frequ\u00eancia dos dados, do ru\u00eddo e de muitos outros fatores.</p> <p>Devido aos algoritmos de sele\u00e7\u00e3o de estrutura desenvolvidos para modelos NARMAX, mesmo modelos lineares como ARMAX podem apresentar desempenho diferente quando obtidos usando o SysIdentPy em compara\u00e7\u00e3o com outras bibliotecas, como Statsmodels. Apresentamos um estudo de caso mostrando exatamente isso no Cap\u00edtulo 10.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#narx","title":"NARX","text":"<p>Se n\u00e3o incluirmos termos de ru\u00eddo \\(e_{k-n_e}\\) na Equa\u00e7\u00e3o (2.19), obtemos modelos NARX:</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.23} \\] <p>A Equa\u00e7\u00e3o (2.24) descreve um modelo NARX polinomial simples:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^2+0.1139y_{k-1}x_{k-1} \\end{align} \\tag{2.24} \\] <p>A \u00fanica diferen\u00e7a no SysIdentPy \u00e9 definir <code>unbiased=False</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False)\n)\n</code></pre> <p>O usu\u00e1rio pode reutilizar os c\u00f3digos fornecidos para modelos lineares a fim de analisar modelos n\u00e3o lineares com diferentes realiza\u00e7\u00f5es de ru\u00eddo.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#narma","title":"NARMA","text":"<p>Se n\u00e3o incluirmos termos de entrada na Equa\u00e7\u00e3o (2.19), obtemos o modelo NARMA:</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times\\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.25} \\] <p>O exemplo a seguir \u00e9 um modelo NARMA polinomial:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^3+0.1139y_{k-3}y_{k-4} + 0.2245e_{k-1} \\end{align} \\tag{2.26} \\] <p>Como a representa\u00e7\u00e3o do modelo n\u00e3o possui entradas, precisamos definir o tipo de modelo como <code>NAR</code> e <code>unbiased=True</code> novamente em <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True),\n    model_type=\"NAR\"\n)\n</code></pre>"},{"location":"pt/book/2-NARMAX-Model-Representation/#nar","title":"NAR","text":"<p>Se n\u00e3o incluirmos termos de entrada e ru\u00eddo na Equa\u00e7\u00e3o (2.19), obtemos o modelo NAR:</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.27} \\] <p>O exemplo a seguir \u00e9 um modelo NAR polinomial:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^2+0.1139y_{k-3}^3 -0.1691y_{k-4}y_{k-5} \\end{align} \\tag{2.28} \\] <p>Nesse caso, precisamos definir o tipo de modelo como <code>NAR</code> e <code>unbiased=False</code> em <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NAR\"\n)\n</code></pre>"},{"location":"pt/book/2-NARMAX-Model-Representation/#nfir","title":"NFIR","text":"<p>Se mantivermos apenas os termos de entrada na Equa\u00e7\u00e3o (2.19), obtemos o modelo NFIR:</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j} \\end{align} \\tag{2.29} \\] <p>O exemplo a seguir \u00e9 um modelo NFIR polinomial:</p> \\[ \\begin{align}   y_k =&amp; 0.7213x_{k-1}-0.5692x_{k-2}^2+0.1139x_{k-3}x_{k-4} -0.1691x_{k-4}^3 \\end{align} \\tag{2.30} \\] <p>Nesse caso, precisamos definir o tipo de modelo como <code>NFIR</code> e <code>unbiased=False</code> em <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NFIR\"\n)\n</code></pre>"},{"location":"pt/book/2-NARMAX-Model-Representation/#modelos-narmax-mistos","title":"Modelos NARMAX Mistos","text":"<p>Em algumas aplica\u00e7\u00f5es, o uso de uma \u00fanica fun\u00e7\u00e3o base pode n\u00e3o fornecer uma descri\u00e7\u00e3o satisfat\u00f3ria da rela\u00e7\u00e3o entre as vari\u00e1veis de entrada (ou independentes) e a vari\u00e1vel de sa\u00edda (ou resposta). Para melhorar o desempenho do modelo, foi proposto o uso de uma combina\u00e7\u00e3o linear de um conjunto de fun\u00e7\u00f5es n\u00e3o lineares para substituir as contrapartes lineares.</p> <p>Voc\u00ea pode obter isso no SysIdentPy utilizando ensembles de fun\u00e7\u00f5es base. \u00c9 poss\u00edvel construir um modelo de Fourier em que os termos possuem intera\u00e7\u00f5es, bem como modelos com fun\u00e7\u00f5es base mistas, usando termos expandidos por fun\u00e7\u00f5es base polinomiais e de Fourier, ou qualquer outra fun\u00e7\u00e3o base dispon\u00edvel no pacote.</p> <p>No SysIdentPy, por enquanto, voc\u00ea s\u00f3 pode misturar uma fun\u00e7\u00e3o base com a fun\u00e7\u00e3o base polinomial. Voc\u00ea pode, por exemplo, misturar Fourier com Polynomial, mas n\u00e3o pode misturar Fourier com Bernstein.</p> <p>Para misturar as fun\u00e7\u00f5es base Fourier ou Bernstein com Polynomial, basta definir <code>ensemble=True</code> na defini\u00e7\u00e3o da fun\u00e7\u00e3o base:</p> <pre><code>from sysidentpy.basis_function import Fourier\n\nbasis_function = Fourier(degree=2, ensemble=True)\n</code></pre>"},{"location":"pt/book/2-NARMAX-Model-Representation/#rede-neural-narx","title":"Rede Neural NARX","text":"<p>Redes neurais s\u00e3o modelos compostos por camadas interconectadas de n\u00f3s (neur\u00f4nios) projetados para tarefas como classifica\u00e7\u00e3o e regress\u00e3o. Cada neur\u00f4nio \u00e9 uma unidade b\u00e1sica dentro dessas redes. Matematicamente, um neur\u00f4nio \u00e9 representado por uma fun\u00e7\u00e3o \\(f\\) que recebe um vetor de entrada \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]\\) e gera uma sa\u00edda \\(y\\). Essa fun\u00e7\u00e3o normalmente envolve uma soma ponderada das entradas, um termo de bias opcional \\(b\\) e uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o \\(\\phi\\):</p> \\[ y = \\phi \\left( \\sum_{i=1}^{n} w_i x_i + b \\right) \\tag{2.31} \\] <p>em que \\(\\mathbf{w} = [w_1, w_2, \\ldots, w_n]\\) s\u00e3o os pesos associados \u00e0s entradas. A fun\u00e7\u00e3o de ativa\u00e7\u00e3o \\(\\phi\\) introduz n\u00e3o linearidade no modelo, permitindo que a rede aprenda padr\u00f5es complexos. Fun\u00e7\u00f5es de ativa\u00e7\u00e3o comuns incluem:</p> <ul> <li> <p>Sigmoid: \\(\\phi(z) = \\frac{1}{1 + e^{-z}}\\)   Produz sa\u00eddas entre 0 e 1, sendo \u00fatil para classifica\u00e7\u00e3o bin\u00e1ria.</p> </li> <li> <p>Tangente Hiperb\u00f3lica (tanh): \\(\\phi(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\)   Produz valores entre -1 e 1, frequentemente usada para centralizar os dados em torno de zero.</p> </li> <li> <p>Rectified Linear Unit (ReLU): \\(\\phi(z) = \\max(0, z)\\)   Retorna zero para valores negativos e o pr\u00f3prio valor de entrada para valores positivos, ajudando a mitigar o problema do gradiente desvanecente.</p> </li> <li> <p>Leaky ReLU: \\(\\phi(z) = \\max(0.01z, z)\\)   Uma varia\u00e7\u00e3o da ReLU que permite um gradiente pequeno e diferente de zero quando a entrada \u00e9 negativa, abordando o problema de \"neur\u00f4nios mortos\".</p> </li> <li> <p>Softmax: \\(\\phi(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\\)   Converte logits em probabilidades para classifica\u00e7\u00e3o multiclasse, garantindo que as sa\u00eddas somem 1.</p> </li> </ul> <p>Cada fun\u00e7\u00e3o de ativa\u00e7\u00e3o possui vantagens espec\u00edficas e \u00e9 escolhida de acordo com as necessidades da rede neural e da tarefa em quest\u00e3o.</p> <p>Como mencionado, uma rede neural \u00e9 composta por m\u00faltiplas camadas, cada uma contendo diversos neur\u00f4nios. Sob esse aspecto, as camadas podem ser categorizadas em:</p> <ul> <li>Camada de Entrada (Input Layer): Camada que recebe os dados de entrada.</li> <li>Camadas Escondidas (Hidden Layers): Camadas intermedi\u00e1rias que processam as entradas por meio de conex\u00f5es ponderadas e fun\u00e7\u00f5es de ativa\u00e7\u00e3o.</li> <li>Camada de Sa\u00edda (Output Layer): Camada final que produz a sa\u00edda da rede.</li> </ul> <p>A pr\u00f3pria rede possui, portanto, uma arquitetura muito simples. A terminologia usada em redes neurais tamb\u00e9m \u00e9 ligeiramente diferente da nota\u00e7\u00e3o padr\u00e3o, universal em Identifica\u00e7\u00e3o de Sistemas e Estat\u00edstica. Em vez de falar de par\u00e2metros do modelo, fala-se em pesos da rede e, em vez de estima\u00e7\u00e3o, fala-se em aprendizado. Essa terminologia foi, sem d\u00favida, introduzida para fazer parecer que algo completamente novo estava sendo discutido, quando alguns dos problemas abordados s\u00e3o bastante tradicionais \u2014 Stephen A. Billings</p> <p>Note que a rede em si \u00e9 simplesmente uma cole\u00e7\u00e3o de unidades de ativa\u00e7\u00e3o n\u00e3o lineares \\(\\phi(\\cdot)\\) que s\u00e3o fun\u00e7\u00f5es est\u00e1ticas simples. N\u00e3o h\u00e1 din\u00e2mica dentro da rede. Isso \u00e9 aceit\u00e1vel para aplica\u00e7\u00f5es como reconhecimento de padr\u00f5es, mas, para usar a rede em Identifica\u00e7\u00e3o de Sistemas, s\u00e3o necess\u00e1rios atrasos de entrada e sa\u00edda, que devem ser fornecidos como entradas, seja explicitamente, seja por meio de um procedimento recorrente. Nesse sentido, se definirmos \\(\\mathcal{F}\\) como uma fun\u00e7\u00e3o neural, podemos adapt\u00e1-la para criar um modelo neural NARX, transformando a arquitetura neural em uma arquitetura NARX. A rede neural NARX, por\u00e9m, n\u00e3o \u00e9 linear nos par\u00e2metros, como os modelos NARMAX baseados em fun\u00e7\u00f5es base. Assim, algoritmos como Orthogonal Least Squares n\u00e3o s\u00e3o adequados para estimar os pesos do modelo.</p> <p>O SysIdentPy oferece suporte a redes NARX em configura\u00e7\u00e3o S\u00e9rie-Paralelo (open-loop) para treinamento, o que torna o processo de treinamento mais simples. Em seguida, convertemos a rede NARX da configura\u00e7\u00e3o S\u00e9rie-Paralelo para a configura\u00e7\u00e3o Paralelo (closed-loop) para predi\u00e7\u00e3o.</p> <p>A configura\u00e7\u00e3o S\u00e9rie-Paralelo nos permite usar <code>pytorch</code> diretamente para o treinamento, portanto o SysIdentPy utiliza <code>pytorch</code> no backend para redes neurais NARX, juntamente com m\u00e9todos auxiliares dispon\u00edveis apenas no SysIdentPy.</p> <p>Um modelo neural NARX simples pode ser representado como uma rede neural Perceptron Multicamadas (MLP) com componente autorregressivo, junto com entradas atrasadas.</p> <p></p> <p>Figura 10. Arquiteturas de rede neural em paralelo e s\u00e9rie-paralelo para modelagem do sistema din\u00e2mico \\(\\mathbf{y}[k]=\\mathbf{F}(\\mathbf{y}[k-1], \\mathbf{y}[k-2], \\mathbf{u}[k-1], \\mathbf{u}[k-2])\\). O operador de atraso \\(q^{-1}\\) \u00e9 tal que \\(\\mathbf{y}[k-1]=q^{-1} \\mathbf{y}[k]\\). Refer\u00eancia: Antonio H. Ribeiro and Luis A. Aguirre</p> <p>Neural NARX n\u00e3o \u00e9 o mesmo que Redes Neurais Recorrentes (RNN). Para mais detalhes, consulte o artigo A Note on the Equivalence of NARX and RNN.</p> <p>Para construir uma rede Neural NARX no SysIdentPy, o usu\u00e1rio deve usar <code>pytorch</code>. Utilizamos <code>pytorch</code> para tornar flex\u00edvel a defini\u00e7\u00e3o da arquitetura da rede. No entanto, isso exige que o usu\u00e1rio tenha uma compreens\u00e3o razo\u00e1vel de como redes neurais funcionam. Veja abaixo um script que mostra como construir um modelo Neural NARX simples no SysIdentPy:</p> <pre><code>from torch import nn\nimport torch\n\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.narmax_tools import regressor_code\n\n# simulated data\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre> <p>O usu\u00e1rio pode utilizar <code>cuda</code> seguindo a mesma abordagem usada ao construir uma rede neural em pytorch:</p> <pre><code>torch.cuda.is_available()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</code></pre> <p>O usu\u00e1rio pode criar um objeto <code>NARXNN</code> e escolher o atraso m\u00e1ximo tanto da entrada quanto da sa\u00edda para construir a matriz de regressors que servir\u00e1 como entrada da rede. Al\u00e9m disso, \u00e9 poss\u00edvel escolher a fun\u00e7\u00e3o de custo, o otimizador, os par\u00e2metros opcionais do otimizador e o n\u00famero de \u00e9pocas.</p> <p>Como constru\u00edmos esse recurso sobre o Pytorch, voc\u00ea pode escolher qualquer fun\u00e7\u00e3o de custo dispon\u00edvel em <code>torch.nn.functional</code>. Clique aqui para ver a lista de fun\u00e7\u00f5es de custo dispon\u00edveis. Basta passar o nome da fun\u00e7\u00e3o de custo desejada.</p> <p>De forma an\u00e1loga, voc\u00ea pode escolher qualquer otimizador dispon\u00edvel em <code>torch.optim</code>. Clique aqui para ver a lista de otimizadores dispon\u00edveis.</p> <pre><code>basis_function = Polynomial(degree=1)\nnarx_net = NARXNN(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    device=device,\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n</code></pre> <p>Como o modelo NARXNN foi definido com \\(ylag=2\\), \\(xlag=2\\) e fun\u00e7\u00e3o base polinomial com \\(degree=1\\), obtemos uma matriz de regressors com 4 features. Precisamos do tamanho dessa matriz de regressors para construir as camadas da nossa rede. Nossos dados de entrada (<code>x_train</code>) possuem apenas uma feature, mas como estamos criando uma rede NARX, uma matriz de regressors \u00e9 constru\u00edda internamente com novas features baseadas em <code>xlag</code> e <code>ylag</code>.</p> <p>Se precisar de ajuda para descobrir quantos regressors s\u00e3o criados internamente, voc\u00ea pode usar a fun\u00e7\u00e3o <code>regressor_code</code> de <code>narmax_tools</code> e obter o tamanho do c\u00f3digo de regressors gerado:</p> <pre><code>basis_function = Polynomial(degree=1)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=2,\n    ylag=2,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=True,\n)\nn_regressors\n&gt;&gt;&gt; 4\n</code></pre> <p>A configura\u00e7\u00e3o da sua rede segue exatamente o mesmo padr\u00e3o de uma rede definida em Pytorch. O c\u00f3digo a seguir representa nossa rede neural NARX:</p> <pre><code>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_regressors, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n</code></pre> <p>O usu\u00e1rio deve passar a rede definida para o estimador <code>NARXNN</code> e configurar <code>cuda</code> se estiver dispon\u00edvel (ou se for necess\u00e1rio):</p> <pre><code>narx_net.net = NARX()\n\nif device == \"cuda\":\n    narx_net.net.to(torch.device(\"cuda\"))\n</code></pre> <p>Como temos fun\u00e7\u00f5es <code>fit</code> (para treinamento) e <code>predict</code> para o NARMAX polinomial, criamos o mesmo padr\u00e3o para a rede NARX. Assim, basta chamar <code>fit</code> e <code>predict</code> da seguinte forma:</p> <pre><code>narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid)\nyhat = narx_net.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Se a configura\u00e7\u00e3o da rede for constru\u00edda antes de chamar o <code>NARXNN</code>, basta passar o modelo para o <code>NARXNN</code> da seguinte forma:</p> <pre><code>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_regressors, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n\n\nnarx_net2 = NARXNN(\n    net=NARX(),\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n\nnarx_net2.fit(X=x_train, y=y_train)\nyhat = narx_net2.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"pt/book/2-NARMAX-Model-Representation/#representacao-geral-de-conjunto-de-modelos","title":"Representa\u00e7\u00e3o Geral de Conjunto de Modelos","text":"<p>Com base na ideia de transformar uma rede neural est\u00e1tica em um modelo neural NARX, podemos estender o m\u00e9todo basicamente para qualquer classe de modelo. O SysIdentPy n\u00e3o tem como objetivo implementar todas as classes de modelos existentes na literatura. Entretanto, criamos uma funcionalidade que permite o uso de qualquer outro pacote de machine learning que siga a API <code>fit</code> e <code>predict</code> dentro do SysIdentPy para convert\u00ea-los em vers\u00f5es NARX desses modelos.</p> <p>Vamos tomar o XGBoost (eXtreme Gradient Boosting) como exemplo. XGBoost \u00e9 uma classe de modelos bem conhecida para tarefas de regress\u00e3o. No entanto, XGBoost n\u00e3o \u00e9 uma escolha comum quando tratamos de identifica\u00e7\u00e3o de sistemas din\u00e2micos, pois foi originalmente projetado para modelar sistemas est\u00e1ticos. Com o SysIdentPy, voc\u00ea pode facilmente transformar XGBoost em um modelo NARX.</p> <p>O Scikit-learn \u00e9 outro excelente exemplo. Voc\u00ea pode transformar qualquer modelo do Scikit-learn em um modelo NARX usando o SysIdentPy. Veremos essas aplica\u00e7\u00f5es em detalhes no Cap\u00edtulo 11, mas o script abaixo ilustra como isso \u00e9 simples:</p> <pre><code>from sysidentpy.general_estimators import NARX\nfrom sysidentpy.basis_function import Polynomial\nfrom sklearn.linear_model import BayesianRidge\nimport xgboost as xgb\n\nbasis_function = Fourier(degree=1)\n# define the scikit estimator\nscikit_estimator = BayesianRidge()\n# transform scikit_estimator into NARX model\ngb_narx = NARX(\n    base_estimator=scikit_estimator,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\ngb_narx.fit(X=x_train, y=y_train)\nyhat = gb_narx.predict(X=x_valid, y=y_valid)\n\n# XGboost examples\nxgb_estimator = xgb.XGBRegressor()\nxgb_narx = NARX(\n    base_estimator=xgb_estimator,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nxgb_narx.fit(X=x_train, y=y_train)\nyhat = xgb_narx.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Voc\u00ea pode usar qualquer outro modelo simplesmente trocando a classe do modelo e passando-a para o par\u00e2metro <code>base_estimator</code> na funcionalidade <code>NARX</code>.</p>"},{"location":"pt/book/2-NARMAX-Model-Representation/#modelos-mimo","title":"Modelos MIMO","text":"<p>Para manter as coisas simples, apenas modelos SISO foram apresentados nas se\u00e7\u00f5es anteriores. No entanto, modelos NARMAX podem ser estendidos de forma natural para o caso MIMO (Billings, S. A. and Chen, S. and Korenberg, M. J.):</p> \\[ \\begin{align} \\t y_{{_i}k}=&amp; F_{{_i}}^\\ell \\bigl[y_{{_1}k-1},  \\dotsc, y_{{_1}k-n^i_{y{_1}}},\\dotsc, y_{{_s}k-1},  \\dotsc, y_{{_s}k-n^i_{y{_s}}}, x_{{_1}k-d}, \\\\ \\t &amp; x_{{_1}k-d-1}, \\dotsc, x_{{_1}k-d-n^i_{x{_1}}}, \\dotsc, x_{{_r}k-d}, x_{{_r}k-d-1}, \\dotsc, x_{{_r}k-d-n^i_{x{_r}}}\\bigr] + \\xi_{{_i}k}, \\end{align} \\tag{2.32} \\] <p>em que, para \\(i = 1, \\dotsc, s\\), cada submodelo linear nos par\u00e2metros pode ter diferentes atrasos m\u00e1ximos. Mais genericamente, considerando</p> \\[ \\begin{align}     Y_k = \\begin{bmatrix}     y_{{_1}k} \\\\     y_{{_2}k} \\\\     \\vdots \\\\     y_{{_s}k}     \\end{bmatrix},     X_k = \\begin{bmatrix}     x_{{_1}k} \\\\     x_{{_2}k} \\\\     \\vdots \\\\     x_{{_r}k}     \\end{bmatrix},     \\Xi_k = \\begin{bmatrix}     \\xi_{{_1}k} \\\\     \\xi_{{_2}k} \\\\     \\vdots \\\\     \\xi_{{_r}k}     \\end{bmatrix}, \\end{align} \\tag{2.33} \\] <p>o modelo MIMO pode ser denotado como</p> \\[ \\begin{equation}              Y_k= F^\\ell[Y_{k-1},  \\dotsc, Y_{k-n_y},X_{k-d}, X_{k-d-1}, \\dotsc, X_{k-d-n_x}] + \\Xi_k, \\end{equation} \\tag{2.34} \\] <p>em que \\(Xk ~= \\{x_{{_1}k}, x_{{_2}k}, \\dotsc, x_{{_r}k}\\}\\in \\mathbb{R}^{n^i_{x{_r}}}\\) e \\(Yk~= \\{y_{{_1}k}, y_{{_2}k}, \\dotsc, y_{{_s}k}\\}\\in \\mathbb{R}^{n^i_{y{_s}}}\\). O n\u00famero de termos poss\u00edveis de um modelo MIMO NARX dado o \\(i\\)-\u00e9simo grau polinomial, \\(\\ell_i\\), \u00e9:</p> \\[ \\begin{equation}     n_{{_{m}}r} = \\sum_{j = 0}^{\\ell_i}n_{ij}, \\end{equation} \\tag{2.35} \\] <p>em que</p> \\[ \\begin{align}     n_{ij} = \\frac{ n_{ij-1} \\biggl[ \\sum\\limits_{k=1}^{s} n^i_{y_k} + \\sum\\limits_{k=1}^{r} n^i_{x_k} + j - 1 \\biggr]}{j}, \\qquad n_{i0}=1, j=1, \\dotsc, \\ell_i. \\end{align} \\tag{2.36} \\] <p>Se \\(s=1\\), temos um modelo MISO que pode ser representado por uma \u00fanica fun\u00e7\u00e3o polinomial. Al\u00e9m disso, um modelo MIMO pode ser decomposto em modelos MISO, como apresentado na figura a seguir:</p> <p></p> <p>Figura 11. Um modelo MIMO decomposto em modelos MISO individuais.</p> <p>O SysIdentPy ainda n\u00e3o oferece suporte a modelos MIMO, apenas modelos MISO. Voc\u00ea pode, no entanto, decompor um sistema MIMO como apresentado na Figura 11 e usar o SysIdentPy para criar modelos para cada subsistema.</p>"},{"location":"pt/book/3-Parameter-Estimation/","title":"3. Estima\u00e7\u00e3o de Par\u00e2metros","text":""},{"location":"pt/book/3-Parameter-Estimation/#minimos-quadrados","title":"M\u00ednimos Quadrados","text":"<p>Considere o modelo NARX descrito em uma forma gen\u00e9rica como</p> \\[ \\begin{equation}     y_k = \\psi^\\top_{k-1}\\hat{\\Theta} + \\xi_k, \\end{equation} \\tag{3.1} \\] <p>onde \\(\\psi^\\top_{k-1} \\in \\mathbb{R}^{n_r \\times n}\\) \u00e9 a matriz de informa\u00e7\u00e3o, tamb\u00e9m conhecida como matriz de regressores. A matriz de informa\u00e7\u00e3o \u00e9 a transforma\u00e7\u00e3o da entrada e sa\u00edda baseada em uma fun\u00e7\u00e3o base e \\(\\hat{\\Theta}~\\in \\mathbb{R}^{n_{\\Theta}}\\) \u00e9 o vetor de par\u00e2metros estimados. O modelo acima tamb\u00e9m pode ser representado na forma matricial como:</p> \\[ \\begin{equation}     y = \\Psi\\hat{\\Theta} + \\Xi, \\end{equation} \\tag{3.2} \\] <p>onde</p> \\[ \\begin{align}     Y = \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_n     \\end{bmatrix},     \\Psi = \\begin{bmatrix}     \\psi_{{_1}} \\\\     \\psi_{{_2}} \\\\     \\vdots \\\\     \\psi_{{_{n_{\\Theta}}}}     \\end{bmatrix}^\\top=     \\begin{bmatrix}     \\psi_{{_1}1} &amp; \\psi_{{_2}1} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}1} \\\\     \\psi_{{_1}2} &amp; \\psi_{{_2}2} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}2} \\\\     \\vdots &amp; \\vdots &amp;       &amp; \\vdots \\\\     \\psi_{{_1}n} &amp; \\psi_{{_2}n} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}n} \\\\     \\end{bmatrix},     \\hat{\\Theta} = \\begin{bmatrix}     \\hat{\\Theta}_1 \\\\     \\hat{\\Theta}_2 \\\\     \\vdots \\\\     \\hat{\\Theta}_{n_\\Theta}     \\end{bmatrix},     \\Xi = \\begin{bmatrix}     \\xi_1 \\\\     \\xi_2 \\\\     \\vdots \\\\     \\xi_n     \\end{bmatrix}. \\end{align} \\tag{3.3} \\] <p>Consideraremos a fun\u00e7\u00e3o base polinomial para manter os exemplos diretos, mas os m\u00e9todos aqui funcionar\u00e3o para qualquer outra fun\u00e7\u00e3o base.</p> <p>O modelo NARX param\u00e9trico \u00e9 linear nos par\u00e2metros \\(\\Theta\\), ent\u00e3o podemos usar algoritmos bem conhecidos, como o algoritmo de M\u00ednimos Quadrados desenvolvido por Gauss em \\(1795\\), para estimar os par\u00e2metros do modelo. A ideia \u00e9 encontrar o vetor de par\u00e2metros que minimiza a norma \\(l2\\), tamb\u00e9m conhecida como soma dos quadrados dos res\u00edduos, descrita como</p> \\[ \\begin{equation}     J_{\\hat{\\Theta}} = \\Xi^\\top \\Xi = (y - \\Psi\\hat{\\Theta})^\\top(y - \\Psi\\hat{\\Theta}) = \\lVert y - \\Psi\\hat{\\Theta} \\rVert^2. \\end{equation} \\tag{3.4} \\] <p>Na Equa\u00e7\u00e3o 3.4, \\(\\Psi\\hat{\\Theta}\\) \u00e9 a predi\u00e7\u00e3o um passo \u00e0 frente de \\(y_k\\), expressa como</p> \\[ \\begin{equation}     \\hat{y}_{1_k} = g(y_{k-1}, u_{k-1}\\lvert ~\\Theta), \\end{equation} \\tag{3.5} \\] <p>onde \\(g\\) \u00e9 alguma fun\u00e7\u00e3o polinomial desconhecida. Se o gradiente de \\(J_{\\Theta}\\) em rela\u00e7\u00e3o a \\(\\Theta\\) \u00e9 igual a zero, ent\u00e3o temos a equa\u00e7\u00e3o normal e a estimativa de M\u00ednimos Quadrados \u00e9 expressa como</p> \\[ \\begin{equation}     \\hat{\\Theta}  = (\\Psi^\\top\\Psi)^{-1}\\Psi^\\top y, \\end{equation} \\tag{3.6} \\] <p>onde \\((\\Psi^\\top\\Psi)^{-1}\\Psi^\\top\\) \u00e9 chamada de pseudo-inversa da matriz \\(\\Psi\\), denotada \\(\\Psi^+ \\in \\mathbb{R}^{n \\times n_r}\\).</p> <p>Para ter um estimador n\u00e3o-viesado, as seguintes s\u00e3o as suposi\u00e7\u00f5es b\u00e1sicas necess\u00e1rias para o m\u00e9todo dos m\u00ednimos quadrados: - A1 - N\u00e3o h\u00e1 correla\u00e7\u00e3o entre o vetor de erros, \\(\\Xi\\), e a matriz de regressores, \\(\\Psi\\). Matematicamente: - \\(\\mathrm{E}\\{[(\\Psi^\\top\\Psi)^{-1}\\Psi^\\top] \\Xi\\} = \\mathrm{E}[(\\Psi^\\top\\Psi)^{-1}\\Psi^\\top] \\mathrm{E}[\\Xi]; \\tag{3.7}\\) - A2 - O vetor de erros \\(\\Xi\\) \u00e9 uma sequ\u00eancia de ru\u00eddo branco com m\u00e9dia zero: - \\(\\mathrm{E}[\\Xi] = 0; \\tag{3.8}\\) - A3 - A matriz de covari\u00e2ncia do vetor de erros \u00e9 - \\(\\mathrm{Cov}[\\hat{\\Theta}] = \\mathrm{E}[(\\Theta - \\hat{\\Theta})(\\Theta - \\hat{\\Theta})^\\top] = \\sigma^2(\\Psi^\\top\\Psi); \\tag{3.9}\\) - A4 - A matriz de regressores, \\(\\Psi\\), tem posto completo.</p> <p>As suposi\u00e7\u00f5es mencionadas acima s\u00e3o necess\u00e1rias para garantir que o algoritmo de M\u00ednimos Quadrados produza um modelo final n\u00e3o-viesado.</p>"},{"location":"pt/book/3-Parameter-Estimation/#exemplo","title":"Exemplo","text":"<p>Vamos ver um exemplo pr\u00e1tico. Considere o modelo</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k} \\tag{3.10} \\] <p>Podemos gerar a entrada <code>X</code> e a sa\u00edda <code>y</code> usando o SysIdentPy. Antes de entrar nos detalhes, vamos executar um modelo simples usando o SysIdentPy. Como sabemos a priori que o sistema que estamos tentando identificar n\u00e3o \u00e9 linear (o sistema simulado tem um termo de intera\u00e7\u00e3o \\(0.1y_{k-1}x_{k-1}\\)) e a ordem \u00e9 2 (o atraso m\u00e1ximo da entrada e sa\u00edda), definiremos os hiperpar\u00e2metros de acordo. Note que este \u00e9 um cen\u00e1rio simulado, e voc\u00ea n\u00e3o ter\u00e1 essa informa\u00e7\u00e3o a priori em uma tarefa de identifica\u00e7\u00e3o real. Mas n\u00e3o se preocupe, a ideia, por enquanto, \u00e9 apenas mostrar como as coisas funcionam e desenvolveremos alguns modelos reais ao longo do livro.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    n_info_values=3,\n    ylag=1,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\n# print the identified model\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nRegressors   Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.56885108E-01\n1         y(k-1)  2.0000E-01  3.96313039E-02\n2  x1(k-1)y(k-1)  1.0001E-01  3.48355000E-03\n</code></pre> <p>Como voc\u00ea pode ver, o modelo final tem os mesmos 3 regressores do sistema simulado e os par\u00e2metros est\u00e3o muito pr\u00f3ximos dos usados para simular o sistema. Isso nos mostra que o M\u00ednimos Quadrados teve bom desempenho para estes dados.</p> <p>Neste exemplo, no entanto, estamos aplicando um algoritmo de Sele\u00e7\u00e3o de Estrutura de Modelo (FROLS), que veremos no cap\u00edtulo 6. Por isso o modelo final tem apenas 3 regressores. O algoritmo de estima\u00e7\u00e3o de par\u00e2metros n\u00e3o escolhe quais termos incluir no modelo, ent\u00e3o se tivermos uma fun\u00e7\u00e3o base expandida com 6 regressores, ele estimar\u00e1 o par\u00e2metro para cada um dos regressores.</p> <p>Para verificar como isso funciona, podemos usar o SysIdentPy sem Sele\u00e7\u00e3o de Estrutura de Modelo gerando a matriz de informa\u00e7\u00e3o e aplicando o algoritmo de estima\u00e7\u00e3o de par\u00e2metros diretamente.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils import build_lagged_matrix\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\ntheta = LeastSquares().optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[-4.1511e-06]\n [ 2.0002e-01]\n [ 1.1237e-05]\n [ 1.0068e-05]\n [ 8.9997e-01]\n [-6.3216e-05]\n [ 1.3298e-04]\n [ 1.0008e-01]\n [ 6.3118e-05]\n [-5.6031e-05]\n [-1.9073e-05]\n [-1.8223e-04]\n [ 1.1307e-04]\n [-1.6601e-04]\n [-8.5068e-05]]\n</code></pre> <p>Neste caso, temos 15 par\u00e2metros do modelo. Se observarmos a expans\u00e3o da fun\u00e7\u00e3o base onde o grau do polin\u00f4mio \u00e9 igual a 2 e os atrasos para <code>y</code> e <code>x</code> s\u00e3o definidos como 2, temos</p> <pre><code>from sysidentpy.utils.narmax_tools import regressor_code\nbasis_function = Polynomial(degree=2)\nregressors = regressor_code(\n    X=x_train,\n    xlag=2,\n    ylag=2,\n    model_type=\"NARMAX\",\n    model_representation=\"Polynomial\",\n    basis_function=basis_function,\n)\nregressors\n\narray([[ 0, 0],\n   [1001, 0],\n   [1002, 0],\n   [2001, 0],\n   [2002, 0],\n   [1001, 1001],\n   [1002, 1001],\n   [2001, 1001],\n   [2002, 1001],\n   [1002, 1002],\n   [2001, 1002],\n   [2002, 1002],\n   [2001, 2001],\n   [2002, 2001],\n   [2002, 2002]]\n   )\n</code></pre> <p>O regressors \u00e9 como o SysIdentPy codifica a fun\u00e7\u00e3o base polinomial seguindo este padr\u00e3o de codifica\u00e7\u00e3o:</p> <ul> <li>\\(0\\) \u00e9 o termo constante,\\n\",</li> <li>\\([1001] = y_{k-1}\\)</li> <li>\\([100n] = y_{k-n}\\)</li> <li>\\([200n] = x1_{k-n}\\)</li> <li>\\([300n] = x2_{k-n}\\)</li> <li>\\([1011, 1001] = y_{k-11} \\\\times y_{k-1}\\)</li> <li>\\([100n, 100m] = y_{k-n} \\times y_{k-m}\\)</li> <li>\\([12001, 1003, 1001] = x11_{k-1} \\times y_{k-3} \\times y_{k-1}\\),</li> <li>e assim por diante</li> </ul> <p>Ent\u00e3o, se voc\u00ea observar os par\u00e2metros, podemos ver que a estima\u00e7\u00e3o do algoritmo de M\u00ednimos Quadrados para os termos que pertencem ao sistema simulado est\u00e3o muito pr\u00f3ximos dos valores reais.</p> <pre><code>[1001, 0] -&gt; [ 2.00002486e-01]\n[2002, 0] -&gt; [ 8.99927332e-01]\n[2001, 1001] -&gt; [ 1.00062340e-01]\n</code></pre> <p>Al\u00e9m disso, os par\u00e2metros estimados para os outros regressores s\u00e3o valores consideravelmente menores do que os estimados para os termos corretos, indicando que os outros podem n\u00e3o ser relevantes para o modelo.</p> <p>Voc\u00ea pode come\u00e7ar a pensar que s\u00f3 precisamos definir uma fun\u00e7\u00e3o base e aplicar alguma t\u00e9cnica de estima\u00e7\u00e3o de par\u00e2metros para construir modelos NARMAX. No entanto, como mencionado antes, o principal objetivo dos m\u00e9todos NARMAX \u00e9 construir o melhor modelo poss\u00edvel mantendo-o simples. E isso \u00e9 verdade para o caso em que aplicamos o algoritmo FROLS. Al\u00e9m disso, ao lidar com identifica\u00e7\u00e3o de sistemas, queremos recuperar a din\u00e2mica do sistema em estudo, ent\u00e3o adicionar mais termos do que o necess\u00e1rio pode levar a comportamentos inesperados, desempenho ruim e modelos inst\u00e1veis. Lembre-se, este \u00e9 apenas um exemplo did\u00e1tico, ent\u00e3o em casos reais a sele\u00e7\u00e3o de estrutura de modelo \u00e9 fundamental.</p> <p>Voc\u00ea pode implementar o m\u00e9todo de M\u00ednimos Quadrados de forma simples como</p> <pre><code>import numpy as np\n\ndef simple_least_squares(psi, y):\n    return np.linalg.pinv(psi.T @ psi) @ psi.T @ y\n\n# use the psi and y data created in previous examples or\n# create them again here to run the example.\ntheta = simple_least_squares(psi, y_train[max_lag:, :])\n\ntheta\n\narray(\n    [\n       [-1.08377785e-05],\n       [ 2.00002486e-01],\n       [ 1.73422294e-05],\n       [-3.50957931e-06],\n       [ 8.99927332e-01],\n       [ 2.04427279e-05],\n       [-1.47542408e-04],\n       [ 1.00062340e-01],\n       [ 4.53379771e-05],\n       [ 8.90006341e-05],\n       [ 1.15234873e-04],\n       [ 1.57770755e-04],\n       [ 1.58414037e-04],\n       [-3.09236444e-05],\n       [-1.60377753e-04]\n    ]\n)\n</code></pre> <p>Como voc\u00ea pode ver, os par\u00e2metros estimados s\u00e3o muito pr\u00f3ximos. No entanto, tenha cuidado ao usar tal abordagem em sistemas subdeterminados, bem determinados ou sobredeterminados. Recomendamos usar os m\u00e9todos <code>lstsq</code> do numpy ou scipy.</p>"},{"location":"pt/book/3-Parameter-Estimation/#minimos-quadrados-totais","title":"M\u00ednimos Quadrados Totais","text":"<p>Esta se\u00e7\u00e3o \u00e9 baseada em Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least squares methods. Signal Processing..</p> <p>O algoritmo de M\u00ednimos Quadrados Totais (Total Least Squares - TLS) \u00e9 um m\u00e9todo estat\u00edstico usado para encontrar a melhor rela\u00e7\u00e3o linear entre vari\u00e1veis quando tanto os sinais de entrada quanto de sa\u00edda apresentam perturba\u00e7\u00e3o de ru\u00eddo branco. Diferente dos m\u00ednimos quadrados ordin\u00e1rios (OLS), que assume que apenas a vari\u00e1vel dependente est\u00e1 sujeita a erro, o TLS considera erros em todas as vari\u00e1veis medidas, fornecendo uma solu\u00e7\u00e3o mais robusta em muitas aplica\u00e7\u00f5es pr\u00e1ticas. O algoritmo foi proposto por Golub e Van Loan.</p> <p>No TLS, assumimos erros tanto em \\(\\mathbf{X}\\) quanto em \\(\\mathbf{Y}\\), denotados como \\(\\Delta \\mathbf{X}\\) e \\(\\Delta \\mathbf{Y}\\), respectivamente. O modelo verdadeiro se torna:</p> \\[ \\mathbf{Y} + \\Delta \\mathbf{Y} = (\\mathbf{X} + \\Delta \\mathbf{X}) \\mathbf{B} \\tag{3.11} \\] <p>Rearranjando, obtemos:</p> \\[ \\Delta \\mathbf{Y} = \\Delta \\mathbf{X} \\mathbf{B} \\tag{3.12} \\]"},{"location":"pt/book/3-Parameter-Estimation/#funcao-objetivo","title":"Fun\u00e7\u00e3o Objetivo","text":"<p>A solu\u00e7\u00e3o TLS minimiza a norma de Frobenius das perturba\u00e7\u00f5es totais em \\(\\mathbf{X}\\) e \\(\\mathbf{Y}\\):</p> \\[ \\min_{\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}} \\|[\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}]\\|_F \\tag{3.13} \\] <p>sujeito a:</p> \\[ (\\mathbf{X} + \\Delta \\mathbf{X}) \\mathbf{B} = \\mathbf{Y} + \\Delta \\mathbf{Y} \\tag{3.14} \\] <p>onde \\(\\| \\cdot \\|_F\\) denota a norma de Frobenius.</p>"},{"location":"pt/book/3-Parameter-Estimation/#solucao-classica","title":"Solu\u00e7\u00e3o Cl\u00e1ssica","text":"<p>A abordagem cl\u00e1ssica para resolver o problema TLS \u00e9 usando a Decomposi\u00e7\u00e3o em Valores Singulares (SVD). A matriz aumentada \\([\\mathbf{X}, \\mathbf{Y}]\\) \u00e9 decomposta como:</p> \\[ [\\mathbf{X}, \\mathbf{Y}] = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\tag{3.15} \\] <p>onde \\(\\mathbf{U}\\) \u00e9 uma matriz ortogonal \\(n \\times n\\), \\(\\Sigma=\\operatorname{diag}\\left(\\sigma_1, \\ldots, \\sigma_{n+d}\\right)\\) \u00e9 uma matriz diagonal de valores singulares; e \\(\\mathbf{V}\\) \u00e9 uma matriz ortogonal definida como</p> \\[ V:=\\left[\\begin{array}{cc} V_{11} &amp; V_{12} \\\\ V_{21} &amp; V_{22} \\end{array}\\right] \\quad \\begin{aligned} \\end{aligned} \\quad \\text { e } \\quad \\Sigma:=\\left[\\begin{array}{cc} \\Sigma_1 &amp; 0 \\\\ 0 &amp; \\Sigma_2 \\end{array}\\right] \\begin{gathered} \\end{gathered} . \\tag{3.16} \\] <p>Uma solu\u00e7\u00e3o de m\u00ednimos quadrados totais existe se e somente se \\(V_{22}\\) for n\u00e3o-singular. Al\u00e9m disso, \u00e9 \u00fanica se e somente se \\(\\sigma_n \\neq \\sigma_{n+1}\\). No caso em que a solu\u00e7\u00e3o de m\u00ednimos quadrados totais existe e \u00e9 \u00fanica, ela \u00e9 dada por</p> \\[ \\widehat{X}_{\\mathrm{tls}}=-V_{12} V_{22}^{-1} \\tag{3.17} \\] <p>e a matriz de corre\u00e7\u00e3o de m\u00ednimos quadrados totais correspondente \u00e9</p> \\[ \\Delta C_{\\mathrm{tls}}:=\\left[\\begin{array}{ll} \\Delta A_{\\mathrm{tls}} &amp; \\Delta B_{\\mathrm{tls}} \\end{array}\\right]=-U \\operatorname{diag}\\left(0, \\Sigma_2\\right) V^{\\top} . \\tag{3.18} \\] <p>Isso \u00e9 implementado no SysIdentPy da seguinte forma:</p> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using Total Least Squares method.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    \"\"\"\n    check_linear_dependence_rows(psi)\n    full = np.hstack((psi, y))\n    n = psi.shape[1]\n    _, _, v = np.linalg.svd(full, full_matrices=True)\n    theta = -v.T[:n, n:] / v.T[n:, n:]\n    return theta.reshape(-1, 1)\n</code></pre> <p>Para us\u00e1-lo na tarefa de modelagem, basta import\u00e1-lo como fizemos no exemplo de M\u00ednimos Quadrados.</p> <p>A partir de agora, os exemplos n\u00e3o incluir\u00e3o a etapa de Sele\u00e7\u00e3o de Estrutura de Modelo. O objetivo aqui \u00e9 focar nos m\u00e9todos de estima\u00e7\u00e3o de par\u00e2metros. No entanto, j\u00e1 fornecemos um exemplo incluindo MSS na se\u00e7\u00e3o de M\u00ednimos Quadrados, ent\u00e3o voc\u00ea n\u00e3o ter\u00e1 nenhum problema para testar isso com outros algoritmos de estima\u00e7\u00e3o de par\u00e2metros.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import TotalLeastSquares\nfrom sysidentpy.utils import build_lagged_matrix\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\ntheta = TotalLeastSquares().optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[ 1.3321e-04]\n [ 2.0014e-01]\n [-1.1771e-04]\n [ 5.8085e-05]\n [ 9.0011e-01]\n [-1.5490e-04]\n [-1.3517e-05]\n [ 9.9824e-02]\n [ 8.2326e-05]\n [-2.2814e-04]\n [-7.0837e-05]\n [-5.4319e-05]\n [-1.7472e-04]\n [-2.0396e-04]\n [ 1.7416e-05]]\n</code></pre>"},{"location":"pt/book/3-Parameter-Estimation/#minimos-quadrados-recursivos","title":"M\u00ednimos Quadrados Recursivos","text":"<p>Considere o modelo de regress\u00e3o</p> \\[ y_k = \\mathbf{\\Psi}_k^T \\theta_k + \\epsilon_k \\tag{3.19}\\] <p>onde: - \\(y_k\\) \u00e9 a sa\u00edda observada no tempo $ k $. - \\(\\mathbf{\\Psi}_k\\) \u00e9 a matriz de informa\u00e7\u00e3o no tempo \\(k\\). - \\(\\theta_k\\) \u00e9 o vetor de par\u00e2metros a ser estimado no tempo \\(k\\). - \\(\\epsilon_k\\) \u00e9 o ru\u00eddo no tempo \\(k\\).</p> <p>O algoritmo de M\u00ednimos Quadrados Recursivos (RLS) atualiza a estimativa do par\u00e2metro \\(\\theta_k\\) recursivamente \u00e0 medida que novos pontos de dados \\((\\mathbf{x}_k, y_k)\\) se tornam dispon\u00edveis, minimizando uma fun\u00e7\u00e3o de custo de m\u00ednimos quadrados lineares ponderados relacionada \u00e0 matriz de informa\u00e7\u00e3o de maneira sequencial. O RLS \u00e9 particularmente \u00fatil em aplica\u00e7\u00f5es em tempo real onde os dados chegam sequencialmente e o modelo precisa de atualiza\u00e7\u00e3o cont\u00ednua ou para modelar sistemas variantes no tempo (se o fator de esquecimento for inclu\u00eddo).</p> <p>Por ser uma estima\u00e7\u00e3o recursiva, \u00e9 \u00fatil relacionar \\(\\hat{\\Theta}_k\\) a \\(\\hat{\\Theta}_{k-1}\\). Em outras palavras, o novo \\(\\hat{\\Theta}_k\\) depende do \u00faltimo valor estimado (k). Al\u00e9m disso, para estimar \\(\\hat{\\Theta}_k\\), precisamos incorporar a informa\u00e7\u00e3o atual presente em \\(y_k\\).</p> <p>Aguirre define o estimador de M\u00ednimos Quadrados Recursivos com fator de esquecimento \\(\\lambda\\) como</p> \\[ \\left\\{\\begin{array}{c} K_k= Q_k\\psi_k = \\frac{P_{k-1} \\psi_k}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda} \\\\ \\hat{\\theta}_k=\\hat{\\theta}_{k-1}+K_k\\left[y(k)-\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1}\\right] \\\\ P_k=\\frac{1}{\\lambda}\\left(P_{k-1}-\\frac{P_{k-1} \\psi_k \\psi_k^{\\mathrm{T}} P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda}\\right) \\end{array}\\right. \\tag{3.20} \\] <p>onde \\(K_k\\) \u00e9 o c\u00e1lculo do vetor de ganho (tamb\u00e9m conhecido como ganho de Kalman), \\(P_k\\) \u00e9 a atualiza\u00e7\u00e3o da matriz de covari\u00e2ncia, e \\(y_k - \\mathbf{\\Psi}_k^T \\theta_{k-1}\\) \u00e9 o erro de estima\u00e7\u00e3o a priori. O fator de esquecimento \\(\\lambda\\) (\\(0 &lt; \\lambda \\leq 1\\)) \u00e9 geralmente definido entre \\(0.94\\) e \\(0.99\\). Se voc\u00ea definir \\(\\lambda = 1\\) voc\u00ea estar\u00e1 usando o algoritmo recursivo tradicional. A equa\u00e7\u00e3o acima considera que o vetor de regressores \\(\\psi(k-1)\\) foi reescrito como \\(\\psi_k\\), j\u00e1 que este vetor \u00e9 atualizado na itera\u00e7\u00e3o \\(k\\) e cont\u00e9m informa\u00e7\u00e3o at\u00e9 o instante de tempo \\(k-1\\). Podemos inicializar a estimativa do par\u00e2metro \\(\\theta_0\\) como</p> \\[ \\theta_0 = \\mathbf{0} \\tag{3.21}\\] <p>e inicializar a inversa da matriz de covari\u00e2ncia \\(\\mathbf{P}_0\\) com um valor grande:</p> \\[ \\mathbf{P}_0 = \\frac{\\mathbf{I}}{\\delta} \\tag{3.22}\\] <p>onde \\(\\delta\\) \u00e9 uma constante positiva pequena, e \\(\\mathbf{I}\\) \u00e9 a matriz identidade.</p> <p>O fator de esquecimento \\(\\lambda\\) controla qu\u00e3o rapidamente o algoritmo esquece dados passados: - \\(\\lambda = 1\\) significa sem esquecimento, e todos os dados passados s\u00e3o igualmente ponderados. - \\(\\lambda &lt; 1\\) significa que quando novos dados est\u00e3o dispon\u00edveis, todos os pesos s\u00e3o multiplicados por \\(\\lambda\\), o que pode ser interpretado como a raz\u00e3o entre pesos consecutivos para os mesmos dados.</p> <p>Voc\u00ea pode acessar o c\u00f3digo fonte para verificar como o SysIdentPy implementa o algoritmo RLS. O exemplo a seguir apresenta como voc\u00ea pode us\u00e1-lo no SysIdentPy.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.utils import build_lagged_matrix\nimport matplotlib.pyplot as plt\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n = 1000, colored_noise = False, sigma = 0.001, train_percentage = 90\n)\n\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\nestimator = RecursiveLeastSquares(lam=0.99)\ntheta = estimator.optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[-1.1778e-04]\n [ 1.9988e-01]\n [-9.3114e-05]\n [ 2.5119e-04]\n [ 9.0006e-01]\n [ 1.8339e-04]\n [-1.1943e-04]\n [ 9.9957e-02]\n [-4.6181e-05]\n [ 1.3155e-04]\n [ 3.4535e-04]\n [ 1.3843e-04]\n [-3.5454e-05]\n [ 1.5669e-04]\n [ 2.4311e-04]]\n</code></pre> <p>Voc\u00ea pode plotar a evolu\u00e7\u00e3o dos par\u00e2metros estimados ao longo do tempo acessando os valores de <code>theta_evolution</code> <pre><code># plotting only the first 50 values\nplt.plot(estimator.theta_evolution.T[:50, :])\nplt.xlabel(\"iterations\")\nplt.ylabel(\"theta\")\n</code></pre> </p> <p>Figura 1. Evolu\u00e7\u00e3o dos par\u00e2metros estimados ao longo do tempo usando o algoritmo RLS.</p>"},{"location":"pt/book/3-Parameter-Estimation/#least-mean-squares","title":"Least Mean Squares","text":"<p>O filtro adaptativo Least Mean Squares (LMS) \u00e9 um algoritmo de gradiente estoc\u00e1stico popular desenvolvido por Widrow e Hoff em 1960. O filtro adaptativo LMS visa alterar adaptativamente seus coeficientes de filtro para alcan\u00e7ar a melhor filtragem poss\u00edvel de um sinal. Isso \u00e9 feito minimizando o erro entre o sinal desejado \\(d(n)\\) e a sa\u00edda do filtro \\(y(n)\\). Podemos derivar o algoritmo LMS a partir da formula\u00e7\u00e3o RLS.</p> <p>No RLS, o \\(\\lambda\\) est\u00e1 relacionado \u00e0 minimiza\u00e7\u00e3o da soma dos quadrados ponderados da inova\u00e7\u00e3o</p> \\[ J_k = \\sum^k_{j=1}\\lambda^{k-j}e^2_j. \\tag{3.23} \\] <p>O \\(Q_k\\) na Equa\u00e7\u00e3o 3.20, definido como</p> \\[ Q_k = \\frac{P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda} \\\\ \\tag{3.24} \\] <p>\u00e9 derivado da forma geral do algoritmo do Filtro de Kalman (KF).</p> \\[ Q_k = \\frac{P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+v_0} \\\\ \\tag{3.25} \\] <p>onde \\(v_0\\) \u00e9 a vari\u00e2ncia do ru\u00eddo na defini\u00e7\u00e3o do KF, na qual a fun\u00e7\u00e3o de custo \u00e9 definida como a soma dos quadrados da inova\u00e7\u00e3o (ru\u00eddo). Voc\u00ea pode verificar os detalhes em Billings, S. A. - Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains.</p> <p>Se alterarmos \\(Q_k\\) na Equa\u00e7\u00e3o 3.25 para uma matriz identidade escalada</p> \\[ Q_k = \\frac{\\mu}{\\Vert \\psi_k \\Vert^2}I \\tag{3.26} \\] <p>onde \\(\\mu \\in \\mathbb{R}^+\\), o \\(Q_k\\) e \\(\\hat{\\theta}_k\\) na Equa\u00e7\u00e3o 3.20 se tornam</p> \\[ \\hat{\\theta}_k=\\hat{\\theta}_{k-1}+\\frac{\\mu\\left[y(k)-\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1}\\right]}{\\Vert \\psi_k \\Vert^2}\\psi_k \\tag{3.27} \\] <p>onde \\(\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1} = \\hat{y}_k\\), que \u00e9 conhecido como algoritmo LMS.</p>"},{"location":"pt/book/3-Parameter-Estimation/#convergencia-e-tamanho-do-passo","title":"Converg\u00eancia e Tamanho do Passo","text":"<p>O par\u00e2metro de tamanho do passo \\(\\mu\\) desempenha um papel crucial no desempenho do algoritmo LMS. Se \\(\\mu\\) for muito grande, o algoritmo pode se tornar inst\u00e1vel e falhar em convergir. Se \\(\\mu\\) for muito pequeno, o algoritmo convergir\u00e1 lentamente. A escolha de \\(\\mu\\) \u00e9 tipicamente:</p> \\[ 0 &lt; \\mu &lt; \\frac{2}{\\lambda_{\\max}} \\tag{3.28} \\] <p>onde \\(\\lambda_{\\max}\\) \u00e9 o maior autovalor da matriz de autocorrela\u00e7\u00e3o do sinal de entrada.</p> <p>No SysIdentPy, voc\u00ea pode usar v\u00e1rias variantes do algoritmo LMS:</p> <ol> <li>LeastMeanSquareMixedNorm</li> <li>LeastMeanSquares</li> <li>LeastMeanSquaresFourth</li> <li>LeastMeanSquaresLeaky</li> <li>LeastMeanSquaresNormalizedLeaky</li> <li>LeastMeanSquaresNormalizedSignRegressor</li> <li>LeastMeanSquaresNormalizedSignSign</li> <li>LeastMeanSquaresSignError</li> <li>LeastMeanSquaresSignSign</li> <li>AffineLeastMeanSquares</li> <li>NormalizedLeastMeanSquares</li> <li>NormalizedLeastMeanSquaresSignError</li> <li>LeastMeanSquaresSignRegressor</li> </ol> <p>Para usar qualquer um dos m\u00e9todos acima, voc\u00ea s\u00f3 precisa import\u00e1-lo e definir o <code>estimator</code> usando a op\u00e7\u00e3o desejada:</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastMeanSquares\nfrom sysidentpy.utils import build_lagged_matrix\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n = 1000, colored_noise = False, sigma = 0.001, train_percentage = 90\n)\n\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\nestimator = LeastMeanSquares(mu=0.1)\ntheta = estimator.optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[ 1.5924e-04]\n [ 1.9950e-01]\n [ 3.2137e-04]\n [ 1.7824e-04]\n [ 8.9951e-01]\n [ 2.7314e-04]\n [ 3.3538e-04]\n [ 1.0062e-01]\n [ 3.5219e-04]\n [ 1.3544e-04]\n [ 3.4149e-04]\n [ 5.6315e-04]\n [-4.6664e-04]\n [ 2.2849e-04]\n [ 1.0536e-04]]\n</code></pre>"},{"location":"pt/book/3-Parameter-Estimation/#algoritmo-extended-least-squares","title":"Algoritmo Extended Least Squares","text":"<p>Vamos mostrar um exemplo do efeito de uma estima\u00e7\u00e3o de par\u00e2metros viesada. Para simplificar, os dados s\u00e3o gerados simulando o seguinte modelo:</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k} \\] <p>Neste caso, conhecemos os valores dos par\u00e2metros verdadeiros, ent\u00e3o ser\u00e1 mais f\u00e1cil entender como eles s\u00e3o afetados por uma estima\u00e7\u00e3o viesada. Os dados s\u00e3o gerados usando um m\u00e9todo do SysIdentPy. Se colored_noise for definido como True no m\u00e9todo, um ru\u00eddo colorido \u00e9 adicionado aos dados:</p> \\[e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\] <p>onde \\(x\\) \u00e9 uma vari\u00e1vel aleat\u00f3ria uniformemente distribu\u00edda e \\(\\nu\\) \u00e9 uma vari\u00e1vel com distribui\u00e7\u00e3o gaussiana com \\(\\mu=0\\) e \\(\\sigma\\) \u00e9 definido pelo usu\u00e1rio.</p> <p>Vamos gerar dados com 1000 amostras com ru\u00eddo branco e selecionar 90% dos dados para treinar o modelo.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n</code></pre> <p>Primeiro, vamos treinar um modelo sem o Algoritmo Extended Least Squares para fins de compara\u00e7\u00e3o.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=False)\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR x1(k-2) 9.0442E-01 7.55518391E-01 y(k-1) 2.7405E-01 7.57565084E-02 x1(k-1)y(k-1) 9.8757E-02 3.12896171E-03 <p>Claramente temos algo errado com o modelo obtido. Os par\u00e2metros estimados diferem dos verdadeiros definidos na equa\u00e7\u00e3o que gerou os dados. Como podemos observar acima, a estrutura do modelo \u00e9 exatamente a mesma que gerou os dados. Voc\u00ea pode ver que o ERR ordenou os termos da maneira correta. E esta \u00e9 uma nota importante sobre o algoritmo ERR: ele \u00e9 muito robusto ao ru\u00eddo colorido!!</p> <p>Essa \u00e9 uma \u00f3tima caracter\u00edstica! No entanto, embora a estrutura esteja correta, os par\u00e2metros do modelo n\u00e3o est\u00e3o corretos! Aqui temos uma estima\u00e7\u00e3o viesada! Por exemplo, o par\u00e2metro real para \\(y_{k-1}\\) \u00e9 \\(0.2\\), n\u00e3o \\(0.274\\).</p> <p>Neste caso, estamos na verdade modelando usando um modelo NARX, n\u00e3o um NARMAX. A parte MA existe para permitir uma estima\u00e7\u00e3o n\u00e3o-viesada dos par\u00e2metros. Para alcan\u00e7ar uma estima\u00e7\u00e3o n\u00e3o-viesada dos par\u00e2metros, temos o algoritmo Extended Least Squares.</p> <p>Antes de aplicar o Algoritmo Extended Least Squares, vamos executar v\u00e1rios modelos NARX para verificar qu\u00e3o diferentes os par\u00e2metros estimados s\u00e3o dos reais.</p> <pre><code>parameters = np.zeros([3, 50])\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\n# Set the theme for seaborn (optional)\nsns.set_theme()\nplt.figure(figsize=(14, 4))\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label='Parameter 1')\nsns.kdeplot(parameters.T[:, 1], label='Parameter 2')\nsns.kdeplot(parameters.T[:, 2], label='Parameter 3')\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color='k', linestyle='--', label='Real Value 0.1')\nplt.axvline(x=0.2, color='k', linestyle='--', label='Real Value 0.2')\nplt.axvline(x=0.9, color='k', linestyle='--', label='Real Value 0.9')\nplt.xlabel('Parameter Value')\nplt.ylabel('Density')\nplt.title('Kernel Density Estimate of Parameters')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Figura 2.: Estimativas de Densidade de Kernel (KDEs) dos par\u00e2metros estimados obtidos de 50 realiza\u00e7\u00f5es de modelos NARX, cada um ajustado a dados com ru\u00eddo colorido. As linhas tracejadas verticais indicam os valores verdadeiros dos par\u00e2metros usados para gerar os dados. Embora a estrutura do modelo seja identificada corretamente, os par\u00e2metros estimados s\u00e3o viesados devido \u00e0 omiss\u00e3o do componente de M\u00e9dia M\u00f3vel (MA), destacando a necessidade do algoritmo Extended Least Squares para alcan\u00e7ar uma estima\u00e7\u00e3o de par\u00e2metros n\u00e3o-viesada.</p> <p>Como mostrado na figura acima, temos um problema para estimar o par\u00e2metro para \\(y_{k-1}\\). Agora usaremos o Algoritmo Extended Least Squares. No SysIdentPy, basta definir <code>unbiased=True</code> na defini\u00e7\u00e3o da estima\u00e7\u00e3o de par\u00e2metros e o algoritmo ELS ser\u00e1 aplicado.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n    model = FROLS(\n        order_selection=False,\n        n_terms=3,\n        ylag=2,\n        xlag=2,\n        elag=2,\n        info_criteria=\"aic\",\n        estimator=estimator,\n        basis_function=basis_function,\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\nplt.figure(figsize=(14, 4))\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label='Parameter 1')\nsns.kdeplot(parameters.T[:, 1], label='Parameter 2')\nsns.kdeplot(parameters.T[:, 2], label='Parameter 3')\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color='k', linestyle='--', label='Real Value 0.1')\nplt.axvline(x=0.2, color='k', linestyle='--', label='Real Value 0.2')\nplt.axvline(x=0.9, color='k', linestyle='--', label='Real Value 0.9')\nplt.xlabel('Parameter Value')\nplt.ylabel('Density')\nplt.title('Kernel Density Estimate of Parameters')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Figura 3. Estimativas de Densidade de Kernel (KDEs) dos par\u00e2metros estimados obtidos de 50 modelos NARX usando o algoritmo Extended Least Squares (ELS) com estima\u00e7\u00e3o n\u00e3o-viesada. As linhas tracejadas verticais indicam os valores verdadeiros dos par\u00e2metros usados para gerar os dados.</p> <p>Diferente da estima\u00e7\u00e3o viesada anterior, estas KDEs na Figura 3 mostram que os par\u00e2metros estimados est\u00e3o agora intimamente alinhados com os valores verdadeiros, demonstrando a efic\u00e1cia do algoritmo ELS em alcan\u00e7ar estima\u00e7\u00e3o de par\u00e2metros n\u00e3o-viesada, mesmo na presen\u00e7a de ru\u00eddo colorido.</p> <p>O algoritmo Extended Least Squares \u00e9 iterativo por natureza. No SysIdentPy, o n\u00famero padr\u00e3o de itera\u00e7\u00f5es \u00e9 definido como 30 (<code>uiter=30</code>). No entanto, a literatura sugere que o algoritmo tipicamente converge rapidamente, frequentemente dentro de 10 a 20 itera\u00e7\u00f5es. Portanto, voc\u00ea pode querer testar diferentes n\u00fameros de itera\u00e7\u00f5es para encontrar o equil\u00edbrio ideal entre velocidade de converg\u00eancia e efici\u00eancia computacional.</p>"},{"location":"pt/book/4-Model-Structure-Selection/","title":"4. Sele\u00e7\u00e3o de Estrutura de Modelo","text":""},{"location":"pt/book/4-Model-Structure-Selection/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Esta se\u00e7\u00e3o \u00e9 baseada, em grande parte, na minha disserta\u00e7\u00e3o de mestrado, que por sua vez se baseia em Billings, S. A.</p> <p>Selecionar a estrutura do modelo \u00e9 um passo crucial para desenvolver modelos capazes de reproduzir corretamente o comportamento de um sistema. Se algumas informa\u00e7\u00f5es pr\u00e9vias sobre o sistema forem conhecidas, por exemplo, a ordem din\u00e2mica e o grau de n\u00e3o linearidade, determinar os termos e ent\u00e3o estimar os par\u00e2metros torna-se uma tarefa trivial. Em cen\u00e1rios reais, entretanto, geralmente n\u00e3o h\u00e1 informa\u00e7\u00e3o sobre quais termos devem ser inclu\u00eddos no modelo e os regressores corretos precisam ser selecionados no contexto de identifica\u00e7\u00e3o. Se o processo de Model Structure Selection (MSS) n\u00e3o for realizado com o devido cuidado, a lei cient\u00edfica que descreve o sistema pode n\u00e3o ser revelada, resultando em interpreta\u00e7\u00f5es equivocadas sobre o comportamento do sistema. Para ilustrar esse cen\u00e1rio, considere o seguinte exemplo.</p> <p>Seja \\(\\mathcal{D}\\) um conjunto de dados arbitr\u00e1rio</p> \\[ \\begin{equation}     \\mathcal{D} = \\{(x_k, y_k), k = 1, 2, \\dotsc, n\\}, \\end{equation} \\] <p>em que \\(x_k \\in \\mathbb{R}^{n_x}\\) e \\(y_k\\in \\mathbb{R}^{n_y}\\) s\u00e3o, respectivamente, a entrada e a sa\u00edda de um sistema desconhecido, e \\(n\\) \u00e9 o n\u00famero de amostras no conjunto de dados. A seguir, considere dois modelos polinomiais NARX constru\u00eddos para descrever esse sistema:</p> \\[ \\begin{align}     y_{ak} &amp;= 0.7077y_{ak-1} + 0.1642u_{k-1} + 0.1280u_{k-2} \\end{align} \\tag{2} \\] \\[ \\begin{align}     y_{bk} &amp;= 0.7103y_{bk-1} + 0.1458u_{k-1} + 0.1631u_{k-2} \\\\            &amp;\\quad - 1467y_{bk-1}^3 + 0.0710y_{bk-2}^3 + 0.0554y_{bk-3}^2u_{k-3}. \\end{align} \\tag{3} \\] <p>A Figura 1 mostra os valores previstos por cada modelo e os dados reais. Como pode ser observado, o modelo n\u00e3o linear 2 parece ajustar melhor os dados do que o modelo linear 1. O sistema original considerado \u00e9 um circuito RLC, composto por um resistor (R), um indutor (L) e um capacitor (C) conectados em s\u00e9rie com uma fonte de tens\u00e3o. Sabe\u2011se que o comportamento de um circuito RLC em s\u00e9rie pode ser descrito com precis\u00e3o por uma equa\u00e7\u00e3o diferencial linear de segunda ordem que relaciona a corrente \\(I(t)\\) e a tens\u00e3o aplicada \\(V(t)\\):</p> \\[ L\\frac{d^2I(t)}{dt^2} + R\\frac{dI(t)}{dt} + \\frac{1}{C}I(t) = \\frac{dV(t)}{dt} \\tag{4} \\] <p>Dada essa rela\u00e7\u00e3o linear, um modelo adequado para o circuito RLC deve refletir essa linearidade de segunda ordem. Embora o Modelo 2, que inclui termos n\u00e3o lineares, possa fornecer um ajuste mais pr\u00f3ximo aos dados, ele \u00e9 claramente superparametrizado. Tal superparametriza\u00e7\u00e3o pode introduzir efeitos n\u00e3o lineares esp\u00farios, frequentemente chamados de \"ghost nonlinearities\", que n\u00e3o correspondem \u00e0 din\u00e2mica real do sistema. Portanto, esses modelos precisam ser interpretados com cautela, pois o uso de um modelo excessivamente complexo pode mascarar a verdadeira natureza linear do sistema e levar a conclus\u00f5es equivocadas sobre o seu comportamento.</p> <p></p> <p>Figura 1. Resultados para dois modelos polinomiais NARX ajustados a dados de um sistema desconhecido. O Modelo 1 (esquerda) \u00e9 linear, enquanto o Modelo 2 (direita) inclui termos n\u00e3o lineares. A figura ilustra que o Modelo 2 fornece um ajuste mais pr\u00f3ximo aos dados em compara\u00e7\u00e3o com o Modelo 1. No entanto, como o sistema original \u00e9 um circuito RLC linear de segunda ordem, o melhor ajuste do Modelo 2 pode ser enganoso devido \u00e0 superparametriza\u00e7\u00e3o. Isso destaca a import\u00e2ncia de considerar as caracter\u00edsticas f\u00edsicas do sistema ao interpretar os resultados, para evitar interpretar como reais n\u00e3o linearidades artificiais. Refer\u00eancia: Meta Model Structure Selection: An Algorithm For Building Polynomial NARX Models For Regression And Classification</p> <p>Identificar corretamente a estrutura de um modelo \u00e9 fundamental para analisar com precis\u00e3o a din\u00e2mica do sistema. Uma estrutura de modelo bem escolhida garante que o modelo reflita o comportamento real do sistema, permitindo uma an\u00e1lise consistente e significativa. Nesse sentido, diversos algoritmos foram desenvolvidos para selecionar os termos apropriados na constru\u00e7\u00e3o de um modelo polinomial NARX. O objetivo principal dos algoritmos de Model Structure Selection (MSS) \u00e9 revelar as caracter\u00edsticas do sistema produzindo o modelo mais simples poss\u00edvel que ainda descreva adequadamente os dados. Embora alguns sistemas exijam, de fato, modelos mais complexos, \u00e9 essencial buscar um equil\u00edbrio entre simplicidade e acur\u00e1cia. Como Einstein disse de forma bastante apropriada:</p> <p>Um modelo deve ser o mais simples poss\u00edvel, mas n\u00e3o mais simples do que isso.</p> <p>Esse princ\u00edpio enfatiza a import\u00e2ncia de evitar complexidade desnecess\u00e1ria, garantindo ao mesmo tempo que o modelo capture as din\u00e2micas essenciais do sistema.</p> <p>Vimos no Cap\u00edtulo 2 que a sele\u00e7\u00e3o de regressores n\u00e3o \u00e9 uma tarefa simples. Se o grau de n\u00e3o linearidade, a ordem do modelo e o n\u00famero de entradas aumentam, o n\u00famero de modelos candidatos se torna grande demais para uma abordagem de for\u00e7a bruta. Considerando o caso MIMO, esse problema \u00e9 ainda mais severo do que no caso SISO quando muitas entradas e sa\u00eddas s\u00e3o necess\u00e1rias. O n\u00famero de todos os modelos distintos pode ser calculado como</p> \\[ \\begin{align}     n_m =     \\begin{cases}     2^{n_r} &amp; \\text{para modelos SISO}, \\\\     2^{n_{{_{m}}r}} &amp; \\text{para modelos MIMO},     \\end{cases} \\end{align} \\tag{5} \\] <p>em que \\(n_r\\) e \\(n_{{_{m}}r}\\) s\u00e3o os valores calculados usando as equa\u00e7\u00f5es apresentadas no Cap\u00edtulo 2.</p> <p>Uma solu\u00e7\u00e3o cl\u00e1ssica para o problema de sele\u00e7\u00e3o de regressores \u00e9 o algoritmo Forward Regression Orthogonal Least Squares (FROLS) associado ao algoritmo Error Reduction Ratio (ERR). Essa t\u00e9cnica \u00e9 baseada no framework de Prediction Error Minimization e seleciona, termo a termo, o regressor mais relevante por meio de uma regress\u00e3o step\u2011wise. O m\u00e9todo FROLS adapta o conjunto de regressores no espa\u00e7o de busca para um conjunto de vetores ortogonais, sobre os quais o ERR avalia a contribui\u00e7\u00e3o individual para a vari\u00e2ncia da sa\u00edda desejada.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#o-algoritmo-forward-regression-orthogonal-least-squares","title":"O algoritmo Forward Regression Orthogonal Least Squares","text":"<p>Considere o modelo NARMAX geral definido na Equa\u00e7\u00e3o 2.23, descrito de forma gen\u00e9rica como</p> \\[ \\begin{equation}     y_k = \\psi^\\top_{k-1}\\hat{\\Theta} + \\xi_k, \\end{equation} \\tag{6} \\] <p>em que \\(\\psi^\\top_{k-1} \\in \\mathbb{R}^{n_r \\times n}\\) \u00e9 um vetor contendo combina\u00e7\u00f5es dos regressores e \\(\\hat{\\Theta} \\in \\mathbb{R}^{n_{\\Theta}}\\) \u00e9 o vetor de par\u00e2metros estimados. De forma mais compacta, o modelo NARMAX pode ser representado em forma matricial como:</p> \\[ \\begin{equation}     y = \\Psi\\hat{\\Theta} + \\Xi, \\end{equation} \\tag{7} \\] <p>onde</p> \\[ \\begin{align}     Y = \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_n     \\end{bmatrix},     \\Psi = \\begin{bmatrix}     \\psi_{{_1}} \\\\     \\psi_{{_2}} \\\\     \\vdots \\\\     \\psi_{{_{n_{\\Theta}}}}     \\end{bmatrix}^\\top=     \\begin{bmatrix}     \\psi_{{_1}1} &amp; \\psi_{{_2}1} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}1} \\\\     \\psi_{{_1}2} &amp; \\psi_{{_2}2} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}2} \\\\     \\vdots &amp; \\vdots &amp;       &amp; \\vdots \\\\     \\psi_{{_1}n} &amp; \\psi_{{_2}n} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}n} \\\\     \\end{bmatrix},     \\hat{\\Theta} = \\begin{bmatrix}     \\hat{\\Theta}_1 \\\\     \\hat{\\Theta}_2 \\\\     \\vdots \\\\     \\hat{\\Theta}_{n_\\Theta}     \\end{bmatrix},     \\Xi = \\begin{bmatrix}     \\xi_1 \\\\     \\xi_2 \\\\     \\vdots \\\\     \\xi_n     \\end{bmatrix}. \\end{align} \\tag{8} \\] <p>Os par\u00e2metros na equa\u00e7\u00e3o acima podem ser estimados por um algoritmo baseado em M\u00ednimos Quadrados, mas isso exigiria otimizar todos os par\u00e2metros ao mesmo tempo devido \u00e0 intera\u00e7\u00e3o entre regressores causada pela falta de ortogonalidade. Consequentemente, a demanda computacional torna\u2011se impratic\u00e1vel para um n\u00famero elevado de regressores. Nesse contexto, o FROLS transforma o modelo n\u00e3o ortogonal apresentado na equa\u00e7\u00e3o acima em um modelo ortogonal.</p> <p>A matriz de regressores \\(\\Psi\\) pode ser decomposta ortogonalmente como</p> \\[ \\begin{equation}     \\Psi = QA, \\end{equation} \\tag{9} \\] <p>em que \\(A \\in \\mathbb{R}^{n_{\\Theta}\\times n_{\\Theta}}\\) \u00e9 uma matriz triangular superior unit\u00e1ria, dada por</p> \\[ \\begin{align} A =     \\begin{bmatrix}     1       &amp; a_{12} &amp; a_{13} &amp; \\dotsc &amp; a_{1n_{\\Theta}} \\\\     0       &amp;   1    &amp; a_{23} &amp; \\dotsc &amp; a_{2n_{\\Theta}} \\\\     0       &amp;   0    &amp;   1    &amp; \\dotsc &amp;     \\vdots       \\\\     \\vdots  &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; a_{n_{\\Theta}-1n_{\\Theta}} \\\\     0       &amp;  0     &amp;  0     &amp;  0     &amp; 1     \\end{bmatrix}, \\end{align} \\tag{10} \\] <p>e \\(Q \\in \\mathbb{R}^{n\\times n_{\\Theta}}\\) \u00e9 uma matriz com colunas ortogonais \\(q_i\\), descrita como</p> \\[ \\begin{equation}     Q =         \\begin{bmatrix}         q_{{_1}} &amp; q_{{_2}} &amp; q_{{_3}} &amp; \\dotsc &amp; q_{{_{n_{\\Theta}}}}         \\end{bmatrix}, \\end{equation} \\tag{11} \\] <p>tal que \\(Q^\\top Q = \\Lambda\\) e \\(\\Lambda\\) \u00e9 diagonal com entradas \\(d_i\\), que podem ser expressas como</p> \\[ \\begin{align}     d_i = q_i^\\top q_i = \\sum^{k=1}_{n}q_{{_i}k}q_{{_i}k}, \\qquad 1\\leq i \\leq n_{\\Theta}. \\end{align} \\] <p>Como o subespa\u00e7o gerado pela base ortogonal \\(Q\\) (Equa\u00e7\u00e3o 11) \u00e9 o mesmo subespa\u00e7o gerado pelo conjunto de base \\(\\Psi\\) (Equa\u00e7\u00e3o 8) (isto \u00e9, cont\u00e9m todas as combina\u00e7\u00f5es lineares poss\u00edveis nesse subespa\u00e7o), podemos reescrever a Equa\u00e7\u00e3o 7 como</p> \\[ \\begin{equation}     Y = \\underbrace{(\\Psi A^{-1})}_{Q}\\underbrace{(A\\Theta)}_{g}+ \\Xi = Qg+\\Xi, \\end{equation} \\tag{12} \\] <p>onde \\(g\\in \\mathbb{R}^{n_\\Theta}\\) \u00e9 um vetor auxiliar de par\u00e2metros. A solu\u00e7\u00e3o do modelo descrito na Equa\u00e7\u00e3o 12 \u00e9 dada por</p> \\[ \\begin{equation}     g = \\left(Q^\\top Q\\right)^{-1}Q^\\top Y = \\Lambda^{-1}Q^\\top Y \\end{equation} \\tag{13} \\] <p>ou, de forma equivalente,</p> \\[ \\begin{equation}     g_{{_i}} = \\frac{q_{{_i}}^\\top Y}{q_{{_i}}^\\top q_{{_i}}}. \\end{equation} \\tag{14} \\] <p>Como os vetores de par\u00e2metros \\(\\Theta\\) e \\(g\\) satisfazem o sistema triangular \\(A\\Theta = g\\), qualquer m\u00e9todo de ortogonaliza\u00e7\u00e3o, como Householder, Gram\u2011Schmidt, Gram\u2011Schmidt modificado ou transforma\u00e7\u00f5es de Givens, pode ser utilizado para resolver a equa\u00e7\u00e3o e estimar os par\u00e2metros originais. Assumindo que \\(E[\\Psi^\\top \\Xi] = 0\\), a vari\u00e2ncia da sa\u00edda pode ser obtida multiplicando a Equa\u00e7\u00e3o 12 por ela mesma e dividindo por \\(n\\), o que resulta em</p> \\[ \\begin{equation}     \\frac{1}{n}Y^\\top Y = \\underbrace{\\frac{1}{n}\\sum^{i = 1}_{n_{\\Theta}}g_{{_i}}^2q^\\top_{{_i}}q_{{_i}}}_{\\text{{vari\u00e2ncia explicada pelos regressores}}} + \\underbrace{\\frac{1}{n}\\Xi^\\top \\Xi}_{\\text{{vari\u00e2ncia n\u00e3o explicada}}}. \\end{equation} \\tag{15} \\] <p>Assim, o ERR associado \u00e0 inclus\u00e3o do regressor \\(q_{{_i}}\\) pode ser expresso como</p> \\[ [\\text{ERR}]_i = \\frac{g_{i}^2 \\cdot q_{i}^\\top q_{i}}{Y^\\top Y}, \\qquad \\text{para } i=1,2,\\dotsc, n_\\Theta. \\] <p>Existem diversas formas de encerrar o algoritmo. Uma abordagem frequentemente utilizada \u00e9 parar quando a vari\u00e2ncia da sa\u00edda n\u00e3o explicada pelo modelo cai abaixo de um limite pr\u00e9\u2011definido \\(\\varepsilon\\):</p> \\[ \\begin{equation}     1 - \\sum_{i = 1}^{n_{\\Theta}}\\text{ERR}_i \\leq \\varepsilon, \\end{equation} \\tag{17} \\]"},{"location":"pt/book/4-Model-Structure-Selection/#mantendo-as-coisas-simples","title":"Mantendo as coisas simples","text":"<p>Para fins did\u00e1ticos, vamos apresentar o FROLS usando exemplos simples, de forma a tornar a intui\u00e7\u00e3o por tr\u00e1s do m\u00e9todo mais clara. Primeiro, definimos o c\u00e1lculo do ERR e, em seguida, explicamos a ideia do FROLS em termos simples.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#caso-ortogonal","title":"Caso ortogonal","text":"<p>Considere o caso em que temos um conjunto de entradas \\(x_1, x_2, \\ldots, x_n\\) e uma sa\u00edda \\(y\\). Suponha que esses vetores de entrada sejam ortogonais entre si.</p> <p>Suponha que queremos construir um modelo para aproximar \\(y\\) usando \\(x_1, x_2, \\ldots, x_n\\) da seguinte forma:</p> \\[ y=\\hat{\\theta}_1 x_1+\\hat{\\theta}_2 x_2+\\ldots+\\hat{\\theta}_n x_n+e \\tag{18} \\] <p>onde \\(\\hat{\\theta}_1, \\hat{\\theta}_2, \\ldots, \\hat{\\theta}_n\\) s\u00e3o par\u00e2metros e \\(e\\) \u00e9 um ru\u00eddo branco, independente de \\(x\\) e \\(y\\) (note a hip\u00f3tese \\(E[\\Psi^\\top \\Xi] = 0\\) mencionada na se\u00e7\u00e3o anterior). Nesse caso, podemos reescrever a equa\u00e7\u00e3o acima como</p> \\[ y = \\hat{\\theta} x \\tag{19} \\] <p>de modo que</p> \\[ \\left\\langle x, y\\right\\rangle = \\left\\langle \\hat{\\theta} x, x\\right\\rangle = \\hat{\\theta} \\left\\langle x, x\\right\\rangle \\tag{20} \\] <p>o que implica</p> \\[ \\hat{\\theta} = \\frac{\\left\\langle x, y\\right\\rangle}{\\left\\langle x, x\\right\\rangle} \\tag{21} \\] <p>Portanto, podemos mostrar que</p> \\[ \\begin{align} &amp; \\left\\langle x_1, y\\right\\rangle=\\hat{\\theta}_1\\left\\langle x_1, x_1\\right\\rangle \\Rightarrow \\hat{\\theta}_1=\\frac{\\left\\langle x_1, y\\right\\rangle}{\\left\\langle x_1, x_1\\right\\rangle}=\\frac{x_1^T y}{x_1^T x_1} \\\\ &amp; \\left\\langle x_2, y\\right\\rangle=\\hat{\\theta}_2\\left\\langle x_2, x_2\\right\\rangle \\Rightarrow \\hat{\\theta}_2=\\frac{\\left\\langle x_2, y\\right\\rangle}{\\left\\langle x_2, x_2\\right\\rangle}=\\frac{x_2^T y}{x_2^T x_2}, \\ldots \\\\ &amp; \\left\\langle x_n, y\\right\\rangle=\\hat{\\theta}_n\\left\\langle x_n, x_n\\right\\rangle \\Rightarrow \\hat{\\theta}_n=\\frac{\\left\\langle x_n, y\\right\\rangle}{\\left\\langle x_n, x_n\\right\\rangle}=\\frac{x_n^T y}{x_n^T x_n}, \\end{align} \\tag{22} \\] <p>Seguindo a mesma ideia, tamb\u00e9m podemos mostrar que</p> \\[ \\langle y, y\\rangle=\\hat{\\theta}_1^2\\left\\langle x_1, x_1\\right\\rangle+\\hat{\\theta}_2^2\\left\\langle x_2, x_2\\right\\rangle+\\ldots+\\hat{\\theta}_n^2\\left\\langle x_n, x_n\\right\\rangle+\\langle e, e\\rangle \\tag{23} \\] <p>que pode ser descrito como</p> \\[ y^T y=\\hat{\\theta}_1^2 x_1^T x_1+\\hat{\\theta}_2^2 x_2^T x_2+\\ldots+\\hat{\\theta}_n^2 x_n^T x_n+e^T e \\tag{24} \\] <p>ou ainda</p> \\[ \\|y\\|^2=\\hat{\\theta}_1^2\\left\\|x_1\\right\\|^2+\\hat{\\theta}_2^2\\left\\|x_2\\right\\|^2+\\ldots+\\hat{\\theta}_n^2\\left\\|x_n\\right\\|^2+\\|e\\|^2 \\tag{25} \\] <p>Dividindo ambos os lados da equa\u00e7\u00e3o por \\(\\|y\\|^2\\) e rearranjando, obtemos</p> \\[ \\frac{\\|e\\|^2}{\\|y\\|^2}=1-\\hat{\\theta}_1^2 \\frac{\\left\\|x_1\\right\\|^2}{\\|y\\|^2}-\\hat{\\theta}_2^2 \\frac{\\left\\|x_2\\right\\|^2}{\\|y\\|^2}-\\ldots-\\hat{\\theta}_n^2 \\frac{\\left\\|x_n\\right\\|^2}{\\|y\\|^2} \\tag{26} \\] <p>Como \\(\\hat{\\theta}_k=\\frac{x_k^T y}{x_k^T x_k}=\\frac{x_k^T y}{\\left\\|x_k\\right\\|^2}, k=1,2, . ., n\\), temos</p> \\[ \\begin{align} \\frac{\\|e\\|^2}{\\|y\\|^2} &amp; =1-\\left(\\frac{x_1^T y}{\\left\\|x_1\\right\\|^2}\\right)^2 \\frac{\\left\\|x_1\\right\\|^2}{\\|y\\|^2}-\\left(\\frac{x_2^T y}{\\left\\|x_2\\right\\|^2}\\right)^2 \\frac{\\left\\|x_2\\right\\|^2}{\\|y\\|^2}-\\ldots-\\left(\\frac{x_n^T y}{\\left\\|x_n\\right\\|^2}\\right)^2 \\frac{\\left\\|x_n\\right\\|^2}{\\|y\\|^2} \\\\ &amp; =1-\\frac{\\left(x_1^T y\\right)^2}{\\left\\|x_1\\right\\|^2\\| y \\|^2}-\\frac{\\left(x_2^T y\\right)^2}{\\left\\|x_2\\right\\|^2\\|y\\|^2}-\\cdots-\\frac{\\left(x_n^T y\\right)^2}{\\left\\|x_n\\right\\|^2\\|y\\|^2} \\\\ &amp; =1-ERR_1 \\quad-ERR_2-\\cdots-ERR_n \\end{align} \\tag{27} \\] <p>onde \\(\\operatorname{ERR}_k(k=1,2 \\ldots, n)\\) \u00e9 o Error Reduction Ratio definido na se\u00e7\u00e3o anterior.</p> <p>Veja o exemplo abaixo usando a base can\u00f4nica:</p> <pre><code>import numpy as np\n\ny = np.array([3, 7, 8])\n# Orthogonal Basis\nx1 = np.array([1, 0, 0])\nx2 = np.array([0, 1, 0])\nx3 = np.array([0, 0, 1])\n\ntheta1 = (x1.T@y)/(x1.T@x1)\ntheta2 = (x2.T@y)/(x2.T@x2)\ntheta3 = (x3.T@y)/(x3.T@x3)\n\nsquared_y = y.T @ y\nerr1 = (x1.T@y)**2/((x1.T@x1) * squared_y)\nerr2 = (x2.T@y)**2/((x2.T@x2) * squared_y)\nerr3 = (x3.T@y)**2/((x3.T@x3) * squared_y)\n\nprint(f\"x1 represents {round(err1*100, 2)}% of the variation in y, \\n x2 represents {round(err2*100, 2)}% of the variation in y, \\n x3 represents {round(err3*100, 2)}% of the variation in y\")\n\nx1 represents 7.38% of the variation in y,\nx2 represents 40.16% of the variation in y,\nx3 represents 52.46% of the variation in y\n</code></pre> <p>Vamos agora analisar o que acontece em um cen\u00e1rio n\u00e3o ortogonal.</p> <pre><code>y = np.array([3, 7, 8])\nx1 = np.array([1, 2, 2])\nx2 = np.array([-1, 0, 2])\nx3 = np.array([0, 0, 1])\n\ntheta1 = (x1.T@y)/(x1.T@x1)\ntheta2 = (x2.T@y)/(x2.T@x2)\ntheta3 = (x3.T@y)/(x3.T@x3)\n\nsquared_y = y.T @ y\nerr1 = (x1.T@y)**2/((x1.T@x1) * squared_y)\nerr2 = (x2.T@y)/((x2.T@x2) * squared_y)\nerr3 = (x3.T@y)**2/((x3.T@x3) * squared_y)\n\nprint(f\"x1 represents {round(err1*100, 2)}% of the variation in y, \\n x2 represents {round(err2*100, 2)}% of the variation in y, \\n x3 represents {round(err3*100, 2)}% of the variation in y\")\n\n&gt;&gt;&gt; x1 represents 99.18% of the variation in y,\n&gt;&gt;&gt; x2 represents 2.13% of the variation in y,\n&gt;&gt;&gt; x3 represents 52.46% of the variation in y\n</code></pre> <p>Nesse caso, \\(x1\\) apresenta o maior valor de \\(err\\), ent\u00e3o o escolhemos como o primeiro vetor ortogonal.</p> <pre><code>q1 = x1.copy()\n\nv1 = x2 - (q1.T@x2)/(q1.T@q1)*q1\nerrv1 = (v1.T@y)**2/((v1.T@v1) * squared_y)\n\nv2 = x3 - (q1.T@x3)/(q1.T@q1)*q1\nerrv2 = (v2.T@y)**2/((v2.T@v2) * squared_y)\n\nprint(f\"v1 represents {round(errv1*100, 2)}% of the variation in y, \\n v2 represents {round(errv2*100, 2)}% of the variation in y\")\n\n&gt;&gt;&gt; v1 represents 0.82% of the variation in y,\n&gt;&gt;&gt; v2 represents 0.66% of the variation in y\n</code></pre> <p>Assim, neste caso, ao somarmos os valores de ERR dos dois primeiros vetores ortogonais, \\(x1\\) e \\(v1\\), obtemos \\(err_3 + errv1 = 100\\%\\). N\u00e3o h\u00e1 necessidade de continuar a busca por mais termos: o modelo com esses dois termos j\u00e1 explica toda a vari\u00e2ncia dos dados.</p> <p>Essa \u00e9 a ideia do algoritmo FROLS. Calculamos o ERR, escolhemos o vetor com maior ERR como o primeiro vetor ortogonal, ortogonalizamos todos os demais vetores em rela\u00e7\u00e3o a ele, calculamos o ERR de cada um deles, escolhemos novamente aquele com maior ERR e repetimos o processo at\u00e9 que algum crit\u00e9rio de parada seja satisfeito.</p> <p>No SysIdentPy, temos dois hiperpar\u00e2metros chamados <code>n_terms</code> e <code>err_tol</code>. Ambos podem ser usados para interromper as itera\u00e7\u00f5es. O primeiro faz com que o algoritmo selecione at\u00e9 <code>n_terms</code> regressores. O segundo interrompe o processo quando \\(\\sum ERR_i &gt; err_{tol}\\). Se ambos forem definidos, o algoritmo p\u00e1ra quando qualquer uma das condi\u00e7\u00f5es for satisfeita.</p> <pre><code>model = FROLS(\n        n_terms=50,\n        ylag=7,\n        xlag=7,\n        basis_function=basis_function,\n        err_tol=0.98\n    )\n</code></pre> <p>O SysIdentPy aplica o m\u00e9todo de Golub\u2011Householder para a decomposi\u00e7\u00e3o ortogonal. Uma discuss\u00e3o mais detalhada sobre Householder e procedimentos de ortogonaliza\u00e7\u00e3o em geral pode ser encontrada em Chen, S. and Billings, S. A. and Luo, W.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#estudo-de-caso","title":"Estudo de caso","text":"<p>Um exemplo usando dados reais ser\u00e1 apresentado utilizando o SysIdentPy. Neste exemplo, construiremos modelos lineares e n\u00e3o lineares para descrever o comportamento de um motor de corrente cont\u00ednua operando como gerador. Detalhes do experimento usado para gerar esses dados podem ser encontrados no artigo (em portugu\u00eas) IDENTIFICA\u00c7\u00c3O DE UM MOTOR/GERADOR CC POR MEIO DE MODELOS POLINOMIAIS AUTORREGRESSIVOS E REDES NEURAIS ARTIFICIAIS</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n\ndf1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n\n# checking the ouput\ndf2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <p></p> <p>Figura 2. Sa\u00edda do sistema eletromec\u00e2nico.</p> <p>Neste exemplo, iremos dizimar os dados usando \\(d = 500\\). A motiva\u00e7\u00e3o para a dizima\u00e7\u00e3o \u00e9 que os dados est\u00e3o superamostrados devido \u00e0 configura\u00e7\u00e3o experimental. Em uma se\u00e7\u00e3o futura discutiremos em mais detalhes como lidar com dados superamostrados no contexto de identifica\u00e7\u00e3o de sistemas. Por ora, considere essa abordagem como a mais apropriada para este caso.</p> <pre><code>x_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre> <p>Neste caso, construiremos um modelo NARX. No SysIdentPy, isso significa definir <code>unbiased=False</code> na classe <code>LeastSquares</code>. Usaremos uma fun\u00e7\u00e3o base <code>Polynomial</code> e definiremos o atraso m\u00e1ximo tanto para a entrada quanto para a sa\u00edda como 2. Essa configura\u00e7\u00e3o resulta em 15 termos na matriz de informa\u00e7\u00e3o; portanto, definiremos <code>n_terms=15</code>. Essa especifica\u00e7\u00e3o \u00e9 necess\u00e1ria porque, neste exemplo, <code>order_selection=False</code>. Discutiremos <code>order_selection</code> com mais detalhes na se\u00e7\u00e3o de Crit\u00e9rios de Informa\u00e7\u00e3o.</p> <p>No SysIdentPy, <code>order_selection</code> \u00e9 <code>True</code> por padr\u00e3o. Quando <code>order_selection=False</code>, o usu\u00e1rio deve informar um valor para <code>n_terms</code>, pois esse hiperpar\u00e2metro \u00e9 opcional e o valor padr\u00e3o \u00e9 <code>None</code>. Se definirmos <code>n_terms=5</code>, por exemplo, o FROLS ir\u00e1 parar ap\u00f3s selecionar os primeiros 5 regressores. N\u00e3o queremos isso neste caso, pois desejamos que o FROLS pare apenas quando <code>e_tol</code> for atingido.</p> <pre><code>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n    ylag=2,\n    xlag=2,\n    estimator=LeastSquares(unbiased=False),\n    basis_function=basis_function,\n    e_tol=0.9999\n    n_terms=15\n)\n</code></pre> <p>O SysIdentPy foi projetado para simplificar o uso de algoritmos como o <code>FROLS</code>. Construir, treinar ou ajustar um modelo \u00e9 feito por meio de uma interface simples chamada <code>fit</code>. Ao utilizar esse m\u00e9todo, todo o processo \u00e9 tratado internamente, sem a necessidade de intera\u00e7\u00e3o adicional do usu\u00e1rio.</p> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <p>O SysIdentPy tamb\u00e9m oferece um m\u00e9todo para recuperar informa\u00e7\u00f5es detalhadas sobre o modelo ajustado. \u00c9 poss\u00edvel inspecionar os termos inclu\u00eddos no modelo, os par\u00e2metros estimados, os valores de Error Reduction Ratio (ERR) e muito mais.</p> <p>Estamos usando <code>pandas</code> aqui apenas para tornar a sa\u00edda mais leg\u00edvel, mas isso \u00e9 opcional.</p> <pre><code>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR y(k-1) 1.0998E+00 9.86000384E-01 x1(k-1)^2 1.0165E+02 7.94805130E-03 y(k-2)^2 -1.9786E-05 2.50905908E-03 x1(k-1)y(k-1) -1.2138E-01 1.43301039E-03 y(k-2) -3.2621E-01 1.02781443E-03 x1(k-1)y(k-2) 5.3596E-02 5.35200312E-04 x1(k-2) 3.4655E+02 2.79648078E-04 x1(k-2)y(k-1) -5.1647E-02 1.12211942E-04 x1(k-2)x1(k-1) -8.2162E+00 4.54743448E-05 y(k-2)y(k-1) 4.0961E-05 3.25346101E-05 &gt;Table 1 <p>A tabela acima mostra que 10 regressores (de um total de 15 dispon\u00edveis) foram suficientes para atingir o <code>e_tol</code> definido, com a soma dos ERR para os regressores selecionados igual a \\(0.99992\\).</p> <p>Em seguida, vamos avaliar o desempenho do modelo usando os dados de valida\u00e7\u00e3o. De forma an\u00e1loga ao m\u00e9todo <code>fit</code>, o SysIdentPy oferece o m\u00e9todo <code>predict</code>. Para obter os valores previstos e plotar os resultados, basta fazer o seguinte:</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n</code></pre> <p></p> <p>Figura 3. Simula\u00e7\u00e3o em regime livre (ou previs\u00e3o em passos infinitos \u00e0 frente) do modelo ajustado.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#criterios-de-informacao","title":"Crit\u00e9rios de Informa\u00e7\u00e3o","text":"<p>Mencionamos que existem diversas maneiras de encerrar o algoritmo e selecionar os termos do modelo, mas at\u00e9 agora s\u00f3 definimos o crit\u00e9rio baseado em ERR. Uma forma alternativa de parar o algoritmo \u00e9 utilizando crit\u00e9rios de informa\u00e7\u00e3o, por exemplo, o Akaike Information Criterion (AIC). Para regress\u00e3o baseada em M\u00ednimos Quadrados, o AIC indica o n\u00famero de regressores ao minimizar a fun\u00e7\u00e3o objetivo (Akaike, H. - A new look at the statistical model identification):</p> \\[ \\begin{equation}     J_{\\text{AIC}} = \\underbrace{n\\log\\left(Var[\\xi_k]\\right)}_{\\text{primeiro termo}}+\\underbrace{2n_{\\Theta}}_{\\text{{segundo termo}}}. \\end{equation} \\tag{28} \\] <p>\u00c9 importante notar que a equa\u00e7\u00e3o acima ilustra o trade\u2011off entre ajuste do modelo e complexidade do modelo. Em particular, esse trade\u2011off envolve equilibrar a capacidade do modelo de ajustar bem os dados (primeiro termo) com a sua complexidade, relacionada ao n\u00famero de par\u00e2metros inclu\u00eddos (segundo termo). \u00c0 medida que mais termos s\u00e3o inclu\u00eddos no modelo, o valor do AIC tende inicialmente a diminuir, atingindo um m\u00ednimo que representa um equil\u00edbrio \u00f3timo entre complexidade e desempenho preditivo. No entanto, se o n\u00famero de par\u00e2metros se tornar excessivo, a penaliza\u00e7\u00e3o por complexidade passa a superar o benef\u00edcio de um melhor ajuste, fazendo com que o valor de AIC volte a crescer. O AIC e v\u00e1rias de suas variantes t\u00eam sido amplamente utilizados na identifica\u00e7\u00e3o de sistemas lineares e n\u00e3o lineares. Ver, por exemplo, Wei, H. and Zhu, D. and Billings, S. A. and Balikhin, M. A. - Forecasting the geomagnetic activity of the Dst index using multiscale radial basis function networks, Martins, S. A. M. and Nepomuceno, E. G. and Barroso, M. F. S. - Improved Structure Detection For Polynomial NARX Models Using a Multiobjective Error Reduction Ratio, Hafiz, F. and Swain, A. and Mendes, E. M. A. M. and Patel, N. - Structure Selection of Polynomial NARX Models Using Two Dimensional (2D) Particle Swarms, Gu, Y. and Wei, H. and Balikhin, M. M. - Nonlinear predictive model selection and model averaging using information criteria e refer\u00eancias a\u00ed citadas.</p> <p>Apesar de bastante eficazes em muitos problemas de sele\u00e7\u00e3o de modelos lineares, crit\u00e9rios de informa\u00e7\u00e3o como AIC podem ter dificuldade em selecionar um n\u00famero adequado de par\u00e2metros quando lidamos com sistemas fortemente n\u00e3o lineares. Al\u00e9m disso, esses crit\u00e9rios podem levar a modelos sub\u00f3timos se o espa\u00e7o de busca n\u00e3o contiver todos os termos necess\u00e1rios para representar adequadamente o modelo \"verdadeiro\". Consequentemente, em sistemas altamente n\u00e3o lineares ou quando componentes importantes do modelo est\u00e3o ausentes, os crit\u00e9rios de informa\u00e7\u00e3o podem n\u00e3o fornecer orienta\u00e7\u00f5es confi\u00e1veis, resultando em modelos com desempenho ruim.</p> <p>Al\u00e9m do AIC, o SysIdentPy disponibiliza outros quatro crit\u00e9rios de informa\u00e7\u00e3o: Bayesian Information Criteria (BIC), Final Prediction Error (FPE), Low of Iterated Logarithm Criteria (LILC) e Corrected Akaike Information Criteria (AICc), definidos, respectivamente, como</p> \\[ \\begin{align} \\operatorname{FPE}\\left(n_\\theta\\right) &amp; =N \\ln \\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]+N \\ln \\left[\\frac{N+n_\\theta}{N-n_\\theta}\\right] \\\\ BI C\\left(n_\\theta\\right) &amp; =N \\ln \\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]+n_\\theta \\ln N \\\\ AICc &amp;=AIC+2 n_p * \\frac{n_p+1}{N-n_p-1} \\\\ LILC &amp;= 2n_{\\theta}\\ln(\\ln(N)) + N \\ln(\\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]) \\end{align} \\tag{29} \\] <p>Para usar qualquer crit\u00e9rio de informa\u00e7\u00e3o no SysIdentPy, defina <code>order_selection=True</code> (como mencionado, esse j\u00e1 \u00e9 o valor padr\u00e3o). Al\u00e9m de <code>order_selection</code>, voc\u00ea pode definir quantos regressores deseja avaliar antes de interromper o algoritmo por meio do hiperpar\u00e2metro <code>n_info_values</code>. O valor padr\u00e3o \u00e9 \\(15\\), mas o usu\u00e1rio deve aument\u00e1\u2011lo de acordo com o n\u00famero de regressores dispon\u00edveis, dado por <code>ylag</code>, <code>xlag</code> e o grau da fun\u00e7\u00e3o base.</p> <p>O uso de crit\u00e9rios de informa\u00e7\u00e3o pode ser computacionalmente custoso, dependendo do n\u00famero de regressores avaliados e da quantidade de amostras. Para calcular o crit\u00e9rio, o algoritmo ERR \u00e9 executado <code>n</code> vezes, onde <code>n</code> \u00e9 o valor definido em <code>n_info_values</code>. Certifique\u2011se de entender bem o funcionamento do m\u00e9todo antes de decidir se realmente precisa utiliz\u00e1\u2011lo.</p> <p>Executando o mesmo exemplo, mas agora usando o crit\u00e9rio de informa\u00e7\u00e3o BIC para selecionar a ordem do modelo, temos:</p> <pre><code>model = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"bic\",\n    estimator=LeastSquares(unbiased=False),\n    basis_function=basis_function\n)\nmodel.fit(X=x_train, y=y_train)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR y(k-1) 1.3666E+00 9.86000384E-01 x1(k-1)^2 1.0500E+02 7.94805130E-03 y(k-2)^2 -5.8577E-05 2.50905908E-03 x1(k-1)y(k-1) -1.2427E-01 1.43301039E-03 y(k-2) -5.1414E-01 1.02781443E-03 x1(k-1)y(k-2) 5.3001E-02 5.35200312E-04 x1(k-2) 3.1144E+02 2.79648078E-04 x1(k-2)y(k-1) -4.8013E-02 1.12211942E-04 x1(k-2)x1(k-1) -8.0561E+00 4.54743448E-05 x1(k-2)y(k-2) 4.1381E-03 3.25346101E-05 1 -5.6653E+01 7.54107553E-06 y(k-2)y(k-1) 1.5679E-04 3.52002717E-06 y(k-1)^2 -9.0164E-05 6.17373260E-06 &gt;Table 2 <p>Nesse caso, em vez de 8 regressores, o modelo final possui 13 termos.</p> <p>Atualmente, o n\u00famero de regressores \u00e9 determinado identificando o \u00edndice da \u00faltima posi\u00e7\u00e3o em que a diferen\u00e7a entre o valor atual do crit\u00e9rio e o valor anterior \u00e9 menor que 0. Para inspecionar esses valores, voc\u00ea pode utilizar a abordagem a seguir:</p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <p></p> <p>Figura 4. O gr\u00e1fico mostra os valores do crit\u00e9rio de informa\u00e7\u00e3o (BIC) em fun\u00e7\u00e3o do n\u00famero de termos inclu\u00eddos no modelo. O processo de sele\u00e7\u00e3o de modelo, com base no BIC, adiciona regresssores iterativamente at\u00e9 que o BIC atinja um m\u00ednimo, indicando o balan\u00e7o \u00f3timo entre complexidade e ajuste. O ponto em que o valor de BIC deixa de diminuir define o n\u00famero \u00f3timo de termos, resultando em um modelo final com 13 termos.</p> <p>A predi\u00e7\u00e3o do modelo neste caso \u00e9 mostrada na Figura 5.</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n</code></pre> <p></p> <p>Figura 5. Simula\u00e7\u00e3o em regime livre (ou previs\u00e3o em passos infinitos \u00e0 frente) do modelo ajustado utilizando o BIC.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#visao-geral-dos-metodos-de-criterio-de-informacao","title":"Vis\u00e3o geral dos m\u00e9todos de Crit\u00e9rio de Informa\u00e7\u00e3o","text":"<p>Nesta se\u00e7\u00e3o, utilizamos dados simulados para fornecer ao leitor uma vis\u00e3o mais clara dos crit\u00e9rios de informa\u00e7\u00e3o dispon\u00edveis no SysIdentPy.</p> <p>Aqui estamos trabalhando com uma estrutura de modelo conhecida, o que nos permite focar apenas em como os diferentes crit\u00e9rios de informa\u00e7\u00e3o se comportam. Em dados reais, o n\u00famero correto de termos do modelo \u00e9 desconhecido, o que torna esses m\u00e9todos ferramentas importantes para guiar a sele\u00e7\u00e3o de modelos.</p> <p>Se voc\u00ea observar as m\u00e9tricas reportadas abaixo, notar\u00e1 que o desempenho \u00e9 excelente para todos os modelos. No entanto, \u00e9 fundamental lembrar que Identifica\u00e7\u00e3o de Sistemas n\u00e3o trata apenas de obter boas m\u00e9tricas de predi\u00e7\u00e3o \u2014 o objetivo \u00e9 encontrar a estrutura de modelo mais adequada. Model Structure Selection est\u00e1 no cora\u00e7\u00e3o dos m\u00e9todos NARMAX!</p> <p>Os dados s\u00e3o gerados pela simula\u00e7\u00e3o do seguinte modelo:</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_k \\tag{30} \\] <p>Se <code>colored_noise</code> for definido como <code>True</code>, o termo de ru\u00eddo \u00e9 dado por:</p> \\[ e_k = 0.8\\nu_{k-1} + \\nu_k \\tag{31} \\] <p>em que \\(x\\) \u00e9 uma vari\u00e1vel aleat\u00f3ria uniformemente distribu\u00edda e \\(\\nu\\) \u00e9 uma vari\u00e1vel Gaussiana com \\(\\mu = 0\\) e \\(\\sigma = 0.1\\).</p> <p>No pr\u00f3ximo exemplo, geraremos dados com 100 amostras, usando ru\u00eddo branco, e selecionaremos 70% dos dados para treinar o modelo.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=100, colored_noise=False, sigma=0.1, train_percentage=70\n)\n</code></pre> <p>A ideia aqui \u00e9 mostrar o impacto dos crit\u00e9rios de informa\u00e7\u00e3o na sele\u00e7\u00e3o do n\u00famero de termos que comp\u00f5em o modelo final. Voc\u00ea ver\u00e1 por que esses crit\u00e9rios s\u00e3o ferramentas auxiliares e por que deixar o algoritmo selecionar o n\u00famero de termos apenas com base no valor m\u00ednimo do crit\u00e9rio nem sempre \u00e9 uma boa ideia quando lidamos com dados muito contaminados por ru\u00eddo (mesmo ru\u00eddo branco).</p>"},{"location":"pt/book/4-Model-Structure-Selection/#aic","title":"AIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <p>Os regressores, a simula\u00e7\u00e3o em regime livre e os valores de AIC s\u00e3o detalhados a seguir.</p> Regressors Parameters ERR x1(k-2) 9.4236E-01 9.26094341E-01 y(k-1) 2.4933E-01 3.35898283E-02 x1(k-1)y(k-1) 1.3001E-01 2.35736200E-03 x1(k-1) 8.4024E-02 4.11741791E-03 x1(k-1)^2 7.0807E-02 2.54231877E-03 x1(k-2)^2 -9.1138E-02 1.39658893E-03 y(k-1)^2 1.1698E-01 1.70257419E-03 x1(k-2)y(k-2) 8.3745E-02 1.11056684E-03 y(k-2)^2 -4.1946E-02 1.01686239E-03 x1(k-2)x1(k-1) 5.9034E-02 7.47435512E-04 &gt;Table 3 <p></p> <p>Figura 5. Simula\u00e7\u00e3o em regime livre (ou previs\u00e3o em passos infinitos \u00e0 frente) do modelo ajustado utilizando AIC.</p> <p></p> <p>Figura 6. O gr\u00e1fico mostra os valores do crit\u00e9rio de informa\u00e7\u00e3o (AIC) em fun\u00e7\u00e3o do n\u00famero de termos inclu\u00eddos no modelo. O processo de sele\u00e7\u00e3o de modelo, baseado no AIC, adiciona regressores iterativamente at\u00e9 que o crit\u00e9rio atinja um m\u00ednimo, indicando o balan\u00e7o \u00f3timo entre complexidade e ajuste. O ponto em que o valor de AIC deixa de diminuir define o n\u00famero \u00f3timo de termos, resultando em um modelo final com 10 termos.</p> <p>Neste caso, obtemos um modelo com 10 termos. Sabemos, por\u00e9m, que o n\u00famero correto \u00e9 3, pois estamos trabalhando com um sistema simulado conhecido.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#aicc","title":"AICc","text":"<p>A \u00fanica modifica\u00e7\u00e3o necess\u00e1ria para usar AICc em vez de AIC \u00e9 alterar o hiperpar\u00e2metro de crit\u00e9rio de informa\u00e7\u00e3o: <code>information_criteria=\"aicc\"</code>.</p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aicc\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.2282E-01 9.26094341E-01 y(k-1) 2.4294E-01 3.35898283E-02 x1(k-1)y(k-1) 1.2753E-01 2.35736200E-03 x1(k-1) 6.9597E-02 4.11741791E-03 x1(k-1)^2 7.0578E-02 2.54231877E-03 x1(k-2)^2 -1.0523E-01 1.39658893E-03 y(k-1)^2 1.0949E-01 1.70257419E-03 x1(k-2)y(k-2) 7.1821E-02 1.11056684E-03 y(k-2)^2 -3.9756E-02 1.01686239E-03 &gt;Table 4 <p></p> <p>Figura 7. Simula\u00e7\u00e3o em regime livre (ou previs\u00e3o em passos infinitos \u00e0 frente) do modelo ajustado utilizando AICc.</p> <p></p> <p>Figura 8. O gr\u00e1fico mostra os valores do crit\u00e9rio de informa\u00e7\u00e3o (AICc) em fun\u00e7\u00e3o do n\u00famero de termos inclu\u00eddos no modelo. O processo de sele\u00e7\u00e3o de modelo, baseado no AICc, adiciona regressores iterativamente at\u00e9 que o crit\u00e9rio atinja um m\u00ednimo, indicando o balan\u00e7o \u00f3timo entre complexidade e ajuste. O ponto em que o valor de AICc deixa de diminuir define o n\u00famero \u00f3timo de termos, resultando em um modelo final com 9 termos.</p> <p>Desta vez, obtemos um modelo com 9 regressores.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#bic","title":"BIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"bic\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1726E-01 9.26094341E-01 y(k-1) 1.8670E-01 3.35898283E-02 &gt;Table 5 <p>Figura 9. Simula\u00e7\u00e3o em regime livre (ou previs\u00e3o em passos infinitos \u00e0 frente) do modelo ajustado utilizando BIC.</p> <p></p> <p>Figura 10. O gr\u00e1fico mostra os valores do crit\u00e9rio de informa\u00e7\u00e3o (BIC) em fun\u00e7\u00e3o do n\u00famero de termos inclu\u00eddos no modelo. O processo de sele\u00e7\u00e3o de modelo, baseado no BIC, adiciona regressores iterativamente at\u00e9 que o crit\u00e9rio atinja um m\u00ednimo, indicando o balan\u00e7o \u00f3timo entre complexidade e ajuste. O ponto em que o valor de BIC deixa de diminuir define o n\u00famero \u00f3timo de termos, resultando em um modelo final com 2 termos.</p> <p>O BIC retornou um modelo com apenas 2 regressores.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#lilc","title":"LILC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"lilc\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1160E-01 9.26094341E-01 y(k-1) 2.3178E-01 3.35898283E-02 x1(k-1)y(k-1) 1.2080E-01 2.35736200E-03 x1(k-1) 6.3113E-02 4.11741791E-03 x1(k-1)^2 5.4088E-02 2.54231877E-03 x1(k-2)^2 -9.0683E-02 1.39658893E-03 y(k-1)^2 8.2157E-02 1.70257419E-03 &gt;Table 6 <p>Figura 11. Simula\u00e7\u00e3o em regime livre (ou previs\u00e3o em passos infinitos \u00e0 frente) do modelo ajustado utilizando LILC.</p> <p></p> <p>Figura 12. O gr\u00e1fico mostra os valores do crit\u00e9rio de informa\u00e7\u00e3o (LILC) em fun\u00e7\u00e3o do n\u00famero de termos inclu\u00eddos no modelo. O processo de sele\u00e7\u00e3o de modelo, baseado no LILC, adiciona regressores iterativamente at\u00e9 que o crit\u00e9rio atinja um m\u00ednimo, indicando o balan\u00e7o \u00f3timo entre complexidade e ajuste. O ponto em que o valor de LILC deixa de diminuir define o n\u00famero \u00f3timo de termos, resultando em um modelo final com 7 termos.</p> <p>O LILC retornou um modelo com 7 regressores.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#fpe","title":"FPE","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"fpe\",\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.4236E-01 9.26094341E-01 y(k-1) 2.4933E-01 3.35898283E-02 x1(k-1)y(k-1) 1.3001E-01 2.35736200E-03 x1(k-1) 8.4024E-02 4.11741791E-03 x1(k-1)^2 7.0807E-02 2.54231877E-03 x1(k-2)^2 -9.1138E-02 1.39658893E-03 y(k-1)^2 1.1698E-01 1.70257419E-03 x1(k-2)y(k-2) 8.3745E-02 1.11056684E-03 y(k-2)^2 -4.1946E-02 1.01686239E-03 x1(k-2)x1(k-1) 5.9034E-02 7.47435512E-04 &gt;Table 7 <p>Figura 13. Simula\u00e7\u00e3o em regime livre (ou previs\u00e3o em passos infinitos \u00e0 frente) do modelo ajustado utilizando FPE.</p> <p></p> <p>Figura 14. O gr\u00e1fico mostra os valores do crit\u00e9rio de informa\u00e7\u00e3o (FPE) em fun\u00e7\u00e3o do n\u00famero de termos inclu\u00eddos no modelo. O processo de sele\u00e7\u00e3o de modelo, baseado no FPE, adiciona regressores iterativamente at\u00e9 que o crit\u00e9rio atinja um m\u00ednimo, indicando o balan\u00e7o \u00f3timo entre complexidade e ajuste. O ponto em que o valor de FPE deixa de diminuir define o n\u00famero \u00f3timo de termos, resultando em um modelo final com 10 termos.</p> <p>O FPE retornou um modelo com 10 regressores.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#meta-model-structure-selection-metamss","title":"Meta Model Structure Selection (MetaMSS)","text":"<p>Esta se\u00e7\u00e3o reflete em grande parte o conte\u00fado de um artigo que publiquei no ArXiv, intitulado \"Meta-Model Structure Selection: Building Polynomial NARX Models for Regression and Classification.\" Esse artigo foi inicialmente escrito para submiss\u00e3o em peri\u00f3dico, com base nos resultados da minha disserta\u00e7\u00e3o de mestrado. No entanto, como passei a atuar como Data Scientist e considerando o longo processo de submiss\u00e3o em peri\u00f3dicos, decidi n\u00e3o seguir com a publica\u00e7\u00e3o formal neste momento. Assim, o artigo permanece dispon\u00edvel apenas no ArXiv.</p> <p>Esse trabalho estende um artigo anterior apresentado em um congresso brasileiro, onde parte dos resultados foi inicialmente divulgada.</p> <p>Esta se\u00e7\u00e3o apresenta uma abordagem meta\u2011heur\u00edstica para sele\u00e7\u00e3o da estrutura de modelos polinomiais NARX em problemas de regress\u00e3o. O m\u00e9todo proposto considera simultaneamente a complexidade do modelo e a contribui\u00e7\u00e3o de cada termo para construir modelos parcimoniosos por meio de uma nova formula\u00e7\u00e3o de fun\u00e7\u00e3o de custo. A robustez do algoritmo \u00e9 avaliada em diversos sistemas simulados e experimentais com diferentes caracter\u00edsticas de n\u00e3o linearidade. Os resultados mostram que o algoritmo \u00e9 capaz de identificar corretamente o modelo quando a estrutura verdadeira \u00e9 conhecida e de produzir modelos parcimoniosos com dados experimentais, mesmo em casos em que m\u00e9todos cl\u00e1ssicos e contempor\u00e2neos frequentemente falham. A nova abordagem \u00e9 comparada com m\u00e9todos cl\u00e1ssicos, como FROLS, e com t\u00e9cnicas recentes baseadas em amostragem aleat\u00f3ria.</p> <p>Comentamos anteriormente que selecionar termos adequados de modelo \u00e9 crucial para capturar corretamente a din\u00e2mica do sistema original. Problemas como superparametriza\u00e7\u00e3o e m\u00e1 condi\u00e7\u00e3o num\u00e9rica frequentemente surgem devido \u00e0s limita\u00e7\u00f5es dos algoritmos de identifica\u00e7\u00e3o existentes em selecionar os termos corretos para o modelo final. Ver, por exemplo, Aguirre, L. A. and Billings, S. A. - Dynamical effects of overparametrization in nonlinear models, Piroddi, L. and Spinelli, W. - An identification algorithm for polynomial NARX models based on simulation error minimization. Tamb\u00e9m mencionamos que um dos algoritmos mais tradicionais para sele\u00e7\u00e3o de estrutura em modelos NARMAX polinomiais \u00e9 o ERR. Diversas variantes do algoritmo FROLS foram desenvolvidas para melhorar o desempenho da sele\u00e7\u00e3o de modelo, como em Billings, S. A., Chen, S., and Korenberg, M. J. - Identification of MIMO non-linear systems using a forward-regression orthogonal estimator, Farina, M. and Piroddi, L. - Simulation Error Minimization\u2013Based Identification of Polynomial Input\u2013Output Recursive Models, Guo, Y., Guo, L. Z., Billings, S. A., and Wei, H. - A New Iterative Orthogonal Forward Regression Algorithm, Mao, K. Z. and Billings, S. A. - VARIABLE SELECTION IN NON-LINEAR SYSTEMS MODELLING. As limita\u00e7\u00f5es do FROLS foram amplamente discutidas na literatura, por exemplo, em Billings, S. A. and Aguirre, L. A., Palumbo, P. and Piroddi, L., Falsone, A., Piroddi, L., and Prandini, M.. A maior parte desses pontos fracos est\u00e1 relacionada a (i) o framework de Prediction Error Minimization (PEM); (ii) a inadequa\u00e7\u00e3o do \u00edndice ERR para medir a import\u00e2ncia absoluta dos regressores; e (iii) o uso de crit\u00e9rios de informa\u00e7\u00e3o como AIC, FPE e BIC para selecionar a ordem do modelo. Em rela\u00e7\u00e3o aos crit\u00e9rios de informa\u00e7\u00e3o, embora funcionem bem para modelos lineares, em um contexto n\u00e3o linear n\u00e3o \u00e9 poss\u00edvel estabelecer uma rela\u00e7\u00e3o simples entre tamanho do modelo e acur\u00e1cia Falsone, A., Piroddi, L., and Prandini, M. - A randomized algorithm for nonlinear model structure selection, Chen, S., Hong, X., and Harris, C. J. - Sparse kernel regression modeling using combined locally regularized orthogonal least squares and D-optimality experimental design.</p> <p>Devido \u00e0s limita\u00e7\u00f5es dos algoritmos baseados em Ordinary Least Squares (OLS), pesquisas recentes propuseram solu\u00e7\u00f5es que se afastam da abordagem cl\u00e1ssica FROLS. Novos m\u00e9todos reformularam o processo de Model Structure Selection (MSS) em um arcabou\u00e7o probabil\u00edstico e passaram a empregar t\u00e9cnicas de amostragem aleat\u00f3ria Falsone, A., Piroddi, L., and Prandini, M. - A randomized algorithm for nonlinear model structure selection, Tempo, R., Calafiore, G., and Dabbene, F. - Randomized Algorithms for Analysis and Control of Uncertain Systems: With Applications, Baldacchino, T., Anderson, S. R., and Kadirkamanathan, V. - Computational system identification for Bayesian NARMAX modelling, Rodriguez-Vazquez, K., Fonseca, C. M., and Fleming, P. J. - Identifying the structure of nonlinear dynamic systems using multiobjective genetic programming, Severino, A. G. V. and Araujo, F. M. U. de. Apesar desses avan\u00e7os, abordagens meta\u2011heur\u00edsticas e probabil\u00edsticas ainda apresentam algumas limita\u00e7\u00f5es. Em particular, muitas dependem de crit\u00e9rios de informa\u00e7\u00e3o como AIC, FPE e BIC para definir a fun\u00e7\u00e3o de custo da otimiza\u00e7\u00e3o, o que frequentemente leva a modelos superparametrizados.</p> <p>Considere \\(\\mathcal{F}\\) como uma classe de fun\u00e7\u00f5es limitadas \\(\\phi: \\mathbf{R} \\mapsto \\mathbf{R}\\). Se as propriedades de \\(\\phi(x)\\) satisfazem</p> \\[ \\begin{align}     &amp;\\lim\\limits_{x \\to \\infty} \\phi(x) = \\alpha \\nonumber \\\\     &amp;\\lim\\limits_{x \\to -\\infty} \\phi(x) = \\beta \\quad \\text{com } \\alpha &gt; \\beta,  \\nonumber \\end{align} \\tag{32} \\] <p>ent\u00e3o \\(\\phi(x)\\) \u00e9 chamada de fun\u00e7\u00e3o sigmoide.</p> <p>No caso particular considerado aqui, seguindo a defini\u00e7\u00e3o da Equa\u00e7\u00e3o 32 com \\(\\alpha = 1\\) e \\(\\beta = 0\\), podemos escrever uma curva em \"S\" como</p> \\[ \\begin{equation}     \\varsigma(x) = \\frac{1}{1+e^{-a(x-c)}}. \\end{equation} \\tag{33} \\] <p>Nesse caso, podemos especificar o par\u00e2metro \\(a\\), que controla a taxa de varia\u00e7\u00e3o. Se \\(a\\) \u00e9 pr\u00f3ximo de zero, a fun\u00e7\u00e3o sigmoide \u00e9 suave. Se \\(a\\) \u00e9 grande, a transi\u00e7\u00e3o da sigmoide \u00e9 abrupta. Se \\(a\\) \u00e9 negativo, a sigmoide varia de 1 para 0. O par\u00e2metro \\(c\\) corresponde ao valor de \\(x\\) para o qual \\(y = 0.5\\).</p> <p>A Sigmoid Linear Unit Function (SiLU) \u00e9 definida como o produto da fun\u00e7\u00e3o sigmoide pela sua entrada</p> \\[ \\begin{equation}     \\text{silu}(x) = x \\varsigma(x), \\end{equation} \\tag{34} \\] <p>que pode ser vista como uma fun\u00e7\u00e3o sigmoide \"mais \u00edngreme\" com overshoot.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#metaheuristicas","title":"Metaheur\u00edsticas","text":"<p>Nas \u00faltimas duas d\u00e9cadas, algoritmos de otimiza\u00e7\u00e3o inspirados na natureza t\u00eam ganhado destaque devido \u00e0 sua flexibilidade, simplicidade, versatilidade e capacidade de evitar m\u00ednimos locais em aplica\u00e7\u00f5es reais.</p> <p>Algoritmos metaheur\u00edsticos s\u00e3o caracterizados por duas propriedades fundamentais: explora\u00e7\u00e3o e explota\u00e7\u00e3o Blum, C. and Roli, A. - Metaheuristics in combinatorial optimization: Overview and conceptual comparison. Exploitation (explora\u00e7\u00e3o local) foca em utilizar informa\u00e7\u00f5es locais para refinar a busca em torno da melhor solu\u00e7\u00e3o atual, melhorando a qualidade das solu\u00e7\u00f5es vizinhas. Por outro lado, exploration (explora\u00e7\u00e3o global) procura investigar regi\u00f5es mais amplas do espa\u00e7o de busca, de modo a descobrir solu\u00e7\u00f5es potencialmente superiores e evitar que o algoritmo fique preso em m\u00ednimos locais.</p> <p>Embora n\u00e3o haja consenso absoluto sobre as defini\u00e7\u00f5es exatas de explora\u00e7\u00e3o e explota\u00e7\u00e3o em computa\u00e7\u00e3o evolutiva, como discutido em Eiben, Agoston E and Schippers, Cornelis A, existe um entendimento geral de que esses conceitos atuam como for\u00e7as opostas e dif\u00edceis de equilibrar. Para lidar com esse desafio, metaheur\u00edsticas h\u00edbridas combinam m\u00faltiplos algoritmos, buscando tirar proveito tanto da explora\u00e7\u00e3o global quanto da explota\u00e7\u00e3o local, resultando em m\u00e9todos de otimiza\u00e7\u00e3o mais robustos.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#o-algoritmo-binary-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-bpsogsa","title":"O algoritmo Binary hybrid Particle Swarm Optimization and Gravitational Search Algorithm (BPSOGSA)","text":"<p>Alcan\u00e7ar um bom equil\u00edbrio entre explora\u00e7\u00e3o e explota\u00e7\u00e3o \u00e9 um dos grandes desafios da maioria dos algoritmos metaheur\u00edsticos. Na abordagem considerada aqui, aumentamos o desempenho e a flexibilidade do processo de busca utilizando uma estrat\u00e9gia h\u00edbrida que combina Binary Particle Swarm Optimization (BPSO) com Gravitational Search Algorithm (GSA), conforme proposto em Mirjalili, S. and Hashim, S. Z. M.. Esse m\u00e9todo h\u00edbrido incorpora uma t\u00e9cnica de coevolu\u00e7\u00e3o heterog\u00eanea de baixo n\u00edvel, originalmente introduzida por Talbi, E. G..</p> <p>O BPSOGSA tira proveito das for\u00e7as de ambos os algoritmos: a componente Particle Swarm Optimization (PSO) \u00e9 especialmente eficiente em explorar o espa\u00e7o de busca em escala global, buscando o \u00f3timo global, enquanto a componente Gravitational Search Algorithm (GSA) \u00e9 eficaz em refinar a busca em torno de solu\u00e7\u00f5es locais dentro de um espa\u00e7o bin\u00e1rio. Essa combina\u00e7\u00e3o visa fornecer uma estrat\u00e9gia de otimiza\u00e7\u00e3o mais abrangente e eficiente, garantindo um melhor equil\u00edbrio entre explora\u00e7\u00e3o e explota\u00e7\u00e3o.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#particle-swarm-optimization-pso-padrao","title":"Particle Swarm Optimization (PSO) padr\u00e3o","text":"<p>No Particle Swarm Optimization (PSO) Kennedy, J. and Eberhart, R. C., Kennedy, J., cada part\u00edcula representa uma solu\u00e7\u00e3o candidata e \u00e9 caracterizada por dois componentes: sua posi\u00e7\u00e3o no espa\u00e7o de busca, denotada por \\(\\vec{x}_{\\,np,d} \\in \\mathbb{R}^{np \\times d}\\), e sua velocidade, \\(\\vec{v}_{\\,np,d} \\in \\mathbb{R}^{np \\times d}\\). Aqui, \\(np = 1, 2, \\ldots, n_a\\), onde \\(n_a\\) \u00e9 o tamanho do enxame, e \\(d\\) \u00e9 a dimensionalidade do problema. A popula\u00e7\u00e3o inicial \u00e9 representada por</p> \\[ \\vec{x}_{\\,np,d} = \\begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,d} \\\\ x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,d} \\\\ \\vdots  &amp; \\vdots  &amp; \\ddots &amp; \\vdots \\\\ x_{n_a,1} &amp; x_{n_a,2} &amp; \\cdots &amp; x_{n_a,d} \\end{bmatrix} \\tag{35} \\] <p>A cada itera\u00e7\u00e3o \\(t\\), a posi\u00e7\u00e3o e a velocidade de uma part\u00edcula s\u00e3o atualizadas pelas seguintes equa\u00e7\u00f5es:</p> \\[ v_{np,d}^{t+1} = \\zeta v_{np,d}^{t} + c_1 \\kappa_1 (pbest_{np}^{t} - x_{np,d}^{t}) + c_2 \\kappa_2 (gbest_{np}^{t} - x_{np,d}^{t}), \\tag{36} \\] <p>em que \\(\\kappa_j \\in \\mathbb{R}\\) para \\(j = [1,2]\\) s\u00e3o vari\u00e1veis aleat\u00f3rias cont\u00ednuas no intervalo \\([0,1]\\), \\(\\zeta \\in \\mathbb{R}\\) \u00e9 o fator de in\u00e9rcia que controla a influ\u00eancia da velocidade anterior na velocidade atual (representando o trade\u2011off entre explora\u00e7\u00e3o e explota\u00e7\u00e3o), \\(c_1\\) \u00e9 o fator cognitivo associado \u00e0 melhor posi\u00e7\u00e3o pessoal \\(pbest\\), e \\(c_2\\) \u00e9 o fator social associado \u00e0 melhor posi\u00e7\u00e3o global \\(gbest\\). A velocidade \\(\\vec{v}_{\\,np,d}\\) \u00e9 tipicamente limitada ao intervalo \\([v_{min}, v_{max}]\\) para evitar que as part\u00edculas saiam do espa\u00e7o de busca. A posi\u00e7\u00e3o \u00e9 ent\u00e3o atualizada por</p> \\[ x_{np,d}^{t+1} = x_{np,d}^{t} + v_{np,d}^{t+1}. \\tag{37} \\]"},{"location":"pt/book/4-Model-Structure-Selection/#gravitational-search-algorithm-gsa-padrao","title":"Gravitational Search Algorithm (GSA) padr\u00e3o","text":"<p>No Gravitational Search Algorithm (GSA) Rashedi, Esmat, Nezamabadi-Pour, Hossein, and Saryazdi, Saeid - GSA: A Gravitational Search Algorithm, os agentes s\u00e3o representados por massas, cujo valor \u00e9 proporcional ao fitness (valor da fun\u00e7\u00e3o objetivo) associado a cada agente. Essas massas interagem mediante for\u00e7as gravitacionais, atraindo\u2011se mutuamente em dire\u00e7\u00e3o a regi\u00f5es mais promissoras do espa\u00e7o de busca. Massas mais pesadas (agentes com melhor fitness) movem\u2011se mais lentamente, enquanto massas mais leves (agentes com pior fitness) tendem a se mover mais rapidamente. Cada massa no GSA possui quatro propriedades: posi\u00e7\u00e3o, massa inercial, massa gravitacional ativa e massa gravitacional passiva. A posi\u00e7\u00e3o de uma massa representa uma solu\u00e7\u00e3o candidata, e suas massas gravitacional e inercial s\u00e3o derivadas da fun\u00e7\u00e3o de fitness.</p> <p>Considere uma popula\u00e7\u00e3o de agentes descrita pelas seguintes equa\u00e7\u00f5es. Em um instante de tempo \\(t\\), a velocidade e a posi\u00e7\u00e3o de cada agente s\u00e3o atualizadas como</p> \\[ \\begin{align}     v_{i,d}^{t+1} &amp;= \\kappa_i \\times v_{i,d}^t + a_{i,d}^t, \\\\     x_{i,d}^{t+1} &amp;= x_{i,d}^t + v_{i,d}^{t+1}. \\end{align} \\tag{38} \\] <p>Aqui, \\(\\kappa_i\\) introduz caracter\u00edsticas estoc\u00e1sticas no processo de busca. A acelera\u00e7\u00e3o \\(a_{i,d}^t\\) \u00e9 calculada de acordo com a lei do movimento Rashedi, Esmat and Nezamabadi-Pour, Hossein and Saryazdi, Saeid:</p> \\[ \\begin{equation}     a_{i,d}^t = \\frac{F_{i,d}^t}{M_{ii}^{t}}, \\end{equation} \\tag{39} \\] <p>em que \\(M_{ii}^{t}\\) \u00e9 a massa inercial do agente \\(i\\) e \\(F_{i,d}^t\\) representa a for\u00e7a gravitacional atuando sobre o agente \\(i\\) na dimens\u00e3o \\(d\\). Os detalhes do c\u00e1lculo de \\(F_{i,d}\\) e \\(M_{ii}\\) podem ser encontrados em Rashedi, Esmat and Nezamabadi-Pour, Hossein and Saryazdi, Saeid.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#o-algoritmo-hibrido-binario-de-otimizacao","title":"O algoritmo h\u00edbrido bin\u00e1rio de otimiza\u00e7\u00e3o","text":"<p>A combina\u00e7\u00e3o dos algoritmos segue a formula\u00e7\u00e3o descrita em Mirjalili, S. and Hashim, S. Z. M. - A new hybrid PSOGSA algorithm for function optimization:</p> \\[ \\begin{align}     v_{i}^{t+1} = \\zeta \\times v_i^t + \\mathrm{c}'_{1} \\times \\kappa \\times a_i^t + \\mathrm{c}'_2 \\times \\kappa \\times (gbest - x_i^t), \\end{align} \\tag{40} \\] <p>onde \\(\\mathrm{c}'_j \\in \\mathbb{R}\\) s\u00e3o coeficientes de acelera\u00e7\u00e3o. Essa formula\u00e7\u00e3o intensifica a fase de explota\u00e7\u00e3o ao incorporar a melhor posi\u00e7\u00e3o encontrada at\u00e9 o momento. Por outro lado, essa mesma caracter\u00edstica pode prejudicar a fase de explora\u00e7\u00e3o. Para contornar esse problema, Mirjalili, S., Mirjalili, S. M., and Lewis, A. - Grey Wolf Optimizer propuseram valores adaptativos para \\(\\mathrm{c}'_j\\), conforme descrito em Mirjalili, S., Wang, Gai-Ge, and Coelho, L. dos S. - Binary optimization using hybrid particle swarm optimization and gravitational search algorithm:</p> \\[ \\begin{align}     \\mathrm{c}_1' &amp;= -2 \\times \\frac{t^3}{\\max(t)^3} + 2, \\\\     \\mathrm{c}_2' &amp;= 2 \\times \\frac{t^3}{\\max(t)^3} + 2. \\end{align} \\tag{41} \\] <p>Em cada itera\u00e7\u00e3o, as posi\u00e7\u00f5es das part\u00edculas s\u00e3o atualizadas segundo as regras acima, sendo o espa\u00e7o cont\u00ednuo mapeado para solu\u00e7\u00f5es discretas por meio de uma fun\u00e7\u00e3o de transfer\u00eancia Mirjalili, S. and Lewis, A. - S-shaped versus V-shaped transfer functions for binary Particle Swarm Optimization:</p> \\[ \\begin{equation}     S(v_{ik}) = \\left|\\frac{2}{\\pi}\\arctan\\left(\\frac{\\pi}{2}v_{ik}\\right)\\right|. \\end{equation} \\tag{42} \\] <p>Com um n\u00famero aleat\u00f3rio \\(\\kappa \\in (0,1)\\) uniformemente distribu\u00eddo, as posi\u00e7\u00f5es dos agentes no espa\u00e7o bin\u00e1rio s\u00e3o atualizadas como</p> \\[ \\begin{equation}     x_{np,d}^{t+1} =     \\begin{cases}         (x_{np,d}^{t})^{-1}, &amp; \\text{se } \\kappa &lt; S(v_{ik}^{t+1}), \\\\         x_{np,d}^{t}, &amp; \\text{se } \\kappa \\geq S(v_{ik}^{t+1}).     \\end{cases} \\end{equation} \\tag{43} \\]"},{"location":"pt/book/4-Model-Structure-Selection/#meta-model-structure-selection-metamss-construindo-narx-para-regressao","title":"Meta-Model Structure Selection (MetaMSS): Construindo NARX para regress\u00e3o","text":"<p>Nesta subse\u00e7\u00e3o, exploramos a abordagem meta\u2011heur\u00edstica para sele\u00e7\u00e3o da estrutura de modelos NARX com BPSOGSA proposta na minha disserta\u00e7\u00e3o de mestrado. A ideia \u00e9 buscar, no espa\u00e7o de decis\u00e3o definido por um dicion\u00e1rio de regressores pr\u00e9\u2011especificado, a estrutura de modelo que minimiza uma fun\u00e7\u00e3o de custo. A fun\u00e7\u00e3o objetivo considerada \u00e9 baseada no root mean squared error (RMSE) da sa\u00edda em regime livre (free\u2011run simulation), acrescida de um termo de penaliza\u00e7\u00e3o que leva em conta a complexidade do modelo e a contribui\u00e7\u00e3o de cada regressor.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#esquema-de-codificacao","title":"Esquema de codifica\u00e7\u00e3o","text":"<p>O processo de uso do BPSOGSA para sele\u00e7\u00e3o de estrutura envolve definir as dimens\u00f5es da fun\u00e7\u00e3o de teste. Em particular, \\(n_y\\), \\(n_x\\) e \\(\\ell\\) s\u00e3o escolhidos de forma a cobrir todos os regressores poss\u00edveis, e uma matriz geral de regressores \\(\\Psi\\) \u00e9 constru\u00edda. O n\u00famero de colunas de \\(\\Psi\\) \u00e9 denotado por \\(noV\\), e o n\u00famero de agentes, por \\(N\\). Uma matriz bin\u00e1ria \\(noV \\times N\\), denotada \\(\\mathcal{X}\\), \u00e9 ent\u00e3o gerada aleatoriamente para representar a posi\u00e7\u00e3o de cada agente no espa\u00e7o de busca. Cada coluna de \\(\\mathcal{X}\\) representa uma solu\u00e7\u00e3o candidata, isto \u00e9, uma estrutura de modelo a ser avaliada em cada itera\u00e7\u00e3o. Nessa matriz, um valor 1 indica que a coluna correspondente de \\(\\Psi\\) \u00e9 inclu\u00edda na matriz reduzida de regressores, enquanto um valor 0 indica a exclus\u00e3o.</p> <p>Como exemplo, considere o caso em que todos os regressores poss\u00edveis s\u00e3o definidos com \\(\\ell = 1\\) e \\(n_y = n_u = 2\\). A matriz \\(\\Psi\\) \u00e9 dada por</p> \\[ \\begin{align} [ \\text{constant} \\quad y(k-1) \\quad y(k-2) \\quad u(k-1) \\quad u(k-2) ] \\end{align} \\tag{44} \\] <p>Com 5 regressores poss\u00edveis, temos \\(noV = 5\\). Supondo \\(N = 5\\), a matriz \\(\\mathcal{X}\\) pode ser representada como</p> \\[ \\begin{equation}     \\mathcal{X} =     \\begin{bmatrix}         0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\         1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\         0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\         0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1 &amp; 0     \\end{bmatrix} \\end{equation} \\tag{45} \\] <p>Cada coluna de \\(\\mathcal{X}\\) \u00e9 transposta para gerar uma solu\u00e7\u00e3o candidata. Por exemplo, a primeira coluna resulta em</p> \\[ \\begin{equation*}     \\mathcal{X} =     \\begin{bmatrix}         \\text{constant} &amp; y(k-1) &amp; y(k-2) &amp; u(k-1) &amp; u(k-2) \\\\         1 &amp; 1 &amp; 1 &amp; 0 &amp; 1     \\end{bmatrix} \\end{equation*} \\tag{46} \\] <p>Neste cen\u00e1rio, o primeiro modelo a ser avaliado \u00e9 \\(\\alpha y(k-1) + \\beta u(k-2)\\), cujos par\u00e2metros \\(\\alpha\\) e \\(\\beta\\) podem ser estimados com qualquer m\u00e9todo de estima\u00e7\u00e3o de par\u00e2metros dispon\u00edvel. O mesmo procedimento \u00e9 repetido para cada coluna de \\(\\mathcal{X}\\).</p>"},{"location":"pt/book/4-Model-Structure-Selection/#formulacao-da-funcao-objetivo","title":"Formula\u00e7\u00e3o da fun\u00e7\u00e3o objetivo","text":"<p>Para cada estrutura de modelo candidata, o sistema linear nos par\u00e2metros pode ser resolvido diretamente utilizando o algoritmo de M\u00ednimos Quadrados (Least Squares) ou qualquer outro m\u00e9todo dispon\u00edvel no SysIdentPy. A vari\u00e2ncia dos par\u00e2metros estimados pode ser calculada como</p> \\[ \\hat{\\sigma}^2 = \\hat{\\sigma}_e^2 V_{jj}, \\tag{47} \\] <p>em que \\(\\hat{\\sigma}_e^2\\) \u00e9 a vari\u00e2ncia do erro estimada, dada por</p> \\[ \\hat{\\sigma}_e^2 = \\frac{1}{N-m} \\sum_{k=1}^{N} (y_k - \\psi_{k-1}^\\top \\hat{\\Theta}), \\tag{48} \\] <p>e \\(V_{jj}\\) \u00e9 o \\(j\\)\u2011\u00e9simo elemento da diagonal de \\((\\Psi^\\top \\Psi)^{-1}\\).</p> <p>O erro padr\u00e3o estimado para o \\(j\\)\u2011\u00e9simo coeficiente de regress\u00e3o \\(\\hat{\\Theta}_j\\) \u00e9 dado pela raiz quadrada positiva dos elementos diagonais de \\(\\hat{\\sigma}^2\\):</p> \\[ \\mathrm{se}(\\hat{\\Theta}_j) = \\sqrt{\\hat{\\sigma}^2_{jj}}. \\tag{49} \\] <p>Para avaliar a relev\u00e2ncia estat\u00edstica de cada regressor, prop\u00f5e\u2011se um teste de penaliza\u00e7\u00e3o baseado no erro padr\u00e3o dos coeficientes de regress\u00e3o. Neste caso, utilizamos o teste t (t\u2011student) para realizar um teste de hip\u00f3tese sobre os coeficientes, avaliando a signific\u00e2ncia de cada regressor no modelo de regress\u00e3o linear m\u00faltipla. As hip\u00f3teses consideradas s\u00e3o</p> \\[ \\begin{align*}    H_0 &amp;: \\Theta_j = 0, \\\\    H_a &amp;: \\Theta_j \\neq 0. \\end{align*} \\tag{50} \\] <p>O valor da estat\u00edstica de teste t \u00e9 calculado como</p> \\[ T_0 = \\frac{\\hat{\\Theta}_j}{\\mathrm{se}(\\hat{\\Theta}_j)}, \\tag{51} \\] <p>que mede quantos desvios padr\u00e3o \\(\\hat{\\Theta}_j\\) est\u00e1 distante de zero. Mais precisamente, se</p> \\[ -t_{\\alpha/2, N-m} &lt; T_0 &lt; t_{\\alpha/2, N-m}, \\tag{52} \\] <p>onde \\(t_{\\alpha/2, N-m}\\) \u00e9 o valor cr\u00edtico da distribui\u00e7\u00e3o t para n\u00edvel de signific\u00e2ncia \\(\\alpha\\) e \\(N-m\\) graus de liberdade, ent\u00e3o, se \\(T_0\\) estiver fora dessa regi\u00e3o de aceita\u00e7\u00e3o, rejeitamos a hip\u00f3tese nula \\(H_0: \\Theta_j = 0\\). Isso implica que \\(\\Theta_j\\) \u00e9 estatisticamente significativo ao n\u00edvel \\(\\alpha\\). Caso contr\u00e1rio, se \\(T_0\\) estiver dentro da regi\u00e3o de aceita\u00e7\u00e3o, n\u00e3o rejeitamos \\(H_0\\) e consideramos que \\(\\Theta_j\\) n\u00e3o \u00e9 significativamente diferente de zero.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#valor-de-penalizacao-baseado-na-derivada-da-sigmoid-linear-unit","title":"Valor de penaliza\u00e7\u00e3o baseado na derivada da Sigmoid Linear Unit","text":"<p>Propomos um valor de penaliza\u00e7\u00e3o baseado na derivada da fun\u00e7\u00e3o sigmoide, definida como</p> \\[ \\dot{\\varsigma}(x(\\varrho)) = \\varsigma(x) [1 + (a(x - c))(1 - \\varsigma(x))]. \\tag{53} \\] <p>Nessa formula\u00e7\u00e3o, os par\u00e2metros s\u00e3o definidos da seguinte forma: \\(x\\) tem dimens\u00e3o \\(noV\\); \\(c = noV / 2\\); e \\(a\\) \u00e9 definido como a raz\u00e3o entre o n\u00famero de regressores do modelo em teste e \\(c\\). Esse procedimento leva a uma curva espec\u00edfica para cada modelo, sendo que o declive da sigmoide se torna mais acentuado \u00e0 medida que o n\u00famero de regressores aumenta. O valor de penaliza\u00e7\u00e3o \\(\\varrho\\) corresponde ao valor de \\(y\\) da curva sigmoide para o n\u00famero de regressores considerado em \\(x\\). Como a derivada da fun\u00e7\u00e3o sigmoide pode assumir valores negativos, normalizamos \\(\\varsigma\\) como</p> \\[ \\varrho = \\varsigma - \\mathrm{min}(\\varsigma), \\tag{54} \\] <p>garantindo que \\(\\varrho \\in \\mathbb{R}^{+}\\).</p> <p>Note que dois modelos distintos com o mesmo n\u00famero de regressores podem apresentar desempenhos bastante diferentes, devido \u00e0 import\u00e2ncia relativa de cada termo. Para lidar com isso, incorporamos o teste t\u2011student na penaliza\u00e7\u00e3o para quantificar a relev\u00e2ncia estat\u00edstica dos regressores. Mais especificamente, calculamos \\(n_{\\Theta, H_{0}}\\), o n\u00famero de regressores considerados n\u00e3o significativos para o modelo. O valor de penaliza\u00e7\u00e3o \u00e9 ent\u00e3o ajustado com base no tamanho efetivo do modelo:</p> \\[ \\mathrm{model\\_size} = n_{\\Theta} + n_{\\Theta, H_{0}}. \\tag{55} \\] <p>A fun\u00e7\u00e3o objetivo que combina o root mean squared error relativo com o termo de penaliza\u00e7\u00e3o \\(\\varrho\\) \u00e9 definida como</p> \\[ \\mathcal{F} = \\frac{\\sqrt{\\sum_{k=1}^{n} (y_k - \\hat{y}_k)^2}}{\\sqrt{\\sum_{k=1}^{n} (y_k - \\bar{y})^2}} \\times \\varrho. \\tag{56} \\] <p>Essa abordagem garante que, mesmo para modelos com o mesmo n\u00famero de regressores, aqueles contendo termos redundantes sejam mais penalizados.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#estudos-de-caso-resultados-de-simulacao","title":"Estudos de caso: resultados de simula\u00e7\u00e3o","text":"<p>Nesta subse\u00e7\u00e3o, apresentamos seis exemplos de simula\u00e7\u00e3o para ilustrar a efic\u00e1cia do algoritmo MetaMSS. Uma an\u00e1lise detalhada do desempenho do algoritmo \u00e9 realizada considerando diferentes configura\u00e7\u00f5es de par\u00e2metros. Os sistemas selecionados s\u00e3o amplamente utilizados como benchmarks em problemas de sele\u00e7\u00e3o de estrutura de modelos e foram extra\u00eddos de Wei, H. and Billings, S. A., \"Model structure selection using an integrated forward orthogonal search algorithm assisted by squared correlation and mutual information\", Falsone, A. and Piroddi, L. and Prandini, M., \"A randomized algorithm for nonlinear model structure selection\", Baldacchino, T. and Anderson, S. R. and Kadirkamanathan, V., \"Computational system identification for Bayesian NARMAX modelling\", Piroddi, L. and Spinelli, W., \"An identification algorithm for polynomial NARX models based on simulation error minimization\", Guo, Y. and Guo, L. Z. and Billings, S. A. and Wei, H., \"A New Iterative Orthogonal Forward Regression Algorithm\", Bonin, M. and Seghezza, V. and Piroddi, L., \"NARX model selection based on simulation error minimization and LASSO\" e Aguirre, L. A. and Barbosa, B. H. G. and Braga, A. P., \"Prediction and simulation errors in parameter estimation for nonlinear systems\". Finalmente, realizamos uma an\u00e1lise comparativa com o Randomized Model Structure Selection (RaMSS), \"A randomized algorithm for nonlinear model structure selection\", o FROLS e o algoritmo Reversible-jump Markov chain Monte Carlo (RJMCMC), \"Computational system identification for Bayesian NARMAX modelling\" para avaliar a efic\u00e1cia do m\u00e9todo proposto.</p> <p>Os modelos de simula\u00e7\u00e3o s\u00e3o definidos como</p> \\[ \\begin{align}     &amp; S_1: \\quad y_k = -1.7y_{k-1} - 0.8y_{k-2} + x_{k-1} + 0.81x_{k-2} + e_k, \\\\     &amp; \\qquad \\quad \\text{com } x_k \\sim \\mathcal{U}(-2, 2) \\text{ e } e_k \\sim \\mathcal{N}(0, 0.01^2); \\\\      &amp; S_2: \\quad y_k = 0.8y_{k-1} + 0.4x_{k-1} + 0.4x_{k-1}^2 + 0.4x_{k-1}^3 + e_k, \\\\     &amp; \\qquad \\quad \\text{com } x_k \\sim \\mathcal{N}(0, 0.3^2) \\text{ e } e_k \\sim \\mathcal{N}(0, 0.01^2). \\\\      &amp; S_3: \\quad y_k = 0.2y_{k-1}^3 + 0.7y_{k-1}x_{k-1} + 0.6x_{k-2}^2 \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 -0.5y_{k-2}+ e_k, \\\\     &amp; \\qquad \\quad \\text{com } x_k \\sim \\mathcal{U}(-1, 1) \\text{ e } e_k \\sim \\mathcal{N}(0, 0.01^2). \\\\      &amp; S_4: \\quad y_k = 0.7y_{k-1}x_{k-1} - 0.5y_{k-2} + 0.6x_{k-2}^2 \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 + e_k, \\\\     &amp; \\qquad \\quad \\text{com } x_k \\sim \\mathcal{U}(-1, 1) \\text{ e } e_k \\sim \\mathcal{N}(0, 0.04^2). \\\\      &amp; S_5: \\quad y_k = 0.7y_{k-1}x_{k-1} - 0.5y_{k-2} + 0.6x_{k-2}^2 \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 + 0.2e_{k-1} \\\\     &amp; \\qquad \\quad - 0.3x_{k-1}e_{k-2} + e_k,\\\\     &amp; \\qquad \\quad \\text{com } x_k \\sim \\mathcal{U}(-1, 1) \\text{ e } e_k \\sim \\mathcal{N}(0, 0.02^2); \\\\      &amp; S_6: \\quad y_k = 0.75y_{k-2} + 0.25x_{k-2} - 0.2y_{k-2}x_{k-2} + e_k \\\\     &amp; \\qquad \\quad \\text{com } x_k \\sim \\mathcal{N}(0, 0.25^2) \\text{ e } e_k \\sim \\mathcal{N}(0, 0.02^2);  \\end{align} \\tag{57} \\] <p>em que \\(\\mathcal{U}(a, b)\\) denota amostras uniformemente distribu\u00eddas em \\([a, b]\\) e \\(\\mathcal{N}(\\eta, \\sigma^2)\\) denota amostras com distribui\u00e7\u00e3o Gaussiana de m\u00e9dia \\(\\eta\\) e desvio padr\u00e3o \\(\\sigma\\). Todas as realiza\u00e7\u00f5es dos sistemas s\u00e3o compostas por 500 amostras de entrada e sa\u00edda. Al\u00e9m disso, a mesma semente aleat\u00f3ria \u00e9 utilizada para garantir reprodutibilidade.</p> <p>Todos os resultados apresentados nesta subse\u00e7\u00e3o s\u00e3o baseados na implementa\u00e7\u00e3o original e foram extra\u00eddos da minha disserta\u00e7\u00e3o de mestrado. Na \u00e9poca, o algoritmo foi implementado em Matlab \\(2018\\)a, executado em um Dell Inspiron \\(5448\\) Core i\\(5-5200\\)U CPU \\(2.20\\)GHz com \\(12\\)GB de RAM. No entanto, n\u00e3o \u00e9 dif\u00edcil adaptar o c\u00f3digo para o SysIdentPy.</p> <p>Seguindo os estudos mencionados, escolhemos lags m\u00e1ximos \\(n_u=n_y=4\\) para entrada e sa\u00edda e grau de n\u00e3o linearidade \\(\\ell = 3\\). Os par\u00e2metros relacionados ao BPSOGSA s\u00e3o detalhados na Tabela 8.</p> Parameters \\(n_u\\) \\(n_y\\) \\(\\ell\\) p-value max_iter n_agents \\(\\alpha\\) \\(G_0\\) Values \\(4\\) \\(4\\) \\(3\\) \\(0.05\\) \\(30\\) \\(10\\) \\(23\\) \\(100\\) &gt;Table 8. Par\u00e2metros usados no MetaMSS <p>Foram realizadas 300 execu\u00e7\u00f5es do algoritmo MetaMSS para cada modelo, com o objetivo de comparar estat\u00edsticas sobre o desempenho do m\u00e9todo. O tempo de execu\u00e7\u00e3o (elapsed time) e a taxa de acerto (correctness), isto \u00e9, a porcentagem de vezes em que a estrutura correta foi selecionada, foram analisados.</p> <p>Os resultados na Tabela 9 foram obtidos com os par\u00e2metros configurados de acordo com a Tabela 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_5\\) \\(S_6\\) Correct model 100\\% 100\\% 100\\% 100\\% 100\\% 100\\% Elapsed time (mean) 5.16s 3.90s 3.40s 2.37s 1.40s 3.80s &gt;Table 9. Desempenho geral do MetaMSS <p>A Tabela 9 mostra que todos os termos dos modelos foram corretamente selecionados usando o MetaMSS. Vale destacar que at\u00e9 mesmo o modelo \\(S_5\\), que possui ru\u00eddo autoregressivo, foi corretamente identificado pelo algoritmo. Esse resultado se deve ao fato de que todos os regressores s\u00e3o avaliados individualmente, e aqueles considerados redundantes s\u00e3o removidos do modelo.</p> <p>A Figura 15 apresenta a converg\u00eancia de cada execu\u00e7\u00e3o do MetaMSS. Observa\u2011se que a maioria das execu\u00e7\u00f5es converge para a estrutura correta com 10 ou menos itera\u00e7\u00f5es. Isso est\u00e1 relacionado ao n\u00famero m\u00e1ximo de itera\u00e7\u00f5es e ao n\u00famero de agentes de busca. O primeiro par\u00e2metro influencia diretamente os coeficientes de acelera\u00e7\u00e3o do algoritmo, que refor\u00e7am a fase de explora\u00e7\u00e3o, enquanto o segundo aumenta o n\u00famero de modelos candidatos avaliados. Intuitivamente, ambos influenciam o tempo de execu\u00e7\u00e3o e, ainda mais importante, a estrutura de modelo selecionada como solu\u00e7\u00e3o final. Assim, escolhas inadequadas desses par\u00e2metros podem levar \u00e0 sele\u00e7\u00e3o de modelos sub ou superparametrizados, uma vez que o algoritmo pode convergir para um \u00f3timo local. A subse\u00e7\u00e3o a seguir apresenta uma an\u00e1lise da influ\u00eancia de <code>max_iter</code> e <code>n_agents</code> no desempenho do algoritmo.</p> <p></p> <p>Figura 15. Converg\u00eancia do MetaMSS para diferentes estruturas de modelo. Cada curva representa a trajet\u00f3ria de converg\u00eancia do algoritmo para uma estrutura espec\u00edfica, de \\(S_1\\) a \\(S_6\\), ao longo de, no m\u00e1ximo, 30 itera\u00e7\u00f5es.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#influencia-dos-parametros-max_iter-e-n_agents","title":"Influ\u00eancia dos par\u00e2metros \\(max\\_iter\\) e \\(n\\_agents\\)","text":"<p>Os modelos simulados s\u00e3o usados para avaliar o desempenho do MetaMSS considerando diferentes configura\u00e7\u00f5es para <code>max_iter</code> e <code>n_agents</code>. Primeiro, fixamos <code>max_iter=30</code> e variamos <code>n_agents</code>. Em seguida, fixamos <code>n_agents</code> e variamos <code>max_iter</code>. Os resultados apresentados nesta subse\u00e7\u00e3o foram obtidos com os demais par\u00e2metros configurados conforme a Tabela 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_5\\) \\(S_6\\) max_iter = 30, n_agents = 1 Correct model \\(65\\%\\) \\(55.66\\%\\) \\(14\\%\\) \\(14\\%\\) \\(7.3\\%\\) \\(20.66\\%\\) Elapsed time (mean) \\(0.26\\)s \\(0.19\\)s \\(0.15\\)s \\(0.11\\)s \\(0.13\\)s \\(0.13\\)s max_iter = 30, n_agents = 5 Correct model \\(100\\%\\) \\(100\\%\\) \\(99\\%\\) \\(98\\%\\) \\(91.66\\%\\) \\(98.33\\%\\) Elapsed time (mean) \\(2.08\\)s \\(1.51\\)s \\(1.41\\)s \\(0.99\\)s \\(0.59\\)s \\(1.13\\)s max_iter = 30, n_agents = 20 Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(12.88\\)s \\(9.10\\)s \\(8.77\\)s \\(5.70\\)s \\(3.37\\)s \\(9.50\\)s max_iter = 5, n_agents = 10 Correct model \\(96.33\\%\\) \\(99\\%\\) \\(86\\%\\) \\(93.66\\%\\) \\(93\\%\\) \\(97.33\\%\\) Elapsed time (mean) \\(0.92\\)s \\(0.73\\)s \\(0.72\\)s \\(0.52\\)s \\(0.29\\)s \\(0.64\\)s max_iter = 15, n_agents = 10 Correct model \\(100\\%\\) \\(100\\%\\) \\(99\\%\\) \\(99\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(2.80\\)s \\(2.33\\)s \\(2.25\\)s \\(1.60\\)s \\(0.90\\)s \\(2.30\\)s max_iter = 50, n_agents = 10 Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(7.38\\)s \\(5.44\\)s \\(4.56\\)s \\(3.01\\)s \\(2.10\\)s \\(4.52\\)s &gt;Table 10. <p>Os resultados agregados da Tabela 10 confirmam o comportamento esperado em rela\u00e7\u00e3o ao tempo de execu\u00e7\u00e3o e \u00e0 taxa de acerto. Ambos aumentam significativamente \u00e0 medida que o n\u00famero de agentes e o n\u00famero m\u00e1ximo de itera\u00e7\u00f5es crescem. O n\u00famero de agentes \u00e9 particularmente relevante, pois amplia a capacidade de explora\u00e7\u00e3o do espa\u00e7o de busca. Todos os sistemas s\u00e3o impactados pelo aumento no n\u00famero de agentes e no m\u00e1ximo de itera\u00e7\u00f5es.</p> <p>Observando todos os sistemas testados, fica claro que uma explora\u00e7\u00e3o mais ampla tem impacto significativo na exatid\u00e3o da sele\u00e7\u00e3o de modelos. Quando poucos agentes s\u00e3o utilizados, o desempenho do MetaMSS se deteriora de forma importante, especialmente para os sistemas \\(S_3\\), \\(S_4\\) e \\(S_5\\). O n\u00famero m\u00e1ximo de itera\u00e7\u00f5es, por sua vez, permite que os agentes explorem, de forma global e local, regi\u00f5es em torno dos modelos candidatos j\u00e1 testados. Assim, quanto maior o n\u00famero de itera\u00e7\u00f5es, mais o algoritmo pode explorar o espa\u00e7o e examinar diferentes conjuntos de regressores.</p> <p>Se esses par\u00e2metros forem escolhidos de forma inadequada, o algoritmo pode n\u00e3o ser capaz de encontrar a estrutura ideal. Nesse sentido, os resultados apresentados aqui se referem apenas aos sistemas analisados. Quanto maior o espa\u00e7o de busca, maior precisar\u00e1 ser o n\u00famero de agentes e o n\u00famero de itera\u00e7\u00f5es. Embora o esfor\u00e7o computacional aumente com valores grandes de <code>n_agents</code> e <code>max_iteration</code>, o algoritmo permanece bastante eficiente em termos de tempo de execu\u00e7\u00e3o para todas as configura\u00e7\u00f5es que garantem a sele\u00e7\u00e3o das estruturas verdadeiras.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#selecao-de-modelos-super-e-subparametrizados","title":"Sele\u00e7\u00e3o de modelos super e sub\u2011parametrizados","text":"<p>Mesmo diante do sucesso na sele\u00e7\u00e3o das estruturas de todos os modelos pelo MetaMSS, \u00e9 natural perguntar como os modelos selecionados diferem do modelo verdadeiro nos casos apresentados na Tabela 10 em que o algoritmo n\u00e3o garantiu 100\\% de acerto. A Figura 16 ilustra a distribui\u00e7\u00e3o do n\u00famero de termos selecionados em cada caso. \u00c9 evidente que o n\u00famero de modelos superparametrizados \u00e9, em geral, maior do que o de modelos subparametrizados. Nos casos em que o n\u00famero de agentes \u00e9 baixo, devido \u00e0 baixa capacidade de explora\u00e7\u00e3o e explota\u00e7\u00e3o, o algoritmo tende a convergir prematuramente, resultando em modelos com muitos regressores esp\u00farios. Em particular, para \\(S_2\\) e \\(S_5\\) com <code>n_agents=1</code>, o algoritmo selecionou modelos com mais de 20 termos. Pode\u2011se argumentar que esse \u00e9 um cen\u00e1rio extremo usado apenas para fins de compara\u00e7\u00e3o, mas a escolha adequada dos par\u00e2metros est\u00e1 intrinsecamente ligada \u00e0 dimens\u00e3o do espa\u00e7o de busca. Para casos com <code>n_agents</code>\\(\\geq 5\\), por exemplo, o n\u00famero de termos esp\u00farios diminui significativamente quando o algoritmo n\u00e3o consegue selecionar o modelo verdadeiro.</p> <p>Al\u00e9m disso, \u00e9 importante destacar a relev\u00e2ncia de uma boa calibra\u00e7\u00e3o dos par\u00e2metros, j\u00e1 que as fases de explora\u00e7\u00e3o e explota\u00e7\u00e3o dependem fortemente deles. Uma converg\u00eancia prematura pode levar \u00e0 sele\u00e7\u00e3o de modelos com o n\u00famero correto de termos, mas com regressores errados. Isso aconteceu em todos os casos com <code>n_agents=1</code>. Por exemplo, para \\(S_3\\), o algoritmo produziu modelos com o n\u00famero correto de termos em 33.33\\% das execu\u00e7\u00f5es, mas a Tabela 10 mostra que apenas 14\\% desses modelos s\u00e3o, de fato, equivalentes \u00e0 estrutura verdadeira.</p> <p></p> <p>Figura 16. Distribui\u00e7\u00e3o do n\u00famero de termos selecionados em cada modelo simulado, considerando diferentes configura\u00e7\u00f5es de <code>max_iter</code> e <code>n_agents</code>.</p> <p>Os sistemas \\(S_1\\), \\(S_2\\), \\(S_3\\), \\(S_4\\) e \\(S_6\\) foram utilizados como benchmark por Bianchi, F., Falsone, A., Prandini, M. and Piroddi, L., o que permite comparar diretamente nossos resultados com aqueles reportados na literatura. Todas as t\u00e9cnicas utilizaram \\(n_y=n_u=4\\) e \\(\\ell = 3\\). O RaMSS e o RaMSS com Conditional Linear Family (C-RaMSS) foram configurados com: \\(K=1\\), \\(\\alpha = 0.997\\), \\(NP = 200\\) e \\(v=0.1\\). O MetaMSS foi ajustado conforme os par\u00e2metros indicados na Tabela 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_6\\) Meta-MSS Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(5.16\\)s \\(3.90\\)s \\(3.40\\)s \\(2.37\\)s \\(3.80\\)s RaMSS- \\(NP=100\\) Correct model \\(90.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(66\\%\\) Elapsed time (mean) \\(3.27\\)s \\(1.24\\)s \\(2.59\\)s \\(1.67\\)s \\(6.66\\)s RaMSS- \\(NP=200\\) Correct model \\(78.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(82\\%\\) Elapsed time (mean) \\(6.25\\)s \\(2.07\\)s \\(4.42\\)s \\(2.77\\)s \\(9.16\\)s C-RaMSS Correct model \\(93.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(18\\)s \\(10.50\\)s \\(16.96\\)s \\(10.56\\)s \\(48.52\\)s &gt; Table 11. An\u00e1lise comparativa entre MetaMSS, RaMSS e C-RaMSS <p>Em termos de taxa de acerto, o MetaMSS supera (ou pelo menos iguala) o RaMSS e o C-RaMSS para todos os sistemas analisados, como mostrado na Tabela 11. Para o sistema \\(S_6\\), por exemplo, a taxa de acerto aumenta em 18\\% em rela\u00e7\u00e3o ao RaMSS, enquanto o tempo de execu\u00e7\u00e3o necess\u00e1rio para que o C-RaMSS atinja 100\\% de acerto \u00e9 1276.84\\% maior do que o do MetaMSS. Al\u00e9m disso, o MetaMSS \u00e9 visivelmente mais eficiente do que o C-RaMSS e possui desempenho computacional semelhante ao RaMSS.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#metamss-vs-frols","title":"MetaMSS vs FROLS","text":"<p>O algoritmo FROLS foi aplicado a todos os sistemas testados, com os resultados resumidos na Tabela 12. O m\u00e9todo foi capaz de selecionar corretamente a estrutura para \\(S_2\\) e \\(S_6\\). No entanto, falhou em identificar dois dos quatro regressores de \\(S_1\\). Para \\(S_3\\), o FROLS incluiu \\(y_{k-1}\\) no lugar do termo correto \\(y_{k-1}^3\\). De forma semelhante, em \\(S_4\\), o termo \\(y_{k-4}\\) foi selecionado em vez de \\(y_{k-2}\\). Para \\(S_5\\), o algoritmo resultou em uma estrutura incorreta ao incluir o termo esp\u00fario \\(y_{k-4}\\).</p> Meta-MSS Regressor Correct FROLS Regressor Correct \\(S_1\\) \\(y_{k-1}\\) yes \\(y_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-2}\\) yes \\(x_{k-4}\\) no \\(S_2\\) \\(y_{k-1}\\) yes \\(y_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}^2\\) yes \\(x_{k-1}^2\\) yes \\(x_{k-1}^3\\) yes \\(x_{k-1}^3\\) yes \\(S_3\\) \\(y_{k-1}^3\\) yes \\(y_{k-1}\\) no \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}\\) yes \\(y_{k-2}\\) yes \\(S_4\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(S_5\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(S_6\\) \\(y_{k-2}\\) yes \\(y_{k-2}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(y_{k-2}x_{k-2}\\) yes \\(y_{k-2}x_{k-1}\\) yes &gt; Table 12. An\u00e1lise comparativa entre MetaMSS e FROLS"},{"location":"pt/book/4-Model-Structure-Selection/#meta-mss-vs-rjmcmc","title":"Meta-MSS vs RJMCMC","text":"<p>O modelo \\(S_4\\) foi tomado do trabalho de Baldacchino, Anderson e Kadirkamanathan (Computational System Identification for Bayesian NARMAX Modelling). No estudo, os lags m\u00e1ximos s\u00e3o \\(n_y = n_u = 4\\) e o grau de n\u00e3o linearidade \u00e9 \\(\\ell = 3\\). Os autores executaram o algoritmo RJMCMC 10 vezes sobre os mesmos dados de entrada e sa\u00edda. O m\u00e9todo RJMCMC identificou a estrutura verdadeira 7 de 10 vezes. Em contraste, o algoritmo MetaMSS identificou a estrutura correta em todas as execu\u00e7\u00f5es. Esses resultados s\u00e3o resumidos na Tabela 13.</p> <p>Al\u00e9m disso, o RJMCMC apresenta algumas desvantagens que s\u00e3o mitigadas pelo MetaMSS. Em particular, o RJMCMC \u00e9 computacionalmente intensivo, exigindo 30.000 itera\u00e7\u00f5es para obter os resultados. Ademais, ele depende de diversas distribui\u00e7\u00f5es de probabilidade para simplificar o processo de estima\u00e7\u00e3o de par\u00e2metros, o que torna as contas mais complexas. J\u00e1 o MetaMSS oferece uma abordagem mais simples e eficiente, evitando esses problemas.</p> Meta-MSS Model Correct RJMCMC Model 1 (\\(7\\times\\)) RJMCMC Model 2 RJMCMC Model 3 RJMCMC Model 4 Correct \\(S_4\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-2}\\) \\(y_{k-2}\\) \\(y_{k-2}\\) \\(y_{k-2}\\) yes \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) yes - - - \\(y_{k-3}x_{k-3}\\) \\(x_{k-4}^2\\) \\(x_{k-1}x_{k-3}^2\\) no &gt; Table 13. An\u00e1lise comparativa entre MetaMSS e RJMCMC."},{"location":"pt/book/4-Model-Structure-Selection/#metamss-usando-sysidentpy","title":"MetaMSS usando SysIdentPy","text":"<p>Considere agora os mesmos dados utilizados na subse\u00e7\u00e3o Overview of the Information Criteria Methods.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\n\nbasis_function = Polynomial(degree=2)\nmodel = MetaMSS(\n    ylag=2,\n    xlag=2,\n    random_state=42,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p>O algoritmo MetaMSS n\u00e3o depende de crit\u00e9rios de informa\u00e7\u00e3o como ERR para sele\u00e7\u00e3o de estrutura de modelo e, por isso, n\u00e3o utiliza esses hiperpar\u00e2metros. O mesmo vale para os algoritmos AOLS e ER. Para mais detalhes sobre o uso desses m\u00e9todos e seus hiperpar\u00e2metros, consulte a documenta\u00e7\u00e3o.</p> <p>No que diz respeito \u00e0 estima\u00e7\u00e3o de par\u00e2metros, o SysIdentPy permite empregar qualquer m\u00e9todo dispon\u00edvel, independentemente do algoritmo de sele\u00e7\u00e3o de estrutura utilizado. Ou seja, o usu\u00e1rio pode combinar m\u00e9todos de estrutura (FROLS, AOLS, ER, MetaMSS, etc.) com diferentes estimadores de par\u00e2metros. Essa flexibilidade permite explorar diversas abordagens de modelagem e personalizar o processo de identifica\u00e7\u00e3o. Embora os exemplos fornecidos utilizem o m\u00e9todo de estima\u00e7\u00e3o padr\u00e3o, o usu\u00e1rio \u00e9 encorajado a testar outras op\u00e7\u00f5es para encontrar a melhor solu\u00e7\u00e3o para o seu problema.</p> <p>Os resultados do MetaMSS s\u00e3o</p> Regressors Parameters ERR y(k-1) 1.8004E-01 0.00000000E+00 x1(k-2) 8.9747E-01 0.00000000E+00 <p></p> <p>Figura 17. Simula\u00e7\u00e3o em regime livre para o modelo ajustado utilizando MetaMSS.</p> <p>O m\u00e9todo <code>results</code> retorna ERR igual a 0 para todos os regressores porque, como mencionado, o algoritmo ERR n\u00e3o \u00e9 executado neste caso.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#accelerated-orthogonal-least-squares-aols-e-entropic-regression-er","title":"Accelerated Orthogonal Least Squares (AOLS) e Entropic Regression (ER)","text":"<p>Al\u00e9m de FROLS e MetaMSS, o SysIdentPy inclui outros dois m\u00e9todos para sele\u00e7\u00e3o de estrutura de modelo: Accelerated Orthogonal Least Squares (AOLS) e Entropic Regression (ER). Nesta se\u00e7\u00e3o n\u00e3o entraremos em detalhes sobre os m\u00e9todos, como fizemos com FROLS e MetaMSS, mas apresentaremos uma vis\u00e3o geral e refer\u00eancias para leitura adicional:</p> <ul> <li>Accelerated Orthogonal Least Squares (AOLS): para uma discuss\u00e3o detalhada sobre AOLS, ver o artigo original aqui.</li> <li>Entropic Regression (ER): detalhes sobre ER podem ser encontrados no artigo original aqui.</li> </ul> <p>A seguir, mostramos como utilizar esses m\u00e9todos no SysIdentPy.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#accelerated-orthogonal-least-squares","title":"Accelerated Orthogonal Least Squares","text":"<pre><code>from sysidentpy.model_structure_selection import AOLS\n\nbasis_function = Polynomial(degree=2)\nmodel = AOLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1542E-01 0.00000000E+00 <p>Figura 18. Simula\u00e7\u00e3o em regime livre para o modelo ajustado com AOLS.</p>"},{"location":"pt/book/4-Model-Structure-Selection/#entropic-regression","title":"Entropic Regression","text":"<pre><code>from sysidentpy.model_structure_selection import ER\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> Regressors Parameters ERR 1 -2.4554E-02 0.00000000E+00 x1(k-2) 9.0273E-01 0.00000000E+00 <p>Figura 19. Simula\u00e7\u00e3o em regime livre para o modelo ajustado com Entropic Regression.</p>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/","title":"5. Estima\u00e7\u00e3o de Par\u00e2metros Multiobjetivo","text":"<p>Multiobjective parameter estimation representa uma mudan\u00e7a de paradigma fundamental na forma como abordamos o problema de ajuste de par\u00e2metros para modelos NARMAX. Em vez de buscar um \u00fanico conjunto de valores de par\u00e2metros que ajuste o modelo de forma \u00f3tima aos dados, abordagens multiobjetivo visam identificar um conjunto de solu\u00e7\u00f5es de par\u00e2metros, conhecido como Pareto front, que fornece um trade-off entre objetivos conflitantes. Esses objetivos frequentemente abrangem um espectro de crit\u00e9rios de desempenho do modelo, como qualidade de ajuste (goodness-of-fit), complexidade do modelo e robustez.</p> <p>O que isso significa? Significa que, quando estamos modelando um sistema din\u00e2mico, na maior parte do tempo estamos construindo modelos que s\u00e3o bons apenas para representar o comportamento din\u00e2mico do sistema em estudo. Isso \u00e9 v\u00e1lido na maioria dos casos porque estamos construindo modelos din\u00e2micos; portanto, se o modelo n\u00e3o tiver um bom desempenho em cen\u00e1rios est\u00e1ticos, isso pode n\u00e3o ser um problema. Entretanto, nem sempre \u00e9 assim, e podemos desejar um modelo que apresente bom desempenho tanto do ponto de vista din\u00e2mico quanto do est\u00e1tico. Nesses casos, m\u00e9todos desenvolvidos apenas para sistemas puramente din\u00e2micos n\u00e3o s\u00e3o adequados e algoritmos multiobjetivo podem nos auxiliar nessa tarefa.</p> <p>A ideia principal na estima\u00e7\u00e3o de par\u00e2metros multiobjetivo \u00e9 a inclus\u00e3o da affine information (informa\u00e7\u00e3o afim). A informa\u00e7\u00e3o afim \u00e9 uma informa\u00e7\u00e3o auxiliar que pode ser definida a priori, como o ganho est\u00e1tico e a fun\u00e7\u00e3o est\u00e1tica do sistema. Formalmente, a informa\u00e7\u00e3o afim pode ser definida como segue:</p> <p>Seja o vetor de par\u00e2metros \\(\\Theta \\in \\mathbb{R}^{n_{\\Theta}}\\), um vetor \\(\\mathrm{v}\\in \\mathbb{R}^p\\) e uma matriz \\(\\mathrm{G}\\in \\mathbb{R}^{n_{\\Theta}\\times p}\\), em que \\(\\mathrm{v}\\) e \\(\\mathrm{G}\\) s\u00e3o assumidos acess\u00edveis. Suponha que \\(\\mathrm{G}\\Theta\\) seja uma estimativa de \\(\\mathrm{v}\\). Ent\u00e3o, \\(\\mathrm{v} = \\mathrm{G}\\Theta + \\xi\\). Logo, \\([\\mathrm{v}, \\mathrm{G}]\\) \u00e9 um par de informa\u00e7\u00e3o afim do sistema.</p>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#multi-objective-optimization-problem","title":"Multi-objective optimization problem","text":"<p>Vamos definir o que \u00e9 um problema multiobjetivo. Dadas \\(m\\) fun\u00e7\u00f5es objetivo</p> \\[ \\begin{equation}     \\mathrm{J}(\\hat{\\Theta}) = [J_1(\\hat{\\Theta}), J_2(\\hat{\\Theta}), \\cdots, J_m(\\hat{\\Theta})]^\\top, \\end{equation}     ag{5.1} \\] <p>em que \\(\\mathrm{J}(\\cdot):\\mathbb{R}^n \\mapsto \\mathbb{R}^m\\), um problema geral de otimiza\u00e7\u00e3o multiobjetivo pode ser escrito como (A. Baykasoglu, S. Owen, e N. Gindy)</p> \\[ \\begin{equation}     \\begin{aligned}          &amp; \\underset{\\Theta}{\\text{minimize}} &amp; &amp; \\mathrm{J}(\\Theta) \\\\          &amp; \\text{subject to} &amp; &amp; \\Theta \\in \\mathrm{S} = \\left\\{\\Theta \\mid \\Theta \\in \\mathrm{A}^n, g_i(\\Theta) \\leq a_i, h_j(\\Theta) = b_j \\right\\}, \\\\          &amp; &amp; &amp; i = 1, \\ldots, m, \\quad j = 1, \\ldots, n     \\end{aligned} \\end{equation}     ag{5.2} \\] <p>em que \\(\\Theta\\) \u00e9 um vetor \\(n\\)-dimensional de vari\u00e1veis de decis\u00e3o, \\(\\mathrm{S}\\) \u00e9 o conjunto de solu\u00e7\u00f5es fact\u00edveis limitado por \\(m\\) restri\u00e7\u00f5es de desigualdade (\\(g_i\\)) e \\(n\\) restri\u00e7\u00f5es de igualdade (\\(h_j\\)), e \\(a_i\\) e \\(b_j\\) s\u00e3o constantes. Para vari\u00e1veis cont\u00ednuas, \\(A = \\mathbb{R}\\), enquanto \\(A\\) cont\u00e9m o conjunto de valores permitidos para vari\u00e1veis discretas.</p> <p>Normalmente, problemas com \\(1 &lt; m &lt; 4\\) s\u00e3o chamados de problemas de otimiza\u00e7\u00e3o multiobjetivo. Quando h\u00e1 mais objetivos (\\(m\\geq 4\\)), eles s\u00e3o chamados de many-objective optimization problems, uma classe emergente de problemas multiobjetivo voltada para a solu\u00e7\u00e3o de tarefas reais complexas e modernas. Mais detalhes podem ser encontrados em (Fleming, P. J., Purshouse, R. C., and Lygoe, R. J., \"Many-Objective Optimization: An Engineering Design Perspective\"), (Li, B., Li, J., Tang, K., and Yao, X., \"A survey on multi-objective evolutionary algorithms for many-objective problems\").</p>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#pareto-optimal-definition-and-pareto-dominance","title":"Pareto Optimal Definition and Pareto Dominance","text":"<p>Considere \\([y^{(1)}, y^{(2)}] \\in \\mathbb{R}^m\\) dois vetores no espa\u00e7o objetivo. Se, e somente se, \\(\\forall i \\in \\{1, \\ldots, m \\}: y_i^{(1)}\\leq y_i^{(2)}\\) e \\(\\exists j \\in \\{1, \\ldots, m \\}: y_j^{(1)} &lt; y_j^{(2)}\\), pode-se dizer que \\(y^{(1)} \\prec y^{(2)}\\) (P. L. Yu, \"Cone convexity, cone extreme points, and non dominated solutions in decision problems with multiobjectives\").</p> <p>O conceito de otimalidade de Pareto \u00e9 geralmente usado para descrever o trade-off entre a minimiza\u00e7\u00e3o de diferentes objetivos. Seguindo a defini\u00e7\u00e3o de Pareto: o \u00f3timo de Pareto \u00e9 qualquer vetor de par\u00e2metros que represente uma solu\u00e7\u00e3o eficiente tal que nenhuma fun\u00e7\u00e3o objetivo possa ser melhorada sem piorar pelo menos uma outra fun\u00e7\u00e3o objetivo; tal vetor ser\u00e1 referido como um Pareto-model.</p> <p>No contexto de identifica\u00e7\u00e3o de sistemas, isso significa encontrar um modelo em que n\u00e3o seja poss\u00edvel obter um melhor desempenho din\u00e2mico sem piorar o desempenho est\u00e1tico.</p> <p>Um conjunto de Pareto hipot\u00e9tico \u00e9 mostrado na Figura 1.</p> <p></p> <p>Figura 1. A figura ilustra o conceito de otimalidade de Pareto, em que cada ponto no espa\u00e7o objetivo representa uma solu\u00e7\u00e3o. A frente de Pareto \u00e9 representada por uma curva, mostrando o trade-off entre dois objetivos conflitantes. Pontos na frente n\u00e3o podem ser melhorados em um dos objetivos sem piorar o outro, destacando o equil\u00edbrio nas solu\u00e7\u00f5es \u00f3timas.</p> <p>Neste caso, assume-se que a estrutura do modelo \u00e9 conhecida e, portanto, existe uma correspond\u00eancia biun\u00edvoca entre cada vetor de par\u00e2metros na solu\u00e7\u00e3o \u00f3tima de Pareto e um modelo (Nepomuceno, E. G., Takahashi, R. H. C., and Aguirre, L. A., \"Multiobjective parameter estimation for non-linear systems: affine information and least-squares formulation\"). Pode-se construir um conjunto de Pareto aplicando o Weighted Sum Method, no qual um conjunto de objetivos \u00e9 escalarizado em um \u00fanico objetivo pela soma de cada objetivo multiplicado por um peso fornecido pelo usu\u00e1rio. Considere</p> \\[ \\begin{equation}     \\mathrm{W} = \\Bigg\\{ w\\mid w \\in \\mathbb{R}^m, w_j\\geq 0 \\quad \\textrm{and} \\quad \\sum^{m}_{j=1}w_j=1 \\Bigg\\} \\end{equation}     ag{5.3} \\] <p>como pesos n\u00e3o negativos. Ent\u00e3o, o problema de otimiza\u00e7\u00e3o convexo pode ser escrito como</p> \\[ \\begin{equation} \\begin{aligned} \\Theta^* &amp;= \\underset{\\Theta}{\\text{argmin}} \\, \\langle w, \\mathrm{J}(\\Theta) \\rangle \\end{aligned} \\end{equation}     ag{5.4} \\] <p>em que \\(w\\) \u00e9 uma combina\u00e7\u00e3o de pesos para as diferentes fun\u00e7\u00f5es objetivo. Portanto, o conjunto de Pareto est\u00e1 associado ao conjunto de realiza\u00e7\u00f5es de \\(w \\in \\mathrm{W}\\). Uma estrat\u00e9gia computacional eficiente em passo \u00fanico foi apresentada em (Nepomuceno, E. G., Takahashi, R. H. C., and Aguirre, L. A., \"Multiobjective parameter estimation for non-linear systems: affine information and least-squares formulation\") para resolver a Equa\u00e7\u00e3o 5.4 por meio de uma formula\u00e7\u00e3o em Least Squares, apresentada na pr\u00f3xima se\u00e7\u00e3o.</p>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#affine-information-least-squares-algorithm","title":"Affine Information Least Squares Algorithm","text":"<p>Considere os \\(m\\) pares de informa\u00e7\u00e3o afim \\([\\mathrm{v}_i \\in \\mathbb{R}^{p_i}, \\mathrm{G}_i \\in \\mathbb{R}^{p_i\\times n}]\\) com \\(i = 1, \\ldots, m\\). Assuma que existe \\(\\mathrm{G}_i\\) de posto coluna completo (full column rank) e seja \\(M\\) um modelo da forma</p> \\[ y = \\Psi\\Theta + \\epsilon.     ag{5.5} \\] <p>Ent\u00e3o, os \\(m\\) pares de informa\u00e7\u00e3o afim podem ser considerados na estima\u00e7\u00e3o de par\u00e2metros resolvendo</p> \\[ \\begin{equation} \\begin{aligned} \\Theta^* &amp;= \\underset{\\Theta}{\\text{argmin}} \\sum_{i=1}^{m} w_i (\\mathrm{v}_i - \\mathrm{G}_i \\Theta)^\\top (\\mathrm{v}_i - \\mathrm{G}_i \\Theta) \\end{aligned} \\end{equation}     ag{5.6} \\] <p>com \\(w = [w_i, \\ldots, w_m]^\\top \\in \\mathrm{W}\\). A solu\u00e7\u00e3o da equa\u00e7\u00e3o acima \u00e9 dada por</p> \\[ \\begin{equation}     \\Theta^* = \\left[\\sum^{m}_{i=1}w_i\\mathrm{G}_i^\\top\\mathrm{G}_i\\right]^{-1}  \\left[\\sum^{m}_{i=1}w_i\\mathrm{G}_i^\\top\\mathrm{v}_i\\right]. \\end{equation}     ag{5.7} \\] <p>Se existir apenas uma informa\u00e7\u00e3o, o problema se reduz \u00e0 solu\u00e7\u00e3o monoobjetivo de Least Squares.</p> <p>Para tornar as coisas mais claras, vamos analisar um estudo de caso detalhado.</p>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#estudo-de-caso-conversor-buck","title":"Estudo de Caso - Conversor Buck","text":"<p>Um conversor Buck \u00e9 um tipo de conversor CC/CC (DC/DC) que reduz a tens\u00e3o (enquanto aumenta a corrente) de sua entrada (fonte de alimenta\u00e7\u00e3o) para sua sa\u00edda (carga). Ele \u00e9 similar a um conversor Boost (elevador) e \u00e9 um tipo de fonte de alimenta\u00e7\u00e3o chaveada (switched-mode power supply, SMPS) que tipicamente cont\u00e9m pelo menos dois semicondutores (um diodo e um transistor, embora conversores Buck modernos substituam o diodo por um segundo transistor usado para retifica\u00e7\u00e3o s\u00edncrona) e pelo menos um elemento de armazenamento de energia, um capacitor, um indutor ou ambos combinados.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.multiobjective_parameter_estimation import AILS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.narmax_tools import set_weights\n</code></pre>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#comportamento-dinamico","title":"Comportamento Din\u00e2mico","text":"<pre><code>df_train = pd.read_csv(r\"datasets/buck_id.csv\")\ndf_valid = pd.read_csv(r\"datasets/buck_valid.csv\")\n\n# Plotting the measured output (identification and validation data)\nplt.figure(1)\nplt.title(\"Output\")\nplt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5)\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Voltage\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Plotting the measured input (identification and validation data)\nplt.figure(2)\nplt.title(\"Input\")\nplt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5)\nplt.ylim(2.1, 2.6)\nplt.ylabel(\"u\")\nplt.xlabel(\"Samples\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#funcao-estatica-do-conversor-buck","title":"Fun\u00e7\u00e3o Est\u00e1tica do Conversor Buck","text":"<p>O duty cycle, representado pelo s\u00edmbolo \\(D\\), \u00e9 definido como a raz\u00e3o entre o tempo em que o sistema permanece ligado (\\(T_{on}\\)) e o tempo total de opera\u00e7\u00e3o do ciclo (\\(T\\)). Matematicamente, pode ser expresso como \\(D=\\frac{T_{on}}{T}\\). O complementar do ciclo de trabalho, representado por \\(D'\\), \u00e9 definido como a raz\u00e3o entre o tempo em que o sistema permanece desligado (\\(T_{off}\\)) e o tempo total de opera\u00e7\u00e3o (\\(T\\)) e pode ser expresso como \\(D'=\\frac{T_{off}}{T}\\).</p> <p>A tens\u00e3o na carga (\\(V_o\\)) est\u00e1 relacionada \u00e0 tens\u00e3o da fonte (\\(V_d\\)) pela equa\u00e7\u00e3o \\(V_o = D\\cdot V_d = (1-D')\\cdot V_d\\). Para este conversor em particular, sabe-se que \\(D\u2032=\\frac{\\bar{u}-1}{3}\\), o que significa que a fun\u00e7\u00e3o est\u00e1tica deste sistema pode ser derivada da teoria como:</p> \\[ V_o = \\frac{4V_d}{3} - \\frac{V_d}{3}\\cdot \\bar{u} \\] <p>Se assumirmos que a tens\u00e3o da fonte \\(V_d\\) \u00e9 igual a 24 V, podemos reescrever a express\u00e3o acima como:</p> \\[ V_o = (4 - \\bar{u})\\cdot 8 \\] <pre><code># Static data\nVd = 24\nUo = np.linspace(0, 4, 50)\nYo = (4 - Uo) * Vd / 3\nUo = Uo.reshape(-1, 1)\nYo = Yo.reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Curve\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{y}$\")\nplt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\")\nplt.show()\n</code></pre> <p></p>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#ganho-estatico-do-conversor-buck","title":"Ganho Est\u00e1tico do Conversor Buck","text":"<p>O ganho de um conversor Buck \u00e9 uma medida de como sua tens\u00e3o de sa\u00edda varia em resposta a altera\u00e7\u00f5es na tens\u00e3o de entrada. Matematicamente, o ganho pode ser calculado como a derivada da fun\u00e7\u00e3o est\u00e1tica do conversor, que descreve a rela\u00e7\u00e3o entre as tens\u00f5es de entrada e sa\u00edda.</p> <p>Neste caso, a fun\u00e7\u00e3o est\u00e1tica do conversor Buck \u00e9 dada por</p> \\[ V_o = (4 - \\bar{u})\\cdot 8 \\] <p>Derivando essa equa\u00e7\u00e3o em rela\u00e7\u00e3o a \\(\\hat{u}\\), obtemos que o ganho do conversor Buck \u00e9 igual a \u22128. Em outras palavras, para cada unidade de aumento na tens\u00e3o de entrada \\(\\hat{u}\\), a tens\u00e3o de sa\u00edda \\(V_o\\) diminui em 8 unidades. Assim,</p> \\[ gain=V_o'=-8 \\] <pre><code># Defining the gain\ngain = -8 * np.ones(len(Uo)).reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Gain\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{gain}$\")\nplt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#construindo-um-modelo-dinamico-usando-a-abordagem-mono-objetivo","title":"Construindo um modelo din\u00e2mico usando a abordagem mono-objetivo","text":"<pre><code>x_train = df_train.input.values.reshape(-1, 1)\ny_train = df_train.y.values.reshape(-1, 1)\nx_valid = df_valid.input.values.reshape(-1, 1)\ny_valid = df_valid.y.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=8,\n    extended_least_squares=False,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=\"least_squares\",\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#affine-information-least-squares-algorithm-ails","title":"Affine Information Least Squares Algorithm (AILS)","text":"<p>AILS \u00e9 um algoritmo de estima\u00e7\u00e3o de par\u00e2metros multiobjetivo, baseado em um conjunto de pares de informa\u00e7\u00e3o afim. A abordagem multiobjetivo proposta no artigo citado e implementada em SysIdentPy leva a um problema de otimiza\u00e7\u00e3o multiobjetivo convexo, que pode ser resolvido por AILS. AILS \u00e9 um esquema n\u00e3o iterativo do tipo Least Squares para encontrar as solu\u00e7\u00f5es do conjunto de Pareto (Pareto-set solutions) para o problema multiobjetivo.</p> <p>Assim, com a estrutura do modelo definida (neste exemplo, usaremos a obtida a partir dos dados din\u00e2micos anteriores), podemos estimar os par\u00e2metros usando a abordagem multiobjetivo.</p> <p>As informa\u00e7\u00f5es sobre a fun\u00e7\u00e3o est\u00e1tica e o ganho est\u00e1tico, al\u00e9m dos dados din\u00e2micos usuais de entrada/sa\u00edda, podem ser usadas para construir o par de informa\u00e7\u00e3o afim utilizado na estima\u00e7\u00e3o dos par\u00e2metros do modelo. Podemos modelar a fun\u00e7\u00e3o de custo como:</p> \\[ \\gamma(\\hat\\theta) = w_1\\cdot J_{LS}(\\hat{\\theta})+w_2\\cdot J_{SF}(\\hat{\\theta})+w_3\\cdot J_{SG}(\\hat{\\theta}) \\]"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#estimacao-de-parametros-multiobjetivo-considerando-3-objetivos-diferentes-o-erro-de-predicao-a-funcao-estatica-e-o-ganho-estatico","title":"Estima\u00e7\u00e3o de par\u00e2metros multiobjetivo considerando 3 objetivos diferentes: o erro de predi\u00e7\u00e3o, a fun\u00e7\u00e3o est\u00e1tica e o ganho est\u00e1tico","text":"<pre><code># you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work\nmo_estimator = AILS(final_model=model.final_model)\n# setting the log-spaced weights of each objective function\nw = set_weights(static_function=True, static_gain=True)\n# you can also use something like\n# w = np.array(\n#       [\n#           [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01],\n#           [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01],\n#           [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98],\n#       ]\n# )\n\n# to set the weights. Each row correspond to each objective\n</code></pre> <p>AILS possui um m\u00e9todo <code>estimate</code> que retorna as fun\u00e7\u00f5es de custo (J), a norma euclidiana das fun\u00e7\u00f5es de custo (E), os par\u00e2metros estimados associados a cada vetor de pesos (theta), a matriz de regressores associada ao ganho est\u00e1tico (HR) e \u00e0 fun\u00e7\u00e3o est\u00e1tica (QR), respectivamente.</p> <pre><code>J, E, theta, HR, QR, position = mo_estimator.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf \\(\\lVert J \\rVert\\) 0.006842 0.003078 0.990080 0.999970 1.095020e-05 0.000013 0.245244 0.007573 0.002347 0.990080 0.999938 2.294665e-05 0.000016 0.245236 0.008382 0.001538 0.990080 0.999885 6.504913e-05 0.000018 0.245223 0.009277 0.000642 0.990080 0.999717 4.505541e-04 0.000021 0.245182 0.006842 0.098663 0.894495 1.000000 7.393246e-08 0.000015 0.245251 ... ... ... ... ... ... ... 0.659632 0.333527 0.006842 0.995896 3.965699e-04 1.000000 0.244489 0.730119 0.263039 0.006842 0.995632 5.602981e-04 0.972842 0.244412 0.808139 0.185020 0.006842 0.995364 8.321071e-04 0.868299 0.244300 0.894495 0.098663 0.006842 0.995100 1.364999e-03 0.660486 0.244160 0.990080 0.003078 0.006842 0.992584 9.825987e-02 0.305492 0.261455 <p>Agora podemos definir \\(\\theta\\) associado a qualquer combina\u00e7\u00e3o de pesos desejada.</p> <pre><code>model.theta = theta[-1, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the last combination of the weights\n\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\n\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 1 2.2930E+00 9.999E-01 y(k-1) 2.3307E-01 2.042E-05 y(k-2) 6.3209E-01 1.108E-06 x1(k-1) -5.9333E-01 4.688E-06 y(k-1)^2 2.7673E-01 3.922E-07 y(k-2)y(k-1) -5.3228E-01 8.389E-07 x1(k-1)y(k-1) 1.6667E-02 5.690E-07 y(k-2)^2 2.5766E-01 3.827E-06"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#os-resultados-dinamicos-para-o-theta-escolhido-sao","title":"Os resultados din\u00e2micos para o theta escolhido s\u00e3o","text":"<pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#o-resultado-do-ganho-estatico-e","title":"O resultado do ganho est\u00e1tico \u00e9","text":"<pre><code>plt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#o-resultado-da-funcao-estatica-e","title":"O resultado da fun\u00e7\u00e3o est\u00e1tica \u00e9","text":"<pre><code>plt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \\u200b\\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#obtendo-a-melhor-combinacao-de-pesos-com-base-na-norma-da-funcao-de-custo","title":"Obtendo a melhor combina\u00e7\u00e3o de pesos com base na norma da fun\u00e7\u00e3o de custo","text":"<p>A vari\u00e1vel <code>position</code> retornada pelo m\u00e9todo <code>estimate</code> fornece a posi\u00e7\u00e3o da melhor combina\u00e7\u00e3o de pesos. A estrutura do modelo \u00e9 exatamente a mesma, mas a ordem dos regressores \u00e9 alterada no m\u00e9todo <code>estimate</code>. Por isso \u00e9 necess\u00e1rio atualizar <code>model.final_model</code>. Os resultados din\u00e2mico, de ganho est\u00e1tico e da fun\u00e7\u00e3o est\u00e1tica para o \\(\\theta\\) escolhido s\u00e3o mostrados a seguir.</p> <pre><code>model.theta = theta[position, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the best combination of the weights\n\n# changing the model.final_model\n\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\n# dynamic results\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\n# static gain\nplt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n\n# static function\nplt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \\u200b\\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> Regressors Parameters ERR 1 1.5405E+00 9.999E-01 y(k-1) 2.9687E-01 2.042E-05 y(k-2) 6.4693E-01 1.108E-06 x1(k-1) -4.1302E-01 4.688E-06 y(k-1)^2 2.7671E-01 3.922E-07 y(k-2)y(k-1) -5.3474E-01 8.389E-07 x1(k-1)y(k-1) 4.0624E-03 5.690E-07 y(k-2)^2 2.5832E-01 3.827E-06 <p></p> <p></p> <p></p> <p>Voc\u00ea tamb\u00e9m pode plotar as solu\u00e7\u00f5es do conjunto de Pareto</p> <pre><code>plt.figure(6)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Pareto-set solutions\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#detalhando-o-ails","title":"Detalhando o AILS","text":"<p>O modelo polinomial NARX constru\u00eddo usando a abordagem mono-objetivo possui a seguinte estrutura:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>Assim, o objetivo ao usar as informa\u00e7\u00f5es da fun\u00e7\u00e3o est\u00e1tica e do ganho est\u00e1tico no cen\u00e1rio multiobjetivo \u00e9 estimar o vetor \\(\\hat{\\theta}\\) com base em:</p> \\[         heta = [w_1\\Psi^T\\Psi + w_2(HR)^T(HR) + w_3(QR)(QR)^T]^{-1} [w_1\\Psi^T y + w_2(HR)^T\\overline{g}+w_3(QR)^T\\overline{y}] \\] <p>A matriz \\(\\Psi\\) \u00e9 constru\u00edda usando a abordagem usual de modelagem din\u00e2mica mono-objetivo no SysIdentPy. No entanto, ainda \u00e9 necess\u00e1rio encontrar as matrizes Q, H e R. O AILS possui m\u00e9todos para calcular todas essas matrizes. Basicamente, para isso, \\(q_i^T\\) \u00e9 primeiro estimado:$$ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_1} &amp; \\overline{y_i}^2 &amp; \\cdots &amp; \\overline{y_i}^l &amp; F_{yu} &amp; \\overline{u_i}^2 &amp; \\cdots &amp; \\overline{u_i}^l \\end{bmatrix} $$</p> <p>onde \\(F_{yu}\\) representa todos os mon\u00f4mios n\u00e3o lineares no modelo que est\u00e3o relacionados a \\(y(k)\\) e \\(u(k)\\), \\(l\\) \u00e9 a maior n\u00e3o linearidade no modelo para termos de entrada e sa\u00edda. Para um modelo com grau de n\u00e3o linearidade igual a 2, podemos obter:</p> \\[ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_i} &amp; \\overline{y_i}^2 &amp; \\overline{u_i}\\:\\overline{y_i} &amp; \\overline{u_i}^2 \\end{bmatrix} \\] <p>\u00c9 poss\u00edvel codificar a matriz \\(q_i^T\\) de forma que ela siga a codifica\u00e7\u00e3o do modelo definida no SysIdentPy. Para isso, 0 \u00e9 considerado como uma constante, \\(y_i\\) igual a 1 e \\(u_i\\) igual a 2. O n\u00famero de colunas indica o grau de n\u00e3o linearidade do sistema e o n\u00famero de linhas reflete o n\u00famero de termos:</p> \\[ q_i = \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\ 2 &amp; 2\\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y_i}\\\\ \\overline{u_i}\\\\ \\overline{y_i}^2\\\\ \\overline{u_i}\\:\\overline{y_i}\\\\ \\overline{u_i}^2\\\\ \\end{bmatrix} \\] <p>Finally, the result can be easily obtained using the \u2018regressor_space\u2019 method of SysIdentPy</p> <pre><code>from sysidentpy.narmax_base import RegressorDictionary\n\nobject_qit = RegressorDictionary(xlag=1, ylag=1)\nR_example = object_qit.regressor_space(n_inputs=1) // 1000\nprint(f\"R = {R_example}\")\n</code></pre> \\[ R = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ 2 &amp; 2 \\end{bmatrix} \\] <p>de modo que:</p> \\[ \\overline{y_i} = q_i^T R\\theta \\] <p>e:</p> \\[ \\overline{g_i} = H R\\theta \\] <p>onde \\(R\\) \u00e9 o mapeamento linear dos regressores est\u00e1ticos representados por \\(q_i^T\\). Al\u00e9m disso, a matriz \\(H\\) cont\u00e9m informa\u00e7\u00e3o afim referente a \\(\\overline{g_i}\\), que \u00e9 igual a \\(\\overline{g_i} = \\frac{d\\overline{y}}{d\\overline{u}}{\\big |}_{(\\overline{u_i}\\:\\overline{y_i})}\\).</p> <p>A partir de agora, come\u00e7aremos a aplicar a estima\u00e7\u00e3o de par\u00e2metros de forma multiobjetivo. Isso ser\u00e1 feito tendo em mente o modelo polinomial NARX do conversor BUCK. Neste contexto, \\(q_i^T\\) ser\u00e1 gen\u00e9rico e assumir\u00e1 um formato espec\u00edfico para o problema em quest\u00e3o. Para esta tarefa, ser\u00e1 utilizado o m\u00e9todo <code>build_linear_mapping</code>, cujo objetivo \u00e9 retornar o \\(q_i^T\\) relacionado ao modelo e a matriz do mapeamento linear \\(R\\):</p> <pre><code>R, qit = mo_estimator.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> \\[ R = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} \\] <p>and</p> \\[ qit = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>Portanto</p> \\[ q_i = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y} \\\\ \\overline{u} \\\\ \\overline{y^2} \\\\ \\overline{u} \\cdot \\overline{y} \\\\ \\end{bmatrix} \\] <p>Voc\u00ea pode notar que o m\u00e9todo produz sa\u00eddas consistentes com o esperado:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>e:</p> \\[ R = \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ \\overline{y^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\\\ \\overline{y}\\:\\overline{u} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\end{bmatrix} \\]"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#validacao","title":"Valida\u00e7\u00e3o","text":"<p>A seguinte estrutura de modelo ser\u00e1 usada para validar a abordagem:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[     herefore \\] \\[ final\\_model = \\begin{bmatrix} 1001 &amp; 0\\\\ 1002 &amp; 0\\\\ 0 &amp; 0\\\\ 2001 &amp; 0\\\\ 2001 &amp; 2001\\\\ 2002 &amp; 2001\\\\ 2002 &amp; 0\\\\ 2002 &amp; 2002 \\end{bmatrix} \\] <p>definindo em c\u00f3digo:</p> <pre><code>final_model = np.array(\n    [\n        [1001, 0],\n        [1002, 0],\n        [0, 0],\n        [2001, 0],\n        [2001, 2001],\n        [2002, 2001],\n        [2002, 0],\n        [2002, 2002],\n    ]\n)\nfinal_model\n</code></pre> 1001 0 1002 0 0 0 2001 0 2001 2001 2002 2001 2002 0 2002 2002 <pre><code>mult2 = AILS(final_model=final_model)\n\ndef psi(X, Y):\n    PSI = np.zeros((len(X), 8))\n    for k in range(2, len(Y)):\n        PSI[k, 0] = Y[k - 1]\n        PSI[k, 1] = Y[k - 2]\n        PSI[k, 2] = 1\n        PSI[k, 3] = X[k - 1]\n        PSI[k, 4] = X[k - 1] ** 2\n        PSI[k, 5] = X[k - 2] * X[k - 1]\n        PSI[k, 6] = X[k - 2]\n        PSI[k, 7] = X[k - 2] ** 2\n    return np.delete(PSI, [0, 1], axis=0)\n</code></pre> <p>O valor de theta com o menor erro quadr\u00e1tico m\u00e9dio obtido com o mesmo c\u00f3digo implementado em Scilab foi:</p> \\[ W_{LS} = 0.3612343 \\] <p>e:</p> \\[ W_{SG} = 0.3548699 \\] <p>e:</p> \\[ W_{SF} = 0.3548699 \\] <pre><code>PSI = psi(x_train, y_train)\nw = np.array([[0.3612343], [0.2838959], [0.3548699]])\nJ, E, theta, HR, QR, position = mult2.estimate(\n    y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\n\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf \\(\\lVert J \\rVert\\) 0.361234 0.35487 0.283896 1.0 1.0 1.0 1.0 A ordem dos pesos \u00e9 diferente devido \u00e0 forma como implementamos em Python, mas os resultados s\u00e3o muito pr\u00f3ximos, como esperado."},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#resultados-dinamicos","title":"Resultados din\u00e2micos","text":"<pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = mult2.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 1 1.4287E+00 9.999E-01 y(k-1) 5.5147E-01 2.042E-05 y(k-2) 4.0449E-01 1.108E-06 x1(k-1) -1.2605E+01 4.688E-06 x1(k-2) 1.2257E+01 3.922E-07 x1(k-1)^2 8.3274E+00 8.389E-07 x1(k-2)x1(k-1) -1.1416E+01 5.690E-07 x1(k-2)^2 3.0846E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#ganho-estatico","title":"Ganho est\u00e1tico","text":"<pre><code>plt.figure(7)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\n\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#funcao-estatica","title":"Fun\u00e7\u00e3o est\u00e1tica","text":"<pre><code>plt.figure(8)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \\u200b\\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#solucoes-do-conjunto-de-pareto","title":"Solu\u00e7\u00f5es do conjunto de Pareto","text":"<pre><code>plt.figure(9)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Optimum pareto-curve\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p>A tabela a seguir mostra os resultados reportados em <code>IniciacaoCientifica2007</code> e os obtidos com a implementa\u00e7\u00e3o do SysIdentPy</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.5514725 0.549144 \\(\\theta_2\\) 0.40449005 0.408028 \\(\\theta_3\\) 1.42867821 1.45097 \\(\\theta_4\\) -12.60548863 -12.55788 \\(\\theta_5\\) 8.32740057 8.1516315 \\(\\theta_6\\) -11.41574116 -11.09728 \\(\\theta_7\\) 12.25729955 12.215782 \\(\\theta_8\\) 3.08461195 2.9319577 <p>onde:</p> \\[ E_{Scilab} = \u00a0 \u00a017.426613 \\] <p>e:</p> \\[ E_{Python} = 17.474865 \\] <p>Nota: como mencionado anteriormente, a ordem dos regressores no modelo muda, mas \u00e9 a mesma estrutura. As tabelas mostram o respectivo par\u00e2metro do regressor referente ao <code>SysIdentPy</code> e <code>IniciacaoCientifica2007</code>, mas a ordem \\(\\Theta_1\\), \\(\\Theta_2\\) e assim por diante n\u00e3o \u00e9 a mesma dos valores em <code>model.final_model</code></p> <pre><code>R, qit = mult2.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> \\[ R = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>and</p> \\[ qit = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 2 \\\\ \\end{bmatrix} \\] <p>estrutura do modelo que ser\u00e1 utilizada (<code>IniciacaoCientifica2007</code>):</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[ q_i = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 2 &amp; 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y} \\\\ \\overline{u} \\\\ \\overline{u^2} \\end{bmatrix} \\]"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#otimizacao-biobjetivo","title":"Otimiza\u00e7\u00e3o biobjetivo","text":""},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#um-caso-de-uso-aplicado-ao-conversor-buck-cc-cc-usando-como-objetivos-a-informacao-da-curva-estatica-e-o-erro-de-predicao-dinamico","title":"Um caso de uso aplicado ao conversor Buck CC-CC usando como objetivos a informa\u00e7\u00e3o da curva est\u00e1tica e o erro de predi\u00e7\u00e3o (din\u00e2mico)","text":"<pre><code>bi_objective = AILS(\n    static_function=True, static_gain=False, final_model=final_model, normalize=True\n)\n</code></pre> <p>o valor de theta com o menor erro quadr\u00e1tico m\u00e9dio obtido atrav\u00e9s da rotina em Scilab foi:</p> \\[ W_{LS} = 0.9931126 \\] <p>e:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective.estimate(\n    y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\n\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\n\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg \\(\\lVert J \\rVert\\) 0.990080 0.009920 0.990863 1.000000 0.990939 0.987127 0.012873 0.990865 0.987032 0.990939 0.984182 0.015818 0.990867 0.974307 0.990939 0.981247 0.018753 0.990870 0.961803 0.990940 0.978320 0.021680 0.990873 0.949509 0.990941 ... ... ... ... ... 0.002555 0.997445 0.999993 0.000072 0.999993 0.002547 0.997453 0.999994 0.000072 0.999994 0.002540 0.997460 0.999996 0.000071 0.999996 0.002532 0.997468 0.999998 0.000071 0.999998 0.002525 0.997475 1.000000 0.000070 1.000000 <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nr\n</code></pre> Regressors Parameters ERR 0 1 1.3873E+00 9.999E-01 1 y(k-1) 5.4941E-01 2.042E-05 2 y(k-2) 4.0804E-01 1.108E-06 3 x1(k-1) -1.2515E+01 4.688E-06 4 x1(k-2) 1.2227E+01 3.922E-07 5 x1(k-1)^2 8.1171E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1047E+01 5.690E-07 7 x1(k-2)^2 2.9043E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(10)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Static Curve Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>onde o melhor \\(\\Theta\\) estimado \u00e9</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54940883 0.5494135 \\(\\theta_2\\) 0.40803995 0.4080312 \\(\\theta_3\\) 1.38725684 3.3857601 \\(\\theta_4\\) -12.51466378 -12.513688 \\(\\theta_5\\) 8.11712897 8.116575 \\(\\theta_6\\) -11.04664789 -11.04592 \\(\\theta_7\\) 12.22693907 12.227184 \\(\\theta_8\\) 2.90425844 2.9038468 <p>onde:</p> \\[ E_{Scilab} = 17.408934 \\] <p>e:</p> \\[ E_{Python} = 17.408947 \\]"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#estimacao-de-parametros-multiobjetivo","title":"Estima\u00e7\u00e3o de par\u00e2metros multiobjetivo","text":""},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#caso-de-uso-considerando-2-objetivos-diferentes-o-erro-de-predicao-e-o-ganho-estatico","title":"Caso de uso considerando 2 objetivos diferentes: o erro de predi\u00e7\u00e3o e o ganho est\u00e1tico","text":"<pre><code>bi_objective_gain = AILS(\n    static_function=False, static_gain=True, final_model=final_model, normalize=False\n)\n</code></pre> <p>o valor de theta com o menor erro quadr\u00e1tico m\u00e9dio obtido atrav\u00e9s da rotina em Scilab foi:</p> \\[ W_{LS} = 0.9931126 \\] <p>e:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(0, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective_gain.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\n\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\n\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg \\(\\lVert J \\rVert\\) 1.000000 0.000000 17.407256 3.579461e+01 39.802849 0.997012 0.002988 17.407528 2.109260e-01 17.408806 0.994033 0.005967 17.407540 2.082067e-01 17.408785 0.991063 0.008937 17.407559 2.056636e-01 17.408774 0.988102 0.011898 17.407585 2.031788e-01 17.408771 ... ... ... ... ... 0.002555 0.997445 17.511596 3.340081e-07 17.511596 0.002547 0.997453 17.511596 3.320125e-07 17.511596 0.002540 0.997460 17.511597 3.300289e-07 17.511597 0.002532 0.997468 17.511598 3.280571e-07 17.511598 0.002525 0.997475 17.511599 3.260972e-07 17.511599 <pre><code># Writing the results\nmodel.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective_gain.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4853E+00 9.999E-01 1 y(k-1) 5.4940E-01 2.042E-05 2 y(k-2) 4.0806E-01 1.108E-06 3 x1(k-1) -1.2581E+01 4.688E-06 4 x1(k-2) 1.2210E+01 3.922E-07 5 x1(k-1)^2 8.1686E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1122E+01 5.690E-07 7 x1(k-2)^2 2.9455E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(12)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\n\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Gain Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>sendo o \\(\\theta\\) selecionado:</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54939785 0.54937289 \\(\\theta_2\\) 0.40805603 0.40810168 \\(\\theta_3\\) 1.48525190 1.48663719 \\(\\theta_4\\) -12.58066084 -12.58127183 \\(\\theta_5\\) 8.16862622 8.16780294 \\(\\theta_6\\) -11.12171897 -11.11998621 \\(\\theta_7\\) 12.20954849 12.20927355 \\(\\theta_8\\) 2.94548501 2.9446532 <p>onde:</p> \\[ E_{Scilab} =  17.408997 \\] <p>e:</p> \\[ E_{Python} = 17.408781 \\]"},{"location":"pt/book/5-Multiobjective-Parameter-Estimation/#informacoes-adicionais","title":"Informa\u00e7\u00f5es Adicionais","text":"<p>Voc\u00ea tamb\u00e9m pode acessar as matrizes Q e H usando os seguintes m\u00e9todos</p> <p>Matriz Q:</p> <pre><code>bi_objective_gain.build_static_function_information(Uo, Yo)[1]\n</code></pre> <p>Matriz H+R:</p> <pre><code>bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1]\n</code></pre>"},{"location":"pt/book/6-Multiobjective-Model-Structure-Selection/","title":"6. Sele\u00e7\u00e3o de Estrutura de Modelo Multiobjetivo","text":"<p>Coming soon</p>"},{"location":"pt/book/7-NARX-Neural-Network/","title":"7. Rede Neural NARX","text":"<p>Coming soon</p>"},{"location":"pt/book/8-Severely-Nonlinear-System-Identification/","title":"8. Identifica\u00e7\u00e3o de Sistemas Severamente N\u00e3o Lineares","text":"<p>At\u00e9 agora, categorizamos os sistemas em duas classes distintas: sistemas lineares e sistemas n\u00e3o lineares. Como mencionado, sistemas lineares foram extensivamente estudados, com diversos m\u00e9todos bem estabelecidos dispon\u00edveis, enquanto sistemas n\u00e3o lineares \u00e9 uma \u00e1rea muito ativa, com v\u00e1rios problemas ainda em aberto para pesquisa. Al\u00e9m de sistemas lineares e n\u00e3o lineares, existem os chamados Sistemas Severamente N\u00e3o Lineares (Severely Nonlinear Systems). Sistemas Severamente N\u00e3o Lineares s\u00e3o aqueles que exibem comportamentos din\u00e2micos altamente complexos e ex\u00f3ticos, como sub-harm\u00f4nicos, comportamento ca\u00f3tico e histerese. Por enquanto, focaremos em sistemas com histerese.</p>"},{"location":"pt/book/8-Severely-Nonlinear-System-Identification/#modelagem-de-histerese-com-modelos-polinomiais-narx","title":"Modelagem de Histerese com Modelos Polinomiais NARX","text":"<p>A n\u00e3o linearidade de histerese \u00e9 um comportamento severamente n\u00e3o linear comumente encontrado em dispositivos eletromagn\u00e9ticos, sensores, semicondutores, materiais inteligentes, entre outros, que possuem efeitos de mem\u00f3ria entre entrada e sa\u00edda quase-est\u00e1ticas (Visintin, A., \"Differential Models of Hysteresis\"), (Ahmad, I., \"Two Degree-of-Freedom Robust Digital Controller Design With Bouc-Wen Hysteresis Compensator for Piezoelectric Positioning Stage\"). Um sistema hister\u00e9tico \u00e9 aquele que exibe um comportamento dependente do caminho, o que significa que sua resposta depende n\u00e3o apenas de seu estado atual, mas tamb\u00e9m de seu hist\u00f3rico. Em um sistema hister\u00e9tico, quando voc\u00ea aplica uma entrada, a resposta do sistema (como deslocamento ou tens\u00e3o) n\u00e3o segue o mesmo caminho de volta ao ponto de partida quando voc\u00ea remove a entrada. Em vez disso, ela forma um padr\u00e3o em formato de la\u00e7o chamado hysteresis loop. Isso ocorre porque o sistema possui a capacidade de preservar uma deforma\u00e7\u00e3o causada por uma entrada, caracterizando um efeito de mem\u00f3ria.</p> <p>A identifica\u00e7\u00e3o de sistemas hister\u00e9ticos utilizando modelos polinomiais NARX \u00e9 tipicamente uma tarefa intrigante, pois os algoritmos tradicionais de Sele\u00e7\u00e3o de Estrutura de Modelo (Model Structure Selection) n\u00e3o funcionam adequadamente (Martins, S. A. M. and Aguirre, L. A., \"Sufficient conditions for rate-independent hysteresis in autoregressive identified models\", Leva, A. and Piroddi, L., \"NARX-based technique for the modelling of magneto-rheological damping devices\"). Martins, S. A. M. and Aguirre, L. A. apresentaram as condi\u00e7\u00f5es suficientes para descrever histerese usando modelos polinomiais, fornecendo o conceito de estrutura limitante (bounding structure) \\(\\mathcal{H}\\). Modelos polinomiais NARX com um \u00fanico equil\u00edbrio podem ser usados para uma caracteriza\u00e7\u00e3o completa do comportamento de histerese adotando o conceito de estrutura limitante.</p> <p>A seguir, s\u00e3o apresentados alguns dos conceitos essenciais e defini\u00e7\u00f5es formais para entender como modelos NARX podem ser usados para descrever sistemas com histerese.</p>"},{"location":"pt/book/8-Severely-Nonlinear-System-Identification/#sinal-quase-estatico-de-carregamento-descarregamento-em-tempo-continuo","title":"Sinal quase-est\u00e1tico de carregamento-descarregamento em tempo cont\u00ednuo","text":"<p>Uma caracter\u00edstica importante para modelar sistemas hister\u00e9ticos \u00e9 o sinal de entrada. Um sinal quase-est\u00e1tico de carregamento-descarregamento (loading-unloading quasi-static signal) \u00e9 um sinal peri\u00f3dico em tempo cont\u00ednuo \\(x_t\\) com per\u00edodo \\(T = (t_f - t_i)\\) e frequ\u00eancia \\(\\omega = 2\\pi f\\), onde \\(x_t\\) aumenta monotonicamente de \\(x_{min}\\) para \\(x_{max}\\), considerando \\(t_i \\leq t \\leq t_m\\) (carregamento) e diminui monotonicamente de \\(x_{max}\\) para \\(x_{min}\\), considerando \\(t_m \\leq t \\leq t_f\\) (descarregamento). Se o sinal de carregamento-descarregamento varia com \\(\\omega \\rightarrow 0\\), o sinal tamb\u00e9m \u00e9 chamado de sinal quase-est\u00e1tico. Visualmente, isso \u00e9 muito mais simples de entender. A imagem a seguir mostra um sinal quase-est\u00e1tico de carregamento-descarregamento em tempo cont\u00ednuo.</p> <p></p> <p>Figura 1. Sinal quase-est\u00e1tico de carregamento-descarregamento em tempo cont\u00ednuo, demonstrando o aumento e diminui\u00e7\u00e3o peri\u00f3dicos do sinal de entrada.</p> <p>Nesse sentido, Martins, S. A. M. and Aguirre, L. A. tamb\u00e9m apresentaram a ideia de transformar as entradas do sistema usando fun\u00e7\u00f5es multivaloradas.</p> <p>Fun\u00e7\u00f5es multivaloradas - Seja \\(\\phi (\\Delta x_{k}): \\mathbb{R} \\rightarrow \\mathbb{R}\\). Se \\(\\Delta x_{k}=x_k-x_{k-1}\\), \\(\\phi (\\Delta x_{k})\\) \u00e9 uma fun\u00e7\u00e3o multivalorada se:</p> \\[ \\begin{equation}     \\phi (\\Delta x_{k})=     \\begin{cases}         \\phi_1, &amp; se \\ \\Delta x_{k} &gt; \\epsilon; \\\\         \\phi_2, &amp; se \\ \\Delta x_{k} &lt; \\epsilon; \\\\         \\phi_3, &amp; se \\ \\Delta x_{k} = \\epsilon; \\\\     \\end{cases} \\end{equation} \\tag{1} \\] <p>onde \\(\\epsilon \\in \\mathbb{R}\\), \\(\\phi_1 \\neq \\phi_2 \\neq \\phi_3\\). Para algumas entradas \\(\\Delta x_{k}\\neq \\epsilon, \\ \\forall{k} \\in \\mathbb{N}\\), e o \u00faltimo valor na equa\u00e7\u00e3o acima n\u00e3o \u00e9 utilizado.</p> <p>Uma fun\u00e7\u00e3o multivalorada frequentemente usada \u00e9 a sign\\((\\cdot): \\mathbb{R} \\rightarrow \\mathbb{R}\\):</p> \\[  \\begin{equation}  sign(x)=     \\begin{cases}         1, &amp; se \\ x &gt; 0; \\\\         -1, &amp; se \\ x &lt; 0; \\\\         0, &amp; se \\ x = 0. \\\\     \\end{cases} \\end{equation} \\tag{2} \\]"},{"location":"pt/book/8-Severely-Nonlinear-System-Identification/#hysteresis-loops-em-tempo-continuo-mathcalh_tomega","title":"Hysteresis loops em tempo cont\u00ednuo \\(\\mathcal{H}_t(\\omega)\\)","text":"<p>Seja \\(x_t\\) um sinal quase-est\u00e1tico de carregamento-descarregamento em tempo cont\u00ednuo aplicado a um sistema em tempo cont\u00ednuo e \\(y_t\\) a sa\u00edda do sistema. \\(\\mathcal{H}_t(\\omega)\\) denota um la\u00e7o fechado no plano \\(x_t - y_t\\), cuja forma depende de \\(\\omega\\). Se o sistema apresenta n\u00e3o linearidade hister\u00e9tica, \\(\\mathcal{H}_t(\\omega)\\) \u00e9 denotado como:</p> \\[ \\begin{equation} \\mathcal{H}_t(\\omega) =     \\begin{cases}         \\mathcal{H}_t(\\omega)^{+}, \\ para \\ t_i \\ \\leq \\ t \\ \\leq \\ t_m, \\\\         \\mathcal{H}_t(\\omega)^{-}; \\ para \\ t_m \\ \\leq \\ t \\ \\leq \\ t_f, \\\\     \\end{cases} \\end{equation} \\tag{3} \\] <p>onde \\(\\mathcal{H}_t(\\omega)^{+} \\neq \\mathcal{H}_t(\\omega)^{-}\\), \\(\\forall t \\neq t_m\\). \\(t_i \\leq t \\leq t_m\\) e \\(t_m \\leq t \\leq t_f\\) correspondem ao regime quando \\(x_t\\) est\u00e1 em carregamento e descarregamento, respectivamente. \\(\\mathcal{H}_t(\\omega)^{+}\\) corresponde \u00e0 parte do la\u00e7o formada no plano \\(x_t - y_t\\), enquanto \\(t_i \\leq t \\leq t_m\\) (quando \\(x_t\\) est\u00e1 em carregamento), enquanto \\(\\mathcal{H}_t(\\omega)^{-}\\) \u00e9 a parte do la\u00e7o formada no plano \\(x_t - y_t\\) para \\(t_m \\leq t \\leq t_f\\) (quando \\(x_t\\) est\u00e1 em descarregamento), como mostrado na Figura 2:</p> <p></p> <p>Figura 2. Exemplo de uma curva de histerese.</p> <p>Rate Independent Hysteresis (RIH) (Visintin, A., \"Differential Models of Hysteresis\") - O comportamento de histerese \u00e9 chamado de rate independent se o caminho \\(ABCD\\), que depende do par \\(x(t), y(t)\\), \u00e9 invariante em rela\u00e7\u00e3o a qualquer difeomorfismo crescente \\(\\varphi : [0,T] \\rightarrow [0,T]\\), ou seja:</p> \\[ \\begin{align}         F(u \\ o \\ \\varphi, y^{0}) = F(u,y^0)\\ o \\ \\varphi &amp; \\ em \\ [0,T]. \\end{align} \\tag{4} \\] <p>Isso significa que, em qualquer instante \\(t\\), \\(y(t)\\) depende apenas de \\(u:[0,T] \\rightarrow \\mathbb{R}\\) e da ordem em que os valores foram atingidos antes de \\(t\\). Em outras palavras, o efeito de mem\u00f3ria n\u00e3o \u00e9 afetado pela frequ\u00eancia da entrada.</p>"},{"location":"pt/book/8-Severely-Nonlinear-System-Identification/#rate-independent-hysteresis-em-modelos-polinomiais-narx","title":"Rate Independent Hysteresis em modelos polinomiais NARX","text":"<p>Martins, S. A. M. and Aguirre, L. A. apresentaram as condi\u00e7\u00f5es suficientes para que modelos NARX representem histerese. Um dos conceitos desenvolvidos \u00e9 a Estrutura Limitante (Bounding Structure) \\(\\mathcal{H}\\).</p> <p>Estrutura Limitante \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A.) - Seja \\(\\mathcal{H}_t(\\omega)\\) a histerese do sistema. \\(\\mathcal{H}= \\lim_{\\omega \\to 0} \\mathcal{H}_t(\\omega)\\) \u00e9 definida como a estrutura limitante que delimita \\(\\mathcal{H}_t(\\omega)\\).</p> <p>Agora, considere um modelo polinomial NARX excitado por um sinal quase-est\u00e1tico de carregamento-descarregamento. Se o modelo possui um ponto de equil\u00edbrio real e est\u00e1vel, cuja localiza\u00e7\u00e3o depende da entrada e do regime de carregamento/descarregamento, o polin\u00f4mio exibir\u00e1 um hysteresis loop Rate Independent \\(\\mathcal{H}_t(\\omega)\\) no plano \\(x-y\\).</p> <p>Aqui est\u00e1 um exemplo. Seja \\(y_k  =  0.8y_{k-1} + 0.4\\phi_{k-1} + 0.2x_{k-1}\\), onde \\(\\phi_{k} = \\rm{sign}(\\Delta(x_{k}))\\) e \\(x_{k} = sin(\\omega k)\\) e \\(\\omega\\) \u00e9 a frequ\u00eancia do sinal de entrada \\(x\\). Os equil\u00edbrios deste modelo s\u00e3o dados por:</p> \\[ \\begin{equation}     \\overline{y}(\\overline{\\phi},\\overline{x})=     \\begin{cases}         \\frac{0.6+0.2\\overline{x}}{1-0.8} \\ = 3 \\ + \\ \\overline{x} \\ , &amp; para \\ carregamento; \\\\         \\frac{-0.6+0.2\\overline{x}}{1-0.8} \\ = -3 \\ + \\ \\overline{x} \\ , &amp; para \\ descarregamento; \\\\     \\end{cases} \\end{equation} \\tag{5} \\] <p>onde \\(\\overline {x}\\) \u00e9 um sinal de entrada quase-est\u00e1tico de carregamento-descarregamento. Como os pontos de equil\u00edbrio s\u00e3o assintoticamente est\u00e1veis, a sa\u00edda converge para \\(\\mathcal{H}_k (w)\\) no plano \\(x-y\\). Note que, para um valor de entrada constante \\(x ~ = ~ 1 ~ = ~ \\overline{x}\\), o equil\u00edbrio est\u00e1 em \\(\\overline{y} ~ = ~ 3\\) para o regime de carregamento e \\(\\overline {y} ~ = ~ -1\\) para o regime de descarregamento. Analogamente, para \\(\\overline {x} ~ = ~ -1\\), o equil\u00edbrio est\u00e1 em \\(\\overline {y} ~ = ~ 1\\) para o regime de carregamento e \\(\\overline {y} ~ = ~ -3\\) para o regime de descarregamento, como mostrado na figura abaixo:</p> <p></p> <p>Figura 3. Exemplo de uma estrutura limitante \\(\\mathcal{H}\\). Os pontos pretos est\u00e3o em \\(\\mathcal{H}_{k}(\\omega)\\) para o modelo \\(y_k  =  0.8y_{k-1} + 0.4\\phi_{k-1} + 0.2x_{k-1}\\). A estrutura limitante \\(\\mathcal{H}\\), em vermelho, confina \\(\\mathcal{H}_{k}(\\omega)\\).</p> <p>Como pode ser observado na Figura 3, se garantirmos as condi\u00e7\u00f5es suficientes propostas por Martins, S. A. M. and Aguirre, L. A., um modelo NARX pode reproduzir um comportamento hister\u00e9tico. O Cap\u00edtulo 10 apresenta um estudo de caso de um sistema com histerese.</p> <p>O c\u00f3digo a seguir pode ser usado para reproduzir o comportamento mostrado na Figura 3. Altere <code>w</code> de \\(1\\) para \\(0.1\\) para ver como a estrutura limitante \\(\\mathcal{H}\\) converge para os equil\u00edbrios do sistema.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nw = 1\nt = np.arange(0, 60.1, 0.1)\ny = np.zeros(len(t))\nx = np.sin(w * t)\n\n# Initialize y and fi\nfi = np.zeros(len(t))\n# Iterate over the time array to calculate y\nfor k in range(1, len(t)):\n    fi[k] = np.sign(x[k] - x[k-1])\n    y[k] = 0.8 * y[k-1] + 0.2 * x[k-1] + 0.4 * fi[k-1]\n\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Example')\nplt.show()\n</code></pre> <p></p> <p>Figura 4. Reprodu\u00e7\u00e3o de uma estrutura limitante \\(\\mathcal{H}\\) usando Python.</p>"},{"location":"pt/book/9-Validation/","title":"9. Valida\u00e7\u00e3o","text":""},{"location":"pt/book/9-Validation/#o-metodo-predict-no-sysidentpy","title":"O M\u00e9todo <code>predict</code> no SysIdentPy","text":"<p>Antes de entrar no processo de valida\u00e7\u00e3o em Identifica\u00e7\u00e3o de Sistemas, \u00e9 essencial entender como o m\u00e9todo <code>predict</code> funciona no SysIdentPy.</p>"},{"location":"pt/book/9-Validation/#usando-o-metodo-predict","title":"Usando o M\u00e9todo <code>predict</code>","text":"<p>Um uso t\u00edpico do m\u00e9todo <code>predict</code> no SysIdentPy \u00e9 assim:</p> <pre><code>yhat = model.predict(X=x_test, y=y_test)\n</code></pre> <p>Usu\u00e1rios do SysIdentPy frequentemente t\u00eam duas d\u00favidas comuns sobre este m\u00e9todo:</p> <ol> <li>Por que precisamos passar os dados de teste, <code>y_test</code>, como argumento no m\u00e9todo <code>predict</code>?</li> <li>Por que os valores iniciais preditos s\u00e3o id\u00eanticos aos valores nos dados de teste?</li> </ol> <p>Para responder a essas perguntas, vamos primeiro explicar os conceitos de predi\u00e7\u00e3o infinitos passos \u00e0 frente, predi\u00e7\u00e3o n passos \u00e0 frente e predi\u00e7\u00e3o um passo \u00e0 frente em sistemas din\u00e2micos.</p>"},{"location":"pt/book/9-Validation/#predicao-infinitos-passos-a-frente","title":"Predi\u00e7\u00e3o Infinitos Passos \u00e0 Frente","text":"<p>A predi\u00e7\u00e3o infinitos passos \u00e0 frente, tamb\u00e9m conhecida como free run simulation, refere-se a fazer predi\u00e7\u00f5es usando valores previamente preditos, \\(\\hat{y}_{k-n_y}\\), no loop de predi\u00e7\u00e3o.</p> <p>Por exemplo, considere os seguintes dados de entrada e sa\u00edda de teste:</p> \\[ x_{test} = [1, 2, 3, 4, 5, 6, 7] \\] \\[ y_{test} = [8, 9, 10, 11, 12, 13, 14] \\] <p>Suponha que queremos validar um modelo \\(m\\) definido por:</p> \\[ m \\rightarrow y_k = 1*y_{k-1} + 2*x_{k-1} \\] <p>Para predizer o primeiro valor, precisamos de acesso tanto a \\(y_{k-1}\\) quanto a \\(x_{k-1}\\). Esse requisito explica por que voc\u00ea precisa passar <code>y_test</code> como argumento no m\u00e9todo <code>predict</code>. Isso tamb\u00e9m responde \u00e0 segunda pergunta: o SysIdentPy requer que o usu\u00e1rio forne\u00e7a as condi\u00e7\u00f5es iniciais explicitamente. Os dados <code>y_test</code> passados no m\u00e9todo <code>predict</code> n\u00e3o s\u00e3o usados inteiramente; apenas os valores iniciais necess\u00e1rios para a estrutura de lags do modelo s\u00e3o usados.</p> <p>Neste exemplo, o lag m\u00e1ximo do modelo \u00e9 1, ent\u00e3o precisamos apenas de 1 condi\u00e7\u00e3o inicial. Os valores preditos, <code>yhat</code>, s\u00e3o ent\u00e3o calculados da seguinte forma:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*10 + 2*2 = 14\nyhat(3) = 1*14 + 2*3 = 20\nyhat(4) = 1*20 + 2*4 = 28\n</code></pre> <p>Como mostrado, o primeiro valor de <code>yhat</code> corresponde ao primeiro valor de <code>y_test</code> porque ele serve como condi\u00e7\u00e3o inicial. Outro ponto importante \u00e9 que o loop de predi\u00e7\u00e3o usa os valores previamente preditos, n\u00e3o os valores reais de <code>y_test</code>, e \u00e9 por isso que \u00e9 chamado de infinitos passos \u00e0 frente ou free run simulation.</p> <p>Em identifica\u00e7\u00e3o de sistemas, frequentemente buscamos modelos que tenham bom desempenho em predi\u00e7\u00f5es infinitos passos \u00e0 frente. Como o erro de predi\u00e7\u00e3o se propaga ao longo do tempo, um modelo que mostra bom desempenho em free run simulation \u00e9 considerado um modelo robusto.</p> <p>No SysIdentPy, os usu\u00e1rios s\u00f3 precisam passar as condi\u00e7\u00f5es iniciais ao realizar uma predi\u00e7\u00e3o infinitos passos \u00e0 frente. Se voc\u00ea passar apenas as condi\u00e7\u00f5es iniciais, os resultados ser\u00e3o os mesmos! Portanto</p> <pre><code>yhat = model.predict(X=x_test, y=y_test)\n</code></pre> <p>\u00e9 na verdade o mesmo que</p> <pre><code>yhat = model.predict(X=x_test, y=y_test[:model.max_lag].reshape(-1, 1))\n</code></pre> <p><code>model.max_lag</code> pode ser acessado ap\u00f3s ajustarmos o modelo usando o c\u00f3digo abaixo.</p> <pre><code>model = FROLS(\n    order_selection=False,\n    ylag=2,\n    xlag=2,\n    estimator=LeastSquares(unbiased=False),\n    basis_function=basis_function,\n    e_tol=0.9999\n    n_terms=15\n)\nmodel.fit(X=x, y=y)\nmodel.max_lag\n</code></pre> <p>\u00c9 importante mencionar que, na vers\u00e3o atual do SysIdentPy, o lag m\u00e1ximo considerado \u00e9 na verdade o lag m\u00e1ximo entre as defini\u00e7\u00f5es de <code>xlag</code> e <code>ylag</code>. Isso \u00e9 importante porque voc\u00ea pode passar <code>ylag = xlag = 10</code> e o modelo final, ap\u00f3s a sele\u00e7\u00e3o de estrutura de modelo, selecionar termos onde o lag m\u00e1ximo \u00e9 3. Voc\u00ea tem que passar 10 condi\u00e7\u00f5es iniciais, mas internamente os c\u00e1lculos s\u00e3o feitos usando os regressores corretos. Isso \u00e9 necess\u00e1rio devido \u00e0 forma como os regressores s\u00e3o criados ap\u00f3s o modelo ser ajustado. Portanto, \u00e9 recomendado usar <code>model.max_lag</code> para ter certeza.</p>"},{"location":"pt/book/9-Validation/#predicao-1-passo-a-frente","title":"Predi\u00e7\u00e3o 1 Passo \u00e0 Frente","text":"<p>A diferen\u00e7a entre predi\u00e7\u00e3o 1 passo \u00e0 frente e predi\u00e7\u00e3o infinitos passos \u00e0 frente \u00e9 que o modelo usa os valores reais anteriores de <code>y_test</code> no loop ao inv\u00e9s dos valores preditos <code>yhat</code>. E essa \u00e9 uma diferen\u00e7a enorme e importante. Vamos fazer a predi\u00e7\u00e3o usando o m\u00e9todo 1 passo \u00e0 frente:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*9 + 2*2 = 13\nyhat(3) = 1*10 + 2*3 = 16\nyhat(4) = 1*11 + 2*4 = 19\ne assim por diante\n</code></pre> <p>O modelo usa valores reais no loop e apenas prediz o pr\u00f3ximo valor. O erro de predi\u00e7\u00e3o, neste caso, \u00e9 sempre corrigido porque n\u00e3o estamos propagando o erro usando os valores preditos no loop.</p> <p>O m\u00e9todo <code>predict</code> do SysIdentPy permite que o usu\u00e1rio realize uma predi\u00e7\u00e3o 1 passo \u00e0 frente configurando <code>steps_ahead=1</code></p> <pre><code>yhat = model.predict(X=x_test, y=y_test, steps_ahead=1)\n</code></pre> <p>Neste caso, como voc\u00ea pode imaginar, precisamos passar todos os dados de <code>y_test</code> porque o m\u00e9todo precisa acessar os valores reais em cada itera\u00e7\u00e3o. Se voc\u00ea passar apenas as condi\u00e7\u00f5es iniciais, <code>yhat</code> ter\u00e1 apenas as condi\u00e7\u00f5es iniciais mais 1 amostra adicional, que \u00e9 a predi\u00e7\u00e3o 1 passo \u00e0 frente. Para predizer outro ponto, voc\u00ea precisaria passar as novas condi\u00e7\u00f5es iniciais novamente e assim por diante. O SysIdentPy j\u00e1 faz tudo isso para voc\u00ea, ent\u00e3o apenas passe todos os dados que voc\u00ea quer validar usando o m\u00e9todo 1 passo \u00e0 frente.</p>"},{"location":"pt/book/9-Validation/#predicao-n-passos-a-frente","title":"Predi\u00e7\u00e3o n Passos \u00e0 Frente","text":"<p>A predi\u00e7\u00e3o n passos \u00e0 frente \u00e9 quase a mesma que a de 1 passo \u00e0 frente, mas aqui voc\u00ea pode definir o n\u00famero de passos \u00e0 frente que quer testar seu modelo. Se voc\u00ea configurar <code>steps_ahead=5</code>, por exemplo, significa que os primeiros 5 valores ser\u00e3o preditos usando <code>yhat</code> no loop, mas ent\u00e3o o processo \u00e9 reiniciado alimentando os valores reais em <code>y_test</code> na pr\u00f3xima itera\u00e7\u00e3o, ent\u00e3o realizando outras 5 predi\u00e7\u00f5es usando o <code>yhat</code> e assim por diante. Vamos verificar o exemplo considerando <code>steps_ahead=2</code>:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*10 + 2*2 = 14\nyhat(3) = 1*10 + 2*3 = 16\nyhat(4) = 1*16 + 2*4 = 24\ne assim por diante\n</code></pre>"},{"location":"pt/book/9-Validation/#desempenho-do-modelo","title":"Desempenho do Modelo","text":"<p>A valida\u00e7\u00e3o de modelos \u00e9 uma das partes mais cruciais em identifica\u00e7\u00e3o de sistemas. Como mencionamos antes, em identifica\u00e7\u00e3o de sistemas estamos tentando modelar a din\u00e2mica do processo para tarefas como projeto de controle. Em tais casos, n\u00e3o podemos apenas confiar em m\u00e9tricas de regress\u00e3o, mas tamb\u00e9m garantir que os res\u00edduos sejam imprevis\u00edveis em v\u00e1rias combina\u00e7\u00f5es de entradas e sa\u00eddas passadas (Billings, S. A. and Voon, W. S. F., \"Structure detection and model validity tests in the identification of nonlinear systems\"). Um teste estat\u00edstico frequentemente usado \u00e9 o RMSE normalizado, chamado RRSE, que pode ser expresso por</p> \\[ \\begin{equation}         \\textrm{RRSE}= \\frac{\\sqrt{\\sum\\limits_{k=1}^{n}(y_k-\\hat{y}_k)^2}}{\\sqrt{\\sum\\limits_{k=1}^{n}(y_k-\\bar{y})^2}}, \\end{equation} \\tag{1} \\] <p>onde \\(\\hat{y}_k \\in \\mathbb{R}\\) \u00e9 a sa\u00edda predita pelo modelo e \\(\\bar{y} \\in \\mathbb{R}\\) \u00e9 a m\u00e9dia da sa\u00edda medida \\(y_k\\). O RRSE fornece alguma indica\u00e7\u00e3o sobre a qualidade do modelo, mas concluir sobre o melhor modelo avaliando apenas essa quantidade pode levar a uma interpreta\u00e7\u00e3o incorreta, como mostrado no exemplo a seguir.</p> <p>Considere os modelos</p> \\[ y_{{_a}k} = 0.7077y_{{_a}k-1} + 0.1642u_{k-1} + 0.1280u_{k-2} \\] <p>e</p> \\[y_{{_b}k}=0.7103y_{{_b}k-1} + 0.1458u_{k-1} + 0.1631u_{k-2} -1467y^3_{{_b}k-1} + 0.0710y^3_{{_b}k-2} +0.0554y^2_{{_b}k-3}u_{k-3}\\] <p>definidos em Meta Model Structure Selection: An Algorithm For Building Polynomial NARX Models For Regression And Classification. O primeiro resulta em \\(RRSE = 0.1202\\) enquanto o \u00faltimo resulta em \\(RRSE~=0.0857\\). Embora o modelo \\(y_{{_b}k}\\) ajuste melhor os dados, ele \u00e9 apenas uma representa\u00e7\u00e3o enviesada para um conjunto de dados e n\u00e3o uma boa descri\u00e7\u00e3o de todo o sistema.</p> <p>O RRSE (ou qualquer outra m\u00e9trica) mostra que testes de valida\u00e7\u00e3o podem precisar ser realizados cuidadosamente. Outra pr\u00e1tica tradicional \u00e9 dividir o conjunto de dados em duas partes. Nesse sentido, pode-se testar os modelos obtidos da parte de estima\u00e7\u00e3o dos dados usando dados espec\u00edficos para valida\u00e7\u00e3o. No entanto, o desempenho de um passo \u00e0 frente de modelos NARX geralmente resulta em interpreta\u00e7\u00f5es equivocadas porque mesmo modelos fortemente enviesados podem ajustar bem os dados. Portanto, uma abordagem de free run simulation geralmente permite uma melhor interpreta\u00e7\u00e3o se o modelo \u00e9 adequado ou n\u00e3o (Billings, S. A.).</p> <p>Testes estat\u00edsticos para modelos SISO baseados nas fun\u00e7\u00f5es de correla\u00e7\u00e3o foram propostos em (Billings, S. A. and Voon, W. S. F., \"A prediction-error and stepwise-regression estimation algorithm for non-linear systems\"), (Model validity tests for non-linear signal processing applications). Os testes s\u00e3o:</p> \\[ \\begin{align}     \\phi_{_{\\xi \\xi}\\tau} &amp;= E\\{\\xi_k \\xi_{k-\\tau}\\} = \\delta_{\\tau}, \\\\     \\phi_{_{\\xi x}\\tau} &amp;= E\\{\\xi_k x_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{\\xi \\xi x}\\tau} &amp;= E\\{\\xi_k \\xi_{k-\\tau} x_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{x^2 \\xi}\\tau} &amp;= E\\{(u^2_k - E\\{x^2_k\\})\\xi_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{x^2 \\xi^2}\\tau} &amp;= E\\{(u^2_k - E\\{x^2_k\\})\\xi^2_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{(y\\xi) x^2}\\tau} &amp;= E\\{(y_k\\xi_k - E\\{y_k\\xi_k\\})(x^2_{k-\\tau} - E\\{x^2_k\\})\\} = 0 \\forall \\tau, \\end{align} \\tag{2} \\] <p>onde \\(\\delta\\) \u00e9 a fun\u00e7\u00e3o delta de Dirac e a fun\u00e7\u00e3o de correla\u00e7\u00e3o cruzada \\(\\phi\\) \u00e9 denotada por (Billings, S. A. and Voon, W. S. F.):</p> \\[ \\begin{equation} \\phi_{{_{ab}}\\tau} = \\frac{\\frac{1}{n}\\sum\\limits_{k=1}^{n-\\tau}(a_k - \\hat{a})(b_{k+\\tau}-\\hat{b})}{\\sqrt{\\frac{1}{n}\\sum\\limits_{k=1}^{n}(a_k-\\hat{a})^2} \\sqrt{\\frac{1}{n}\\sum\\limits_{k=1}^{n}(b_k-\\hat{b})^2}} = \\frac{\\sum\\limits_{k=1}^{n-\\tau}(a_k - \\hat{a})(b_{k+\\tau}-\\hat{b})}{\\sqrt{\\sum\\limits_{k=1}^{n}(a_k-\\hat{a})^2} \\sqrt{\\sum\\limits_{k=1}^{n}(b_k-\\hat{b})^2}}, \\end{equation} \\tag{3} \\] <p>onde \\(a\\) e \\(b\\) s\u00e3o duas sequ\u00eancias de sinais. Se os testes s\u00e3o verdadeiros, ent\u00e3o os res\u00edduos do modelo podem ser considerados como ru\u00eddo branco.</p>"},{"location":"pt/book/9-Validation/#metricas-disponiveis-no-sysidentpy","title":"M\u00e9tricas Dispon\u00edveis no SysIdentPy","text":"<p>O SysIdentPy fornece as seguintes m\u00e9tricas de regress\u00e3o prontas para uso:</p> <ul> <li>forecast_error</li> <li>mean_forecast_error</li> <li>mean_squared_error</li> <li>root_mean_squared_error</li> <li>normalized_root_mean_squared_error</li> <li>root_relative_squared_error</li> <li>mean_absolute_error</li> <li>mean_squared_log_error</li> <li>median_absolute_error</li> <li>explained_variance_score</li> <li>r2_score</li> <li>symmetric_mean_absolute_percentage_error</li> </ul> <p>Para us\u00e1-las, o usu\u00e1rio s\u00f3 precisa importar a m\u00e9trica desejada usando, por exemplo</p> <pre><code>from sysidentpy.metrics import root_relative_squared_error\n</code></pre> <p>O SysIdentPy tamb\u00e9m fornece m\u00e9todos para calcular e analisar a correla\u00e7\u00e3o dos res\u00edduos</p> <pre><code>from sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <p>Vamos verificar as m\u00e9tricas do sistema eletromec\u00e2nico modelado no Cap\u00edtulo 4.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.metrics import root_relative_squared_error\n\ndf1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"bic\",\n    estimator=LeastSquares(unbiased=False),\n    basis_function=basis_function\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p> <p>O RRSE \u00e9 0.0800, que \u00e9 uma m\u00e9trica muito boa. No entanto, podemos ver que os res\u00edduos t\u00eam algumas autocorrela\u00e7\u00f5es altas e com a entrada. Isso significa que nosso modelo talvez n\u00e3o seja bom o suficiente como poderia ser.</p> <p>Vamos verificar o que acontece se aumentarmos <code>xlag</code>, <code>ylag</code> e mudarmos o algoritmo de estima\u00e7\u00e3o de par\u00e2metros de Least Squares para Recursive Least Squares</p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=50,\n    ylag=5,\n    xlag=5,\n    info_criteria=\"bic\",\n    estimator=RecursiveLeastSquares(unbiased=False),\n    basis_function=basis_function\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>Agora o RRSE \u00e9 0.0568 e temos uma melhor correla\u00e7\u00e3o residual!</p> <p></p> <p></p> <p></p> <p>No final das contas, o melhor modelo ser\u00e1 aquele que satisfaz as necessidades do usu\u00e1rio. No entanto, \u00e9 importante entender como analisar os modelos para que voc\u00ea possa ter uma ideia se pode obter algumas melhorias sem muito trabalho.</p> <p>Por curiosidade, vamos verificar como o modelo se comporta se executarmos uma predi\u00e7\u00e3o 1 passo \u00e0 frente. N\u00e3o precisamos ajustar o modelo novamente, apenas fazer outra predi\u00e7\u00e3o usando a op\u00e7\u00e3o 1 passo.</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid, steps_ahead=1)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>O mesmo modelo, mas avaliando a predi\u00e7\u00e3o 1 passo \u00e0 frente, agora retorna um RRSE\\(= 0.02044\\) e os res\u00edduos est\u00e3o ainda melhores. Mas lembre-se, isso \u00e9 esperado, como explicado na se\u00e7\u00e3o anterior.</p> <p></p> <p></p> <p></p>"},{"location":"pt/changelog/changelog/","title":"Hist\u00f3rico de Altera\u00e7\u00f5es","text":"<p>O changelog \u00e9 mantido em ingl\u00eas</p> <p>O hist\u00f3rico de altera\u00e7\u00f5es (changelog) \u00e9 mantido em ingl\u00eas para facilitar a manuten\u00e7\u00e3o e consist\u00eancia com as releases do GitHub. Para ver as altera\u00e7\u00f5es detalhadas, consulte a vers\u00e3o em ingl\u00eas do changelog ou as releases no GitHub.</p>"},{"location":"pt/changelog/changelog/#resumo-das-versoes-recentes","title":"Resumo das Vers\u00f5es Recentes","text":""},{"location":"pt/changelog/changelog/#v060","title":"v0.6.0","text":"<ul> <li>Introdu\u00e7\u00e3o da classe <code>OFRBase</code> para algoritmos baseados em Error Reduction Ratio (ERR)</li> <li>Implementa\u00e7\u00e3o do algoritmo Ultra Orthogonal Forward Regression (UOFR)</li> <li>Corre\u00e7\u00e3o de typo: <code>Bersntein</code> \u2192 <code>Bernstein</code></li> <li>Cobertura de testes aumentada para 92%</li> <li>Descontinua\u00e7\u00e3o do suporte ao Python 3.7</li> </ul>"},{"location":"pt/changelog/changelog/#v053","title":"v0.5.3","text":"<ul> <li>Corre\u00e7\u00e3o do bug da fun\u00e7\u00e3o base Bilinear para modelos com mais de 2 entradas</li> </ul>"},{"location":"pt/changelog/changelog/#v052","title":"v0.5.2","text":"<ul> <li>Corre\u00e7\u00e3o do bug das fun\u00e7\u00f5es base Polynomial e Bilinear para modelos com mais de 3 entradas</li> </ul>"},{"location":"pt/changelog/changelog/#v051","title":"v0.5.1","text":"<ul> <li>Corre\u00e7\u00f5es menores e melhorias de estabilidade</li> </ul> <p>Para o hist\u00f3rico completo de altera\u00e7\u00f5es, visite:</p> <ul> <li>Changelog em Ingl\u00eas</li> <li>Releases no GitHub</li> </ul>"},{"location":"pt/community-support/community-overview/","title":"Comunidade","text":"\ud83d\udcac Obter Ajuda <p>Precisa de assist\u00eancia? Explore v\u00e1rias op\u00e7\u00f5es para obter ajuda, incluindo f\u00f3runs de discuss\u00e3o, issues no GitHub e canais de suporte da comunidade.</p> \ud83e\udd1d Meetups <p>Conecte-se com outros usu\u00e1rios do SysIdentPy atrav\u00e9s de meetups, eventos online e discuss\u00f5es da comunidade. Fique atualizado sobre os pr\u00f3ximos encontros e compartilhe sua experi\u00eancia.</p>"},{"location":"pt/community-support/get-help/","title":"Obter Ajuda","text":"<p>Antes de pedir ajuda a outras pessoas, geralmente \u00e9 uma boa ideia tentar se ajudar primeiro. O SysIdentPy inclui v\u00e1rios exemplos na documenta\u00e7\u00e3o com dicas e notas sobre o pacote que podem ajud\u00e1-lo. No entanto, se voc\u00ea tiver algum problema e n\u00e3o encontrar a resposta, entre em contato usando qualquer m\u00e9todo descrito abaixo.</p>"},{"location":"pt/community-support/get-help/#conecte-se-com-o-autor","title":"Conecte-se com o autor","text":"<p>Voc\u00ea pode:</p> <ul> <li>Seguir no GitHub.</li> <li>Seguir no Twitter.</li> <li>Conectar no LinkedIn.<ul> <li>Vou come\u00e7ar a usar o Twitter mais frequentemente \ud83e\udd37\u200d\u2642 (provavelmente).</li> </ul> </li> <li>Ler o que escrevo (ou me seguir) no Medium.</li> </ul>"},{"location":"pt/community-support/get-help/#criar-issues","title":"Criar issues","text":"<p>Voc\u00ea pode criar uma nova issue no reposit\u00f3rio do GitHub, por exemplo para:</p> <ul> <li>Fazer uma pergunta ou perguntar sobre um problema.</li> <li>Sugerir um novo recurso.</li> </ul>"},{"location":"pt/community-support/get-help/#junte-se-ao-chat","title":"Junte-se ao chat","text":"<p>Junte-se ao \ud83d\udc65 servidor de chat do Discord \ud83d\udc65 e converse com outros membros da comunidade SysIdentPy.</p> <p>Voc\u00ea pode usar o chat para qualquer coisa</p> <p>Tenha em mente que voc\u00ea pode usar o chat para falar sobre qualquer coisa relacionada ao SysIdentPy. Conversas sobre identifica\u00e7\u00e3o de sistemas, sistemas din\u00e2micos, novos artigos, issues, novos recursos s\u00e3o permitidas, mas tenha em mente que se alguma das perguntas puder ajudar outros usu\u00e1rios, pedirei gentilmente que voc\u00ea abra uma discuss\u00e3o ou uma issue no Github tamb\u00e9m.</p> <p>Posso garantir que sempre respondo a tudo, mesmo que demore um pouco.</p>"},{"location":"pt/community-support/meetups/ai-networks-meetup/#sobre-a-ai-networks","title":"Sobre a AI Networks","text":"<p>Fundada em 5 de agosto de 2019, a AI Networks rapidamente se estabeleceu como uma das comunidades mais engajadas e focadas no campo da intelig\u00eancia artificial. Nossa comunidade prospera com uma paix\u00e3o compartilhada por conhecimento e um compromisso em explorar os \u00faltimos avan\u00e7os em IA e machine learning.</p> <p>Ao celebrarmos nosso 5\u00ba anivers\u00e1rio em agosto de 2024, refletimos sobre cinco anos de rica troca de conhecimento e discuss\u00f5es vibrantes.</p> <p>Nossa comunidade atualmente hospeda quatro grupos especializados:</p> <ul> <li>AI/ML Brasil - Principal 01 &amp; 02: Nossos grupos principais dedicados a uma ampla gama de t\u00f3picos de IA e machine learning.</li> <li>AI/ML Brasil - Prompt Engineering: Focado na arte e ci\u00eancia de projetar prompts eficazes para sistemas de IA.</li> <li>AI/ML Brasil - Neuroci\u00eancia: Explorando a interse\u00e7\u00e3o entre intelig\u00eancia artificial e neuroci\u00eancia.</li> <li>AI/ML Brasil - Divulga\u00e7\u00e3o Servi\u00e7os IA: Dedicado \u00e0 dissemina\u00e7\u00e3o e promo\u00e7\u00e3o de servi\u00e7os de IA.</li> </ul> <p>Junte-se a n\u00f3s para conectar-se com pessoas que pensam de forma semelhante, participar de discuss\u00f5es instigantes e ficar na vanguarda da inova\u00e7\u00e3o em IA.</p> <p>Em Portugu\u00eas</p> <p>Talk: Modelando Sistemas Din\u00e2micos | AI Networks Meetup</p> <p>Basta clicar no link acima para assistir ao v\u00eddeo.</p>"},{"location":"pt/community-support/meetups/estatidados/","title":"Estatidados Meetup","text":"<p>Estatidados \u00e9 uma grande comunidade de estat\u00edstica e ci\u00eancia de dados no Brasil. Eles organizam um meetup online que re\u00fane pesquisadores e desenvolvedores de destaque da comunidade de Estat\u00edstica e Ci\u00eancia de Dados para participar de um conjunto diversificado de palestras cobrindo as tend\u00eancias atuais no campo de Data Science.</p> <p>Wilson Rocha, o mantenedor do SysIdentPy, deu uma palestra no meetup sobre o SysIdentPy e o v\u00eddeo est\u00e1 dispon\u00edvel abaixo:</p> <p>Em Portugu\u00eas</p> <p>Talk: SysIdentPy: Uma Biblioteca Para Cria\u00e7\u00e3o de Modelos N\u00e3o Lineares para S\u00e9ries Temporais</p> <p>Basta clicar no link acima para assistir ao v\u00eddeo.</p>"},{"location":"pt/community-support/meetups/gcom-meetup/","title":"GCoM Meetup","text":"<p>GCoM (Grupo de Controle e Modelagem) \u00e9 um grupo de pesquisa do Departamento de Engenharia El\u00e9trica da Universidade Federal de S\u00e3o Jo\u00e3o del-Rei. O GCoM trabalha em duas \u00e1reas principais: An\u00e1lise e Modelagem de Sistemas e Sistemas de Controle. Eles se dedicam a aplicar conhecimentos de Engenharia El\u00e9trica, Matem\u00e1tica, Ci\u00eancia da Computa\u00e7\u00e3o e F\u00edsica para construir e analisar modelos que imitam e controlam sistemas din\u00e2micos reais. Algumas das aplica\u00e7\u00f5es incluem controle robusto de sistemas incertos, Tecnologia Assistiva, identifica\u00e7\u00e3o de sistemas com histerese e Din\u00e2mica Ca\u00f3tica. Recentemente, um grande esfor\u00e7o tem sido empreendido para melhor compreender o papel que a computa\u00e7\u00e3o num\u00e9rica desempenha na modelagem e controle de sistemas din\u00e2micos n\u00e3o lineares.</p> <p>Wilson Rocha, o mantenedor do SysIdentPy, deu uma palestra no meetup sobre o SysIdentPy e o v\u00eddeo est\u00e1 dispon\u00edvel abaixo:</p> <p>Em Portugu\u00eas</p> <p>Talk: Aplica\u00e7\u00f5es em Data Science e Identifica\u00e7\u00e3o de Sistemas com o SysIdentPy</p> <p>Basta clicar no link acima para assistir ao v\u00eddeo.</p>"},{"location":"pt/community-support/meetups/nubank-meetup-open-source/","title":"Nubank Meetup","text":"<p>Nubank \u00e9 uma empresa l\u00edder em tecnologia financeira na Am\u00e9rica Latina com mais de 54 milh\u00f5es de clientes no Brasil, M\u00e9xico e Col\u00f4mbia. Eles organizam um meetup online que re\u00fane pesquisadores e desenvolvedores de destaque da comunidade de Machine Learning (ML) para participar de um conjunto diversificado de palestras cobrindo as tend\u00eancias atuais no desenvolvimento de ML.</p> <p>Wilson Rocha, o mantenedor do SysIdentPy, se juntou a Bruno Rocha (engenheiro de software na Red Hat) e Tatyana Zabanova (Data Scientist no Nubank) em uma conversa sobre a experi\u00eancia de criar um pacote open source. O v\u00eddeo est\u00e1 dispon\u00edvel abaixo:</p> <p>Em Portugu\u00eas</p> <p>Talk: Painel sobre desenvolvimento de bibliotecas em Data Science | Nubank DS &amp; ML Meetup</p> <p>Basta clicar no link acima para assistir ao v\u00eddeo.</p>"},{"location":"pt/community-support/meetups/nubank-meetup/","title":"Nubank Meetup","text":"<p>Nubank \u00e9 uma empresa l\u00edder em tecnologia financeira na Am\u00e9rica Latina com mais de 54 milh\u00f5es de clientes no Brasil, M\u00e9xico e Col\u00f4mbia. Eles organizam um meetup online que re\u00fane pesquisadores e desenvolvedores de destaque da comunidade de Machine Learning (ML) para participar de um conjunto diversificado de palestras cobrindo as tend\u00eancias atuais no desenvolvimento de ML.</p> <p>Wilson Rocha, o mantenedor do SysIdentPy, deu uma palestra no meetup sobre o SysIdentPy e o v\u00eddeo est\u00e1 dispon\u00edvel abaixo:</p> <p>Em Portugu\u00eas</p> <p>Talk: SysIdentPy: Uma Biblioteca Para Cria\u00e7\u00e3o de Modelos N\u00e3o Lineares para S\u00e9ries Temporais</p> <p>Basta clicar no link acima para assistir ao v\u00eddeo.</p>"},{"location":"pt/developer-guide/contribute/","title":"Contribuindo","text":"<p>O SysIdentPy \u00e9 um projeto comunit\u00e1rio, portanto todas as contribui\u00e7\u00f5es s\u00e3o bem-vindas! Existem muitos casos de uso poss\u00edveis na \u00e1rea de Identifica\u00e7\u00e3o de Sistemas e n\u00e3o podemos testar todos os cen\u00e1rios sem a sua ajuda! Se voc\u00ea encontrar algum bug ou tiver sugest\u00f5es, por favor reporte-os no issue tracker no GitHub.</p> <p>Recebemos novos contribuidores de todos os n\u00edveis de experi\u00eancia. Os objetivos da comunidade SysIdentPy s\u00e3o ser prestativa, acolhedora e eficaz.</p>"},{"location":"pt/developer-guide/contribute/#ajude-outros-com-issues-no-github","title":"Ajude outros com issues no GitHub","text":"<p>Voc\u00ea pode ver as issues existentes e tentar ajudar outros, na maioria das vezes s\u00e3o perguntas para as quais voc\u00ea j\u00e1 pode saber a resposta.</p>"},{"location":"pt/developer-guide/contribute/#acompanhe-o-repositorio-do-github","title":"Acompanhe o reposit\u00f3rio do GitHub","text":"<p>Voc\u00ea pode acompanhar o SysIdentPy no GitHub (clicando no bot\u00e3o \"watch\" no canto superior direito):</p> <p>Se voc\u00ea selecionar \"Watching\" em vez de \"Releases only\", receber\u00e1 notifica\u00e7\u00f5es quando algu\u00e9m criar uma nova issue.</p> <p>Assim voc\u00ea pode tentar ajud\u00e1-los a resolver essas issues.</p>"},{"location":"pt/developer-guide/contribute/#documentacao","title":"Documenta\u00e7\u00e3o","text":"<p>A documenta\u00e7\u00e3o \u00e9 t\u00e3o importante quanto a pr\u00f3pria biblioteca. O ingl\u00eas n\u00e3o \u00e9 a l\u00edngua principal dos autores, ent\u00e3o se voc\u00ea encontrar algum erro de digita\u00e7\u00e3o ou algo errado, n\u00e3o hesite em nos avisar.</p>"},{"location":"pt/developer-guide/contribute/#criar-um-pull-request","title":"Criar um Pull Request","text":"<p>Voc\u00ea pode contribuir com o c\u00f3digo-fonte atrav\u00e9s de Pull Requests, por exemplo:</p> <ul> <li>Para corrigir um erro de digita\u00e7\u00e3o que voc\u00ea encontrou na documenta\u00e7\u00e3o.</li> <li>Para compartilhar um artigo, v\u00eddeo ou podcast que voc\u00ea criou ou encontrou sobre o SysIdentPy.</li> <li>Para propor novas se\u00e7\u00f5es de documenta\u00e7\u00e3o.</li> <li>Para corrigir uma issue/bug existente.</li> <li>Para adicionar um novo recurso.</li> </ul>"},{"location":"pt/developer-guide/contribute/#ambiente-de-desenvolvimento","title":"Ambiente de desenvolvimento","text":"<p>Estes s\u00e3o alguns passos b\u00e1sicos para nos ajudar com o c\u00f3digo:</p> <ul> <li> Instalar e configurar o Git no seu computador.</li> <li> Fork o SysIdentPy.</li> <li> Clone o fork na sua m\u00e1quina local.</li> <li> Criar uma nova branch.</li> <li> Fazer altera\u00e7\u00f5es seguindo o estilo de codifica\u00e7\u00e3o do projeto (ou sugerindo melhorias).</li> <li> Executar os testes.</li> <li> Escrever e/ou adaptar testes existentes se necess\u00e1rio.</li> <li> Adicionar documenta\u00e7\u00e3o se necess\u00e1rio.</li> <li> Commit.</li> <li> Push para o seu fork.</li> <li> Abrir um pull_request.</li> </ul>"},{"location":"pt/developer-guide/contribute/#ambiente","title":"Ambiente","text":"<p>Clone o reposit\u00f3rio usando</p> <pre><code>git clone https://github.com/wilsonrljr/sysidentpy.git\n</code></pre> <p>Se voc\u00ea j\u00e1 clonou o reposit\u00f3rio e sabe que precisa mergulhar fundo no c\u00f3digo, aqui est\u00e3o algumas diretrizes para configurar seu ambiente.</p>"},{"location":"pt/developer-guide/contribute/#ambiente-virtual-com-venv","title":"Ambiente virtual com <code>venv</code>","text":"<p>Voc\u00ea pode criar um ambiente virtual em um diret\u00f3rio usando o m\u00f3dulo <code>venv</code> do Python ou Conda:</p> venvconda <pre><code>$ python -m venv env\n</code></pre> <pre><code>conda create -n env\n</code></pre> <p>Isso criar\u00e1 um diret\u00f3rio <code>./env/</code> com os bin\u00e1rios do Python e ent\u00e3o voc\u00ea poder\u00e1 instalar pacotes para esse ambiente isolado.</p>"},{"location":"pt/developer-guide/contribute/#ativar-o-ambiente","title":"Ativar o ambiente","text":"<p>Se voc\u00ea criou o ambiente usando o m\u00f3dulo <code>venv</code> do Python, ative-o com:</p> Linux, macOSWindows PowerShellWindows Bash <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\Activate.ps1\n</code></pre> <p>Ou se voc\u00ea usa Bash no Windows (ex: Git Bash):</p> <pre><code>source ./env/Scripts/activate\n</code></pre> <p>Se voc\u00ea criou o ambiente usando Conda, ative-o com:</p> Conda Bash <pre><code>conda activate env\n</code></pre> <p>Para verificar se funcionou, use:</p> Linux, macOS, Windows BashWindows PowerShell <pre><code>$ which pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <pre><code>$ Get-Command pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <p>Se mostrar o bin\u00e1rio <code>pip</code> em <code>env/bin/pip</code>, ent\u00e3o funcionou.</p> <p>Tip</p> <p>Toda vez que voc\u00ea instalar um novo pacote com <code>pip</code> nesse ambiente, ative o ambiente novamente.</p> <p>Note</p> <p>Usamos o pacote <code>pytest</code> para testes. As fun\u00e7\u00f5es de teste est\u00e3o localizadas em subdiret\u00f3rios de testes em cada pasta dentro do SysIdentPy, que verificam a validade dos algoritmos.</p>"},{"location":"pt/developer-guide/contribute/#dependencias","title":"Depend\u00eancias","text":"<p>Instale o SysIdentPy com as op\u00e7\u00f5es <code>dev</code> e <code>docs</code> para obter todas as depend\u00eancias necess\u00e1rias para executar os testes</p> Depend\u00eancias Dev e Docs <pre><code>pip install \"sysidentpy[dev, docs]\"\n</code></pre>"},{"location":"pt/developer-guide/contribute/#documentacao_1","title":"Documenta\u00e7\u00e3o","text":"<p>Primeiro, certifique-se de configurar seu ambiente conforme descrito acima, isso instalar\u00e1 todos os requisitos.</p> <p>A documenta\u00e7\u00e3o usa MkDocs e Material for MKDocs.</p> <p>Toda a documenta\u00e7\u00e3o est\u00e1 em formato Markdown no diret\u00f3rio <code>./docs/</code>.</p>"},{"location":"pt/developer-guide/contribute/#verificar-as-alteracoes","title":"Verificar as altera\u00e7\u00f5es","text":"<p>Durante o desenvolvimento local, voc\u00ea pode servir o site localmente e verificar quaisquer altera\u00e7\u00f5es. Isso ajuda a garantir que:</p> <ul> <li>Todas as suas modifica\u00e7\u00f5es foram aplicadas.</li> <li>Os arquivos n\u00e3o modificados est\u00e3o sendo exibidos conforme esperado.</li> </ul> <pre><code>$ mkdocs serve\n\nINFO     -  [13:25:00] Browser connected: http://127.0.0.1:8000\n</code></pre> <p>Isso servir\u00e1 a documenta\u00e7\u00e3o em <code>http://127.0.0.1:8008</code>.</p> <p>Dessa forma, voc\u00ea pode continuar editando os arquivos fonte e ver as altera\u00e7\u00f5es ao vivo.</p> <p>Warning</p> <p>Se alguma modifica\u00e7\u00e3o quebrar o build, voc\u00ea ter\u00e1 que servir o site novamente. Sempre verifique seu <code>console</code> para garantir que est\u00e1 servindo o site.</p>"},{"location":"pt/developer-guide/contribute/#executar-testes-localmente","title":"Executar testes localmente","text":"<p>\u00c9 sempre bom verificar se suas implementa\u00e7\u00f5es/modifica\u00e7\u00f5es n\u00e3o quebram nenhuma outra parte do pacote. Voc\u00ea pode executar os testes do SysIdentPy localmente usando <code>pytest</code> na respectiva pasta para realizar todos os testes dos sub-pacotes correspondentes.</p>"},{"location":"pt/developer-guide/contribute/#exemplo-de-como-executar-os-testes","title":"Exemplo de como executar os testes:","text":"<p>Abra um emulador de terminal de sua escolha e v\u00e1 para o diret\u00f3rio principal, ex:</p> <pre><code>\\sysidentpy\\\n</code></pre> <p>Basta digitar <code>pytest</code> no emulador de terminal</p> <pre><code>pytest\n</code></pre> <p>e voc\u00ea obt\u00e9m um resultado como:</p> <pre><code>========== test session starts ==========\n\nplatform linux -- Python 3.7.6, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\n\nrootdir: ~/sysidentpy\n\nplugins: cov-2.8.1\n\ncollected 12 items\n\ntests/test_regression.py ............ [100%]\n\n========== 12 passed in 2.45s ==================\n</code></pre>"},{"location":"pt/developer-guide/documentation-overview/","title":"Proposta de Reestrutura\u00e7\u00e3o da Documenta\u00e7\u00e3o do SysIdentPy","text":"<p>Este documento descreve uma reorganiza\u00e7\u00e3o da documenta\u00e7\u00e3o do SysIdentPy para melhorar a descoberta de conte\u00fado, reduzir a fric\u00e7\u00e3o para iniciantes e alinhar com padr\u00f5es modernos de documenta\u00e7\u00e3o. A estrutura seguir\u00e1 quatro categorias principais: Tutoriais, How-Tos, Explica\u00e7\u00f5es e Refer\u00eancia da API, com se\u00e7\u00f5es adicionais para contribuidores e exemplos do mundo real.</p> <p>Agradecimentos: Esta reestrutura\u00e7\u00e3o da documenta\u00e7\u00e3o se inspira no NEP 44 do NumPy, adaptando seus princ\u00edpios de clareza e organiza\u00e7\u00e3o l\u00f3gica \u00e0s necessidades espec\u00edficas do SysIdentPy no dom\u00ednio de identifica\u00e7\u00e3o de sistemas e previs\u00e3o de s\u00e9ries temporais, enquanto enfatiza tutoriais e reprodutibilidade.</p>"},{"location":"pt/developer-guide/documentation-overview/#motivacao-e-escopo","title":"Motiva\u00e7\u00e3o e Escopo","text":"<p>A documenta\u00e7\u00e3o atual do SysIdentPy (como muitos pacotes Python cient\u00edficos) mistura explica\u00e7\u00f5es conceituais, exemplos de c\u00f3digo e refer\u00eancias de API, o que pode sobrecarregar novos usu\u00e1rios. Ao adotar uma estrutura centrada no usu\u00e1rio inspirada no Di\u00e1taxis, pretendemos:</p> <ul> <li>Separar caminhos de aprendizado para iniciantes (Tutoriais) e praticantes (How-Tos).</li> <li>Melhorar o material para entendimento conceitual (Explica\u00e7\u00f5es).</li> <li>Manter um Guia de Refer\u00eancia limpo e pesquis\u00e1vel.</li> <li>Destacar os recursos do SysIdentPy.</li> </ul> <p>Uma estrutura de documenta\u00e7\u00e3o bem organizada pode melhorar significativamente a experi\u00eancia da nossa comunidade, fornecendo recursos espec\u00edficos para diferentes grupos de usu\u00e1rios:</p> <ul> <li> <p>Para Iniciantes: Um caminho claro e guiado com tutoriais e instru\u00e7\u00f5es passo a passo ajuda novos usu\u00e1rios a superar a curva de aprendizado.</p> </li> <li> <p>Para Pesquisadores: Recursos como fun\u00e7\u00f5es base personalizadas e configura\u00e7\u00f5es de modelo podem ser facilmente descobertos e compreendidos. Com se\u00e7\u00f5es claramente definidas, pesquisadores podem localizar rapidamente as informa\u00e7\u00f5es necess\u00e1rias para experimentar novos m\u00e9todos.</p> </li> <li> <p>Para Usu\u00e1rios Corporativos/Industriais: Guias de benchmarking e exemplos de compara\u00e7\u00e3o de modelos s\u00e3o facilmente acess\u00edveis, facilitando para profissionais da ind\u00fastria avaliar e escolher as ferramentas certas para suas necessidades espec\u00edficas.</p> </li> </ul> <p>O objetivo \u00e9 estruturar a documenta\u00e7\u00e3o para atender \u00e0s necessidades espec\u00edficas desses diversos grupos de usu\u00e1rios, tornando o processo de aprendizado mais r\u00e1pido e eficiente para todos na comunidade.</p>"},{"location":"pt/developer-guide/documentation-overview/#estrutura-proposta","title":"Estrutura Proposta","text":"<p>Aqui est\u00e1 uma vis\u00e3o geral das principais se\u00e7\u00f5es da documenta\u00e7\u00e3o, descrevendo o prop\u00f3sito e o conte\u00fado proposto para cada uma:</p> <ul> <li>Primeiros Passos</li> <li>Guia do Usu\u00e1rio</li> <li>Guia do Desenvolvedor</li> <li>Comunidade &amp; Suporte</li> <li>Sobre</li> </ul>"},{"location":"pt/developer-guide/documentation-overview/#guia-do-usuario","title":"Guia do Usu\u00e1rio","text":"<p>A se\u00e7\u00e3o Guia do Usu\u00e1rio \u00e9 projetada para fornecer uma compreens\u00e3o abrangente do SysIdentPy, cobrindo conceitos essenciais, exemplos pr\u00e1ticos e recursos avan\u00e7ados. A estrutura proposta inclui:</p>"},{"location":"pt/developer-guide/documentation-overview/#tutoriais","title":"Tutoriais","text":"<p>P\u00fablico: Novos usu\u00e1rios com experi\u00eancia m\u00ednima em identifica\u00e7\u00e3o de sistemas.</p> <p>Conte\u00fado Sugerido:</p> <ul> <li> Guia do Iniciante   Comece do zero com guias f\u00e1ceis de seguir projetados para aqueles novos no SysIdentPy e modelos NARMAX.</li> <li> Tutoriais Espec\u00edficos por Dom\u00ednio   Exemplos e casos de uso para \u00e1reas como engenharia, sa\u00fade, finan\u00e7as e outras.</li> </ul> <p>Formato: Jupyter Notebooks com explica\u00e7\u00f5es narrativas e c\u00f3digo.</p>"},{"location":"pt/developer-guide/documentation-overview/#how-tos","title":"How-Tos","text":"<p>P\u00fablico: Praticantes resolvendo problemas espec\u00edficos.</p> <p>Conte\u00fado Sugerido:</p> <ul> <li> Otimiza\u00e7\u00e3o de Modelos</li> <li> Customiza\u00e7\u00f5es Avan\u00e7adas</li> <li> An\u00e1lise de Erros</li> <li> Reprodutibilidade</li> </ul> <p>Formato: Arquivos markdown curtos e focados em tarefas com snippets de c\u00f3digo.</p>"},{"location":"pt/developer-guide/documentation-overview/#explicacoes","title":"Explica\u00e7\u00f5es","text":"<p>P\u00fablico: Usu\u00e1rios buscando fundamentos matem\u00e1ticos rigorosos.</p> <ul> <li> Livro Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy. Oferece contexto te\u00f3rico para os algoritmos do SysIdentPy atrav\u00e9s de um livro.</li> </ul>"},{"location":"pt/developer-guide/documentation-overview/#referencia-da-api","title":"Refer\u00eancia da API","text":"<p>P\u00fablico: Usu\u00e1rios avan\u00e7ados precisando de detalhes da API.</p> <ul> <li> Refer\u00eancia da API   Acesse o c\u00f3digo-fonte completo do SysIdentPy com m\u00f3dulos e m\u00e9todos bem documentados.</li> </ul> <p>Formato: Documenta\u00e7\u00e3o de API gerada automaticamente com se\u00e7\u00f5es \"Veja Tamb\u00e9m\" com links cruzados.</p>"},{"location":"pt/developer-guide/documentation-overview/#guia-do-desenvolvedor","title":"Guia do Desenvolvedor","text":"<p>A se\u00e7\u00e3o Guia do Desenvolvedor visa fornecer informa\u00e7\u00f5es claras sobre a estrutura interna do SysIdentPy, focando em detalhes de implementa\u00e7\u00e3o, exemplos de c\u00f3digo e op\u00e7\u00f5es de customiza\u00e7\u00e3o. A estrutura proposta inclui:</p>"},{"location":"pt/developer-guide/documentation-overview/#como-contribuir","title":"Como Contribuir","text":"<p>P\u00fablico: Mantenedores e contribuidores de c\u00f3digo aberto.</p> <ul> <li> Guia do Contribuidor</li> </ul>"},{"location":"pt/developer-guide/documentation-overview/#guia-de-documentacao","title":"Guia de Documenta\u00e7\u00e3o","text":"<p>P\u00fablico: Mantenedores e contribuidores de c\u00f3digo aberto.</p> <ul> <li> Escrevendo um tutorial</li> <li> Criando um guia how-to</li> <li> Criando conte\u00fado para o livro</li> </ul>"},{"location":"pt/developer-guide/documentation-overview/#comunidade-suporte","title":"Comunidade &amp; Suporte","text":"<p>P\u00fablico: Indiv\u00edduos de todos os n\u00edveis de experi\u00eancia, de iniciantes a especialistas, com interesse em Python e SysIdentPy.</p> <ul> <li> Obter Ajuda</li> <li> Workshops</li> <li> Sugest\u00f5es de Leitura</li> <li> Discuss\u00f5es da Comunidade</li> </ul>"},{"location":"pt/developer-guide/how-to-add-a-translation/","title":"Como Adicionar uma Tradu\u00e7\u00e3o","text":"<p>Guia para criar ou melhorar tradu\u00e7\u00f5es da documenta\u00e7\u00e3o em qualquer idioma.</p> <p>Usamos MkDocs + Material + <code>mkdocs-static-i18n</code>. Ingl\u00eas \u00e9 o fallback. Qualquer idioma novo replica a estrutura de pastas. Se faltar uma p\u00e1gina traduzida, aparece a vers\u00e3o inglesa.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#1-visao-geral","title":"1. Vis\u00e3o geral","text":"<p>Tr\u00eas cen\u00e1rios comuns:</p> <ol> <li>Melhorar uma p\u00e1gina j\u00e1 traduzida.</li> <li>Traduzir uma p\u00e1gina que s\u00f3 existe em ingl\u00eas.</li> <li>Adicionar um idioma totalmente novo.</li> </ol> <p>Tudo abaixo cobre esses casos.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#2-estrutura-de-pastas","title":"2. Estrutura de pastas","text":"<pre><code>/docs\n  en/\n    developer-guide/\n    getting-started/\n    user-guide/\n  &lt;locale&gt;/\n    (mesmos caminhos relativos)\n</code></pre> <p><code>&lt;locale&gt;</code> exemplos: <code>pt</code>, <code>es</code>, <code>fr</code>, <code>de</code>, <code>it</code>, <code>ja</code>, <code>zh</code>, <code>ru</code>. Use c\u00f3digos curtos (BCP\u201147). Evite variantes regionais salvo necessidade (<code>pt-BR</code>, <code>pt-PT</code>).</p> <p>Os caminhos relativos devem ser id\u00eanticos:</p> <pre><code>Ingl\u00eas: docs/en/developer-guide/how-to-add-a-translation.md\nEspanhol: docs/es/developer-guide/how-to-add-a-translation.md\nFranc\u00eas:  docs/fr/developer-guide/how-to-add-a-translation.md\n</code></pre>"},{"location":"pt/developer-guide/how-to-add-a-translation/#3-inicio-rapido-traduzir-ou-melhorar","title":"3. In\u00edcio r\u00e1pido (traduzir ou melhorar)","text":"<ol> <li>Fork e clone.</li> <li>Crie / ative ambiente virtual.</li> <li>Instale extras de docs:     <pre><code>pip install -e \".[docs]\"\n</code></pre></li> <li>Servidor local:     <pre><code>mkdocs serve\n</code></pre></li> <li>Abra a URL e use o seletor de idioma.</li> </ol> <p>Reinicie se arquivos novos n\u00e3o aparecerem.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#4-adicionando-um-novo-idioma-setup-inicial","title":"4. Adicionando um novo idioma (setup inicial)","text":"<p>Se a pasta j\u00e1 existe (ex: <code>pt/</code>), pule.</p> <ol> <li>Escolha c\u00f3digo (ex: <code>es</code>).</li> <li>Crie <code>docs/es/</code>.</li> <li>Copie <code>docs/en/index.md</code> para <code>docs/es/index.md</code> e traduza.</li> <li>(Opcional) Comece s\u00f3 com index + p\u00e1ginas principais para PR menor.</li> <li>Edite <code>mkdocs.yml</code> em <code>i18n.languages</code>:     <pre><code>- locale: es\n  name: Espa\u00f1ol\n  build: true\n  site_description: &lt;slogan traduzido&gt;\n  theme:\n    docs_dir: docs/es/\n    custom_dir: docs/es/\n    site_dir: site/es/\n    logo: overrides/assets/img/logotype-sysidentpy.svg\n</code></pre></li> <li>N\u00e3o duplique a navega\u00e7\u00e3o; o plugin mapeia automaticamente.</li> <li>Rode <code>mkdocs serve</code> e confirme o idioma no seletor.</li> </ol> <p>Para variantes regionais (ex: <code>pt-BR</code>) mantenha consist\u00eancia no nome da pasta e no <code>locale</code>.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#5-nova-pagina-em-ingles-fonte","title":"5. Nova p\u00e1gina em ingl\u00eas (fonte)","text":"<ol> <li>Crie em <code>docs/en/...</code>.</li> <li>Front matter:     <pre><code>---\ntemplate: overrides/main.html\ntitle: T\u00edtulo\n---\n</code></pre></li> <li>Adicione no <code>nav</code> do <code>mkdocs.yml</code> (apenas uma vez).</li> <li>Verifique build.</li> <li>(Opcional) Coment\u00e1rio para tradutores:     <pre><code>&lt;!-- Nota para tradu\u00e7\u00e3o: manter \"NARMAX\" em ingl\u00eas. --&gt;\n</code></pre></li> </ol>"},{"location":"pt/developer-guide/how-to-add-a-translation/#6-criando-o-arquivo-traduzido","title":"6. Criando o arquivo traduzido","text":"<ol> <li> <p>Caminho espelhado: <code>docs/&lt;locale&gt;/&lt;mesmo&gt;.md</code>.</p> </li> <li> <p>Copie o original.</p> </li> <li> <p>Traduza s\u00f3 texto natural. Preserve:</p> <ul> <li>Blocos de c\u00f3digo (coment\u00e1rios apenas se ajudar)</li> <li>Identificadores (fun\u00e7\u00f5es, classes, imports)</li> <li>Caminhos, chaves de config, URLs</li> <li>Alvos de links relativos</li> </ul> </li> <li> <p>Mantenha hierarquia de t\u00edtulos.</p> </li> <li> <p>Preserve tipos de admonitions (<code>!!! note</code>, etc.). T\u00edtulo interno pode ser traduzido.</p> </li> <li> <p>Parte pendente? Use:    <pre><code>!!! note \"Tradu\u00e7\u00e3o pendente\"\n    Este par\u00e1grafo ainda ser\u00e1 traduzido.\n</code></pre></p> </li> <li> <p>Remova notas pendentes antes de finalizar (se concluir).</p> </li> </ol>"},{"location":"pt/developer-guide/how-to-add-a-translation/#7-links-internos","title":"7. Links internos","text":"<p>Use links relativos: <pre><code>Veja o [guia de contribui\u00e7\u00e3o](contribute.md).\n</code></pre> O plugin resolve por idioma. Evite hardcode <code>/en/</code> ou outro prefixo.</p> <p>\u00c2ncoras: se traduzir t\u00edtulo, o slug muda; ajuste refer\u00eancias <code>(#ancora)</code>.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#8-imagens-e-midia","title":"8. Imagens e m\u00eddia","text":"<p>Se a imagem cont\u00e9m texto:</p> <ul> <li>Op\u00e7\u00e3o A: localizar imagem dentro de <code>docs/&lt;locale&gt;/assets/</code> com mesmo nome.</li> <li>Op\u00e7\u00e3o B: reutilizar imagem inglesa se o texto n\u00e3o atrapalha.</li> </ul> <p>SVG: manter s\u00edmbolos ou termos t\u00e9cnicos; traduzir r\u00f3tulos descritivos.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#9-formatacao-estilo","title":"9. Formata\u00e7\u00e3o &amp; estilo","text":"Aspecto Regra N\u00fameros Mantenha precis\u00e3o; separador decimal local \u00e9 opcional. Unidades N\u00e3o traduzir (ms, Hz, etc.). APIs Nunca traduzir identificadores. Aspas Use padr\u00e3o local sem quebrar Markdown. Capitaliza\u00e7\u00e3o Igual s\u00f3 para nomes pr\u00f3prios / APIs. Tom Neutro, direto. <p>Evite blocos n\u00e3o revisados de tradu\u00e7\u00e3o autom\u00e1tica. Prefira frases curtas.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#10-glossario-de-traducao","title":"10. Gloss\u00e1rio de tradu\u00e7\u00e3o","text":"<p>Mantenha estes termos consistentes. Adicione equivalentes para outros idiomas conforme necess\u00e1rio:</p> Termo em Ingl\u00eas Portugu\u00eas (pt) Conceitos centrais model structure estrutura do modelo parameter estimation estima\u00e7\u00e3o de par\u00e2metros residual analysis an\u00e1lise dos res\u00edduos time series s\u00e9rie temporal identification identifica\u00e7\u00e3o Termos t\u00e9cnicos basis function fun\u00e7\u00e3o de base regression regress\u00e3o algorithm algoritmo validation valida\u00e7\u00e3o simulation simula\u00e7\u00e3o Termos de desenvolvimento feature funcionalidade pull request (PR) pull request (PR) branch branch commit commit documentation documenta\u00e7\u00e3o <p>Para outros idiomas, siga padr\u00f5es similares. Prefira clareza \u00e0 tradu\u00e7\u00e3o literal.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#11-checklist-de-revisao-arquivo-traduzido","title":"11. Checklist de revis\u00e3o (arquivo traduzido)","text":"<ul> <li> Build local OK.</li> <li> Caminho espelhado correto.</li> <li> Links relativos sem <code>/en/</code> fixo.</li> <li> Blocos de c\u00f3digo intactos (coment\u00e1rios revisados).</li> <li> Terminologia consistente.</li> <li> Sem notas pendentes (ou marcadas claramente se parcial).</li> <li> Front matter com <code>title:</code> traduzido.</li> </ul>"},{"location":"pt/developer-guide/how-to-add-a-translation/#12-commit-pr","title":"12. Commit &amp; PR","text":"<p>Inclua arquivo ingl\u00eas + traduzido se a p\u00e1gina \u00e9 nova; caso contr\u00e1rio s\u00f3 o traduzido.</p> <p>Exemplo: <pre><code>git add docs/en/developer-guide/new-topic.md docs/es/developer-guide/new-topic.md\ngit commit -m \"docs: adicionar tradu\u00e7\u00e3o em espanhol de new-topic\"\n</code></pre></p> <p>Template de descri\u00e7\u00e3o de PR:</p> <ul> <li>Idioma: <code>&lt;locale&gt;</code></li> <li>P\u00e1ginas: lista</li> <li>Trechos ainda em ingl\u00eas: (se houver)</li> <li>Termos novos de gloss\u00e1rio: (se houver)</li> <li>Notas para revis\u00e3o: contexto, termos dif\u00edceis</li> </ul> <p>Tradu\u00e7\u00f5es parciais s\u00e3o aceit\u00e1veis \u2014 marque claramente.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#13-atualizando-traducoes","title":"13. Atualizando tradu\u00e7\u00f5es","text":"<p>Quando o ingl\u00eas mudar:</p> <ol> <li>Veja o diff.</li> <li>Aplique mudan\u00e7as equivalentes.</li> <li>Sem tempo para traduzir? Deixe em ingl\u00eas + nota tempor\u00e1ria.</li> <li>Remova a nota ao finalizar.</li> </ol> <p>Prefira PRs menores.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#14-problemas-comuns","title":"14. Problemas comuns","text":"Sintoma Causa Corre\u00e7\u00e3o P\u00e1gina s\u00f3 em ingl\u00eas Falta arquivo no locale Criar arquivo espelhado Erro de build Entrada <code>i18n</code> incorreta Corrigir <code>locale</code> / indenta\u00e7\u00e3o Link 404 Caminho diferente do ingl\u00eas Sincronizar caminho \u00c2ncora quebrada T\u00edtulo mudou Ajustar slug / t\u00edtulo Idioma n\u00e3o aparece Faltou adicionar em <code>mkdocs.yml</code> Adicionar e reiniciar"},{"location":"pt/developer-guide/how-to-add-a-translation/#15-automacao-opcional","title":"15. Automa\u00e7\u00e3o (opcional)","text":"<p>Scripts podem copiar estrutura base, mas revise manualmente termos t\u00e9cnicos. N\u00e3o sobrescreva tradu\u00e7\u00f5es existentes.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#16-duvidas","title":"16. D\u00favidas","text":"<p>Abra Issue ou Discussion para confirmar termos antes de traduzir grandes trechos. Feedback cedo evita retrabalho.</p> <p>Obrigado por tornar a documenta\u00e7\u00e3o acess\u00edvel a mais pessoas.</p>"},{"location":"pt/developer-guide/how-to-write-a-how-to-guide/","title":"Como Escrever um Guia How-to","text":"<p>Em breve</p>"},{"location":"pt/developer-guide/how-to-write-a-tutorial/","title":"Como Escrever um Tutorial","text":"<p>Em breve</p>"},{"location":"pt/getting-started/getting-started/","title":"Primeiros Passos","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o do SysIdentPy! Aprenda como come\u00e7ar a usar o SysIdentPy no seu projeto. Em seguida, explore os principais conceitos e descubra recursos adicionais para modelar sistemas din\u00e2micos e s\u00e9ries temporais.</p>          \ud83d\udcda Em busca de mais detalhes sobre modelos NARMAX? \u25bc <p>             Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e um conjunto de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>             Esse livro oferece uma orienta\u00e7\u00e3o detalhada para auxiliar no seu trabalho com o SysIdentPy.         </p> <p>             \ud83d\udee0\ufe0f Voc\u00ea tamb\u00e9m pode explorar os tutoriais na documenta\u00e7\u00e3o para exemplos pr\u00e1ticos.         </p>"},{"location":"pt/getting-started/getting-started/#o-que-e-o-sysidentpy","title":"O que \u00e9 o SysIdentPy","text":"<p>SysIdentPy \u00e9 uma biblioteca Python de c\u00f3digo aberto para Identifica\u00e7\u00e3o de Sistemas usando modelos NARMAX, constru\u00edda sobre o NumPy e distribu\u00edda sob a licen\u00e7a BSD de 3 cl\u00e1usulas. SysIdentPy disponibiliza uma estrutura flex\u00edvel e f\u00e1cil de usar para construir modelos din\u00e2micos n\u00e3o lineares para s\u00e9ries temporais e sistemas din\u00e2micos.</p> <p>Com o SysIdentPy, voc\u00ea pode:</p> <ul> <li>Construir e customizar modelos n\u00e3o lineares para previs\u00e3o de s\u00e9ries temporais e sistemas din\u00e2micos.</li> <li>Utilizar t\u00e9cnicas inovadoras para sele\u00e7\u00e3o de estrutura e estima\u00e7\u00e3o de par\u00e2metros do modelo.</li> <li>Experimentar modelos NARX neurais e outros algoritmos avan\u00e7ados.</li> </ul>"},{"location":"pt/getting-started/getting-started/#instalacao","title":"Instala\u00e7\u00e3o","text":"<p>SysIdentPy \u00e9 publicado como um pacote Python e pode ser instalado com <code>pip</code>, de prefer\u00eancia em um ambiente virtual. Caso n\u00e3o tenha experi\u00eancia, role a p\u00e1gina e expanda a caixa de ajuda. Instale com:</p> \u00daltima Vers\u00e3o <pre><code>pip install sysidentpy</code></pre> Suporte NARX Neural <pre><code>pip install sysidentpy[\"all\"]</code></pre> Vers\u00e3o Espec\u00edfica <pre><code>pip install sysidentpy==\"0.5.3\"</code></pre> Do Git <pre><code>pip install git+https://github.com/wilsonrljr/sysidentpy.git</code></pre>          \u2753 Como gerenciar as depend\u00eancias do meu projeto? \u25bc <p>             Se voc\u00ea n\u00e3o tem experi\u00eancia pr\u00e9via com Python, recomendamos a leitura de                              Using Python's pip to Manage Your Projects' Dependencies             , que \u00e9 uma excelente introdu\u00e7\u00e3o \u00e0 mec\u00e2nica de gerenciamento de pacotes em Python e ajuda na solu\u00e7\u00e3o de erros.         </p>"},{"location":"pt/getting-started/getting-started/#quais-sao-os-principais-recursos-do-sysidentpy","title":"Quais s\u00e3o os principais recursos do SysIdentPy?","text":"\ud83e\udde9 Filosofia NARMAX <p>Construa varia\u00e7\u00f5es como NARX, NAR, ARMA, NFIR e outras.</p> \ud83d\udcdd Sele\u00e7\u00e3o da Estrutura <p>Use m\u00e9todos como FROLS, MetaMSS e combina\u00e7\u00f5es com t\u00e9cnicas de estima\u00e7\u00e3o de par\u00e2metros.</p> \ud83d\udd17 Fun\u00e7\u00f5es Base <p>Escolha entre 8+ fun\u00e7\u00f5es base, combinando tipos lineares e n\u00e3o lineares para modelos NARMAX personalizados.</p> \ud83c\udfaf Estima\u00e7\u00e3o de Par\u00e2metros <p>Mais de 15 m\u00e9todos para explorar diferentes cen\u00e1rios em conjunto com t\u00e9cnicas de sele\u00e7\u00e3o de estrutura.</p> \u2696\ufe0f T\u00e9cnicas Multiobjetivo <p>Minimize diferentes fun\u00e7\u00f5es objetivo usando informa\u00e7\u00e3o afim para estima\u00e7\u00e3o de par\u00e2metros.</p> \ud83d\udd04 Simula\u00e7\u00e3o de Modelos <p>Reproduza resultados de artigos com SimulateNARMAX. Teste e compare modelos publicados em artigos.</p> \ud83e\udd16 NARX Neural (PyTorch) <p>Integre com PyTorch para arquiteturas NARX neurais usando qualquer otimizador e fun\u00e7\u00e3o de custo.</p> \ud83d\udee0\ufe0f Estimadores Gerais <p>Compat\u00edvel com scikit-learn, CatBoost e mais para criar modelos NARMAX.</p>"},{"location":"pt/getting-started/getting-started/#recursos-adicionais","title":"Recursos adicionais","text":"<ul> <li> \ud83e\udd1d Contribua com o SysIdentPy </li> <li> \ud83d\udcdc Informa\u00e7\u00f5es de Licen\u00e7a </li> <li> \ud83c\udd98 Ajuda &amp; Suporte </li> <li> \ud83d\udcc5 Palestras </li> <li> \ud83d\udc96 Torne-se um Patrocinador </li> <li> \ud83e\udde9 Explore o C\u00f3digo Fonte </li> </ul>"},{"location":"pt/getting-started/getting-started/#voce-gosta-do-sysidentpy","title":"Voc\u00ea gosta do SysIdentPy?","text":"<p>Gostaria de ajudar o SysIdentPy, outros usu\u00e1rios e o criador da biblioteca? Voc\u00ea pode \"dar uma estrela\" ao projeto no GitHub clicando no bot\u00e3o de estrela no canto superior direito da p\u00e1gina: https://github.com/wilsonrljr/sysidentpy. \u2b50\ufe0f</p> <p>Ao marcar um reposit\u00f3rio com estrela, voc\u00ea o encontra mais facilmente no futuro, recebe sugest\u00f5es de projetos relacionados no GitHub e ainda valoriza o trabalho do mantenedor.</p> <p>Considere, tamb\u00e9m, apoiar o projeto tornando-se um sponsor. Seu apoio ajuda a manter o desenvolvimento ativo e garante a evolu\u00e7\u00e3o cont\u00ednua do SysIdentPy.</p> <p> \u00a0 Seja um  Patrocinador no GitHub</p>"},{"location":"pt/getting-started/license/","title":"Licen\u00e7a","text":"<p>Licen\u00e7a BSD de 3 Cl\u00e1usulas</p> <p>Copyright \u00a9 2019, Wilson Rocha; Luan Pascoal; Samuel Oliveira; Samir Martins Todos os direitos reservados.</p> <p>A redistribui\u00e7\u00e3o e o uso nas formas de c\u00f3digo-fonte e bin\u00e1rio, com ou sem modifica\u00e7\u00e3o, s\u00e3o permitidos desde que as seguintes condi\u00e7\u00f5es sejam atendidas:</p> <ul> <li> <p>As redistribui\u00e7\u00f5es do c\u00f3digo-fonte devem manter o aviso de direitos autorais acima, esta lista de condi\u00e7\u00f5es e o aviso de isen\u00e7\u00e3o de responsabilidade a seguir.</p> </li> <li> <p>As redistribui\u00e7\u00f5es em formato bin\u00e1rio devem reproduzir o aviso de direitos autorais acima, esta lista de condi\u00e7\u00f5es e o aviso de isen\u00e7\u00e3o de responsabilidade a seguir na documenta\u00e7\u00e3o e/ou em outros materiais fornecidos com a distribui\u00e7\u00e3o.</p> </li> <li> <p>Nem o nome do detentor dos direitos autorais nem os nomes de seus colaboradores podem ser usados para endossar ou promover produtos derivados deste software sem permiss\u00e3o pr\u00e9via espec\u00edfica por escrito.</p> </li> </ul> <p>ESTE SOFTWARE \u00c9 FORNECIDO PELOS DETENTORES DOS DIREITOS AUTORAIS E COLABORADORES \"COMO EST\u00c1\" E QUAISQUER GARANTIAS EXPRESSAS OU IMPL\u00cdCITAS, INCLUINDO, MAS N\u00c3O SE LIMITANDO A, GARANTIAS IMPL\u00cdCITAS DE COMERCIALIZA\u00c7\u00c3O E ADEQUA\u00c7\u00c3O A UM PROP\u00d3SITO ESPEC\u00cdFICO S\u00c3O REJEITADAS. EM NENHUMA HIP\u00d3TESE O DETENTOR DOS DIREITOS AUTORAIS OU OS COLABORADORES SER\u00c3O RESPONS\u00c1VEIS POR QUAISQUER DANOS DIRETOS, INDIRETOS, INCIDENTAIS, ESPECIAIS, EXEMPLARES OU CONSEQUENTES (INCLUINDO, MAS N\u00c3O SE LIMITANDO A, AQUISI\u00c7\u00c3O DE BENS OU SERVI\u00c7OS SUBSTITUTOS; PERDA DE USO, DADOS OU LUCROS; OU INTERRUP\u00c7\u00c3O DE NEG\u00d3CIOS) CAUSADOS E SOB QUALQUER TEORIA DE RESPONSABILIDADE, SEJA EM CONTRATO, RESPONSABILIDADE OBJETIVA OU DELITO (INCLUINDO NEGLIG\u00caNCIA OU OUTROS) DECORRENTES DE QUALQUER FORMA DO USO DESTE SOFTWARE, MESMO QUE AVISADOS DA POSSIBILIDADE DE TAIS DANOS.</p>"},{"location":"pt/getting-started/narmax-intro/","title":"Introdu\u00e7\u00e3o","text":"<p>Autor: Wilson Rocha Lacerda Junior</p> <p>Este \u00e9 o primeiro de uma s\u00e9rie de publica\u00e7\u00f5es explicando um pouco sobre modelos NARMAX<sup>1</sup>. Espero que o conte\u00fado dessas publica\u00e7\u00f5es ajude aqueles que usam ou gostariam de usar a biblioteca SysIdentPy.</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla gama de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o detalhada para auxiliar seu trabalho com o SysIdentPy.</p> <p>Voc\u00ea tamb\u00e9m pode explorar os tutoriais na documenta\u00e7\u00e3o para exemplos pr\u00e1ticos.</p>"},{"location":"pt/getting-started/narmax-intro/#identificacao-de-sistemas","title":"Identifica\u00e7\u00e3o de Sistemas","text":"<p>Como usarei o termo Identifica\u00e7\u00e3o de Sistemas aqui e ali, deixe-me fazer uma breve defini\u00e7\u00e3o.</p> <p> Identifica\u00e7\u00e3o de sistemas \u00e9 uma das principais \u00e1reas que lida com a modelagem de processos baseados em dados. Neste contexto, o termo \"sistema\" pode ser interpretado como qualquer conjunto de opera\u00e7\u00f5es que processam uma ou mais entradas e retornam uma ou mais sa\u00eddas. Exemplos incluem sistemas el\u00e9tricos, sistemas mec\u00e2nicos, sistemas biol\u00f3gicos, sistemas financeiros, sistemas qu\u00edmicos... literalmente qualquer coisa que voc\u00ea possa relacionar a dados de entrada e sa\u00edda. A demanda de eletricidade \u00e9 parte de um sistema cujas entradas podem ser, por exemplo, tamanho da popula\u00e7\u00e3o, quantidade de \u00e1gua nos reservat\u00f3rios, esta\u00e7\u00e3o do ano, eventos. O pre\u00e7o de um im\u00f3vel \u00e9 a sa\u00edda de um sistema cujas entradas podem ser a cidade, renda per capita, bairro, n\u00famero de quartos, idade do im\u00f3vel, entre muitos outros. Voc\u00ea entendeu a ideia.</p> <p> Embora existam muitas coisas relacionadas com Machine Learning, Statistical Learning e outros campos, cada \u00e1rea tem suas particularidades.</p>"},{"location":"pt/getting-started/narmax-intro/#entao-o-que-e-um-modelo-narmax","title":"Ent\u00e3o, o que \u00e9 um modelo NARMAX?","text":"<p>Voc\u00ea deve ter notado a semelhan\u00e7a entre o acr\u00f4nimo NARMAX com os conhecidos modelos ARX, ARMAX, etc., que s\u00e3o amplamente utilizados para previs\u00e3o de s\u00e9ries temporais. E essa semelhan\u00e7a n\u00e3o \u00e9 por acaso. Os modelos Autorregressivos com M\u00e9dia M\u00f3vel e Entrada Ex\u00f3gena (ARMAX) e suas varia\u00e7\u00f5es AR, ARX, ARMA (para citar apenas alguns) s\u00e3o uma das representa\u00e7\u00f5es matem\u00e1ticas mais utilizadas para identifica\u00e7\u00e3o de sistemas lineares.</p> <p> Vamos voltar ao modelo. Eu disse que a fam\u00edlia de modelos ARX \u00e9 comumente usada para modelar sistemas lineares. Linear \u00e9 a palavra-chave aqui. Para cen\u00e1rios n\u00e3o lineares, temos a classe NARMAX. Como relatado por Billings (um dos criadores do modelo NARMAX) no livro [Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains], NARMAX come\u00e7ou como um nome de modelo, mas logo se tornou uma filosofia quando se trata de identificar sistemas n\u00e3o lineares. Obter modelos NARMAX consiste em realizar as seguintes etapas:</p> <p>Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains</p> <ul> <li>Testes din\u00e2micos e coleta de dados;</li> <li>Escolha da representa\u00e7\u00e3o matem\u00e1tica;</li> <li>Detec\u00e7\u00e3o da estrutura do modelo;</li> <li>Estima\u00e7\u00e3o de par\u00e2metros;</li> <li>Valida\u00e7\u00e3o;</li> <li>An\u00e1lise do modelo.</li> </ul> <p>Abordaremos cada uma dessas etapas em publica\u00e7\u00f5es futuras. A ideia deste texto \u00e9 apresentar uma vis\u00e3o geral dos modelos NARMAX.</p> <p> Modelos NARMAX n\u00e3o s\u00e3o, entretanto, uma simples extens\u00e3o dos modelos ARMAX. Modelos NARMAX s\u00e3o capazes de representar os mais diferentes e complexos sistemas n\u00e3o lineares. Introduzidos em 1981 pelo Engenheiro Eletricista Stephen A. Billings, os modelos NARMAX podem ser descritos como:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>onde \\(n_y\\in \\mathbb{N}\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) s\u00e3o os atrasos m\u00e1ximos para a sa\u00edda e entrada do sistema, respectivamente; \\(x_k \\in \\mathbb{R}^{n_x}\\) \u00e9 a entrada do sistema e \\(y_k \\in \\mathbb{R}^{n_y}\\) \u00e9 a sa\u00edda do sistema no tempo discreto \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) representa incertezas e poss\u00edvel ru\u00eddo no tempo discreto \\(k\\). Neste caso, \\(\\mathcal{F}^\\ell\\) \u00e9 alguma fun\u00e7\u00e3o n\u00e3o linear dos regressores de entrada e sa\u00edda com grau de n\u00e3o linearidade \\(\\ell \\in \\mathbb{N}\\) e \\(d\\) \u00e9 um atraso de tempo tipicamente definido como \\(d=1\\).</p> <p>Se n\u00e3o incluirmos os termos de ru\u00eddo, \\(e_{k-n_e}\\), temos modelos NARX. Se definirmos \\(\\ell = 1\\), lidamos com modelos ARMAX; se \\(\\ell = 1\\) e n\u00e3o incluirmos termos de entrada e ru\u00eddo, torna-se um modelo AR (ARX se incluirmos entradas, ARMA se incluirmos termos de ru\u00eddo); se \\(\\ell&gt;1\\) e n\u00e3o h\u00e1 termos de entrada, temos o NARMA. Se n\u00e3o h\u00e1 termos de entrada ou ru\u00eddo, temos NAR. Existem v\u00e1rias variantes, mas isso \u00e9 suficiente por enquanto.</p>"},{"location":"pt/getting-started/narmax-intro/#representacao-narmax","title":"Representa\u00e7\u00e3o NARMAX","text":"<p>Existem v\u00e1rias representa\u00e7\u00f5es de fun\u00e7\u00f5es n\u00e3o lineares para aproximar o mapeamento desconhecido \\(\\mathrm{f}[\\cdot]\\) nos m\u00e9todos NARMAX, por exemplo:</p> <ul> <li>redes neurais;</li> <li>modelos baseados em l\u00f3gica fuzzy;</li> <li>fun\u00e7\u00f5es de base radial;</li> <li>base wavelet;</li> <li>base polinomial;</li> <li>modelos aditivos generalizados;</li> </ul> <p>O restante deste texto contempla m\u00e9todos relacionados aos modelos polinomiais na forma de pot\u00eancia, que \u00e9 a representa\u00e7\u00e3o mais comumente utilizada. O NARMAX Polinomial \u00e9 um modelo matem\u00e1tico baseado em equa\u00e7\u00f5es de diferen\u00e7a e relaciona a sa\u00edda atual como uma fun\u00e7\u00e3o de entradas e sa\u00eddas passadas.</p>"},{"location":"pt/getting-started/narmax-intro/#narmax-polinomial","title":"NARMAX Polinomial","text":"<p>O modelo NARMAX polinomial com pontos de equil\u00edbrio assintoticamente est\u00e1veis pode ser descrito como:</p> \\[\\begin{align}     y_k =&amp; \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m}\\\\     &amp;+ \\sum_{i=1}^{p}\\sum_{j=1}^{q}\\Theta_{ye}^{ij}y_{k-i} e_{k-j} + \\sum_{i=1}^{p}\\sum_{m=1}^{r}\\Theta_{yx}^{im}y_{k-i} x_{k-m} \\\\     &amp;+ \\sum_{j=1}^{q}\\sum_{m=1}^{r}\\Theta_{e x}^{jm}e_{k-j} x_{k-m} \\\\     &amp;+ \\sum_{i=1}^{p}\\sum_{j=1}^{q}\\sum_{m=1}^{r}\\Theta_{y e x}^{ijm}y_{k-i} e_{k-j} x_{k-m} \\\\     &amp;+ \\sum_{m_1=1}^{r} \\sum_{m_2=m_1}^{r}\\Theta_{x^2}^{m_1 m_2} x_{k-m_1} x_{k-m_2} \\dotsc \\\\     &amp;+ \\sum_{m_1=1}^{r} \\dotsc \\sum_{m_l=m_{l-1}}^{r} \\Theta_{x^l}^{m_1, \\dotsc, m_2} x_{k-m_1} x_{k-m_l} \\end{align}\\] <p>onde \\(\\sum\\nolimits_{0}\\), \\(c_{y}^{i}\\), \\(c_{e}^{j}\\), \\(c_{x}^{m}\\), \\(c_{y\\e}^{ij}\\), \\(c_{yx}^{im}\\), \\(c_{e x}^{jm}\\), \\(c_{y e x}^{ijm}\\), \\(c_{x^2}^{m_1 m_2} \\dotsc c_{x^l}^{m_1, \\dotsc, ml}\\) s\u00e3o par\u00e2metros constantes.</p> <p> Vamos dar uma olhada em um exemplo de modelo NARMAX para facilitar o entendimento. O seguinte \u00e9 um modelo NARMAX de grau~\\(2\\), identificado a partir de dados experimentais de um motor/gerador CC sem conhecimento pr\u00e9vio da forma do modelo. Se voc\u00ea quiser mais informa\u00e7\u00f5es sobre o processo de identifica\u00e7\u00e3o, escrevi um artigo comparando um NARMAX polinomial com um modelo NARX neural usando esses dados (EM PORTUGU\u00caS: Identifica\u00e7\u00e3o de um motor/gerador CC por meio de modelos polinomiais autorregressivos e redes neurais artificiais)</p> \\[\\begin{align}     y_k =&amp; 1.7813y_{k-1}-0,7962y_{k-2}+0,0339x_{k-1} -0,1597x_{k-1} y_{k-1} +0,0338x_{k-2} \\\\     &amp; + 0,1297x_{k-1}y_{k-2} - 0,1396x_{k-2}y_{k-1}+ 0,1086x_{k-2}y_{k-2}+0,0085y_{k-2}^2 + 0.1938e_{k-1}e_{k-2} \\end{align}\\] <p>Mas como esses termos foram selecionados? Como os par\u00e2metros foram estimados? Essas perguntas nos levar\u00e3o aos t\u00f3picos de sele\u00e7\u00e3o de estrutura do modelo e estima\u00e7\u00e3o de par\u00e2metros, mas, por enquanto, vamos discutir esses t\u00f3picos de maneira mais simples.</p> <p> Primeiro, a \"estrutura\" de um modelo \u00e9 o conjunto de termos (tamb\u00e9m chamados de regressores) inclu\u00eddos no modelo final. Os par\u00e2metros s\u00e3o os valores que multiplicam cada um desses termos. E olhando para o exemplo acima, podemos notar algo realmente importante sobre os modelos NARMAX polinomiais tratados neste texto: eles t\u00eam uma estrutura n\u00e3o linear, mas s\u00e3o lineares nos par\u00e2metros. Voc\u00ea ver\u00e1 como essa observa\u00e7\u00e3o \u00e9 importante no post sobre estima\u00e7\u00e3o de par\u00e2metros.</p> <p> Nesse sentido, considere o caso onde temos os dados de entrada e sa\u00edda de algum sistema. Por simplicidade, suponha uma entrada e uma sa\u00edda. Temos os dados, mas n\u00e3o sabemos quais atrasos escolher para a entrada ou a sa\u00edda. Al\u00e9m disso, n\u00e3o sabemos nada sobre a n\u00e3o linearidade do sistema. Ent\u00e3o, temos que definir alguns valores para os atrasos m\u00e1ximos da entrada, sa\u00edda e dos termos de ru\u00eddo, al\u00e9m da escolha do valor de \\(\\ell\\). Vale notar que muitas suposi\u00e7\u00f5es feitas para casos lineares n\u00e3o s\u00e3o v\u00e1lidas no cen\u00e1rio n\u00e3o linear e, portanto, selecionar os atrasos m\u00e1ximos n\u00e3o \u00e9 trivial. Ent\u00e3o, como esses valores podem tornar a modelagem mais dif\u00edcil?</p> <p> Ent\u00e3o temos uma entrada e uma sa\u00edda (desconsidere os termos de ru\u00eddo por enquanto). E se escolhermos \\(n_y = n_x = \\ell = 2\\)? Com esses valores, temos as seguintes possibilidades para compor o modelo final:</p> \\[\\begin{align}     &amp; constant, y_{k-1}, y_{k-2}, y_{k-1}^2, y_{k-2}^2, x_{k-1}, x_{k-2}, x_{k-1}^2, x_{k-2}^2,y_{k-1}y_{k-2},\\\\     &amp; y_{k-1}x_{k-1}, y_{k-1}x_{k-2}, y_{k-2}x_{k-1}, y_{k-2}x_{k-2}, x_{k-1}x_{k-2} . \\end{align}\\] <p>Ent\u00e3o temos \\(15\\) termos candidatos para compor o modelo final.</p> <p> Novamente, n\u00e3o sabemos quais desses termos s\u00e3o significativos para compor o modelo. Algu\u00e9m poderia decidir usar todos os termos porque s\u00e3o apenas \\(15\\). Isso, mesmo em um cen\u00e1rio simples como este, pode levar a uma representa\u00e7\u00e3o muito errada do sistema que voc\u00ea est\u00e1 tentando modelar. Ok, e se executarmos um algoritmo de for\u00e7a bruta para testar os regressores candidatos e selecionar apenas os significativos? Neste caso, temos \\(2^{15} = 32768\\) poss\u00edveis estruturas de modelo para serem testadas.</p> <p> Voc\u00ea pode pensar que est\u00e1 tudo bem, temos poder computacional para isso. Mas este caso \u00e9 muito simples e o sistema pode ter atrasos iguais a \\(10\\) para entrada e sa\u00edda. Se definirmos \\(n_y = n_x = 10\\) e \\(\\ell=2\\), o n\u00famero de modelos poss\u00edveis a serem testados aumenta para \\(2^{231}=3.4508732\\times10^{69}\\). Se a n\u00e3o linearidade for definida como \\(3\\), ent\u00e3o temos \\(2^{1771} = 1.3308291989700907535925992... \\times 10^{533}\\) modelos candidatos.</p> <p> Agora, pense no caso quando temos n\u00e3o 1, mas 5, 10 ou mais entradas... e temos que incluir termos para o ru\u00eddo, e os atrasos m\u00e1ximos s\u00e3o maiores que 10... e a n\u00e3o linearidade \u00e9 maior que 3...</p> <p> E o problema n\u00e3o \u00e9 resolvido apenas identificando os termos mais significativos. Como voc\u00ea escolhe o n\u00famero de termos para incluir no modelo final? N\u00e3o se trata apenas de verificar a relev\u00e2ncia de cada regressor, temos que pensar no impacto de incluir \\(5\\), \\(10\\) ou \\(50\\) regressores no modelo. E n\u00e3o esque\u00e7a: ap\u00f3s selecionar os termos, temos que estimar seus par\u00e2metros.</p> <p> Como voc\u00ea pode ver, selecionar os termos mais significativos de um enorme dicion\u00e1rio de termos poss\u00edveis n\u00e3o \u00e9 uma tarefa f\u00e1cil. E \u00e9 dif\u00edcil n\u00e3o apenas por causa do complexo problema combinat\u00f3rio e da incerteza sobre a ordem do modelo. Identificar os termos mais significativos em um cen\u00e1rio n\u00e3o linear \u00e9 muito dif\u00edcil porque depende do tipo de n\u00e3o linearidade (singularidade esparsa ou comportamento quase singular, efeitos de mem\u00f3ria ou amortecimento e muitos outros), resposta din\u00e2mica (sistemas espa\u00e7o-temporais, dependentes do tempo), resposta em regime permanente, frequ\u00eancia dos dados, o ru\u00eddo...</p> <p> Apesar de toda essa complexidade, os modelos NARMAX s\u00e3o amplamente utilizados porque s\u00e3o capazes de representar sistemas complexos com modelos simples e transparentes, cujos termos s\u00e3o selecionados usando algoritmos robustos para sele\u00e7\u00e3o de estrutura do modelo. A sele\u00e7\u00e3o de estrutura do modelo \u00e9 o n\u00facleo dos m\u00e9todos NARMAX e a comunidade cient\u00edfica \u00e9 muito ativa em melhorar m\u00e9todos cl\u00e1ssicos e desenvolver novos. Como eu disse, apresentarei alguns desses m\u00e9todos em outro post.</p> <p> Espero que esta publica\u00e7\u00e3o tenha servido como uma breve introdu\u00e7\u00e3o aos modelos NARMAX. Al\u00e9m disso, espero ter despertado seu interesse nessa classe de modelos. Os links para os outros textos ser\u00e3o disponibilizados em breve, mas sinta-se \u00e0 vontade para entrar em contato conosco se estiver interessado em colaborar com a biblioteca SysIdentPy ou se quiser esclarecer qualquer d\u00favida.</p> <ol> <li> <p>Modelos Autorregressivos N\u00e3o Lineares com M\u00e9dia M\u00f3vel e Entrada Ex\u00f3gena.\u00a0\u21a9</p> </li> </ol>"},{"location":"pt/getting-started/quickstart-guide/","title":"Uso B\u00e1sico","text":""},{"location":"pt/getting-started/quickstart-guide/#1-pre-requisitos","title":"1. Pr\u00e9-requisitos","text":"<p>Voc\u00ea precisa conhecer um pouco de Python.</p> <p>Para executar os exemplos, al\u00e9m do NumPy voc\u00ea precisar\u00e1 do <code>pandas</code> instalado.</p> <pre><code>pip install sysidentpy pandas\n# Opcional: Para redes neurais e recursos avan\u00e7ados\npip install sysidentpy[\"all\"]\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#2-principais-recursos","title":"2. Principais Recursos","text":"<p>SysIdentPy oferece uma estrutura flex\u00edvel para construir, validar e visualizar modelos n\u00e3o lineares de s\u00e9ries temporais e sistemas din\u00e2micos. O processo de modelagem envolve algumas etapas: definir a representa\u00e7\u00e3o matem\u00e1tica, escolher o algoritmo de estima\u00e7\u00e3o de par\u00e2metros, selecionar a estrutura do modelo e determinar o horizonte de previs\u00e3o.</p> <p>Os seguintes recursos est\u00e3o dispon\u00edveis no SysIdentPy:</p>"},{"location":"pt/getting-started/quickstart-guide/#classes-de-modelo","title":"Classes de Modelo","text":"<ul> <li>NARMAX, NARX, NARMA, NAR, NFIR, ARMAX, ARX, AR e suas variantes.</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#representacoes-matematicas","title":"Representa\u00e7\u00f5es Matem\u00e1ticas","text":"<ul> <li>Polynomial (Polinomial)</li> <li>Neural</li> <li>Fourier</li> <li>Laguerre</li> <li>Bernstein</li> <li>Bilinear</li> <li>Legendre</li> <li>Hermite</li> <li>HermiteNormalized</li> </ul> <p>Voc\u00ea tamb\u00e9m pode definir modelos NARX como Bayesian e Gradient Boosting usando a classe GeneralNARX, que oferece integra\u00e7\u00e3o direta com v\u00e1rios algoritmos de aprendizado de m\u00e1quina.</p>"},{"location":"pt/getting-started/quickstart-guide/#algoritmos-de-selecao-de-estrutura","title":"Algoritmos de Sele\u00e7\u00e3o de Estrutura","text":"<ul> <li>Forward Regression Orthogonal Least Squares (FROLS)</li> <li>Meta-model Structure Selection (MeMoSS / MetaMSS)</li> <li>Accelerated Orthogonal Least Squares (AOLS)</li> <li>Entropic Regression (ER)</li> <li>Ultra Orthogonal Least Squares (UOLS)</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#metodos-de-estimacao-de-parametros","title":"M\u00e9todos de Estima\u00e7\u00e3o de Par\u00e2metros","text":"<ul> <li>M\u00ednimos Quadrados (MQ)</li> <li>Total Least Squares (TLS)</li> <li>M\u00ednimos Quadrados Recursivos (MQR)</li> <li>Ridge Regression</li> <li>Non-Negative Least Squares (NNLS)</li> <li>Least Squares Minimal Residues (LSMR)</li> <li>Bounded Variable Least Squares (BVLS)</li> <li>Least Mean Squares (LMS) e suas variantes:</li> <li>Affine LMS</li> <li>LMS with Sign Error</li> <li>Normalized LMS</li> <li>LMS with Normalized Sign Error</li> <li>LMS with Sign Regressor</li> <li>Normalized LMS with Sign Sign</li> <li>Leaky LMS</li> <li>Fourth-Order LMS</li> <li>Mixed Norm LMS</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#criterios-de-selecao-de-ordem","title":"Crit\u00e9rios de Sele\u00e7\u00e3o de Ordem","text":"<ul> <li>Crit\u00e9rio de Informa\u00e7\u00e3o de Akaike (AIC)</li> <li>Crit\u00e9rio de Informa\u00e7\u00e3o de Akaike Corrigido (AICc)</li> <li>Crit\u00e9rio de Informa\u00e7\u00e3o Bayesiano (BIC)</li> <li>Final Prediction Error (FPE)</li> <li>Khundrin's Law of Iterated Logarithm Criterion (LILC)</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#metodos-de-previsao","title":"M\u00e9todos de Previs\u00e3o","text":"<ul> <li>Um passo \u00e0 frente (one-step ahead)</li> <li>n passos \u00e0 frente (n-steps ahead)</li> <li>Infinito passos \u00e0 frente / simula\u00e7\u00e3o livre (infinity-steps / free run simulation)</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#ferramentas-de-visualizacao","title":"Ferramentas de Visualiza\u00e7\u00e3o","text":"<ul> <li>Gr\u00e1ficos de previs\u00e3o</li> <li>An\u00e1lise de res\u00edduos</li> <li>Visualiza\u00e7\u00e3o da estrutura do modelo</li> <li>Visualiza\u00e7\u00e3o de par\u00e2metros</li> </ul> <p>Como voc\u00ea pode ver, o SysIdentPy suporta diversas combina\u00e7\u00f5es de modelos. N\u00e3o se preocupe em escolher todas as configura\u00e7\u00f5es logo no come\u00e7o. Vamos come\u00e7ar com as configura\u00e7\u00f5es padr\u00e3o.</p>          \ud83d\udcda Em busca de mais detalhes sobre modelos NARMAX? \u25bc <p>             Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e um conjunto de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>             Esse livro oferece uma orienta\u00e7\u00e3o detalhada para auxiliar no seu trabalho com o SysIdentPy.         </p> <p>             \ud83d\udee0\ufe0f Voc\u00ea tamb\u00e9m pode explorar os tutoriais na documenta\u00e7\u00e3o para exemplos pr\u00e1ticos.         </p>"},{"location":"pt/getting-started/quickstart-guide/#3-guia-rapido","title":"3. Guia R\u00e1pido","text":"<p>Para manter as coisas simples, vamos carregar alguns dados simulados para os exemplos.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\n\n# Gera um conjunto de dados de um sistema din\u00e2mico simulado.\nx_train, x_valid, y_train, y_valid = get_siso_data(\n        n=300,\n        colored_noise=False,\n        sigma=0.0001,\n        train_percentage=80\n)\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#construa-seu-primeiro-modelo-narx","title":"Construa seu primeiro modelo NARX","text":"<p>Com os dados carregados, vamos construir um modelo NARX Polinomial. Usando as configura\u00e7\u00f5es padr\u00e3o, voc\u00ea precisa definir pelo menos o m\u00e9todo de sele\u00e7\u00e3o de estrutura e a representa\u00e7\u00e3o matem\u00e1tica (fun\u00e7\u00e3o base).</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\n</code></pre> <p>O m\u00e9todo de sele\u00e7\u00e3o de estrutura (MSS) habilita as opera\u00e7\u00f5es de \"treinamento\" e previs\u00e3o do modelo.</p> <p>Embora diferentes algoritmos tenham diferentes hiperpar\u00e2metros, esse n\u00e3o \u00e9 o foco aqui. Mostraremos como modific\u00e1-los, mas n\u00e3o discutiremos as melhores configura\u00e7\u00f5es nesse guia.</p> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Para avaliar o desempenho, voc\u00ea pode usar qualquer m\u00e9trica dispon\u00edvel na biblioteca. Exemplo com Root Relative Squared Error (RRSE):</p> <pre><code>from sysidentpy.metrics import root_relative_squared_error\n\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00014\n</code></pre> <p>Para visualizar a equa\u00e7\u00e3o final do modelo polinomial, use a fun\u00e7\u00e3o <code>results</code>. Ela requer a seguinte configura\u00e7\u00e3o:</p> <ul> <li><code>final_model</code>: Regressoras selecionadas ap\u00f3s o ajuste</li> <li><code>theta</code>: Par\u00e2metros estimados</li> <li><code>err</code>: Error Reduction Ratio (ERR)</li> </ul> <pre><code>from sysidentpy.utils.display_results import results\n\nr = pd.DataFrame(\n        results(\n                model.final_model, model.theta, model.err,\n                model.n_terms, err_precision=8, dtype='sci'\n        ),\n        columns=['Regressores', 'Par\u00e2metros', 'ERR'])\nprint(r)\n</code></pre> <p>Resultado (exemplo):</p> <pre><code>Regressores     Par\u00e2metros        ERR\n0        x1(k-2)     0.9000  0.95556574\n1         y(k-1)     0.1999  0.04107943\n2  x1(k-1)y(k-1)     0.1000  0.00335113\n</code></pre> <p>Para visualizar o desempenho do modelo:</p> <p><pre><code>from sysidentpy.utils.plotting import plot_results\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> </p> <p>Analisar res\u00edduos de um modelo \u00e9 essencial. Podemos calcular a autocorrela\u00e7\u00e3o dos res\u00edduos e correla\u00e7\u00e3o cruzada entre res\u00edduos e entradas conforme exemplo abaixo:</p> <pre><code>from sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n        compute_residues_autocorrelation,\n        compute_cross_correlation,\n)\n\n# Autocorrela\u00e7\u00e3o\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Res\u00edduos\", ylabel=\"$e^2$\")\n\n# Correla\u00e7\u00e3o cruzada com uma entrada\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Res\u00edduos\", ylabel=\"$x_1e$\")\n</code></pre> <p>C\u00f3digo completo para refer\u00eancia:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n        compute_residues_autocorrelation,\n        compute_cross_correlation,\n)\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n        n=300,\n        colored_noise=False,\n        sigma=0.0001,\n        train_percentage=80\n)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n        results(\n                model.final_model, model.theta, model.err,\n                model.n_terms, err_precision=8, dtype='sci'\n                ),\n        columns=['Regressores', 'Par\u00e2metros', 'ERR'])\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000, figsize=(15, 4))\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Res\u00edduos\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Res\u00edduos\", ylabel=\"$x_1e$\")\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#personalizando-a-configuracao-do-modelo","title":"Personalizando a configura\u00e7\u00e3o do modelo","text":""},{"location":"pt/getting-started/quickstart-guide/#selecao-de-estrutura","title":"Sele\u00e7\u00e3o de Estrutura","text":"<p>Para usar o algoritmo AOLS em vez de <code>FROLS</code>:</p> <pre><code>from sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = AOLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Usando MetaMSS:</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = MetaMSS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Usando Entropic Regression (ER):</p> <pre><code>from sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#estimacao-de-parametros","title":"Estima\u00e7\u00e3o de Par\u00e2metros","text":"<p>Listar algoritmos dispon\u00edveis:</p> <pre><code>from sysidentpy import parameter_estimation\nprint(\"Algoritmos dispon\u00edveis:\", parameter_estimation.__all__)\n</code></pre> <p>Definir estimador espec\u00edfico (ex: LSMR):</p> <pre><code>from sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquaresMinimalResidual\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        estimator=LeastSquaresMinimalResidual(),\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#funcao-base-representacao-matematica","title":"Fun\u00e7\u00e3o Base (Representa\u00e7\u00e3o Matem\u00e1tica)","text":"<p>Listar fun\u00e7\u00f5es base:</p> <pre><code>from sysidentpy import basis_function\nprint(\"Fun\u00e7\u00f5es base dispon\u00edveis:\", basis_function.__all__)\n</code></pre> <p>Exemplo com Fourier:</p> <pre><code>from sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.basis_function import Fourier\nfrom sysidentpy.parameter_estimation import LeastSquaresMinimalResidual\n\nbasis_function = Fourier(degree=2)\nmodel = AOLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        estimator=LeastSquaresMinimalResidual(),\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Note</p> <pre><code>O m\u00e9todo `results` suporta apenas a base **Polynomial** no momento. Suporte a todas as fun\u00e7\u00f5es de base est\u00e1 planejado para a vers\u00e3o 1.0.\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#customizando-o-tipo-de-modelo","title":"Customizando o Tipo de Modelo","text":"<p>Diferen\u00e7a entre NARX e ARX: presen\u00e7a de termos n\u00e3o lineares. <code>degree=2</code> (Polynomial) permite um modelo potencialmente NARX; <code>degree=1</code> resulta em ARX. Por\u00e9m, a linearidade final depende da equa\u00e7\u00e3o obtida pelo m\u00e9todo de sele\u00e7\u00e3o de estrutura.</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=1)  # ARX\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Para criar um NAR:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n        ylag=2,\n        basis_function=basis_function,\n        model_type=\"NAR\",\n)\nmodel.fit(y=y_train)\nyhat = model.predict(y=y_valid, forecast_horizon=23)\n</code></pre> <p>Para NFIR (apenas entradas):</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n        xlag=2,\n        basis_function=basis_function,\n        model_type=\"NFIR\",\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>          \ud83d\udcda Quer saber mais detalhes sobre condi\u00e7\u00f5es iniciais? \u25bc <p>             Veja o cap\u00edtulo 9 do nosso livro para entender por que modelos autorregressivos precisam de condi\u00e7\u00f5es iniciais:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy"},{"location":"pt/getting-started/quickstart-guide/#horizonte-de-previsao","title":"Horizonte de Previs\u00e3o","text":"<p>Por padr\u00e3o, <code>predict</code> realiza previs\u00e3o de infinitos passos a frente (ou simula\u00e7\u00e3o livre). Para um n\u00famero espec\u00edfico de passos \u00e0 frente, use <code>steps_ahead</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\n\nyhat_1 = model.predict(X=x_valid, y=y_valid, steps_ahead=1)\nyhat_4 = model.predict(X=x_valid, y=y_valid, steps_ahead=4)\n</code></pre>          \ud83d\udcda Mais detalhes sobre previs\u00e3o com diferentes passos a frente? \u25bc <p>             Veja o cap\u00edtulo 9 do nosso livro para saber como funcionam previs\u00f5es um passo, n-passos e infinitos passos a frente:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy"},{"location":"pt/getting-started/quickstart-guide/#selecao-de-ordem","title":"Sele\u00e7\u00e3o de Ordem","text":"<p>A sele\u00e7\u00e3o de ordem \u00e9 uma abordagem cl\u00e1ssica para determinar automaticamente a ordem \u00f3tima do modelo ao utilizar o algoritmo FROLS. Esse processo auxilia na identifica\u00e7\u00e3o da melhor combina\u00e7\u00e3o dos atrasos e regressores por meio da avalia\u00e7\u00e3o de diferentes modelos com base em um crit\u00e9rio de informa\u00e7\u00e3o.</p> <p>Important</p> <pre><code>Crit\u00e9rios de informa\u00e7\u00e3o *s\u00f3 se aplicam* ao algoritmo **FROLS**.\n</code></pre> <p>Habilite com: 1. <code>order_selection=True</code> 2. <code>info_criteria=\"bic\"</code> (ou <code>\"aic\"</code>, <code>\"aicc\"</code>, <code>\"fpe\"</code>, <code>\"lilc\"</code>).</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        order_selection=True,\n        info_criteria=\"bic\"\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Controlar n\u00famero de regressores testados: <code>n_info_values</code>.</p> <pre><code>model = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        order_selection=True,\n        info_criteria=\"bic\",\n        n_info_values=50\n)\n</code></pre> <p>Important</p> <pre><code>Aumentar `n_info_values` pode melhorar a precis\u00e3o, mas aumenta o tempo computacional.\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#rede-neural-narx","title":"Rede Neural NARX","text":"<p>Exemplo com PyTorch:</p> <pre><code>from torch import nn\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\n\nbasis_function = Polynomial(degree=1)\n\nclass NARX(nn.Module):\n        def __init__(self):\n                super().__init__()\n                self.lin = nn.Linear(4, 10)\n                self.lin2 = nn.Linear(10, 10)\n                self.lin3 = nn.Linear(10, 1)\n                self.tanh = nn.Tanh()\n\n        def forward(self, xb):\n                z = self.lin(xb)\n                z = self.tanh(z)\n                z = self.lin2(z)\n                z = self.tanh(z)\n                z = self.lin3(z)\n                return z\n\nnarx_net = NARXNN(\n        net=NARX(),\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        model_type=\"NARMAX\",\n        loss_func='mse_loss',\n        optimizer='Adam',\n        epochs=200,\n        verbose=False,\n        optim_params={'betas': (0.9, 0.999), 'eps': 1e-05}\n)\n\nnarx_net.fit(X=x_train, y=y_train)\nyhat = narx_net.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=1000, figsize=(15, 4))\n</code></pre> <p></p>"},{"location":"pt/getting-started/quickstart-guide/#estimadores-gerais","title":"Estimadores Gerais","text":"<p>Voc\u00ea pode integrar qualquer estimador (scikit-learn, xgboost, catboost etc.) desde que eles sigam o padr\u00e3o <code>fit</code> e <code>predict</code>.</p> <p>Exemplo CatBoost NARX:</p> <pre><code>from sysidentpy.general_estimators import NARX\nfrom catboost import CatBoostRegressor\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\n\nbasis_function = Polynomial(degree=1)\ncatboost_narx = NARX(\n        base_estimator=CatBoostRegressor(\n                iterations=300,\n                learning_rate=0.1,\n                depth=6),\n        xlag=2,\n        ylag=2,\n        basis_function=basis_function,\n        model_type=\"NARMAX\",\n        fit_params={'verbose': False}\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=200)\n</code></pre> <p></p> <p>Sem NARX (para compara\u00e7\u00e3o):</p> <pre><code>from catboost import CatBoostRegressor\nfrom sysidentpy.utils.plotting import plot_results\n\ncatboost = CatBoostRegressor(\n        iterations=300,\n        learning_rate=0.1,\n        depth=6\n)\ncatboost.fit(x_train, y_train, verbose=False)\nplot_results(y=y_valid, yhat=catboost.predict(x_valid), figsize=(15, 4))\n</code></pre> <p></p> <p>Voc\u00ea ainda pode explorar combina\u00e7\u00f5es: usar fun\u00e7\u00e3o base Fourier, previs\u00e3o multi-passos, diferentes estimadores etc.</p> <p>Este \u00e9 apenas um guia r\u00e1pido. Para tutoriais completos, guias passo a passo, explica\u00e7\u00f5es detalhadas e casos avan\u00e7ados, veja a documenta\u00e7\u00e3o e o livro.</p>"},{"location":"pt/landing-page/about-us/","title":"Hist\u00f3rico do Projeto","text":"<p>O projeto foi iniciado por Wilson R. L. Junior, Luan Pascoal e Samir A. M. Martins como um projeto para a disciplina de Identifica\u00e7\u00e3o de Sistemas. Trabalhamos com Identifica\u00e7\u00e3o de Sistemas por v\u00e1rios anos (Sistemas N\u00e3o Lineares, Machine Learning, Sistemas Ca\u00f3ticos, modelos Hister\u00e9ticos, etc.) durante muitos anos.</p> <p> Todo trabalho que faz\u00edamos era usando uma \u00f3tima ferramenta, mas paga: Matlab. Come\u00e7amos a procurar alternativas gratuitas para construir modelos NARMAX e suas variantes (AR, ARX, ARMAX, NAR, NARX, NFIR, Neural NARX, etc.) usando os m\u00e9todos conhecidos na comunidade de Identifica\u00e7\u00e3o de Sistemas, mas n\u00e3o encontramos nenhum pacote escrito em Python com os recursos que precis\u00e1vamos para continuar nossa pesquisa.  Al\u00e9m disso, sempre foi muito dif\u00edcil encontrar c\u00f3digo-fonte dos artigos que trabalham com modelos NARMAX e reproduzir resultados era algo realmente dif\u00edcil de fazer.</p> <p> Nesse contexto, o SysIdentPy foi idealizado com o seguinte objetivo: ser um pacote gratuito e de c\u00f3digo aberto para ajudar a comunidade a projetar modelos NARMAX. Mais do que isso, ser uma alternativa gratuita e robusta a uma das ferramentas mais utilizadas para construir modelos NARMAX, que \u00e9 o System Identification Toolbox do Matlab.</p> <p> Samuel se juntou ao projeto no in\u00edcio de 2019 para nos ajudar a alcan\u00e7ar nosso objetivo.</p> <p></p>"},{"location":"pt/landing-page/about-us/#mantenedores-ativos","title":"Mantenedores Ativos","text":"<p>O projeto \u00e9 mantido ativamente por Wilson Rocha Lacerda Junior e est\u00e1 em busca de colaboradores.</p>"},{"location":"pt/landing-page/about-us/#citacao","title":"Cita\u00e7\u00e3o","text":"<p>Se voc\u00ea usar o SysIdentPy no seu projeto, por favor me avise.</p> <p>Enviar email </p> <p>Se voc\u00ea usar o SysIdentPy em sua publica\u00e7\u00e3o cient\u00edfica, agradecer\u00edamos cita\u00e7\u00f5es ao seguinte artigo:</p> <p>Lacerda et al., (2020). SysIdentPy: A Python package for System Identification using NARMAX models. Journal of Open Source Software, 5(54), 2384, https://doi.org/10.21105/joss.02384 <pre><code>    @article{Lacerda2020,\n      doi = {10.21105/joss.02384},\n      url = {https://doi.org/10.21105/joss.02384},\n      year = {2020},\n      publisher = {The Open Journal},\n      volume = {5},\n      number = {54},\n      pages = {2384},\n      author = {Wilson Rocha Lacerda Junior and Luan Pascoal Costa da Andrade and Samuel Carlos Pessoa Oliveira and Samir Angelo Milani Martins},\n      title = {SysIdentPy: A Python package for System Identification using NARMAX models},\n      journal = {Journal of Open Source Software}\n    }\n</code></pre></p>"},{"location":"pt/landing-page/about-us/#inspiracao","title":"Inspira\u00e7\u00e3o","text":"<p>A documenta\u00e7\u00e3o e estrutura (inclusive esta se\u00e7\u00e3o) s\u00e3o abertamente inspiradas no sklearn, einsteinpy e muitos outros, pois os usamos (e continuamos usando) para aprender.</p>"},{"location":"pt/landing-page/about-us/#futuro","title":"Futuro","text":"<p>O SysIdentPy j\u00e1 \u00e9 \u00fatil para muitos pesquisadores e empresas constru\u00edrem modelos NARX para sistemas din\u00e2micos. Mas ainda h\u00e1 muitas melhorias e recursos por vir. O SysIdentPy tem um grande futuro pela frente, e sua ajuda \u00e9 muito apreciada.</p>"},{"location":"pt/landing-page/about-us/#colaboradores","title":"Colaboradores","text":""},{"location":"pt/landing-page/sponsor/","title":"Patrocinadores","text":"<p>Como um projeto gratuito e de c\u00f3digo aberto, o SysIdentPy depende do apoio da comunidade para seu desenvolvimento. Se voc\u00ea trabalha para uma organiza\u00e7\u00e3o que usa e se beneficia do SysIdentPy, por favor considere nos apoiar.</p> <p>O SysIdentPy n\u00e3o segue a estrat\u00e9gia de lan\u00e7amento sponsorware, o que significa que todos os recursos s\u00e3o liberados ao p\u00fablico ao mesmo tempo. O SysIdentPy \u00e9 um projeto impulsionado pela comunidade, por\u00e9m patroc\u00ednios ajudar\u00e3o a garantir sua sustentabilidade.</p> <p>O principal objetivo dos patroc\u00ednios \u00e9 tornar este projeto sustent\u00e1vel. Sua doa\u00e7\u00e3o vai para apoiar uma variedade de servi\u00e7os e desenvolvimento, pois d\u00e3o aos mantenedores deste projeto tempo para trabalhar no desenvolvimento de novos recursos, corre\u00e7\u00e3o de bugs, melhoria de estabilidade, triagem de issues e suporte geral.</p> <p>Leia mais para saber como se tornar um patrocinador!</p>"},{"location":"pt/landing-page/sponsor/#patrocinios","title":"Patroc\u00ednios","text":"<p>Toda doa\u00e7\u00e3o conta e seria muito apreciada!</p> <p>Patroc\u00ednios come\u00e7am a partir de $1 por m\u00eas.<sup>1</sup></p>"},{"location":"pt/landing-page/sponsor/#como-se-tornar-um-patrocinador","title":"Como se tornar um patrocinador","text":"<p>Obrigado pelo seu interesse em patrocinar! Para se tornar um patrocinador eleg\u00edvel com sua conta do GitHub, visite o perfil de patrocinador do wilsonrljr, e complete um patroc\u00ednio de $1 por m\u00eas ou mais. Voc\u00ea pode usar sua conta pessoal ou de organiza\u00e7\u00e3o do GitHub para patrocinar.</p> <p> \u00a0 Junte-se aos nossos  incr\u00edveis patrocinadores</p> <p>Se voc\u00ea est\u00e1 no Brasil, pode me apoiar fazendo uma doa\u00e7\u00e3o via Pix. Basta escanear o QR code abaixo.</p> <p> </p> <p>Agradecimentos especiais aos nossos patrocinadores:</p> <p> Patrocinadores Mensais</p> <p> </p> <p> Patrocinadores Individuais</p> <p> </p>"},{"location":"pt/landing-page/sponsor/#apoiado-por","title":"Apoiado por","text":""},{"location":"pt/landing-page/sponsor/#metas","title":"Metas","text":"<p>A se\u00e7\u00e3o a seguir lista todas as metas de financiamento. Cada meta cont\u00e9m uma lista de recursos prefixados com um s\u00edmbolo de verifica\u00e7\u00e3o, indicando se um recurso est\u00e1  j\u00e1 dispon\u00edvel ou  planejado, mas ainda n\u00e3o implementado. Quando a meta de financiamento \u00e9 atingida, os recursos s\u00e3o liberados para disponibilidade geral.</p>"},{"location":"pt/landing-page/sponsor/#perguntas-frequentes","title":"Perguntas frequentes","text":"N\u00e3o quero mais patrocinar. Posso cancelar meu patroc\u00ednio? <p>Sim, voc\u00ea pode cancelar seu patroc\u00ednio a qualquer momento! Se voc\u00ea n\u00e3o quiser mais patrocinar o SysIdentPy no GitHub Sponsors, pode solicitar um cancelamento que entrar\u00e1 em vigor no final do ciclo de cobran\u00e7a. Lembre-se: patroc\u00ednios n\u00e3o s\u00e3o reembols\u00e1veis!</p> <p>Se voc\u00ea tiver algum problema ou mais perguntas, entre em contato pelo wilsonrljr@outlook.com.</p> N\u00e3o queremos pagar pelo patroc\u00ednio todo m\u00eas. Existem outras op\u00e7\u00f5es? <p>Sim. Voc\u00ea pode patrocinar anualmente mudando sua conta do GitHub para um ciclo de cobran\u00e7a anual ou simplesmente escolher uma doa\u00e7\u00e3o \u00fanica.</p> <p>Se voc\u00ea tiver algum problema ou mais perguntas, entre em contato pelo wilsonrljr@outlook.com.</p> <ol> <li> <p>Note que $1 por m\u00eas \u00e9 o valor m\u00ednimo para se tornar um patrocinador no Programa GitHub Sponsor.\u00a0\u21a9</p> </li> </ol>"},{"location":"pt/user-guide/overview/","title":"Vis\u00e3o Geral","text":"\ud83e\udde9 Tutoriais <p>Siga tutoriais pr\u00e1ticos sobre como construir modelos como NARX, Neural NARX, NFIR e variantes. Aprenda as etapas essenciais, desde a prepara\u00e7\u00e3o dos dados at\u00e9 a avalia\u00e7\u00e3o do modelo, usando os principais recursos do SysIdentPy.</p> \ud83d\udcdd Livro <p>Procurando mais detalhes sobre modelos NARMAX? Nosso livro, Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy, aborda a teoria por tr\u00e1s dos modelos e m\u00e9todos, mostrando como implement\u00e1-los usando o SysIdentPy atrav\u00e9s de uma ampla variedade de exemplos e benchmarks.</p> \ud83d\udd17 How To <p>Encontre guias pr\u00e1ticos sobre uma variedade de t\u00f3picos, incluindo sele\u00e7\u00e3o de fun\u00e7\u00f5es base, configura\u00e7\u00e3o de par\u00e2metros do modelo, realiza\u00e7\u00e3o de previs\u00f5es e avalia\u00e7\u00e3o de desempenho do modelo. Obtenha solu\u00e7\u00f5es diretas para tarefas comuns de modelagem.</p> \ud83c\udfaf API <p>Explore a refer\u00eancia completa da API com documenta\u00e7\u00e3o detalhada do c\u00f3digo-fonte do SysIdentPy. Entenda estruturas de classes, m\u00e9todos e par\u00e2metros para estender e personalizar funcionalidades para seus projetos.</p>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/","title":"Criar uma Rede Neural NARX","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/#treinamento-series-parallel-e-predicao-parallel","title":"Treinamento Series-Parallel e Predi\u00e7\u00e3o Parallel","text":"<p>Atualmente, o SysIdentPy suporta um processo de treinamento de rede Feedforward Series-Parallel (malha aberta), o que torna o processo de treinamento mais f\u00e1cil. Convertemos a rede NARX da configura\u00e7\u00e3o Series-Parallel para a configura\u00e7\u00e3o Parallel (malha fechada) para predi\u00e7\u00e3o.</p> <p>A configura\u00e7\u00e3o Series-Parallel nos permite usar o Pytorch diretamente para treinamento, ent\u00e3o podemos usar todo o poder da biblioteca Pytorch para construir nosso modelo de Rede Neural NARX!</p> <p></p> <p>O leitor \u00e9 direcionado ao seguinte artigo para uma discuss\u00e3o mais aprofundada sobre as configura\u00e7\u00f5es Series-Parallel e Parallel em redes neurais NARX:</p> <p>Parallel Training Considered Harmful?: Comparing series-parallel and parallel feedforward network training</p>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/#construindo-uma-rede-neural-narx","title":"Construindo uma Rede Neural NARX","text":"<p>Primeiro, importe os pacotes necess\u00e1rios</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>from torch import nn\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.utils.narmax_tools import regressor_code\nimport torch\n</code></pre> <pre><code>torch.cuda.is_available()\n</code></pre> <pre><code>False\n</code></pre> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</code></pre> <pre><code>Using cpu device\n</code></pre>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/#obtendo-os-dados","title":"Obtendo os dados","text":"<p>Os dados s\u00e3o gerados simulando o seguinte modelo:</p> <p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\).</p> <p>Se colored_noise for definido como True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\),</p> <p>onde \\(x\\) \u00e9 uma vari\u00e1vel aleat\u00f3ria uniformemente distribu\u00edda e \\(\\nu\\) \u00e9 uma vari\u00e1vel com distribui\u00e7\u00e3o gaussiana com \\(\\mu=0\\) e \\(\\sigma=0.1\\)</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/#escolhendo-os-parametros-narx-funcao-de-perda-e-otimizador","title":"Escolhendo os par\u00e2metros NARX, fun\u00e7\u00e3o de perda e otimizador","text":"<p>Voc\u00ea pode criar um objeto NARXNN e escolher o lag m\u00e1ximo tanto da entrada quanto da sa\u00edda para construir a matriz de regressores que servir\u00e1 como entrada da rede.</p> <p>Al\u00e9m disso, voc\u00ea pode escolher a fun\u00e7\u00e3o de perda, o otimizador, os par\u00e2metros opcionais do otimizador e o n\u00famero de \u00e9pocas.</p> <p>Como constru\u00edmos esta funcionalidade sobre o Pytorch, voc\u00ea pode escolher qualquer fun\u00e7\u00e3o de perda do torch.nn.functional. Clique aqui para uma lista das fun\u00e7\u00f5es de perda dispon\u00edveis. Voc\u00ea s\u00f3 precisa passar o nome da fun\u00e7\u00e3o de perda desejada.</p> <p>Da mesma forma, voc\u00ea pode escolher qualquer otimizador do torch.optim. Clique aqui para uma lista de otimizadores dispon\u00edveis.</p> <pre><code>basis_function = Polynomial(degree=1)\n\nnarx_net = NARXNN(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    device=\"cuda\",\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\neural_network\\narx_nn.py:156: UserWarning: No CUDA available. We set the device as CPU\n  self.device = self._check_cuda(device)\n</code></pre> <p>Como definimos nosso NARXNN usando \\(ylag=2\\), \\(xlag=2\\) e uma fun\u00e7\u00e3o base polinomial com \\(degree=1\\), temos uma matriz de regressores com 4 features. Precisamos do tamanho da matriz de regressores para construir as camadas da nossa rede. Nossos dados de entrada (x_train) t\u00eam apenas uma feature, mas como estamos criando uma rede NARX, uma matriz de regressores \u00e9 constru\u00edda internamente com novas features baseadas no xlag e ylag.</p> <p>Se voc\u00ea precisar de ajuda para descobrir quantos regressores s\u00e3o criados internamente, pode usar a fun\u00e7\u00e3o narmax_tools regressor_code e verificar o tamanho do c\u00f3digo de regressores gerado:</p> <pre><code>basis_function = Polynomial(degree=1)\n\nregressors = regressor_code(\n    X=x_train,\n    xlag=2,\n    ylag=2,\n    model_type=\"NARMAX\",\n    model_representation=\"neural_network\",\n    basis_function=basis_function,\n)\n</code></pre> <pre><code>n_features = regressors.shape[0]  # the number of features of the NARX net\nn_features\n</code></pre> <pre><code>4\n</code></pre> <pre><code>regressors\n</code></pre> <pre><code>array([[1001],\n       [1002],\n       [2001],\n       [2002]])\n</code></pre>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/#construindo-a-rede-neural-narx","title":"Construindo a Rede Neural NARX","text":"<p>A configura\u00e7\u00e3o da sua rede segue exatamente o mesmo padr\u00e3o de uma rede definida em Pytorch. O c\u00f3digo a seguir representa nossa rede neural NARX.</p> <pre><code>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n</code></pre> <p>Precisamos passar a rede definida para nosso estimador NARXNN.</p> <pre><code>narx_net.net = NARX()\n</code></pre> <pre><code>if device == \"cuda\":\n    narx_net.net.to(torch.device(\"cuda\"))\n</code></pre>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/#fit-e-predict","title":"Fit e Predict","text":"<p>Como temos uma fun\u00e7\u00e3o fit (para treinamento) e predict para o NARMAX Polinomial, criamos o mesmo padr\u00e3o para a rede NARX. Ent\u00e3o, voc\u00ea s\u00f3 precisa usar fit e predict da seguinte forma:</p> <pre><code>narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid)\n</code></pre> <pre><code>&lt;sysidentpy.neural_network.narx_nn.NARXNN at 0x19ddfff3890&gt;\n</code></pre> <pre><code>yhat = narx_net.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/#resultados","title":"Resultados","text":"<p>Agora mostramos os resultados</p> <pre><code>print(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>MSE:  0.00013103585914746256\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/#nota","title":"Nota","text":"<p>Se voc\u00ea construir a configura\u00e7\u00e3o da rede antes de chamar o NARXNN, pode simplesmente passar o modelo para o NARXNN da seguinte forma:</p> <pre><code>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n\n\nnarx_net2 = NARXNN(\n    net=NARX(),\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n\nnarx_net2.fit(X=x_train, y=y_train)\nyhat = narx_net2.predict(X=x_valid, y=y_valid)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>MSE:  0.00010086796658327408\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"pt/user-guide/how-to/create-a-narx-neural-network/#nota_1","title":"Nota","text":"<p>Lembre-se que voc\u00ea pode usar predi\u00e7\u00e3o n-steps-ahead e modelos NAR e NFIR. Verifique como us\u00e1-los em seus respectivos exemplos.</p>"},{"location":"pt/user-guide/how-to/create-custom-basis-function/","title":"Criar uma Fun\u00e7\u00e3o Base Customizada","text":"<p>Este guia espelha o exemplo dispon\u00edvel em <code>examples/custom-basis-function.ipynb</code> e mostra como \u00e9 f\u00e1cil integrar seu pr\u00f3prio gerador de features no SysIdentPy.</p> <p>Neste how-to, estendemos a classe <code>BaseBasisFunction</code> para criar um mapeamento de features harm\u00f4nicas usando apenas NumPy. A nova classe funciona exatamente como as fun\u00e7\u00f5es base nativas, podendo ser reutilizada em qualquer estimador que espera a mesma interface.</p>"},{"location":"pt/user-guide/how-to/create-custom-basis-function/#requisitos","title":"Requisitos","text":"<p>Voc\u00ea pode reutilizar o ambiente do projeto ou instalar um conjunto m\u00ednimo de pacotes:</p> <pre><code>sysidentpy\nnumpy\nmatplotlib\n</code></pre> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>O exemplo roda inteiramente em CPU.</li> <li>Nenhum dataset adicional \u00e9 necess\u00e1rio.</li> </ul>"},{"location":"pt/user-guide/how-to/create-custom-basis-function/#gerando-um-dataset-sintetico","title":"Gerando um dataset sint\u00e9tico","text":"<p>Constru\u00edmos um sistema SISO simples com um forte componente senoidal controlado pela entrada. As primeiras 1600 amostras s\u00e3o usadas para treinamento e o restante para valida\u00e7\u00e3o.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function.basis_function_base import BaseBasisFunction\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=50\n)\n</code></pre>"},{"location":"pt/user-guide/how-to/create-custom-basis-function/#implementando-a-funcao-base-customizada","title":"Implementando a fun\u00e7\u00e3o base customizada","text":"<p>A nova classe <code>HarmonicBasis</code> precisa implementar apenas os m\u00e9todos <code>fit</code> e <code>transform</code>. Internamente, criamos uma matriz que cont\u00e9m os sinais brutos mais as transforma\u00e7\u00f5es seno/cosseno para as harm\u00f4nicas solicitadas. Como a classe herda de <code>BaseBasisFunction</code>, o SysIdentPy pode utiliz\u00e1-la da mesma forma que qualquer op\u00e7\u00e3o nativa.</p> <pre><code>class HarmonicBasis(BaseBasisFunction):\n    \"\"\"Mapeia regressores defasados para features seno/cosseno.\"\"\"\n\n    def __init__(self, harmonics=(1,), include_linear=True, scale=np.pi):\n        super().__init__(degree=1)\n        self.harmonics = tuple(harmonics)\n        self.include_linear = include_linear\n        self.scale = scale\n\n    def _build_matrix(self, data, predefined_regressors):\n        features = []\n        if self.include_linear:\n            features.append(data)\n        for harmonic in self.harmonics:\n            scaled = self.scale * harmonic * data\n            features.append(np.sin(scaled))\n            features.append(np.cos(scaled))\n        if not features:\n            raise ValueError(\"The basis needs at least one active transformation.\")\n        psi = np.hstack(features)\n        if predefined_regressors is not None:\n            idx = np.asarray(predefined_regressors, dtype=int)\n            psi = psi[:, idx]\n        return psi\n\n    def fit(\n        self,\n        data,\n        max_lag=1,\n        ylag=1,\n        xlag=1,\n        model_type=\"NARMAX\",\n        predefined_regressors=None,\n    ):\n        psi = self._build_matrix(data, predefined_regressors)\n        return psi[max_lag:, :]\n\n    def transform(\n        self,\n        data,\n        max_lag=1,\n        ylag=1,\n        xlag=1,\n        model_type=\"NARMAX\",\n        predefined_regressors=None,\n    ):\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"pt/user-guide/how-to/create-custom-basis-function/#treinando-com-a-funcao-base-customizada","title":"Treinando com a fun\u00e7\u00e3o base customizada","text":"<p>O fluxo de trabalho \u00e9 id\u00eantico a todos os outros exemplos. Simplesmente passamos uma inst\u00e2ncia de <code>HarmonicBasis</code> para o <code>FROLS</code> e procedemos com treinamento, avalia\u00e7\u00e3o e visualiza\u00e7\u00e3o.</p> <pre><code>basis_function = HarmonicBasis(harmonics=(1, 2, 3), include_linear=True, scale=np.pi)\n\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    order_selection=True,\n    n_info_values=20,\n    info_criteria=\"aic\",\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NARX\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid[model.max_lag:], yhat[model.max_lag:])\nprint(f\"RRSE (validation): {rrse:.4f}\")\n</code></pre> <pre><code>plot_results(\n    y=y_valid[model.max_lag:],\n    yhat=yhat[model.max_lag:],\n    n=400,\n    figsize=(12, 4),\n    title=\"Validation results with HarmonicBasis\",\n)\n</code></pre>"},{"location":"pt/user-guide/how-to/create-custom-basis-function/#conclusao","title":"Conclus\u00e3o","text":"<p>Com apenas algumas linhas de c\u00f3digo, constru\u00edmos um substituto direto para as fun\u00e7\u00f5es base nativas. Qualquer transforma\u00e7\u00e3o NumPy/SciPy/Scikit-Learn pode ser exportada para uma classe como <code>HarmonicBasis</code>, permitindo reutilizar mapeamentos de features personalizados em todos os estimadores do SysIdentPy sem alterar o restante do seu workflow.</p>"},{"location":"pt/user-guide/how-to/save-and-load-models/","title":"Salvar e Carregar Modelos","text":"<p>Exemplo criado por Samir Angelo Milani Martins</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p>"},{"location":"pt/user-guide/how-to/save-and-load-models/#obtendo-o-modelo-usando-frols","title":"Obtendo o modelo usando FROLS","text":"<pre><code>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.save_load import save_model, load_model\n\n# Generating 1 input 1 output sample data from a benchmark system\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nyhat = model.predict(X=x_valid, y=y_valid)\n\n# Gathering results\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n</code></pre>"},{"location":"pt/user-guide/how-to/save-and-load-models/#salvando-o-modelo-obtido-no-arquivo-model_namesyspy","title":"Salvando o modelo obtido no arquivo \"model_name.syspy\"","text":"<pre><code># save_model(model_variable, file_name.syspy, path (optional))\nsave_model(model=model, file_name=\"model_name.syspy\")\n</code></pre>"},{"location":"pt/user-guide/how-to/save-and-load-models/#carregando-o-modelo-e-verificando-se-tudo-ocorreu-corretamente","title":"Carregando o modelo e verificando se tudo ocorreu corretamente","text":"<pre><code># load_model(file_name.syspy, path (optional))\nloaded_model = load_model(file_name=\"model_name.syspy\")\n\n# Predicting output with loaded_model\nyhat_loaded = loaded_model.predict(X=x_valid, y=y_valid)\n\nr_loaded = pd.DataFrame(\n    results(\n        loaded_model.final_model,\n        loaded_model.theta,\n        loaded_model.err,\n        loaded_model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\n# Printing both: original model and model loaded from file\nprint(\"\\n Original model \\n\", r)\nprint(\"\\n Model Loaded from file \\n\", r_loaded)\n\n# Checking predictions from both: original model and model loaded from file\nif (yhat == yhat_loaded).all():\n    print(\"\\n Predictions are the same!\")\n\n# Ploting results\nplot_results(y=y_valid, yhat=yhat_loaded, n=1000)\n</code></pre> <pre><code> Original model \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56631676E-01\n1         y(k-1)  1.9999E-01  3.99688899E-02\n2  x1(k-1)y(k-1)  1.0000E-01  3.39940092E-03\n\n Model Loaded from file \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56631676E-01\n1         y(k-1)  1.9999E-01  3.99688899E-02\n2  x1(k-1)y(k-1)  1.0000E-01  3.39940092E-03\n\n Predictions are the same!\n</code></pre>"},{"location":"pt/user-guide/how-to/set-specific-lags/","title":"Definir Lags Espec\u00edficos","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <p>Diferentes formas de definir o lag m\u00e1ximo para entrada e sa\u00edda</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n</code></pre>"},{"location":"pt/user-guide/how-to/set-specific-lags/#definindo-lags-usando-um-intervalo-de-valores","title":"Definindo lags usando um intervalo de valores","text":"<p>Se voc\u00ea passar valores inteiros para ylag e xlag, os lags s\u00e3o definidos como um intervalo de 1 at\u00e9 ylag e de 1 at\u00e9 xlag.</p> <p>Por exemplo: se ylag=4, ent\u00e3o os regressores candidatos s\u00e3o \\(y_{k-1}, y_{k-2}, y_{k-3}, y_{k-4}\\)</p> <pre><code>basis_function = Polynomial(degree=1)\n\nmodel = FROLS(\n    order_selection=True,\n    ylag=4,\n    xlag=4,\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n</code></pre>"},{"location":"pt/user-guide/how-to/set-specific-lags/#definindo-lags-especificos-usando-listas","title":"Definindo lags espec\u00edficos usando listas","text":"<p>Se voc\u00ea passar ylag e xlag como uma lista, apenas os lags relacionados aos valores na lista ser\u00e3o criados. \\(y_{k-1}, y_{k-4}\\),  \\(x_{k-1}, x_{k-4}\\)</p> <pre><code>model = FROLS(\n    order_selection=True,\n    ylag=[1, 4],\n    xlag=[1, 4],\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n</code></pre>"},{"location":"pt/user-guide/how-to/set-specific-lags/#definindo-lags-para-modelos-multiple-input-single-output-miso","title":"Definindo lags para modelos Multiple Input Single Output (MISO)","text":"<p>O exemplo a seguir mostra como definir lags espec\u00edficos para cada entrada. Note que precisamos usar uma lista aninhada neste caso.</p> <pre><code># O exemplo considera um modelo com 2 entradas, mas voc\u00ea pode usar o mesmo para qualquer quantidade de entradas.\n\nmodel = FROLS(\n    order_selection=True,\n    ylag=[1, 4],\n    xlag=[[1, 2, 3, 4], [1, 7]],\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n# Os lags definidos s\u00e3o:\n# x1(k-1), x1(k-2), x(k-3), x(k-4)\n# x2(k-1), x1(k-7)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"pt/user-guide/how-to/simulating-existing-models/","title":"Simular Modelos Existentes","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.simulation import SimulateNARMAX\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre>"},{"location":"pt/user-guide/how-to/simulating-existing-models/#gerando-dados-de-amostra-com-1-entrada-e-1-saida","title":"Gerando dados de amostra com 1 entrada e 1 sa\u00edda","text":""},{"location":"pt/user-guide/how-to/simulating-existing-models/#os-dados-sao-gerados-simulando-o-seguinte-modelo","title":"Os dados s\u00e3o gerados simulando o seguinte modelo:","text":"<p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}\\)</p> <p>Se colored_noise for definido como True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>onde \\(x\\) \u00e9 uma vari\u00e1vel aleat\u00f3ria uniformemente distribu\u00edda e \\(\\nu\\) \u00e9 uma vari\u00e1vel com distribui\u00e7\u00e3o gaussiana com \\(\\mu=0\\) e \\(\\sigma=0.1\\)</p> <p>No pr\u00f3ximo exemplo, geraremos dados com 1000 amostras com ru\u00eddo branco e selecionando 90% dos dados para treinar o modelo.</p> <pre><code>x_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre>"},{"location":"pt/user-guide/how-to/simulating-existing-models/#definindo-o-modelo","title":"Definindo o modelo","text":"<p>J\u00e1 sabemos que os dados gerados s\u00e3o resultado do modelo \\(\ud835\udc66_\ud835\udc58=0.2\ud835\udc66_{\ud835\udc58\u22121}+0.1\ud835\udc66_{\ud835\udc58\u22121}\ud835\udc65_{\ud835\udc58\u22121}+0.9\ud835\udc65_{\ud835\udc58\u22122}+\ud835\udc52_\ud835\udc58\\). Assim, podemos criar um modelo com esses regressores seguindo um padr\u00e3o de codifica\u00e7\u00e3o: - \\(0\\) \u00e9 o termo constante, - \\([1001] = y_{k-1}\\) - \\([100n] = y_{k-n}\\) - \\([200n] = x1_{k-n}\\) - \\([300n] = x2_{k-n}\\) - \\([1011, 1001] = y_{k-11} \\times y_{k-1}\\) - \\([100n, 100m] = y_{k-n} \\times y_{k-m}\\) - \\([12001, 1003, 1001] = x11_{k-1} \\times y_{k-3} \\times y_{k-1}\\) - e assim por diante</p>"},{"location":"pt/user-guide/how-to/simulating-existing-models/#nota-importante","title":"Nota Importante","text":"<p>A ordem dos arrays importa.</p> <p>Se voc\u00ea usar [2001, 1001], funcionar\u00e1, mas [1001, 2001] n\u00e3o (o regressor ser\u00e1 ignorado). Sempre coloque o maior valor primeiro: - \\([2003, 2001]\\) funciona - \\([2001, 2003]\\) n\u00e3o funciona</p> <p>Trataremos esta limita\u00e7\u00e3o em uma atualiza\u00e7\u00e3o futura.</p> <pre><code>s = SimulateNARMAX(\n    basis_function=Polynomial(), calculate_err=True, estimate_parameter=False\n)\n\n# the model must be a numpy array\nmodel = np.array(\n    [\n        [1001, 0],  # y(k-1)\n        [2001, 1001],  # x1(k-1)y(k-1)\n        [2002, 0],  # x1(k-2)\n    ]\n)\n# theta must be a numpy array of shape (n, 1) where n is the number of regressors\ntheta = np.array([[0.2, 0.9, 0.1]]).T\n</code></pre>"},{"location":"pt/user-guide/how-to/simulating-existing-models/#simulando-o-modelo","title":"Simulando o modelo","text":"<p>Ap\u00f3s definir o modelo e theta, s\u00f3 precisamos usar o m\u00e9todo simulate.</p> <p>O m\u00e9todo simulate retorna os valores preditos e os resultados onde podemos visualizar os regressores, par\u00e2metros e valores de ERR.</p> <pre><code>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0         y(k-1)  2.0000E-01  0.00000000E+00\n1        x1(k-2)  9.0000E-01  0.00000000E+00\n2  x1(k-1)y(k-1)  1.0000E-01  0.00000000E+00\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"pt/user-guide/how-to/simulating-existing-models/#opcoes","title":"Op\u00e7\u00f5es","text":"<p>Voc\u00ea pode definir o <code>steps_ahead</code> para executar a predi\u00e7\u00e3o/simula\u00e7\u00e3o:</p> <pre><code>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=1,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.001980394341423956\n</code></pre> <pre><code>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=21,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.0019394741034286557\n</code></pre>"},{"location":"pt/user-guide/how-to/simulating-existing-models/#estimando-os-parametros","title":"Estimando os par\u00e2metros","text":"<p>Se voc\u00ea tiver apenas a estrutura do modelo, pode criar um objeto com <code>estimate_parameter=True</code> e escolher o m\u00e9todo de estima\u00e7\u00e3o usando <code>estimator</code>. Neste caso, voc\u00ea precisa passar os dados de treinamento para estima\u00e7\u00e3o dos par\u00e2metros.</p> <p>Quando <code>estimate_parameter=True</code>, tamb\u00e9m calculamos o ERR considerando apenas os regressores definidos pelo usu\u00e1rio.</p> <pre><code>s = SimulateNARMAX(\n    basis_function=Polynomial(),\n    estimate_parameter=True,\n    estimator=LeastSquares(),\n    calculate_err=True,\n)\n\nyhat = s.simulate(\n    X_train=x_train,\n    y_train=y_train,\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    # theta will be estimated using the defined estimator\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0         y(k-1)  1.9999E-01  9.57682046E-01\n1        x1(k-2)  9.0003E-01  3.87716434E-02\n2  x1(k-1)y(k-1)  1.0009E-01  3.54306118E-03\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"pt/user-guide/how-to/use-extended-least-squares/","title":"Usar Extended Least Squares","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <p>Para usar o algoritmo Extended Least Squares (ELS), defina o par\u00e2metro <code>unbiased</code> como <code>True</code> ao definir o algoritmo de estima\u00e7\u00e3o de par\u00e2metros.</p> <pre><code>from sysidentpy.parameter_estimation import LeastSquares\n\nestimator = LeastSquares(unbiased=True)\n</code></pre> <p>O hiperpar\u00e2metro <code>unbiased</code> est\u00e1 dispon\u00edvel em todos os algoritmos de estima\u00e7\u00e3o de par\u00e2metros, com valor padr\u00e3o <code>False</code>.</p> <p>Al\u00e9m disso, o algoritmo Extended Least Squares \u00e9 iterativo. No SysIdentPy, o n\u00famero padr\u00e3o de itera\u00e7\u00f5es \u00e9 definido como 20 (<code>uiter=20</code>), j\u00e1 que estudos na literatura indicam que o algoritmo tipicamente converge entre 10 e 20 itera\u00e7\u00f5es. No entanto, voc\u00ea pode ajustar este valor para qualquer n\u00famero de itera\u00e7\u00f5es que preferir.</p> <pre><code>from sysidentpy.parameter_estimation import LeastSquares\n\nestimator = LeastSquares(unbiased=True, uiter=40)\n</code></pre> <p>Um exemplo simples, por\u00e9m completo, demonstrando a estima\u00e7\u00e3o de par\u00e2metros usando o algoritmo Extended Least Squares (ELS) \u00e9 mostrado abaixo.</p> <p>(Dados simulados s\u00e3o usados para fins ilustrativos.)</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model = FROLS(\n        order_selection=False,\n        n_terms=3,\n        ylag=2,\n        xlag=2,\n        elag=2,\n        info_criteria=\"aic\",\n        estimator=estimator,\n        basis_function=basis_function,\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\nplt.figure(figsize=(14, 4))\n\n# Compute and plot KDE for each parameter using scipy's gaussian_kde\nx_grid = np.linspace(np.min(parameters), np.max(parameters), 1000)\n\nfor i, label in enumerate([\"Parameter 1\", \"Parameter 2\", \"Parameter 3\"]):\n    kde = gaussian_kde(parameters[i, :])\n    plt.plot(x_grid, kde(x_grid), label=label)\n\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters (Matplotlib only)\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"pt/user-guide/tutorials/NFIR-model-overview/","title":"Modelo NFIR - Vis\u00e3o Geral","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <p>Este exemplo mostra como usar o SysIdentPy para construir modelos NFIR. Modelos NFIR s\u00e3o modelos sem realimenta\u00e7\u00e3o de sa\u00edda. Em outras palavras, n\u00e3o h\u00e1 regressores \\(y(k-n_y)\\), apenas \\(x(k-n_x)\\).</p> <p>O modelo NFIR pode ser descrito como:</p> \\[     y_k= F^\\ell[x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>onde \\(n_x \\in \\mathbb{N}\\) \u00e9 o lag m\u00e1ximo para a entrada do sistema; \\(x_k \\in \\mathbb{R}^{n_x}\\) \u00e9 a entrada do sistema no tempo discreto \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) representa incertezas e poss\u00edvel ru\u00eddo no tempo discreto \\(k\\). Neste caso, \\(\\mathcal{F}^\\ell\\) \u00e9 alguma fun\u00e7\u00e3o n\u00e3o-linear dos regressores de entrada com grau de n\u00e3o-linearidade \\(\\ell \\in \\mathbb{N}\\) e \\(d\\) \u00e9 um atraso de tempo tipicamente definido como \\(d=1\\).</p> <p>\u00c9 importante notar que o tamanho do modelo NFIR \u00e9 geralmente significativamente maior comparado ao tamanho de seu equivalente modelo NARMAX. Esta desvantagem pode ser notada na dimensionalidade de modelos lineares e leva a cen\u00e1rios ainda mais complexos no caso n\u00e3o-linear.</p> <p>Portanto, se voc\u00ea est\u00e1 procurando modelos parcimoniosos e compactos, considere usar modelos NARMAX. No entanto, ao comparar modelos NFIR e NARMAX, \u00e9 geralmente mais desafiador estabelecer estabilidade, particularmente em um contexto orientado a controle, com modelos NARMAX do que com modelos NFIR.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import pandas as pd\nimport numpy as np\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.model_structure_selection import AOLS, FROLS\n</code></pre> <p>NFIR x NARMAX</p> <p>Vamos reproduzir o mesmo exemplo fornecido na \"se\u00e7\u00e3o de funcionalidades principais\". Naquele exemplo, usamos um modelo NARX com xlag e ylag iguais a 2 e um grau de n\u00e3o-linearidade igual a 2. Isso resultou em um modelo com 3 regressores e um RRSE (m\u00e9trica de valida\u00e7\u00e3o) igual a \\(0.000184\\)</p> Regressores Par\u00e2metros ERR x1(k-2) 9.0001E-01 9.57505011E-01 y(k-1) 2.0001E-01 3.89117583E-02 x1(k-1)y(k-1) 9.9992E-02 3.58319976E-03"},{"location":"pt/user-guide/tutorials/NFIR-model-overview/#entao-o-que-acontece-se-eu-usar-um-modelo-nfir-com-a-mesma-configuracao","title":"Ent\u00e3o, o que acontece se eu usar um modelo NFIR com a mesma configura\u00e7\u00e3o?","text":"<pre><code>np.random.seed(seed=42)\n# gerando dados simulados\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:618: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 6\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n0.2129700627690414\n  Regressors  Parameters             ERR\n0    x1(k-2)  8.9017E-01  9.55432286E-01\n</code></pre> <p>No caso NFIR, obtivemos um modelo com 1 regressor, mas com RRSE significativamente pior (\\(0.21\\))</p> Regressores Par\u00e2metros ERR x1(k-2) 8.9017E-01 9.55432286E-01 <p>Ent\u00e3o, para obter um modelo NFIR melhor, temos que definir um modelo de ordem mais alta. Em outras palavras, temos que definir um lag m\u00e1ximo maior para construir o modelo.</p> <p>Vamos definir xlag=3.</p> <pre><code>np.random.seed(seed=42)\n# gerando dados simulados\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=3,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.04314951932710626\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9980E-01  9.55367779E-01\n1         x1(k-3)  1.7832E-01  3.94348076E-02\n2  x1(k-3)x1(k-1)  9.1104E-02  3.33315478E-03\n</code></pre> <p></p> <p>Agora, o modelo tem 3 regressores, mas o RRSE ainda \u00e9 pior (\\(0.04\\)).</p> Regressores Par\u00e2metros ERR x1(k-2) 8.9980E-01 9.55367779E-01 x1(k-3) 1.7832E-01 3.94348076E-02 x1(k-3)x1(k-1) 9.1104E-02 3.33315478E-03 <p>Vamos definir xlag=5.</p> <pre><code>np.random.seed(seed=42)\n# gerando dados simulados\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=5,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.004209451216121233\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9978E-01  9.55485306E-01\n1         x1(k-3)  1.7979E-01  3.93181813E-02\n2  x1(k-3)x1(k-1)  8.9706E-02  3.33141271E-03\n3         x1(k-4)  3.5772E-02  1.54789285E-03\n4  x1(k-4)x1(k-2)  1.7615E-02  1.09675506E-04\n5  x1(k-4)x1(k-1)  1.7871E-02  1.13215338E-04\n6         x1(k-5)  6.9594E-03  6.23773643E-05\n7  x1(k-5)x1(k-1)  4.1353E-03  6.10794551E-06\n8  x1(k-5)x1(k-3)  3.4007E-03  3.98364615E-06\n9  x1(k-5)x1(k-2)  2.9798E-03  3.42693984E-06\n</code></pre> <p></p> <p>Agora o RRSE est\u00e1 mais pr\u00f3ximo do modelo NARMAX, mas o modelo NFIR tem 10 regressores. Ent\u00e3o, como mencionado antes, a ordem dos modelos NFIR \u00e9 geralmente maior que a do modelo NARMAX para obter resultados compar\u00e1veis.</p> Regressores Par\u00e2metros ERR x1(k-2) 8.9978E-01 9.55485306E-01 x1(k-3) 1.7979E-01 3.93181813E-02 x1(k-3)x1(k-1) 8.9706E-02 3.33141271E-03 x1(k-4) 3.5772E-02 1.54789285E-03 x1(k-4)x1(k-2) 1.7615E-02 1.09675506E-04 x1(k-4)x1(k-1) 1.7871E-02 1.13215338E-04 x1(k-5) 6.9594E-03 6.23773643E-05 x1(k-5)x1(k-1) 4.1353E-03 6.10794551E-06 x1(k-5)x1(k-3) 3.4007E-03 3.98364615E-06 x1(k-5)x1(k-2) 2.9798E-03 3.42693984E-06 <p>xlag = 35</p> <pre><code>np.random.seed(seed=42)\n# gerando dados simulados\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=35,\n    n_info_values=200,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.0033427508754120074\n        Regressors  Parameters             ERR\n0          x1(k-2)  9.0009E-01  9.55386378E-01\n1          x1(k-3)  1.8001E-01  3.94178379E-02\n2   x1(k-3)x1(k-1)  9.0886E-02  3.32874170E-03\n3          x1(k-4)  3.5412E-02  1.54540871E-03\n4   x1(k-4)x1(k-2)  1.8743E-02  1.12751104E-04\n5   x1(k-4)x1(k-1)  1.8378E-02  1.13878189E-04\n6          x1(k-5)  6.7236E-03  6.27406151E-05\n7   x1(k-5)x1(k-1)  4.4974E-03  6.32909200E-06\n8   x1(k-5)x1(k-3)  3.5420E-03  3.95779051E-06\n9   x1(k-5)x1(k-2)  5.5656E-03  3.39231220E-06\n10         x1(k-6)  1.5079E-03  2.37202762E-06\n11  x1(k-6)x1(k-2)  1.8768E-03  3.65196792E-07\n12  x1(k-6)x1(k-3)  1.0685E-03  2.92529290E-07\n13  x1(k-6)x1(k-1)  6.3191E-04  2.55676107E-07\n</code></pre> <p></p> <p>Agora o RRSE est\u00e1 mais pr\u00f3ximo do modelo NARMAX, mas o modelo NFIR tem 14 regressores, com <code>RRSE=0.0033</code>. Ent\u00e3o, como voc\u00ea pode verificar nestes exemplos, a ordem dos modelos NFIR \u00e9 geralmente maior que a do modelo NARMAX para obter resultados compar\u00e1veis, mesmo tentando diferentes algoritmos de estima\u00e7\u00e3o de par\u00e2metros.</p> Regressores Par\u00e2metros ERR x1(k-2) 9.0009E-01 9.55386378E-01 x1(k-3) 1.8001E-01 3.94178379E-02 x1(k-3)x1(k-1) 9.0886E-02 3.32874170E-03 x1(k-4) 3.5412E-02 1.54540871E-03 x1(k-4)x1(k-2) 1.8743E-02 1.12751104E-04 x1(k-4)x1(k-1) 1.8378E-02 1.13878189E-04 x1(k-5) 6.7236E-03 6.27406151E-05 x1(k-5)x1(k-1) 4.4974E-03 6.32909200E-06 x1(k-5)x1(k-3) 3.5420E-03 3.95779051E-06 x1(k-5)x1(k-2) 5.5656E-03 3.39231220E-06 x1(k-6) 1.5079E-03 2.37202762E-06 x1(k-6)x1(k-2) 1.8768E-03 3.65196792E-07 x1(k-6)x1(k-3) 1.0685E-03 2.92529290E-07 x1(k-6)x1(k-1) 6.3191E-04 2.55676107E-07"},{"location":"pt/user-guide/tutorials/PV-forecasting-benchmark/","title":"Benchmark de Previs\u00e3o Fotovoltaica","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro fornece orienta\u00e7\u00f5es detalhadas para apoiar seu trabalho com o SysIdentPy.</p>"},{"location":"pt/user-guide/tutorials/PV-forecasting-benchmark/#nota","title":"Nota","text":"<p>O exemplo a seguir n\u00e3o tem a inten\u00e7\u00e3o de afirmar que uma biblioteca \u00e9 melhor que outra. O foco principal destes exemplos \u00e9 mostrar que o SysIdentPy pode ser uma boa alternativa para pessoas que desejam modelar s\u00e9ries temporais.</p> <p>Compararemos os resultados obtidos com a biblioteca neural prophet.</p> <p>Por quest\u00e3o de brevidade, do SysIdentPy apenas os m\u00e9todos MetaMSS, AOLS e FROLS (com fun\u00e7\u00e3o base polinomial) ser\u00e3o utilizados. Consulte a documenta\u00e7\u00e3o do SysIdentPy para conhecer outras formas de modelagem com a biblioteca.</p> <p>Compararemos um previsor de 1 passo \u00e0 frente em dados de irradi\u00e2ncia solar (que pode ser um proxy para produ\u00e7\u00e3o fotovoltaica). A configura\u00e7\u00e3o do modelo neuralprophet foi retirada da documenta\u00e7\u00e3o do neuralprophet (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>O treinamento ocorrer\u00e1 em 80% dos dados, reservando os \u00faltimos 20% para valida\u00e7\u00e3o.</p> <p>Nota: os dados usados neste exemplo podem ser encontrados no github do neuralprophet.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</code></pre>"},{"location":"pt/user-guide/tutorials/air-passenger-benchmark/","title":"Benchmark de Passageiros A\u00e9reos","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro fornece orienta\u00e7\u00f5es detalhadas para apoiar seu trabalho com o SysIdentPy.</p>"},{"location":"pt/user-guide/tutorials/air-passenger-benchmark/#nota","title":"Nota","text":"<p>O exemplo a seguir n\u00e3o tem a inten\u00e7\u00e3o de afirmar que uma biblioteca \u00e9 melhor que outra. O foco principal destes exemplos \u00e9 mostrar que o SysIdentPy pode ser uma boa alternativa para pessoas que desejam modelar s\u00e9ries temporais.</p> <p>Compararemos os resultados obtidos usando as bibliotecas sktime e neural prophet.</p> <p>Do sktime, os seguintes modelos ser\u00e3o utilizados:</p> <ul> <li> <p>AutoARIMA</p> </li> <li> <p>BATS</p> </li> <li> <p>TBATS</p> </li> <li> <p>Exponential Smoothing</p> </li> <li> <p>Prophet</p> </li> <li> <p>AutoETS</p> </li> </ul> <p>Por quest\u00e3o de brevidade, do SysIdentPy apenas os m\u00e9todos MetaMSS, AOLS, FROLS (com fun\u00e7\u00e3o base polinomial) e NARXNN ser\u00e3o utilizados. Consulte a documenta\u00e7\u00e3o do SysIdentPy para conhecer outras formas de modelagem com a biblioteca.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport scipy.signal.signaltools\n\n\ndef _centered(arr, newsize):\n    # Return the center newsize portion of the array.\n    newsize = np.asarray(newsize)\n    currsize = np.array(arr.shape)\n    startind = (currsize - newsize) // 2\n    endind = startind + newsize\n    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n    return arr[tuple(myslice)]\n\n\nscipy.signal.signaltools._centered = _centered\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom torch import nn\n\n# from sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.ets import AutoETS\nfrom sktime.forecasting.arima import ARIMA, AutoARIMA\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.fbprophet import Prophet\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.bats import BATS\n\n# from sktime.forecasting.model_evaluation import evaluate\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.performance_metrics.forecasting import mean_squared_error\nfrom sktime.utils.plotting import plot_series\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</code></pre>"},{"location":"pt/user-guide/tutorials/air-passenger-benchmark/#dados-de-passageiros-aereos","title":"Dados de passageiros a\u00e9reos","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 amostras para teste\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</code></pre> <pre><code>121 23\n</code></pre>"},{"location":"pt/user-guide/tutorials/air-passenger-benchmark/#resultados","title":"Resultados","text":"No. Pacote Erro Quadr\u00e1tico M\u00e9dio 1 SysIdentPy (Modelo Neural) 316.54 2 SysIdentPy (MetaMSS) 450.99 3 SysIdentPy (AOLS) 476.64 4 NeuralProphet 501.24 5 SysIdentPy (FROLS) 805.95 6 Exponential Smoothing 910.52 7 Prophet 1186.00 8 AutoArima 1714.47 9 Arima Manual 2085.42 10 ETS 2590.05 11 BATS 7286.64 12 TBATS 7448.43"},{"location":"pt/user-guide/tutorials/air-passenger-benchmark/#sysidentpy-frols","title":"SysIdentPy FROLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    ylag=13,  # os lags para todos os modelos ser\u00e3o 13\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\nsysidentpy.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(y=y_test, forecast_horizon=23)\nfrols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</code></pre> <pre><code>805.9521186338106\n</code></pre>"},{"location":"pt/user-guide/tutorials/air-passenger-benchmark/#sysidentpy-neural-narx","title":"SysIdentPy Neural NARX","text":"<pre><code>import torch\n\ntorch.manual_seed(42)\n\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=36)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nx_train = np.zeros_like(y_train)\nx_test = np.zeros_like(y_test)\n\n\nclass NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(13, 20)\n        self.lin2 = nn.Linear(20, 20)\n        self.lin3 = nn.Linear(20, 20)\n        self.lin4 = nn.Linear(20, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.relu(z)\n        z = self.lin2(z)\n        z = self.relu(z)\n        z = self.lin3(z)\n        z = self.relu(z)\n        z = self.lin4(z)\n        return z\n\n\nnarx_net = NARXNN(\n    net=NARX(),\n    ylag=13,\n    model_type=\"NAR\",\n    basis_function=Polynomial(degree=1),\n    epochs=900,\n    verbose=False,\n    learning_rate=2.5e-02,\n    optim_params={},  # par\u00e2metros opcionais do otimizador\n)\n\nnarx_net.fit(y=y_train)\nyhat = narx_net.predict(y=y_test, forecast_horizon=23)\nnarxnet_loss = loss(\n    pd.Series(y_test.flatten()[narx_net.max_lag :]),\n    pd.Series(yhat.flatten()[narx_net.max_lag :]),\n)\nprint(narxnet_loss)\nplot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :])\n</code></pre> <pre><code>316.54086775668776\n</code></pre> <pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 amostras para teste\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</code></pre> <pre><code>121 23\n</code></pre>"},{"location":"pt/user-guide/tutorials/air-passenger-benchmark/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\n\ndf = pd.read_csv(r\".\\datasets\\air_passengers.csv\")\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\ndf_train = df.iloc[:-23, :].copy()\ndf_test = df.iloc[-23:, :].copy()\n\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\n\nmetrics = m.fit(df_train, freq=\"MS\")\n\nfuture = m.make_future_dataframe(\n    df_train, periods=23, n_historic_predictions=len(df_train)\n)\n\nforecast = m.predict(future)\nplt.plot(forecast[\"yhat1\"].values[-23:])\nplt.plot(df_test[\"y\"].values)\nneuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values)\nneuralprophet_loss\n</code></pre> <pre><code>501.24794023767436\n</code></pre> <pre><code>results = {\n    \"Exponential Smoothing\": es_loss,\n    \"ETS\": ets_loss,\n    \"AutoArima\": autoarima_loss,\n    \"Arima Manual\": manualarima_loss,\n    \"BATS\": bats_loss,\n    \"TBATS\": tbats_loss,\n    \"Prophet\": prophet_loss,\n    \"SysIdentPy (Modelo Polinomial)\": frols_loss,\n    \"SysIdentPy (Modelo Neural)\": narxnet_loss,\n    \"SysIdentPy (AOLS)\": aols_loss,\n    \"SysIdentPy (MetaMSS)\": metamss_loss,\n    \"NeuralProphet\": neuralprophet_loss,\n}\n\nsorted(results.items(), key=lambda result: result[1])\n</code></pre> <pre><code>[('SysIdentPy (Modelo Neural)', 316.54086775668776),\n ('SysIdentPy (MetaMSS)', 450.992127624293),\n ('SysIdentPy (AOLS)', 476.64996316992523),\n ('NeuralProphet', 501.24794023767436),\n ('SysIdentPy (Modelo Polinomial)', 805.9521186338106),\n ('Exponential Smoothing', 910.462659260655),\n ('Prophet', 1186.0045566050442),\n ('AutoArima', 1714.4753226965322),\n ('ETS', 1739.117296439066),\n ('Arima Manual', 2085.425167938668),\n ('BATS', 7286.6484525676415),\n ('TBATS', 7448.434672875093)]\n</code></pre>"},{"location":"pt/user-guide/tutorials/aols-overview/","title":"AOLS - Vis\u00e3o Geral","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <pre><code>import pandas as pd\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.model_structure_selection import AOLS\n\n# gerando dados simulados\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre> <pre><code>basis_function = Polynomial(degree=2)\nmodel = AOLS(xlag=3, ylag=3, k=5, L=1, basis_function=basis_function)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS at 0x25cd3b406d0&gt;\n</code></pre> <pre><code>yhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0018996279285613828\n      Regressors   Parameters             ERR\n0         y(k-1)   1.9999E-01  0.00000000E+00\n1        x1(k-2)   9.0003E-01  0.00000000E+00\n2  x1(k-1)y(k-1)   9.9954E-02  0.00000000E+00\n3  x1(k-3)y(k-1)  -2.1442E-04  0.00000000E+00\n4      x1(k-1)^2   3.3714E-04  0.00000000E+00\n</code></pre> <pre><code>plot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"pt/user-guide/tutorials/basis-function-overview/","title":"Fun\u00e7\u00f5es Base - Vis\u00e3o Geral","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <p>Este notebook n\u00e3o tem como objetivo encontrar os melhores modelos poss\u00edveis para identifica\u00e7\u00e3o de sistemas. Em vez disso, serve como uma demonstra\u00e7\u00e3o simples das fun\u00e7\u00f5es base dispon\u00edveis no SysIdentPy. O objetivo \u00e9 apresentar cada fun\u00e7\u00e3o base com c\u00f3digo m\u00ednimo para ilustrar como us\u00e1-las dentro do framework SysIdentPy.</p> <p>Usamos configura\u00e7\u00f5es b\u00e1sicas para sele\u00e7\u00e3o de estrutura do modelo e estima\u00e7\u00e3o de par\u00e2metros, mas para aplica\u00e7\u00f5es do mundo real, voc\u00ea pode precisar ajustar os hiperpar\u00e2metros e explorar m\u00e9todos mais avan\u00e7ados para alcan\u00e7ar resultados \u00f3timos.</p> <p>Para mais detalhes sobre o SysIdentPy e como aproveitar totalmente suas capacidades, consulte a documenta\u00e7\u00e3o oficial e o livro.</p>"},{"location":"pt/user-guide/tutorials/basis-function-overview/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Neste exemplo, exploraremos como usar o SysIdentPy para aplicar v\u00e1rias fun\u00e7\u00f5es base para identifica\u00e7\u00e3o de sistemas e sele\u00e7\u00e3o de estrutura do modelo. Usaremos um dataset simulado e aplicaremos o algoritmo FROLS com diferentes fun\u00e7\u00f5es base. Cada fun\u00e7\u00e3o base ser\u00e1 avaliada e os resultados ser\u00e3o plotados para comparar seu desempenho.</p> <p>Voc\u00ea pode aprender mais sobre as fun\u00e7\u00f5es base do SysIdentPy consultando a documenta\u00e7\u00e3o oficial.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy import basis_function\n</code></pre>"},{"location":"pt/user-guide/tutorials/basis-function-overview/#gerando-dados-simulados","title":"Gerando Dados Simulados","text":"<p>Come\u00e7amos gerando dados simulados Single-Input Single-Output (SISO) usando a fun\u00e7\u00e3o get_siso_data. Este utilit\u00e1rio nos permite criar dados realistas para tarefas de identifica\u00e7\u00e3o de sistemas. Para mais detalhes sobre como personalizar o processo de gera\u00e7\u00e3o de dados, visite a documenta\u00e7\u00e3o de utilit\u00e1rios de dados.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n</code></pre>"},{"location":"pt/user-guide/tutorials/basis-function-overview/#funcoes-base-no-sysidentpy","title":"Fun\u00e7\u00f5es Base no SysIdentPy","text":"<p>O SysIdentPy fornece v\u00e1rias fun\u00e7\u00f5es base que podem ser usadas na identifica\u00e7\u00e3o de sistemas. As fun\u00e7\u00f5es base transformam os dados de entrada em um espa\u00e7o de features, permitindo a identifica\u00e7\u00e3o de sistemas n\u00e3o-lineares.</p> <p>O seguinte trecho de c\u00f3digo carrega e instancia dinamicamente cada fun\u00e7\u00e3o base dispon\u00edvel. Voc\u00ea pode explorar a lista completa de fun\u00e7\u00f5es base dispon\u00edveis no SysIdentPy visitando a documenta\u00e7\u00e3o de fun\u00e7\u00f5es base.</p> <pre><code>basis_function.__all__\n</code></pre> <pre><code>['Bernstein',\n 'Bilinear',\n 'Fourier',\n 'Hermite',\n 'HermiteNormalized',\n 'Laguerre',\n 'Legendre',\n 'Polynomial']\n</code></pre> <pre><code>import inspect\nfrom sysidentpy import basis_function\n\nfor basis_name, bf in inspect.getmembers(basis_function):\n    if inspect.isclass(bf):\n        estimator = LeastSquares()\n        model = FROLS(\n            order_selection=True,\n            n_info_values=15,\n            ylag=2,\n            xlag=2,\n            info_criteria=\"aic\",\n            estimator=estimator,\n            err_tol=None,\n            basis_function=bf(degree=5),\n        )\n\n        model.fit(X=x_train, y=y_train)\n        yhat = model.predict(X=x_valid, y=y_valid)\n\n        plot_results(\n            y=y_valid,\n            yhat=yhat,\n            n=100,\n            title=f\"{basis_name}\",\n            xlabel=\"Samples\",\n            ylabel=r\"y, $\\hat{y}$\",\n            data_color=\"#1f77b4\",\n            model_color=\"#ff7f0e\",\n            marker=\"o\",\n            model_marker=\"*\",\n            linewidth=1.5,\n            figsize=(10, 6),\n            style=\"seaborn-v0_8-notebook\",\n            facecolor=\"white\",\n        )\n</code></pre> <pre><code>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning:  `bias` and `n` are deprecated in 0.5.0 and will be removed in 1.0.0. Use `include_bias` and `degree`, respectively, instead.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"pt/user-guide/tutorials/coupled-eletric-device/","title":"Dispositivo El\u00e9trico Acoplado","text":"<p>Nota: O exemplo mostrado neste notebook \u00e9 retirado do livro Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>O dataset CE8 de acionamentos el\u00e9tricos acoplados dataset - Nonlinear Benchmark apresenta um caso de uso interessante para demonstrar o desempenho do SysIdentPy. Este sistema envolve dois motores el\u00e9tricos acionando uma polia com uma correia flex\u00edvel, criando um ambiente din\u00e2mico ideal para testar ferramentas de identifica\u00e7\u00e3o de sistemas.</p> <p>O site de benchmarks n\u00e3o lineares representa uma contribui\u00e7\u00e3o significativa para a comunidade de identifica\u00e7\u00e3o de sistemas e aprendizado de m\u00e1quina. Os usu\u00e1rios s\u00e3o encorajados a explorar todos os artigos referenciados no site.</p>"},{"location":"pt/user-guide/tutorials/coupled-eletric-device/#visao-geral-do-sistema","title":"Vis\u00e3o Geral do Sistema","text":"<p>O sistema CE8, ilustrado na Figura 1, apresenta: - Dois Motores El\u00e9tricos: Estes motores controlam independentemente a tens\u00e3o e a velocidade da correia, fornecendo controle sim\u00e9trico em torno do zero. Isso permite movimentos tanto hor\u00e1rios quanto anti-hor\u00e1rios. - Mecanismo de Polia: A polia \u00e9 suportada por uma mola, introduzindo um modo din\u00e2mico levemente amortecido que adiciona complexidade ao sistema. - Foco no Controle de Velocidade: O foco principal \u00e9 o sistema de controle de velocidade. A velocidade angular da polia \u00e9 medida usando um contador de pulsos, que \u00e9 insens\u00edvel \u00e0 dire\u00e7\u00e3o da velocidade.</p> <p></p> <p>Figura 1. Design do sistema CE8.</p>"},{"location":"pt/user-guide/tutorials/coupled-eletric-device/#sensor-e-filtragem","title":"Sensor e Filtragem","text":"<p>O processo de medi\u00e7\u00e3o envolve: - Contador de Pulsos: Este sensor mede a velocidade angular da polia sem considerar a dire\u00e7\u00e3o. - Filtragem Anal\u00f3gica Passa-Baixa: Reduz o ru\u00eddo de alta frequ\u00eancia, seguido por filtragem anti-aliasing para preparar o sinal para processamento digital. Os efeitos din\u00e2micos s\u00e3o principalmente influenciados pelas constantes de tempo do acionamento el\u00e9trico e pela mola, com a filtragem passa-baixa tendo impacto m\u00ednimo na sa\u00edda.</p>"},{"location":"pt/user-guide/tutorials/coupled-eletric-device/#resultados-sota","title":"Resultados SOTA","text":"<p>O SysIdentPy pode ser usado para construir modelos robustos para identificar e modelar as din\u00e2micas complexas do sistema CE8. O desempenho ser\u00e1 comparado com um benchmark fornecido por Max D. Champneys, Gerben I. Beintema, Roland T\u00f3th, Maarten Schoukens, and Timothy J. Rogers - Baselines for Nonlinear Benchmarks, Workshop on Nonlinear System Identification Benchmarks, 2024.</p> <p></p> <p>O benchmark avalia a m\u00e9trica m\u00e9dia entre os dois experimentos. Por isso o m\u00e9todo SOTA n\u00e3o tem a melhor m\u00e9trica para o <code>teste 1</code>, mas ainda \u00e9 o melhor no geral. O objetivo deste estudo de caso n\u00e3o \u00e9 apenas demonstrar a robustez do SysIdentPy, mas tamb\u00e9m fornecer insights valiosos sobre suas aplica\u00e7\u00f5es pr\u00e1ticas em sistemas din\u00e2micos do mundo real.</p>"},{"location":"pt/user-guide/tutorials/coupled-eletric-device/#pacotes-e-versoes-necessarios","title":"Pacotes e Vers\u00f5es Necess\u00e1rios","text":"<p>Para garantir que voc\u00ea possa replicar este estudo de caso, \u00e9 essencial usar vers\u00f5es espec\u00edficas dos pacotes necess\u00e1rios. Abaixo est\u00e1 uma lista dos pacotes junto com suas respectivas vers\u00f5es necess\u00e1rias para executar os estudos de caso de forma eficaz.</p> <p>Para instalar todos os pacotes necess\u00e1rios, voc\u00ea pode criar um arquivo <code>requirements.txt</code> com o seguinte conte\u00fado:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Ent\u00e3o, instale os pacotes usando: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Certifique-se de usar um ambiente virtual para evitar conflitos entre vers\u00f5es de pacotes.</li> <li>As vers\u00f5es especificadas s\u00e3o baseadas na compatibilidade com os exemplos de c\u00f3digo fornecidos. Se voc\u00ea estiver usando vers\u00f5es diferentes, alguns ajustes no c\u00f3digo podem ser necess\u00e1rios.</li> </ul>"},{"location":"pt/user-guide/tutorials/coupled-eletric-device/#configuracao-do-sysidentpy","title":"Configura\u00e7\u00e3o do SysIdentPy","text":"<p>Nesta se\u00e7\u00e3o, demonstraremos a aplica\u00e7\u00e3o do SysIdentPy ao dataset CE8 de acionamentos el\u00e9tricos acoplados. Este exemplo mostra o desempenho robusto do SysIdentPy na modelagem e identifica\u00e7\u00e3o de sistemas din\u00e2micos complexos. O c\u00f3digo a seguir ir\u00e1 gui\u00e1-lo atrav\u00e9s do processo de carregamento do dataset, configura\u00e7\u00e3o dos par\u00e2metros do SysIdentPy e constru\u00e7\u00e3o de um modelo para o sistema CE8.</p> <p>Este exemplo pr\u00e1tico ajudar\u00e1 os usu\u00e1rios a entender como utilizar efetivamente o SysIdentPy para suas pr\u00f3prias tarefas de identifica\u00e7\u00e3o de sistemas, aproveitando seus recursos avan\u00e7ados para lidar com as complexidades de sistemas din\u00e2micos do mundo real. Vamos mergulhar no c\u00f3digo e explorar as capacidades do SysIdentPy.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.CED(atleast_2d=True)\ndata_train_1, data_train_2 = train_val\ndata_test_1, data_test_2 = test\n</code></pre> <p>Usamos o pacote <code>nonlinear_benchmarks</code> para carregar os dados. O usu\u00e1rio \u00e9 direcionado \u00e0 documenta\u00e7\u00e3o do pacote GerbenBeintema - nonlinear_benchmarks: The official dataload for nonlinear benchmark datasets para verificar os detalhes de como us\u00e1-lo.</p> <p>O gr\u00e1fico a seguir detalha os dados de treinamento e teste de ambos os experimentos. Aqui estamos tentando obter dois modelos, um para cada experimento, que tenham um desempenho melhor que os baselines mencionados.</p> <pre><code>plt.plot(data_train_1.u)\nplt.plot(data_train_1.y)\nplt.title(\"Experimento 1: dados de treinamento\")\nplt.show()\n\nplt.plot(data_test_1.u)\nplt.plot(data_test_1.y)\nplt.title(\"Experimento 1: dados de teste\")\nplt.show()\n\nplt.plot(data_train_2.u)\nplt.plot(data_train_2.y)\nplt.title(\"Experimento 2: dados de treinamento\")\nplt.show()\n\nplt.plot(data_test_2.u)\nplt.plot(data_test_2.y)\nplt.title(\"Experimento 2: dados de teste\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"pt/user-guide/tutorials/coupled-eletric-device/#resultados","title":"Resultados","text":"<p>Primeiro, definiremos exatamente a mesma configura\u00e7\u00e3o para construir modelos para ambos os experimentos. Podemos ter modelos melhores otimizando as configura\u00e7\u00f5es individualmente, mas come\u00e7aremos de forma simples.</p> <p>Uma configura\u00e7\u00e3o b\u00e1sica do FROLS usando uma fun\u00e7\u00e3o base polinomial com grau igual a 2 \u00e9 definida. O crit\u00e9rio de informa\u00e7\u00e3o ser\u00e1 o padr\u00e3o, o <code>aic</code>. Os <code>xlag</code> e <code>ylag</code> s\u00e3o definidos como \\(7\\) neste primeiro exemplo.</p> <p>Modelo para o experimento 1:</p> <pre><code>y_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n    n_info_values=120,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Simula\u00e7\u00e3o Free Run. Modelo 1 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <p></p> <p>Modelo para o experimento 2:</p> <pre><code>y_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n    n_info_values=120,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Simula\u00e7\u00e3o Free Run. Modelo 2 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <p></p> <p>A primeira configura\u00e7\u00e3o para o experimento 1 j\u00e1 \u00e9 melhor que os modelos LTI ARX, LTI SS, GRU, LSTM, MLP NARX, MLP FIR, OLSTM e SOTA mostrados na tabela do benchmark. Melhor que 8 de 11 modelos mostrados no benchmark. Para o experimento 2, \u00e9 melhor que LTI ARX, LTI SS, GRU, RNN, LSTM, OLSTM e pNARX (7 de 11). \u00c9 um bom come\u00e7o, mas vamos verificar se o desempenho melhora se definirmos um lag maior para <code>xlag</code> e <code>ylag</code>.</p> <p>A m\u00e9trica m\u00e9dia \u00e9 \\((0.1131 + 0.1059)/2 = 0.1095\\), o que \u00e9 muito bom, mas pior que o SOTA (\\(0.0945\\)). Agora vamos aumentar os lags para <code>x</code> e <code>y</code> para verificar se obtemos um modelo melhor. Antes de aumentar os lags, o crit\u00e9rio de informa\u00e7\u00e3o \u00e9 mostrado:</p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Crit\u00e9rio de Informa\u00e7\u00e3o\")\n</code></pre> <pre><code>Text(0, 0.5, 'Crit\u00e9rio de Informa\u00e7\u00e3o')\n</code></pre> <p></p> <p>Pode-se observar que ap\u00f3s 22 regressores, adicionar novos regressores n\u00e3o melhora o desempenho do modelo (considerando a configura\u00e7\u00e3o definida para aquele modelo). Como queremos experimentar modelos com lags maiores e grau de n\u00e3o linearidade maior, o crit\u00e9rio de parada ser\u00e1 alterado para <code>err_tol</code> em vez de crit\u00e9rio de informa\u00e7\u00e3o. Isso far\u00e1 o algoritmo rodar consideravelmente mais r\u00e1pido.</p> <pre><code># experimento 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9996,\n    n_terms=22,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Simula\u00e7\u00e3o Free Run. Modelo 1 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>(22, 2) 0.9970964868326048\n</code></pre> <p></p> <pre><code># experimento 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aicc\",\n    err_tol=0.9996,\n    n_terms=22,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Simula\u00e7\u00e3o Free Run. Modelo 2 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <p></p> <p>No primeiro experimento, o modelo mostrou uma leve melhoria, enquanto o desempenho do segundo experimento experimentou uma pequena queda. Aumentar as configura\u00e7\u00f5es de lag com estas configura\u00e7\u00f5es n\u00e3o resultou em mudan\u00e7as significativas. Portanto, vamos definir o grau polinomial para \\(3\\) e aumentar o n\u00famero de termos para construir o modelo para <code>n_terms=40</code> se o <code>err_tol</code> n\u00e3o for atingido. \u00c9 importante notar que estes valores s\u00e3o escolhidos empiricamente. Tamb\u00e9m poder\u00edamos ajustar a t\u00e9cnica de estima\u00e7\u00e3o de par\u00e2metros, o <code>err_tol</code>, o algoritmo de sele\u00e7\u00e3o de estrutura do modelo e a fun\u00e7\u00e3o base, entre outros fatores. Os usu\u00e1rios s\u00e3o encorajados a empregar t\u00e9cnicas de ajuste de hiperpar\u00e2metros para encontrar as combina\u00e7\u00f5es \u00f3timas de hiperpar\u00e2metros.</p> <pre><code># experimento 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9996,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Simula\u00e7\u00e3o Free Run. Modelo 1 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>(40, 3) 0.9982136069197526\n</code></pre> <p></p> <pre><code># experimento 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aicc\",\n    err_tol=0.9996,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Simula\u00e7\u00e3o Free Run. Modelo 2 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <p></p> <p>Como mostrado no gr\u00e1fico, superamos os resultados do estado da arte (SOTA) com uma m\u00e9trica m\u00e9dia de \\((0.0969 + 0.0731)/2 = 0.0849\\). Al\u00e9m disso, a m\u00e9trica para o primeiro experimento iguala o melhor modelo no benchmark, e a m\u00e9trica para o segundo experimento supera levemente o melhor modelo do benchmark. Usando a mesma configura\u00e7\u00e3o para ambos os modelos, alcan\u00e7amos os melhores resultados gerais!</p>"},{"location":"pt/user-guide/tutorials/electromechanical-system-identification-entropic-regression/","title":"Identifica\u00e7\u00e3o de Sistema Eletromec\u00e2nico - Entropic Regression","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro fornece orienta\u00e7\u00f5es detalhadas para apoiar seu trabalho com o SysIdentPy.</p> <p>Mais detalhes sobre estes dados podem ser encontrados no seguinte artigo (em portugu\u00eas): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>df1 = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/generator/x_cc.csv\"\n)\ndf2 = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/generator/y_cc.csv\"\n)\n</code></pre> <pre><code>df2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code># decimaremos os dados usando d=500 neste exemplo\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre>"},{"location":"pt/user-guide/tutorials/electromechanical-system-identification-entropic-regression/#construindo-um-modelo-narx-polinomial-usando-o-algoritmo-entropic-regression","title":"Construindo um Modelo NARX Polinomial usando o Algoritmo Entropic Regression","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = ER(\n    ylag=6,\n    xlag=6,\n    n_perm=2,\n    k=2,\n    skip_forward=True,\n    estimator=estimator,\n    basis_function=basis_function,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressores\", \"Par\u00e2metros\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Res\u00edduos\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Res\u00edduos\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.03276775133089435\n        Regressores   Par\u00e2metros             ERR\n0                1  -6.7052E+02  0.00000000E+00\n1           y(k-1)   9.6022E-01  0.00000000E+00\n2           y(k-5)  -3.0769E-02  0.00000000E+00\n3          x1(k-2)   7.3733E+02  0.00000000E+00\n4         y(k-1)^2   1.5897E-04  0.00000000E+00\n5     y(k-2)y(k-1)  -2.2080E-04  0.00000000E+00\n6     y(k-3)y(k-1)   2.9946E-06  0.00000000E+00\n7     y(k-5)y(k-1)   4.9779E-06  0.00000000E+00\n8    x1(k-1)y(k-1)  -1.7036E-01  0.00000000E+00\n9    x1(k-2)y(k-1)  -2.0748E-01  0.00000000E+00\n10   x1(k-4)y(k-1)   8.3724E-03  0.00000000E+00\n11        y(k-2)^2   7.3635E-05  0.00000000E+00\n12   x1(k-1)y(k-2)   1.2028E-01  0.00000000E+00\n13   x1(k-2)y(k-2)   8.0270E-02  0.00000000E+00\n14   x1(k-3)y(k-2)  -3.0208E-03  0.00000000E+00\n15   x1(k-4)y(k-2)  -8.8307E-03  0.00000000E+00\n16   x1(k-1)y(k-3)  -4.9095E-02  0.00000000E+00\n17   x1(k-1)y(k-4)   1.2375E-02  0.00000000E+00\n18       x1(k-1)^2   1.1682E+02  0.00000000E+00\n19  x1(k-3)x1(k-2)   5.2777E+00  0.00000000E+00\n</code></pre>"},{"location":"pt/user-guide/tutorials/electromechanical-system-identification-metamss/","title":"Identifica\u00e7\u00e3o de Sistema Eletromec\u00e2nico - MetaMSS","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro fornece orienta\u00e7\u00f5es detalhadas para apoiar seu trabalho com o SysIdentPy.</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import MetaMSS, FROLS\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>df1 = pd.read_csv(\"./datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"./datasets/y_cc.csv\")\n\ndf2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code>df1.iloc[::500].values.shape\n</code></pre> <pre><code>(1000, 1)\n</code></pre> <p>Decimaremos os dados usando d=500 neste exemplo. Al\u00e9m disso, separamos os dados do MetaMSS para usar a mesma quantidade de amostras na valida\u00e7\u00e3o de predi\u00e7\u00e3o. Como o MetaMSS precisa de dados de treino e teste para otimizar os par\u00e2metros do modelo, neste caso, usaremos 400 amostras para treinar em vez de 500 amostras usadas para os outros modelos.</p> <pre><code># decimaremos os dados usando d=500 neste exemplo\nx_train, x_test = np.split(df1.iloc[::500].values, 2)\ny_train, y_test = np.split(df2.iloc[::500].values, 2)\n</code></pre> <pre><code>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = MetaMSS(\n    xlag=5,\n    ylag=5,\n    estimator=estimator,\n    maxiter=5,\n    n_agents=15,\n    basis_function=basis_function,\n    random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS at 0x229e13e3150&gt;\n</code></pre> <pre><code>yhat = model.predict(X=x_test, y=y_test, steps_ahead=None)\nrrse = root_relative_squared_error(y_test[model.max_lag :, :], yhat[model.max_lag :, :])\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressores\", \"Par\u00e2metros\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Res\u00edduos\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Res\u00edduos\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.035919583498004094\n        Regressores   Par\u00e2metros             ERR\n0                1  -6.1606E+02  0.00000000E+00\n1           y(k-1)   1.3117E+00  0.00000000E+00\n2           y(k-2)  -3.0579E-01  0.00000000E+00\n3          x1(k-1)   5.7920E+02  0.00000000E+00\n4          x1(k-3)  -1.8750E-01  0.00000000E+00\n5    x1(k-1)y(k-1)  -1.7305E-01  0.00000000E+00\n6    x1(k-2)y(k-1)  -1.1660E-01  0.00000000E+00\n7    x1(k-1)y(k-2)   1.2182E-01  0.00000000E+00\n8    x1(k-2)y(k-2)   3.4112E-02  0.00000000E+00\n9    x1(k-1)y(k-3)  -4.8970E-02  0.00000000E+00\n10   x1(k-1)y(k-4)   1.3846E-02  0.00000000E+00\n11       x1(k-2)^2   1.0290E+02  0.00000000E+00\n12  x1(k-3)x1(k-2)   8.6745E-01  0.00000000E+00\n13  x1(k-4)x1(k-2)   3.4336E-01  0.00000000E+00\n14  x1(k-5)x1(k-2)   2.7815E-01  0.00000000E+00\n15       x1(k-3)^2  -9.3749E-01  0.00000000E+00\n16  x1(k-4)x1(k-3)   6.1039E-01  0.00000000E+00\n17  x1(k-5)x1(k-3)   3.9361E-02  0.00000000E+00\n18       x1(k-4)^2  -4.6335E-01  0.00000000E+00\n19  x1(k-5)x1(k-4)  -9.5668E-02  0.00000000E+00\n20       x1(k-5)^2   3.6922E-01  0.00000000E+00\n</code></pre> <p></p> <p></p> <p></p> <pre><code># Plotando a evolu\u00e7\u00e3o dos agentes\nplt.plot(model.best_by_iter)\nmodel.best_by_iter[-1]\n</code></pre> <pre><code>0.0017530517788608157\n</code></pre> <p></p> <pre><code># Voc\u00ea tem acesso a todos os modelos testados\n# model.tested_models\n</code></pre> <pre><code>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nxlag = ylag = 5\n\nestimators = [\n    (\n        \"NARX_KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX_ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            ylag=ylag,\n            xlag=xlag,\n            basis_function=basis_function,\n            info_criteria=\"bic\",\n            err_tol=None,\n        ),\n    ),\n    (\n        \"MetaMSS\",\n        MetaMSS(\n            norm=-2,\n            xlag=xlag,\n            ylag=ylag,\n            estimator=estimator,\n            maxiter=5,\n            n_agents=15,\n            loss_func=\"metamss_loss\",\n            basis_function=basis_function,\n            random_state=42,\n        ),\n    ),\n]\n\n\nall_results = {}\nfor model_name, modelo in estimators:\n    all_results[\"%s\" % model_name] = []\n    modelo.fit(X=x_train, y=y_train)\n    yhat = modelo.predict(X=x_test, y=y_test)\n    if model_name in [\"FROLS-Polynomial_NARX\", \"MetaMSS\"]:\n        result = root_relative_squared_error(\n            y_test[modelo.max_lag :], yhat[modelo.max_lag :]\n        )\n    else:\n        result = root_relative_squared_error(y_test, yhat)\n    all_results[\"%s\" % model_name].append(result)\n    print(model_name, \"%.3f\" % np.mean(result))\n</code></pre> <pre><code>NARX_KNeighborsRegressor 1.158\nNARX_DecisionTreeRegressor 0.203\nNARX_RandomForestRegressor 0.146\nNARX_Catboost 0.120\nNARX_ARD 0.083\nFROLS-Polynomial_NARX 0.057\nMetaMSS 0.036\n</code></pre> <pre><code>for model_name, metric in sorted(\n    all_results.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(model_name, np.mean(metric))\n</code></pre> <pre><code>MetaMSS 0.035919583498004094\nFROLS-Polynomial_NARX 0.05729765719062527\nNARX_ARD 0.08265856190495872\nNARX_Catboost 0.12034851661643597\nNARX_RandomForestRegressor 0.14557973585496042\nNARX_DecisionTreeRegressor 0.203057724881072\nNARX_KNeighborsRegressor 1.157787546845798\n</code></pre>"},{"location":"pt/user-guide/tutorials/electromechanical-system-identification-overview/","title":"Identifica\u00e7\u00e3o de Sistema Eletromec\u00e2nico - Vis\u00e3o Geral","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro fornece orienta\u00e7\u00f5es detalhadas para apoiar seu trabalho com o SysIdentPy.</p> <p>Mais detalhes sobre estes dados podem ser encontrados no seguinte artigo (em portugu\u00eas): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>df1 = pd.read_csv(\"../examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"../examples/datasets/y_cc.csv\")\n</code></pre> <pre><code>df2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code># decimaremos os dados usando d=500 neste exemplo\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre>"},{"location":"pt/user-guide/tutorials/electromechanical-system-identification-overview/#construindo-um-modelo-narx-polinomial","title":"Construindo um Modelo NARX Polinomial","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=100,\n    ylag=5,\n    xlag=5,\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressores\", \"Par\u00e2metros\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Res\u00edduos\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Res\u00edduos\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.05681502501595064\n        Regressores   Par\u00e2metros             ERR\n0           y(k-1)   1.5935E+00  9.86000310E-01\n1        x1(k-1)^2   1.1202E+02  7.94813324E-03\n2         y(k-2)^2  -1.7469E-05  2.50921747E-03\n3    x1(k-1)y(k-1)  -1.5994E-01  1.43297462E-03\n4           y(k-2)  -7.4013E-01  1.02774988E-03\n5    x1(k-1)y(k-2)   1.0771E-01  5.35195948E-04\n6     y(k-3)y(k-1)   4.2578E-05  3.46258211E-04\n7        x1(k-4)^2  -6.1823E+00  6.91218347E-05\n8    x1(k-1)y(k-3)  -3.0064E-02  2.83751722E-05\n9     y(k-4)y(k-1)  -1.4505E-05  2.01620114E-05\n10  x1(k-4)x1(k-1)  -2.7490E+00  1.09189469E-05\n11    y(k-4)y(k-2)   7.2062E-06  1.27131624E-05\n12   x1(k-5)y(k-1)  -8.5557E-04  6.53111914E-06\n13  x1(k-3)x1(k-2)  -9.8645E-01  4.24331903E-06\n14  x1(k-2)x1(k-1)  -2.3609E+00  6.41299982E-06\n15         x1(k-3)  -2.0121E+02  6.43059002E-06\n16   x1(k-1)y(k-5)   3.0338E-03  2.76577885E-06\n17   x1(k-3)y(k-1)   3.2426E-02  2.79523223E-06\n18   x1(k-4)y(k-1)   5.9510E-03  1.62218750E-06\n19               1  -4.2071E+01  1.13359933E-06\n</code></pre>"},{"location":"pt/user-guide/tutorials/electromechanical-system-identification-overview/#testando-diferentes-modelos-autorregressivos","title":"Testando Diferentes Modelos Autorregressivos","text":"<pre><code>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    AdaBoostRegressor,\n    GradientBoostingRegressor,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nbasis_function = Polynomial(degree=2)\nxlag = 5\nylag = 5\n\nestimators = [\n    (\n        \"KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            model_type=\"NARMAX\",\n        ),\n    ),\n    (\n        \"NARX-DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX-ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            ylag=xlag,\n            xlag=ylag,\n            info_criteria=\"bic\",\n            estimator=estimator,\n            basis_function=basis_function,\n            err_tol=None,\n        ),\n    ),\n]\n\nall_results = {}\nfor model_name, modelo in estimators:\n    all_results[\"%s\" % model_name] = []\n    modelo.fit(X=x_train, y=y_train)\n    yhat = modelo.predict(X=x_valid, y=y_valid)\n    result = root_relative_squared_error(\n        y_valid[modelo.max_lag :], yhat[modelo.max_lag :]\n    )\n    all_results[\"%s\" % model_name].append(result)\n    print(model_name, \"%.3f\" % np.mean(result))\n</code></pre> <pre><code>KNeighborsRegressor 1.168\nNARX-DecisionTreeRegressor 0.190\nNARX-RandomForestRegressor 0.151\nNARX-Catboost 0.121\nNARX-ARD 0.083\nFROLS-Polynomial_NARX 0.057\n</code></pre> <pre><code>for model_name, metric in sorted(\n    all_results.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(model_name, np.mean(metric))\n</code></pre> <pre><code>FROLS-Polynomial_NARX 0.05729765719062527\nNARX-ARD 0.08336072971138789\nNARX-Catboost 0.12137085298392238\nNARX-RandomForestRegressor 0.15102205613876338\nNARX-DecisionTreeRegressor 0.19018792321900427\nKNeighborsRegressor 1.1676227184643708\n</code></pre>"},{"location":"pt/user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/","title":"Aeronave F-16 - Predi\u00e7\u00e3o N-passos \u00e0 Frente","text":"<p>Nota: Os exemplos a seguir n\u00e3o tentam replicar os resultados dos manuscritos citados. Mesmo os par\u00e2metros do modelo como ylag e xlag e o tamanho dos dados de identifica\u00e7\u00e3o e valida\u00e7\u00e3o n\u00e3o s\u00e3o os mesmos dos artigos citados. Al\u00e9m disso, ajuste de taxa de amostragem e outras prepara\u00e7\u00f5es de dados diferentes n\u00e3o s\u00e3o tratados aqui.</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro fornece orienta\u00e7\u00f5es detalhadas para apoiar seu trabalho com o SysIdentPy.</p> <p>O texto a seguir foi retirado do link http://www.nonlinearbenchmark.org/#F16.</p> <p>Nota: O leitor \u00e9 encaminhado ao site mencionado para uma refer\u00eancia completa sobre o experimento. Por enquanto, este notebook \u00e9 apenas um exemplo simples do desempenho do SysIdentPy em um conjunto de dados do mundo real. Um estudo mais detalhado deste sistema ser\u00e1 publicado no futuro.</p> <p>O benchmark do Ground Vibration Test da F-16 apresenta um sistema de alta ordem com n\u00e3o linearidades de folga e atrito na interface de montagem das cargas \u00fateis.</p> <p>Os dados experimentais disponibilizados aos participantes do Workshop foram adquiridos em uma aeronave F-16 em escala real na ocasi\u00e3o do Siemens LMS Ground Vibration Testing Master Class, realizado em setembro de 2014 na base militar de Saffraanberg, Sint-Truiden, B\u00e9lgica.</p> <p>Durante a campanha de testes, duas cargas \u00fateis simuladas foram montadas nas pontas das asas para simular as propriedades de massa e in\u00e9rcia de dispositivos reais que tipicamente equipam uma F-16 em voo. A estrutura da aeronave foi instrumentada com aceler\u00f4metros. Um excitador foi fixado sob a asa direita para aplicar sinais de entrada. A fonte dominante de n\u00e3o linearidade nas din\u00e2micas estruturais era esperada originar das interfaces de montagem das duas cargas \u00fateis. Essas interfaces consistem em elementos de conex\u00e3o em forma de T no lado da carga \u00fatil, deslizados atrav\u00e9s de um trilho fixado ao lado da asa. Uma investiga\u00e7\u00e3o preliminar mostrou que a conex\u00e3o traseira da interface asa direita-carga \u00fatil era a fonte predominante de distor\u00e7\u00f5es n\u00e3o lineares nas din\u00e2micas da aeronave e, portanto, \u00e9 o foco deste estudo de benchmark.</p> <p>Uma formula\u00e7\u00e3o detalhada do problema de identifica\u00e7\u00e3o pode ser encontrada aqui. Todos os arquivos fornecidos e informa\u00e7\u00f5es sobre o sistema de benchmark da aeronave F-16 est\u00e3o dispon\u00edveis para download aqui. Este arquivo zip cont\u00e9m uma descri\u00e7\u00e3o detalhada do sistema, os conjuntos de dados de estima\u00e7\u00e3o e teste, e algumas fotos da configura\u00e7\u00e3o. Os dados est\u00e3o dispon\u00edveis nos formatos de arquivo .csv e .mat.</p> <p>Por favor, cite o benchmark F16 como:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n</code></pre>"},{"location":"pt/user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/#preparando-os-dados","title":"Preparando os Dados","text":"<pre><code>f_16 = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/f_16_vibration_test/f-16.txt\",\n    header=None,\n    names=[\"x1\", \"x2\", \"y\"],\n)\n</code></pre> <pre><code>f_16.shape\n</code></pre> <pre><code>(32768, 3)\n</code></pre> <pre><code>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</code></pre> <pre><code>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</code></pre> <pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"pt/user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/#construindo-o-modelo","title":"Construindo o Modelo","text":"<pre><code>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x25d368a8910&gt;\n</code></pre>"},{"location":"pt/user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/#definindo-o-horizonte-de-previsao","title":"Definindo o Horizonte de Previs\u00e3o","text":"<p>Para realizar uma predi\u00e7\u00e3o de n-passos \u00e0 frente, voc\u00ea s\u00f3 precisa definir o argumento \"steps_ahead\".</p> <p>Nota O valor padr\u00e3o para steps_ahead \u00e9 None e realiza uma predi\u00e7\u00e3o de infinitos passos \u00e0 frente</p> <pre><code>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=1)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressores\", \"Par\u00e2metros\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</code></pre> <pre><code>0.09610207940697202\n   Regressores   Par\u00e2metros             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</code></pre> <p></p> <pre><code>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=5)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</code></pre> <pre><code>0.2168472873799118\n</code></pre> <p></p> <pre><code>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=None)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</code></pre> <pre><code>0.2910089654603829\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/f-16-aircraft/","title":"Aeronave F-16","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro fornece orienta\u00e7\u00f5es detalhadas para apoiar seu trabalho com o SysIdentPy.</p> <p>Nota: Os exemplos a seguir n\u00e3o tentam replicar os resultados dos manuscritos citados. Mesmo os par\u00e2metros do modelo como ylag e xlag e o tamanho dos dados de identifica\u00e7\u00e3o e valida\u00e7\u00e3o n\u00e3o s\u00e3o os mesmos dos artigos citados. Al\u00e9m disso, ajuste de taxa de amostragem e outras prepara\u00e7\u00f5es de dados diferentes n\u00e3o s\u00e3o tratados aqui.</p>"},{"location":"pt/user-guide/tutorials/f-16-aircraft/#referencia","title":"Refer\u00eancia","text":"<p>O texto a seguir foi retirado do link http://www.nonlinearbenchmark.org/#F16.</p> <p>Nota: O leitor \u00e9 encaminhado ao site mencionado para uma refer\u00eancia completa sobre o experimento. Por enquanto, este notebook \u00e9 apenas um exemplo simples do desempenho do SysIdentPy em um conjunto de dados do mundo real. Um estudo mais detalhado deste sistema ser\u00e1 publicado no futuro.</p> <p>O benchmark do Ground Vibration Test da F-16 apresenta um sistema de alta ordem com n\u00e3o linearidades de folga e atrito na interface de montagem das cargas \u00fateis.</p> <p>Os dados experimentais disponibilizados aos participantes do Workshop foram adquiridos em uma aeronave F-16 em escala real na ocasi\u00e3o do Siemens LMS Ground Vibration Testing Master Class, realizado em setembro de 2014 na base militar de Saffraanberg, Sint-Truiden, B\u00e9lgica.</p> <p>Durante a campanha de testes, duas cargas \u00fateis simuladas foram montadas nas pontas das asas para simular as propriedades de massa e in\u00e9rcia de dispositivos reais que tipicamente equipam uma F-16 em voo. A estrutura da aeronave foi instrumentada com aceler\u00f4metros. Um excitador foi fixado sob a asa direita para aplicar sinais de entrada. A fonte dominante de n\u00e3o linearidade nas din\u00e2micas estruturais era esperada originar das interfaces de montagem das duas cargas \u00fateis. Essas interfaces consistem em elementos de conex\u00e3o em forma de T no lado da carga \u00fatil, deslizados atrav\u00e9s de um trilho fixado ao lado da asa. Uma investiga\u00e7\u00e3o preliminar mostrou que a conex\u00e3o traseira da interface asa direita-carga \u00fatil era a fonte predominante de distor\u00e7\u00f5es n\u00e3o lineares nas din\u00e2micas da aeronave e, portanto, \u00e9 o foco deste estudo de benchmark.</p> <p>Uma formula\u00e7\u00e3o detalhada do problema de identifica\u00e7\u00e3o pode ser encontrada aqui. Todos os arquivos fornecidos e informa\u00e7\u00f5es sobre o sistema de benchmark da aeronave F-16 est\u00e3o dispon\u00edveis para download aqui. Este arquivo zip cont\u00e9m uma descri\u00e7\u00e3o detalhada do sistema, os conjuntos de dados de estima\u00e7\u00e3o e teste, e algumas fotos da configura\u00e7\u00e3o. Os dados est\u00e3o dispon\u00edveis nos formatos de arquivo .csv e .mat.</p> <p>Por favor, cite o benchmark F16 como:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>f_16 = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/f_16_vibration_test/f-16.txt\",\n    header=None,\n    names=[\"x1\", \"x2\", \"y\"],\n)\n</code></pre> <pre><code>f_16.shape\n</code></pre> <pre><code>(32768, 3)\n</code></pre>"},{"location":"pt/user-guide/tutorials/f-16-aircraft/#visualizando-os-dados","title":"Visualizando os Dados","text":"<pre><code>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre>"},{"location":"pt/user-guide/tutorials/f-16-aircraft/#dividindo-os-dados","title":"Dividindo os Dados","text":"<pre><code>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</code></pre>"},{"location":"pt/user-guide/tutorials/f-16-aircraft/#configurando-os-lags-de-entrada","title":"Configurando os Lags de Entrada","text":"<pre><code>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</code></pre> <pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"pt/user-guide/tutorials/f-16-aircraft/#treinamento-e-avaliacao-do-modelo","title":"Treinamento e Avalia\u00e7\u00e3o do Modelo","text":"<pre><code>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\ny_hat = model.predict(X=x_val, y=y_val)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressores\", \"Par\u00e2metros\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.2910089654603829\n   Regressores   Par\u00e2metros             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</code></pre> <pre><code>plot_results(y=y_val, yhat=y_hat, n=1000)\nee = compute_residues_autocorrelation(y_val, y_hat)\nplot_residues_correlation(data=ee, title=\"Res\u00edduos\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0])\nplot_residues_correlation(data=x1e, title=\"Res\u00edduos\", ylabel=\"$x_1e$\")\n</code></pre>"},{"location":"pt/user-guide/tutorials/f-16-aircraft/#grafico-de-criterios-de-informacao","title":"Gr\u00e1fico de Crit\u00e9rios de Informa\u00e7\u00e3o","text":"<pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Crit\u00e9rio de Informa\u00e7\u00e3o\")\n\n# Voc\u00ea pode usar o gr\u00e1fico abaixo para escolher o \"n_terms\" e executar o modelo novamente com o valor mais adequado de termos.\n</code></pre> <pre><code>Text(0, 0.5, 'Crit\u00e9rio de Informa\u00e7\u00e3o')\n</code></pre>"},{"location":"pt/user-guide/tutorials/fourier-NARX-overview/","title":"Fourier NARX - Vis\u00e3o Geral","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <p>Este exemplo mostra como alterar ou adicionar uma nova fun\u00e7\u00e3o base pode melhorar o modelo.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\n\nnp.seterr(all=\"ignore\")\nnp.random.seed(1)\n\n%matplotlib inline\n</code></pre>"},{"location":"pt/user-guide/tutorials/fourier-NARX-overview/#definindo-o-sistema","title":"Definindo o sistema","text":"<pre><code># Sistema simulado\ndef system_equation(y, u):\n    yk = (\n        (0.2 - 0.75 * np.cos(-y[0] ** 2)) * np.cos(y[0])\n        - (0.15 + 0.45 * np.cos(-y[0] ** 2)) * np.cos(y[1])\n        + np.cos(u[0])\n        + 0.2 * u[1]\n        + 0.7 * u[0] * u[1]\n    )\n    return yk\n\n\nrepetition = 5\nrandom_samples = 200\ntotal_time = repetition * random_samples\nn = np.arange(0, total_time)\n\n# Gerando entrada\nx = np.random.normal(size=(random_samples,)).repeat(repetition)\n\n\n_, ax = plt.subplots(figsize=(12, 6))\nax.step(n, x)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$x[n]$\", fontsize=18)\nplt.show()\n</code></pre>"},{"location":"pt/user-guide/tutorials/fourier-NARX-overview/#simulando-o-sistema","title":"Simulando o sistema","text":"<pre><code>y = np.empty_like(x)\n# Condi\u00e7\u00f5es Iniciais\ny0 = [0, 0]\n\n# Simular\ny[0:2] = y0\nfor i in range(2, len(y)):\n    y[i] = system_equation(\n        [y[i - 1], y[i - 2]], [x[i - 1], x[i - 2]]\n    ) + np.random.normal(scale=0.1)\n\n# Plot\n_, ax = plt.subplots(figsize=(12, 6))\nax.plot(n, y)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.grid()\nplt.show()\n</code></pre>"},{"location":"pt/user-guide/tutorials/fourier-NARX-overview/#adicionando-ruido-ao-sistema","title":"Adicionando ru\u00eddo ao sistema","text":"<pre><code># Dados sem ru\u00eddo\nynoise_free = y.copy()\n\n# Gerar ru\u00eddo\nv = np.random.normal(scale=0.5, size=y.shape)\n\n# Dados corrompidos com ru\u00eddo\nynoisy = ynoise_free + v\n\n# Plot\n_, ax = plt.subplots(figsize=(14, 8))\nax.plot(n, ynoise_free, label=\"Dados sem ru\u00eddo\")\nax.plot(n, ynoisy, label=\"Dados corrompidos\")\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.legend(fontsize=18)\nplt.show()\n</code></pre>"},{"location":"pt/user-guide/tutorials/fourier-NARX-overview/#gerando-dados-de-treinamento-e-teste","title":"Gerando dados de treinamento e teste","text":"<pre><code>n_train = 700\n\n# Dados de identifica\u00e7\u00e3o\ny_train = ynoisy[:n_train].reshape(-1, 1)\nx_train = x[:n_train].reshape(-1, 1)\n\n# Dados de valida\u00e7\u00e3o\ny_test = ynoise_free[n_train:].reshape(-1, 1)\nx_test = x[n_train:].reshape(-1, 1)\n</code></pre>"},{"location":"pt/user-guide/tutorials/fourier-NARX-overview/#funcao-base-polinomial","title":"Fun\u00e7\u00e3o Base Polinomial","text":"<p>Como voc\u00ea pode ver abaixo, usar apenas a fun\u00e7\u00e3o base polinomial com os seguintes par\u00e2metros n\u00e3o resulta em um modelo ruim. No entanto, vamos verificar como \u00e9 o desempenho usando a Fun\u00e7\u00e3o Base de Fourier.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    estimator=estimator,\n    err_tol=None,\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</code></pre> <pre><code>0.6768251106751224\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/fourier-NARX-overview/#combinando-uma-funcao-base-de-fourier","title":"Combinando uma Fun\u00e7\u00e3o Base de Fourier","text":"<p>Neste caso, adicionar a Fun\u00e7\u00e3o Base de Fourier resolve o problema e retorna um modelo capaz de predizer o sistema definido.</p> <pre><code>basis_function = Fourier(degree=2, n=2, p=2 * np.pi, ensemble=True)\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=70,\n    xlag=2,\n    ylag=2,  # os lags para todos os modelos ser\u00e3o 13\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    err_tol=None,\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</code></pre> <pre><code>0.3742244715879492\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/fourier-NARX-overview/#importante","title":"Importante","text":"<p>Atualmente voc\u00ea n\u00e3o pode obter a representa\u00e7\u00e3o do modelo usando <code>sysidentpy.regressor_code</code> para modelos Fourier NARX. Na verdade, voc\u00ea pode usar o m\u00e9todo, mas a representa\u00e7\u00e3o n\u00e3o \u00e9 precisa porque n\u00e3o deixamos claro quais s\u00e3o os regressores relacionados ao polin\u00f4mio ou relacionados \u00e0 fun\u00e7\u00e3o base de Fourier. Esta \u00e9 uma melhoria a ser feita em futuras atualiza\u00e7\u00f5es!</p>"},{"location":"pt/user-guide/tutorials/general-NARX-models/","title":"Modelos NARX Gerais","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <p>Neste exemplo, criaremos modelos NARX usando diferentes estimadores como GradientBoostingRegressor, Bayesian Regression, Automatic Relevance Determination (ARD) Regression e Catboost.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.general_estimators import NARX\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code># dataset simulado\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=10000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre>"},{"location":"pt/user-guide/tutorials/general-NARX-models/#importancia-da-arquitetura-narx","title":"Import\u00e2ncia da arquitetura NARX","text":"<p>Para ter uma ideia da import\u00e2ncia da arquitetura NARX, vamos observar o desempenho dos modelos sem a configura\u00e7\u00e3o NARX.</p> <pre><code>catboost = CatBoostRegressor(iterations=300, learning_rate=0.1, depth=6)\n</code></pre> <pre><code>gb = GradientBoostingRegressor(\n    loss=\"quantile\",\n    alpha=0.90,\n    n_estimators=250,\n    max_depth=10,\n    learning_rate=0.1,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\n</code></pre> <pre><code>def plot_results_tmp(y_valid, yhat):\n    _, ax = plt.subplots(figsize=(14, 8))\n    ax.plot(y_valid[:200], label=\"Data\", marker=\"o\")\n    ax.plot(yhat[:200], label=\"Prediction\", marker=\"*\")\n    ax.set_xlabel(\"$n$\", fontsize=18)\n    ax.set_ylabel(\"$y[n]$\", fontsize=18)\n    ax.grid()\n    ax.legend(fontsize=18)\n    plt.show()\n</code></pre> <pre><code>catboost.fit(x_train, y_train, verbose=False)\nplot_results_tmp(y_valid, catboost.predict(x_valid))\n</code></pre> <p></p> <pre><code>gb.fit(x_train, y_train.ravel())\nplot_results_tmp(y_valid, gb.predict(x_valid))\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/general-NARX-models/#introduzindo-a-configuracao-narx-usando-sysidentpy","title":"Introduzindo a configura\u00e7\u00e3o NARX usando SysIdentPy","text":"<p>Como voc\u00ea pode ver, basta passar o estimador base desejado para a classe NARX do SysIdentPy para construir o modelo NARX! Voc\u00ea pode escolher os lags das vari\u00e1veis de entrada e sa\u00edda para construir a matriz de regressores.</p> <p>Mantemos o m\u00e9todo fit/predict para tornar o processo direto.</p>"},{"location":"pt/user-guide/tutorials/general-NARX-models/#narx-com-catboost","title":"NARX com Catboost","text":"<pre><code>basis_function = Fourier(degree=1)\n\ncatboost_narx = NARX(\n    base_estimator=CatBoostRegressor(iterations=300, learning_rate=0.1, depth=8),\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    fit_params={\"verbose\": False},\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid, steps_ahead=1)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>MSE:  0.00024145290395678653\n</code></pre>"},{"location":"pt/user-guide/tutorials/general-NARX-models/#narx-com-gradient-boosting","title":"NARX com Gradient Boosting","text":"<pre><code>basis_function = Fourier(degree=1)\n\ngb_narx = NARX(\n    base_estimator=GradientBoostingRegressor(\n        loss=\"quantile\",\n        alpha=0.90,\n        n_estimators=250,\n        max_depth=10,\n        learning_rate=0.1,\n        min_samples_leaf=9,\n        min_samples_split=9,\n    ),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\ngb_narx.fit(X=x_train, y=y_train)\nyhat = gb_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.0011824693986863938\n</code></pre>"},{"location":"pt/user-guide/tutorials/general-NARX-models/#narx-com-ard","title":"NARX com ARD","text":"<pre><code>from sysidentpy.general_estimators import NARX\n\nARD_narx = NARX(\n    base_estimator=ARDRegression(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nARD_narx.fit(X=x_train, y=y_train)\nyhat = ARD_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.0011058934497373794\n</code></pre>"},{"location":"pt/user-guide/tutorials/general-NARX-models/#narx-com-bayesian-ridge","title":"NARX com Bayesian Ridge","text":"<pre><code>from sysidentpy.general_estimators import NARX\n\nBayesianRidge_narx = NARX(\n    base_estimator=BayesianRidge(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nBayesianRidge_narx.fit(X=x_train, y=y_train)\nyhat = BayesianRidge_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.0011077874945734536\n</code></pre>"},{"location":"pt/user-guide/tutorials/general-NARX-models/#nota","title":"Nota","text":"<p>Lembre-se que voc\u00ea pode usar predi\u00e7\u00e3o n-steps-ahead e modelos NAR e NFIR agora. Confira como us\u00e1-los em seus respectivos exemplos.</p>"},{"location":"pt/user-guide/tutorials/importance-of-extended-least-squares/","title":"Import\u00e2ncia do Extended Least Squares","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <p>Aqui importamos o modelo NARMAX, a m\u00e9trica para avalia\u00e7\u00e3o do modelo e os m\u00e9todos para gerar dados de amostra para testes. Tamb\u00e9m importamos pandas para uso espec\u00edfico.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n</code></pre>"},{"location":"pt/user-guide/tutorials/importance-of-extended-least-squares/#gerando-dados-de-amostra-com-1-entrada-e-1-saida","title":"Gerando dados de amostra com 1 entrada e 1 sa\u00edda","text":"<p>Os dados s\u00e3o gerados simulando o seguinte modelo: \\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}\\)</p> <p>Se colored_noise for definido como True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>onde \\(x\\) \u00e9 uma vari\u00e1vel aleat\u00f3ria uniformemente distribu\u00edda e \\(\\nu\\) \u00e9 uma vari\u00e1vel com distribui\u00e7\u00e3o gaussiana com \\(\\mu=0\\) e \\(\\sigma\\) definido pelo usu\u00e1rio.</p> <p>No pr\u00f3ximo exemplo, geraremos dados com 3000 amostras com ru\u00eddo colorido e selecionando 90% dos dados para treinar o modelo.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n</code></pre>"},{"location":"pt/user-guide/tutorials/importance-of-extended-least-squares/#construindo-o-modelo","title":"Construindo o modelo","text":"<p>Primeiro, treinaremos um modelo sem o Algoritmo Extended Least Squares para fins de compara\u00e7\u00e3o.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=False)\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.5499799245432233\n</code></pre> <p>Claramente, h\u00e1 algo errado com o modelo obtido. Veja o notebook basic_steps para comparar os resultados obtidos usando os mesmos dados mas sem ru\u00eddo colorido. Mas vamos ver o que est\u00e1 errado.</p> <pre><code>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0        x1(k-2)  8.9976E-01  7.41682256E-01\n1         y(k-1)  2.8734E-01  8.33321202E-02\n2  x1(k-1)y(k-1)  1.2348E-01  5.10334067E-03\n</code></pre>"},{"location":"pt/user-guide/tutorials/importance-of-extended-least-squares/#estimacao-de-parametros-com-bias","title":"Estima\u00e7\u00e3o de par\u00e2metros com bias","text":"<p>Como podemos observar acima, a estrutura do modelo \u00e9 exatamente a mesma que gerou os dados. Voc\u00ea pode ver que o ERR ordenou os termos da forma correta. E esta \u00e9 uma nota importante sobre o algoritmo Error Reduction Ratio usado aqui: ele \u00e9 muito robusto ao ru\u00eddo colorido!!</p> <p>Esta \u00e9 uma grande caracter\u00edstica! No entanto, embora a estrutura esteja correta, os par\u00e2metros do modelo n\u00e3o est\u00e3o ok! Aqui temos uma estima\u00e7\u00e3o com bias! O par\u00e2metro real para \\(y_{k-1}\\) \u00e9 \\(0.2\\), n\u00e3o \\(0.3\\).</p> <p>Neste caso, estamos na verdade modelando usando um modelo NARX, n\u00e3o NARMAX. A parte MA existe para permitir uma estima\u00e7\u00e3o n\u00e3o-enviesada dos par\u00e2metros. Para alcan\u00e7ar uma estima\u00e7\u00e3o n\u00e3o-enviesada dos par\u00e2metros, temos o algoritmo Extended Least Squares. Lembre-se, se os dados t\u00eam apenas ru\u00eddo branco, NARX \u00e9 suficiente.</p> <p>Antes de aplicar o Algoritmo Extended Least Squares, executaremos v\u00e1rios modelos NARX para verificar qu\u00e3o diferentes os par\u00e2metros estimados s\u00e3o dos reais.</p> <pre><code>parameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\n# Definir o tema para seaborn (opcional)\nsns.set_theme()\n\nplt.figure(figsize=(14, 4))\n\n# Plotar KDE para cada par\u00e2metro\nsns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\")\nsns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\")\nsns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")\n\n# Plotar linhas verticais onde os valores reais devem estar\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/importance-of-extended-least-squares/#usando-o-algoritmo-extended-least-squares","title":"Usando o algoritmo Extended Least Squares","text":"<p>Como mostrado na figura acima, temos um problema para estimar o par\u00e2metro para \\(y_{k-1}\\). Agora usaremos o Algoritmo Extended Least Squares.</p> <p>No SysIdentPy, basta definir extended_least_squares como True e o algoritmo ser\u00e1 aplicado.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model = FROLS(\n        order_selection=False,\n        n_terms=3,\n        ylag=2,\n        xlag=2,\n        elag=2,\n        info_criteria=\"aic\",\n        estimator=estimator,\n        basis_function=basis_function,\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\n\nplt.figure(figsize=(14, 4))\n\n# Plotar KDE para cada par\u00e2metro\nsns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\")\nsns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\")\nsns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")\n\n# Plotar linhas verticais onde os valores reais devem estar\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>\u00d3timo! Agora temos uma estima\u00e7\u00e3o n\u00e3o-enviesada dos par\u00e2metros!</p>"},{"location":"pt/user-guide/tutorials/importance-of-extended-least-squares/#nota","title":"Nota","text":"<p>Nota: O Extended Least Squares \u00e9 um algoritmo iterativo. No SysIdentPy, o padr\u00e3o \u00e9 30 itera\u00e7\u00f5es (<code>uiter=30</code>) porque \u00e9 conhecido da literatura que o algoritmo converge rapidamente (cerca de 10 ou 20 itera\u00e7\u00f5es).</p>"},{"location":"pt/user-guide/tutorials/information-criteria-overview/","title":"Crit\u00e9rios de Informa\u00e7\u00e3o - Vis\u00e3o Geral","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p>"},{"location":"pt/user-guide/tutorials/information-criteria-overview/#comparando-diferentes-metodos-de-criterios-de-informacao","title":"Comparando diferentes m\u00e9todos de crit\u00e9rios de informa\u00e7\u00e3o","text":"<p>Aqui importamos o modelo NARMAX, a m\u00e9trica para avalia\u00e7\u00e3o do modelo e os m\u00e9todos para gerar dados de amostra para testes. Tamb\u00e9m importamos o pandas para uso espec\u00edfico.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n</code></pre>"},{"location":"pt/user-guide/tutorials/information-criteria-overview/#gerando-dados-de-amostra","title":"Gerando dados de amostra","text":"<p>Os dados s\u00e3o gerados simulando o seguinte modelo: \\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\)</p> <p>Se colored_noise for definido como True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>onde \\(x\\) \u00e9 uma vari\u00e1vel aleat\u00f3ria uniformemente distribu\u00edda e \\(\\nu\\) \u00e9 uma vari\u00e1vel com distribui\u00e7\u00e3o gaussiana com \\(\\mu=0\\) e \\(\\sigma=0.1\\)</p> <p>No pr\u00f3ximo exemplo, geraremos dados com 3000 amostras com ru\u00eddo branco e selecionando 90% dos dados para treinar o modelo.</p> <pre><code>x_train, x_test, y_train, y_test = get_siso_data(\n    n=100, colored_noise=False, sigma=0.1, train_percentage=70\n)\n</code></pre> <p>A ideia \u00e9 mostrar o impacto do crit\u00e9rio de informa\u00e7\u00e3o na sele\u00e7\u00e3o do n\u00famero de termos para compor o modelo final. Voc\u00ea ver\u00e1 por que \u00e9 uma ferramenta auxiliar e deixar o algoritmo selecionar o n\u00famero de termos com base no valor m\u00ednimo n\u00e3o \u00e9 uma boa ideia ao lidar com dados altamente corrompidos por ru\u00eddo (mesmo ru\u00eddo branco).</p> <p>Nota: Voc\u00ea pode encontrar resultados diferentes ao executar os exemplos. Isso se deve ao fato de n\u00e3o estarmos definindo um gerador aleat\u00f3rio fixo para os dados de amostra. No entanto, a an\u00e1lise principal permanece.</p>"},{"location":"pt/user-guide/tutorials/information-criteria-overview/#aic","title":"AIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    # estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\nmodel.fit(X=x_train, y=y_train)\n\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.1681621129389993\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2076E-01  9.41001395E-01\n1          y(k-1)   1.7063E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7342E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -9.7902E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   4.9319E-02  1.85339089E-03\n5        y(k-2)^2  -5.6743E-02  1.02439383E-03\n6         x1(k-1)  -2.0179E-02  6.78305323E-04\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-273.81858224, -311.60797635, -331.34011486, -338.49936124,\n       -342.10339048, -342.27073244, -342.82764626, -342.16492383,\n       -341.04704839, -339.58437034, -337.79642875, -336.20531349,\n       -333.72427584, -331.48645717, -329.53042523])\n</code></pre> <p>Como pode ser visto acima, o valor m\u00ednimo faz o algoritmo escolher um modelo com 4 termos. No entanto, se voc\u00ea verificar o gr\u00e1fico, 3 termos \u00e9 a melhor escolha. Aumentar o n\u00famero de termos de 3 em diante n\u00e3o leva a um modelo melhor, j\u00e1 que a diferen\u00e7a \u00e9 muito pequena.</p> <p>Neste caso, voc\u00ea deve executar o modelo novamente com os par\u00e2metros n_terms=3! O algoritmo ERR ordenou os termos de forma correta, ent\u00e3o voc\u00ea obter\u00e1 a estrutura exata do modelo novamente!</p>"},{"location":"pt/user-guide/tutorials/information-criteria-overview/#aicc","title":"AICc","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aicc\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.1697650652880654\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2659E-01  9.41001395E-01\n1          y(k-1)   1.7219E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03\n4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03\n5        y(k-2)^2  -4.8117E-02  1.02439383E-03\n6         x1(k-1)  -2.4728E-02  6.78305323E-04\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-273.99834706, -311.49708248, -331.28179881, -338.08184328,\n       -341.32119362, -341.3373076 , -341.51818541, -340.50119461,\n       -339.16231408, -336.96924137, -334.34018578, -331.63849434,\n       -328.92685178, -325.8181513 , -322.55039655])\n</code></pre> <p>Como pode ser visto acima, o valor m\u00ednimo faz o algoritmo escolher um modelo com 4 termos. O AICc, no entanto, tem diferen\u00e7as importantes em compara\u00e7\u00e3o com o AIC quando o n\u00famero de amostras \u00e9 pequeno.</p> <p>Neste caso, voc\u00ea deve executar o modelo novamente com os par\u00e2metros n_terms=3! O algoritmo ERR ordenou os termos de forma correta, ent\u00e3o voc\u00ea obter\u00e1 a estrutura exata do modelo novamente!</p>"},{"location":"pt/user-guide/tutorials/information-criteria-overview/#bic","title":"BIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.16868631871856155\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.3050E-01  9.41001395E-01\n1          y(k-1)   1.7980E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-271.83944541, -307.24268246, -324.99827569, -329.8387331 ,\n       -331.19139703, -329.39731055, -327.84829815, -325.18581094,\n       -322.29019301, -318.63381344, -314.63988674, -310.67712915,\n       -306.81399235, -302.66957173, -298.4885502 ])\n</code></pre> <p>O BIC fez um trabalho melhor neste caso! A forma como ele penaliza o modelo em rela\u00e7\u00e3o ao n\u00famero de termos garante que o valor m\u00ednimo aqui foi exatamente o n\u00famero esperado de termos para compor o modelo. Bom, mas nem sempre o melhor m\u00e9todo!</p>"},{"location":"pt/user-guide/tutorials/information-criteria-overview/#lilc","title":"LILC","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"lilc\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.16868631871856155\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.3050E-01  9.41001395E-01\n1          y(k-1)   1.7980E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-273.17951619, -309.92282401, -329.01848803, -335.19901621,\n       -337.89175092, -337.43773522, -337.22879359, -335.90637716,\n       -334.35083001, -332.03452122, -329.3806653 , -326.75797849,\n       -324.23491246, -321.43056262, -318.58961187])\n</code></pre> <p>O LILC tamb\u00e9m inclui termos esp\u00farios. Como o AIC, ele falha em selecionar automaticamente os termos corretos, mas voc\u00ea pode selecionar o n\u00famero certo com base no gr\u00e1fico acima!</p>"},{"location":"pt/user-guide/tutorials/information-criteria-overview/#fpe","title":"FPE","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"fpe\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.1697650652880654\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2659E-01  9.41001395E-01\n1          y(k-1)   1.7219E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03\n4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03\n5        y(k-2)^2  -4.8117E-02  1.02439383E-03\n6         x1(k-1)  -2.4728E-02  6.78305323E-04\n\n\n\n\n\nText(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-274.05880892, -311.68054386, -331.65290152, -338.70751749,\n       -342.27085495, -342.68306863, -343.33508312, -342.86743567,\n       -342.15953986, -340.68281499, -338.85950374, -337.05732543,\n       -335.3437066 , -333.33668596, -331.27985457])\n</code></pre> <p>O FPE tamb\u00e9m falhou em selecionar automaticamente o n\u00famero correto de termos! Mas, como apontamos antes, o Crit\u00e9rio de Informa\u00e7\u00e3o \u00e9 uma ferramenta auxiliar! Se voc\u00ea olhar os gr\u00e1ficos, todos os m\u00e9todos permitem que voc\u00ea escolha o n\u00famero correto de termos!</p>"},{"location":"pt/user-guide/tutorials/information-criteria-overview/#nota-importante","title":"Nota Importante","text":"<p>Aqui estamos lidando com uma estrutura de modelo conhecida! Em rela\u00e7\u00e3o a dados reais, n\u00e3o conhecemos o n\u00famero correto de termos, ent\u00e3o os m\u00e9todos acima se destacam como excelentes ferramentas para ajud\u00e1-lo!</p> <p>Se voc\u00ea verificar as m\u00e9tricas acima, mesmo com os modelos com mais termos, voc\u00ea ver\u00e1 m\u00e9tricas excelentes! Mas a Identifica\u00e7\u00e3o de Sistemas sempre busca a melhor estrutura de modelo! A Sele\u00e7\u00e3o de Estrutura do Modelo \u00e9 o n\u00facleo dos m\u00e9todos NARMAX! Neste sentido, os exemplos s\u00e3o para mostrar conceitos b\u00e1sicos e como os algoritmos funcionam!</p>"},{"location":"pt/user-guide/tutorials/load-forecasting-benchmark/","title":"Benchmark de Previs\u00e3o de Carga","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro fornece orienta\u00e7\u00f5es detalhadas para apoiar seu trabalho com o SysIdentPy.</p>"},{"location":"pt/user-guide/tutorials/load-forecasting-benchmark/#nota","title":"Nota","text":"<p>O exemplo a seguir n\u00e3o pretende dizer que uma biblioteca \u00e9 melhor que outra. O foco principal desses exemplos \u00e9 mostrar que o SysIdentPy pode ser uma boa alternativa para pessoas que buscam modelar s\u00e9ries temporais.</p> <p>Compararemos os resultados obtidos contra a biblioteca neural prophet.</p> <p>Por quest\u00f5es de brevidade, do SysIdentPy apenas os m\u00e9todos MetaMSS, AOLS e FROLS (com fun\u00e7\u00e3o de base polinomial) ser\u00e3o usados. Consulte a documenta\u00e7\u00e3o do SysIdentPy para aprender outras formas de modelagem com a biblioteca.</p> <p>Compararemos um previsor de 1 passo \u00e0 frente no consumo de eletricidade de um edif\u00edcio. A configura\u00e7\u00e3o do modelo neuralprophet foi retirada da documenta\u00e7\u00e3o do neuralprophet (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>O treinamento ocorrer\u00e1 em 80% dos dados, reservando os \u00faltimos 20% para valida\u00e7\u00e3o.</p> <p>Nota: os dados usados neste exemplo podem ser encontrados no github do neuralprophet.</p>"},{"location":"pt/user-guide/tutorials/load-forecasting-benchmark/#resultados-do-benchmark","title":"Resultados do Benchmark:","text":"No. Pacote Erro Quadr\u00e1tico M\u00e9dio 1 SysIdentPy (FROLS) 4183 2 SysIdentPy (MetaMSS) 5264 3 SysIdentPy (AOLS) 5264 4 NeuralProphet 11471 <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom sktime.datasets import load_airline\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</code></pre>"},{"location":"pt/user-guide/tutorials/m4-benchmark/","title":"Benchmark M4","text":"<p>Nota: O exemplo mostrado neste notebook \u00e9 retirado do livro complementar Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>O dataset M4 \u00e9 um recurso bem conhecido para previs\u00e3o de s\u00e9ries temporais, oferecendo uma ampla gama de s\u00e9ries de dados usadas para testar e melhorar m\u00e9todos de previs\u00e3o. Criado para a competi\u00e7\u00e3o M4 organizada por Spyros Makridakis, este dataset impulsionou muitos avan\u00e7os em t\u00e9cnicas de previs\u00e3o.</p> <p>O dataset M4 inclui 100.000 s\u00e9ries temporais de v\u00e1rios campos como demografia, finan\u00e7as, ind\u00fastria, macroeconomia e microeconomia, que foram selecionados aleatoriamente do banco de dados ForeDeCk. As s\u00e9ries v\u00eam em diferentes frequ\u00eancias (anual, trimestral, mensal, semanal, di\u00e1ria e hor\u00e1ria), tornando-o uma cole\u00e7\u00e3o abrangente para testar m\u00e9todos de previs\u00e3o.</p> <p>Neste estudo de caso, focaremos no subconjunto hor\u00e1rio do dataset M4. Este subconjunto consiste em dados de s\u00e9ries temporais registrados por hora, fornecendo uma vis\u00e3o detalhada e de alta frequ\u00eancia das mudan\u00e7as ao longo do tempo. Dados hor\u00e1rios apresentam desafios \u00fanicos devido \u00e0 sua granularidade e ao potencial de capturar flutua\u00e7\u00f5es e padr\u00f5es de curto prazo.</p> <p>O dataset M4 fornece um benchmark padr\u00e3o para comparar diferentes m\u00e9todos de previs\u00e3o, permitindo que pesquisadores e profissionais avaliem seus modelos de forma consistente. Com s\u00e9ries de v\u00e1rios dom\u00ednios e frequ\u00eancias, o dataset M4 representa desafios de previs\u00e3o do mundo real, tornando-o valioso para desenvolver t\u00e9cnicas de previs\u00e3o robustas. A competi\u00e7\u00e3o e o dataset em si levaram \u00e0 cria\u00e7\u00e3o de novos algoritmos e m\u00e9todos, melhorando significativamente a precis\u00e3o e confiabilidade das previs\u00f5es.</p> <p>Apresentaremos um passo a passo completo usando o dataset hor\u00e1rio M4 para demonstrar as capacidades do SysIdentPy. O SysIdentPy oferece uma variedade de ferramentas e t\u00e9cnicas projetadas para lidar efetivamente com as complexidades de dados de s\u00e9ries temporais, mas focaremos em uma configura\u00e7\u00e3o r\u00e1pida e f\u00e1cil para este caso. Abordaremos a sele\u00e7\u00e3o de modelos e m\u00e9tricas de avalia\u00e7\u00e3o espec\u00edficas para o dataset hor\u00e1rio.</p> <p>Ao final deste estudo de caso, voc\u00ea ter\u00e1 uma compreens\u00e3o s\u00f3lida de como usar o SysIdentPy para previs\u00e3o com o dataset hor\u00e1rio M4, preparando voc\u00ea para enfrentar desafios de previs\u00e3o semelhantes em cen\u00e1rios do mundo real.</p>"},{"location":"pt/user-guide/tutorials/m4-benchmark/#pacotes-necessarios-e-versoes","title":"Pacotes Necess\u00e1rios e Vers\u00f5es","text":"<p>Para garantir que voc\u00ea possa replicar este estudo de caso, \u00e9 essencial usar vers\u00f5es espec\u00edficas dos pacotes necess\u00e1rios. Abaixo est\u00e1 uma lista dos pacotes junto com suas respectivas vers\u00f5es necess\u00e1rias para executar os estudos de caso efetivamente.</p> <p>Para instalar todos os pacotes necess\u00e1rios, voc\u00ea pode criar um arquivo <code>requirements.txt</code> com o seguinte conte\u00fado:</p> <pre><code>sysidentpy==0.4.0\ndatasetsforecast==0.0.8\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\ns3fs==2024.6.1\n</code></pre> <p>Ent\u00e3o, instale os pacotes usando: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Certifique-se de usar um ambiente virtual para evitar conflitos entre vers\u00f5es de pacotes.</li> <li>As vers\u00f5es especificadas s\u00e3o baseadas na compatibilidade com os exemplos de c\u00f3digo fornecidos. Se voc\u00ea estiver usando vers\u00f5es diferentes, alguns ajustes no c\u00f3digo podem ser necess\u00e1rios.</li> </ul>"},{"location":"pt/user-guide/tutorials/m4-benchmark/#configuracao-do-sysidentpy","title":"Configura\u00e7\u00e3o do SysIdentPy","text":"<p>Nesta se\u00e7\u00e3o, demonstraremos a aplica\u00e7\u00e3o do SysIdentPy ao dataset Silver box. O c\u00f3digo a seguir guiar\u00e1 voc\u00ea atrav\u00e9s do processo de carregamento do dataset, configura\u00e7\u00e3o dos par\u00e2metros do SysIdentPy e constru\u00e7\u00e3o de um modelo para o sistema mencionado.</p> <pre><code>import warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import SettingWithCopyWarning\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import (\n    root_relative_squared_error,\n    symmetric_mean_absolute_percentage_error,\n)\nfrom sysidentpy.utils.plotting import plot_results\n\nfrom datasetsforecast.m4 import M4, M4Evaluation\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=UserWarning)\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\ntrain = pd.read_csv(\"https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\")\ntest = pd.read_csv(\n    \"https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\"\n).rename(columns={\"y\": \"y_test\"})\n</code></pre> <p>Os gr\u00e1ficos a seguir fornecem uma visualiza\u00e7\u00e3o dos dados de treinamento para um pequeno subconjunto das s\u00e9ries temporais. O gr\u00e1fico mostra os dados brutos, dando uma vis\u00e3o dos padr\u00f5es e comportamentos inerentes a cada s\u00e9rie.</p> <p>Ao observar os dados, voc\u00ea pode ter uma no\u00e7\u00e3o da variedade e complexidade das s\u00e9ries temporais com as quais estamos trabalhando. Os gr\u00e1ficos podem revelar caracter\u00edsticas importantes como tend\u00eancias, padr\u00f5es sazonais e anomalias potenciais dentro das s\u00e9ries temporais. Entender esses elementos \u00e9 crucial para o desenvolvimento de modelos de previs\u00e3o precisos.</p> <p>No entanto, ao lidar com um grande n\u00famero de s\u00e9ries temporais diferentes, \u00e9 comum come\u00e7ar com suposi\u00e7\u00f5es amplas em vez de an\u00e1lises individuais detalhadas. Neste contexto, adotaremos uma abordagem semelhante. Em vez de entrar nos detalhes de cada dataset, faremos algumas suposi\u00e7\u00f5es gerais e veremos como o SysIdentPy as trata.</p> <p>Esta abordagem fornece um ponto de partida pr\u00e1tico, demonstrando como o SysIdentPy pode gerenciar diferentes tipos de dados de s\u00e9ries temporais sem muito trabalho. \u00c0 medida que voc\u00ea se familiariza mais com a ferramenta, pode refinar seus modelos com insights mais detalhados. Por enquanto, vamos focar em usar o SysIdentPy para criar as previs\u00f5es com base nessas suposi\u00e7\u00f5es iniciais.</p> <p>Nossa primeira suposi\u00e7\u00e3o \u00e9 que h\u00e1 um padr\u00e3o sazonal de 24 horas nas s\u00e9ries. Examinando os gr\u00e1ficos abaixo, isso parece razo\u00e1vel. Portanto, come\u00e7aremos a construir nossos modelos com <code>ylag=24</code>.</p> <pre><code>ax = (\n    train[train[\"unique_id\"] == \"H10\"]\n    .reset_index(drop=True)[\"y\"]\n    .plot(figsize=(15, 2), title=\"H10\")\n)\nxcoords = [a for a in range(24, 24 * 30, 24)]\n\nfor xc in xcoords:\n    plt.axvline(x=xc, color=\"red\", linestyle=\"--\", alpha=0.5)\n</code></pre> <p></p> <p>Vamos verificar e construir um modelo para o grupo <code>H20</code> antes de extrapolar as configura\u00e7\u00f5es para todos os grupos. Como n\u00e3o h\u00e1 features de entrada, usaremos um modelo tipo <code>NAR</code> no SysIdentPy. Para manter as coisas simples e r\u00e1pidas, come\u00e7aremos com fun\u00e7\u00e3o de base Polinomial com grau \\(1\\).</p> <pre><code>unique_id = \"H20\"\ny_id = train[train[\"unique_id\"] == unique_id][\"y\"].values.reshape(-1, 1)\ny_val = test[test[\"unique_id\"] == unique_id][\"y_test\"].values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    order_selection=True,\n    ylag=24,\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\n\nmodel.fit(y=y_id)\ny_val = np.concatenate([y_id[-model.max_lag :], y_val])\ny_hat = model.predict(y=y_val, forecast_horizon=48)\nsmape = symmetric_mean_absolute_percentage_error(\n    y_val[model.max_lag : :], y_hat[model.max_lag : :]\n)\n\nplot_results(\n    y=y_val[model.max_lag :],\n    yhat=y_hat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Grupo: {unique_id} - SMAPE {round(smape, 4)}\",\n)\n</code></pre> <p></p> <p>Provavelmente, os resultados n\u00e3o s\u00e3o \u00f3timos e n\u00e3o funcionar\u00e3o para todos os grupos. No entanto, vamos verificar como esta configura\u00e7\u00e3o se compara ao modelo vencedor da competi\u00e7\u00e3o de s\u00e9ries temporais M4: o Exponential Smoothing with Recurrent Neural Networks (ESRNN).</p> <pre><code>esrnn_url = (\n    \"https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-118.zip\"\n)\nesrnn_forecasts = M4Evaluation.load_benchmark(\"data\", \"Hourly\", esrnn_url)\nesrnn_evaluation = M4Evaluation.evaluate(\"data\", \"Hourly\", esrnn_forecasts)\n\nesrnn_evaluation\n</code></pre> SMAPE MASE OWA Hourly 9.328443 0.893046 0.440163 <p>O c\u00f3digo a seguir levou apenas 49 segundos para rodar na minha m\u00e1quina (processador AMD Ryzen 5 5600x, 32GB RAM a 3600MHz). Devido \u00e0 sua efici\u00eancia, n\u00e3o criei uma vers\u00e3o paralela. Ao final deste caso de uso, voc\u00ea ver\u00e1 como o SysIdentPy pode ser r\u00e1pido e eficaz, entregando bons resultados sem muita otimiza\u00e7\u00e3o.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n    y_id = data[\"y\"].values.reshape(-1, 1)\n    basis_function = Polynomial(degree=1)\n    model = FROLS(\n        ylag=24,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        n_info_values=25,\n    )\n    try:\n        model.fit(y=y_id)\n        y_val = y_id[-model.max_lag :].reshape(-1, 1)\n        y_hat = model.predict(y=y_val, forecast_horizon=48)\n        r.append(\n            [\n                u_id * len(y_hat[model.max_lag : :]),\n                ds_test,\n                y_hat[model.max_lag : :].ravel(),\n            ]\n        )\n    except Exception:\n        print(f\"Problema com {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(\n    [\"unique_id\", \"ds\", \"NARMAX_1\"]\n)\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)  # .clip(lower=10)\npivot_df = results_1.pivot(index=\"unique_id\", columns=\"ds\", values=\"NARMAX_1\")\nresults = pivot_df.to_numpy()\n\nM4Evaluation.evaluate(\"data\", \"Hourly\", results)\n</code></pre> SMAPE MASE OWA Hourly 16.034196 0.958083 0.636132 <p>Os resultados iniciais s\u00e3o razo\u00e1veis, mas n\u00e3o correspondem exatamente ao desempenho do <code>ESRNN</code>. Esses resultados s\u00e3o baseados apenas em nossa primeira suposi\u00e7\u00e3o. Para entender melhor o desempenho, vamos examinar os grupos com os piores resultados.</p> <p></p> <p>O gr\u00e1fico a seguir ilustra dois desses grupos, <code>H147</code> e <code>H136</code>. Ambos exibem um padr\u00e3o sazonal de 24 horas.</p> <p></p> <p></p> <p>No entanto, uma observa\u00e7\u00e3o mais atenta revela um insight adicional: al\u00e9m do padr\u00e3o di\u00e1rio, essas s\u00e9ries tamb\u00e9m mostram um padr\u00e3o semanal. Observe como os dados parecem quando dividimos a s\u00e9rie em segmentos semanais.</p> <p></p> <pre><code>xcoords = list(range(0, 168 * 5, 168))\nfiltered_train = train[train[\"unique_id\"] == \"H147\"].reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(10, 1.5 * len(xcoords[1:])))\nfor i, start in enumerate(xcoords[:-1]):\n    end = xcoords[i + 1]\n    ax = fig.add_subplot(len(xcoords[1:]), 1, i + 1)\n    filtered_train[\"y\"].iloc[start:end].plot(ax=ax)\n    ax.set_title(f\"H147 -&gt; Fatia {i+1}: Hora {start} a {end-1}\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Portanto, construiremos modelos definindo <code>ylag=168</code>.</p> <p>Note que este \u00e9 um n\u00famero muito alto para lags, ent\u00e3o tenha cuidado se quiser tentar com graus polinomiais mais altos porque o tempo para rodar os modelos pode aumentar significativamente. Tentei algumas configura\u00e7\u00f5es com grau polinomial igual a 2 e levou apenas \\(6\\) minutos para rodar (ainda menos, usando <code>AOLS</code>), sem fazer o c\u00f3digo rodar em paralelo. Como voc\u00ea pode ver, o SysIdentPy pode ser muito r\u00e1pido e voc\u00ea pode torn\u00e1-lo mais r\u00e1pido aplicando paraleliza\u00e7\u00e3o.</p> <pre><code># isso levou 2min para rodar no meu computador.\nr = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n    y_id = data[\"y\"].values.reshape(-1, 1)\n    basis_function = Polynomial(degree=1)\n    model = FROLS(\n        ylag=168,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n    )\n    try:\n        model.fit(y=y_id)\n        y_val = y_id[-model.max_lag :].reshape(-1, 1)\n        y_hat = model.predict(y=y_val, forecast_horizon=48)\n        r.append(\n            [\n                u_id * len(y_hat[model.max_lag : :]),\n                ds_test,\n                y_hat[model.max_lag : :].ravel(),\n            ]\n        )\n    except Exception:\n        print(f\"Problema com {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(\n    [\"unique_id\", \"ds\", \"NARMAX_1\"]\n)\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)  # .clip(lower=10)\npivot_df = results_1.pivot(index=\"unique_id\", columns=\"ds\", values=\"NARMAX_1\")\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate(\"data\", \"Hourly\", results)\n</code></pre> SMAPE MASE OWA Hourly 10.475998 0.773749 0.446471 <p>Agora, os resultados est\u00e3o muito mais pr\u00f3ximos dos do modelo <code>ESRNN</code>! Enquanto o Erro Percentual Absoluto M\u00e9dio Sim\u00e9trico (<code>SMAPE</code>) \u00e9 ligeiramente pior, o Erro Escalado Absoluto M\u00e9dio (<code>MASE</code>) \u00e9 melhor quando comparado ao <code>ESRNN</code>, levando a uma m\u00e9trica de M\u00e9dia Ponderada Geral (<code>OWA</code>) muito semelhante. Notavelmente, esses resultados s\u00e3o alcan\u00e7ados usando apenas modelos <code>AR</code> simples. A seguir, vamos ver se o m\u00e9todo <code>AOLS</code> pode fornecer resultados ainda melhores.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n    y_id = data[\"y\"].values.reshape(-1, 1)\n    basis_function = Polynomial(degree=1)\n    model = AOLS(\n        ylag=168,\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        # devido \u00e0s configura\u00e7\u00f5es de lag alto, k foi aumentado para 6 como um palpite inicial\n        k=6,\n    )\n    try:\n        model.fit(y=y_id)\n        y_val = y_id[-model.max_lag :].reshape(-1, 1)\n        y_hat = model.predict(y=y_val, forecast_horizon=48)\n        r.append(\n            [\n                u_id * len(y_hat[model.max_lag : :]),\n                ds_test,\n                y_hat[model.max_lag : :].ravel(),\n            ]\n        )\n    except Exception:\n        print(f\"Problema com {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(\n    [\"unique_id\", \"ds\", \"NARMAX_1\"]\n)\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)  # .clip(lower=10)\npivot_df = results_1.pivot(index=\"unique_id\", columns=\"ds\", values=\"NARMAX_1\")\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate(\"data\", \"Hourly\", results)\n</code></pre> SMAPE MASE OWA Hourly 9.951141 0.809965 0.439755 <p>A M\u00e9dia Ponderada Geral (<code>OWA</code>) \u00e9 ainda melhor que a do modelo <code>ESRNN</code>! Al\u00e9m disso, o m\u00e9todo <code>AOLS</code> foi incrivelmente eficiente, levando apenas 6 segundos para rodar. Esta combina\u00e7\u00e3o de alto desempenho e execu\u00e7\u00e3o r\u00e1pida torna o <code>AOLS</code> uma alternativa atraente para previs\u00e3o de s\u00e9ries temporais em casos com m\u00faltiplas s\u00e9ries.</p> <p>Antes de terminar, vamos verificar como o desempenho do modelo <code>H147</code> melhorou com a configura\u00e7\u00e3o <code>ylag=168</code>.</p> <p></p> <p>Com base no artigo de benchmark M4, tamb\u00e9m poder\u00edamos limitar as previs\u00f5es menores que 10 para 10 e os resultados seriam ligeiramente melhores. Mas isso fica a crit\u00e9rio do usu\u00e1rio.</p> <p>Poder\u00edamos alcan\u00e7ar um desempenho ainda melhor com algum ajuste fino da configura\u00e7\u00e3o do modelo. No entanto, deixarei a explora\u00e7\u00e3o desses ajustes alternativos como um exerc\u00edcio para o usu\u00e1rio. Por\u00e9m, tenha em mente que experimentar com diferentes configura\u00e7\u00f5es nem sempre garante resultados melhores. Um conhecimento te\u00f3rico mais profundo pode frequentemente lev\u00e1-lo a melhores configura\u00e7\u00f5es e, portanto, melhores resultados.</p>"},{"location":"pt/user-guide/tutorials/model-with-multiple-inputs/","title":"Modelo com M\u00faltiplas Entradas","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p>"},{"location":"pt/user-guide/tutorials/model-with-multiple-inputs/#gerando-dados-de-amostra-com-2-entradas-e-1-saida","title":"Gerando dados de amostra com 2 entradas e 1 sa\u00edda","text":"<p>Os dados s\u00e3o gerados simulando o seguinte modelo:</p> <p>\\(y_k = 0.4y_{k-1}^2 + 0.1y_{k-1}x1_{k-1} + 0.6x2_{k-1} -0.3x1_{k-1}x2_{k-2} + e_{k}\\)</p> <p>Se colored_noise for definido como True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>onde \\(x\\) \u00e9 uma vari\u00e1vel aleat\u00f3ria uniformemente distribu\u00edda e \\(\\nu\\) \u00e9 uma vari\u00e1vel com distribui\u00e7\u00e3o gaussiana com \\(\\mu=0\\) e \\(\\sigma=0.001\\)</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.generate_data import get_miso_data\n</code></pre> <pre><code>x_train, x_valid, y_train, y_valid = get_miso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre> <p>Existe uma diferen\u00e7a espec\u00edfica para dados com m\u00faltiplas entradas.</p> <ul> <li>Voc\u00ea precisa passar os lags para cada entrada em uma lista aninhada (e.g., [[1, 2], [1, 2]])</li> </ul> <p>As demais configura\u00e7\u00f5es permanecem as mesmas.</p>"},{"location":"pt/user-guide/tutorials/model-with-multiple-inputs/#construindo-o-modelo","title":"Construindo o modelo","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_terms=4,\n    ylag=2,\n    xlag=[[1, 2], [1, 2]],\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1a88cc17350&gt;\n</code></pre>"},{"location":"pt/user-guide/tutorials/model-with-multiple-inputs/#avaliacao-do-modelo","title":"Avalia\u00e7\u00e3o do modelo","text":"<pre><code>yhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.00314141814133057\n       Regressors   Parameters             ERR\n0         x2(k-1)   5.9999E-01  9.15006949E-01\n1  x2(k-2)x1(k-1)  -3.0010E-01  4.31748224E-02\n2        y(k-1)^2   3.9976E-01  4.15131661E-02\n3   x1(k-1)y(k-1)   1.0028E-01  2.96827987E-04\n</code></pre> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre>"},{"location":"pt/user-guide/tutorials/modeling-a-magneto-rheological-damper-device/","title":"Modelagem de um Dispositivo Amortecedor Magneto-Reol\u00f3gico","text":"<p>Nota: O exemplo mostrado neste notebook \u00e9 retirado do livro complementar Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>Os efeitos de mem\u00f3ria entre entrada e sa\u00edda quase-est\u00e1ticas tornam a modelagem de sistemas hister\u00e9ticos muito dif\u00edcil. Modelos baseados em f\u00edsica s\u00e3o frequentemente usados para descrever os loops de histerese, mas esses modelos geralmente carecem da simplicidade e efici\u00eancia requeridas em aplica\u00e7\u00f5es pr\u00e1ticas envolvendo caracteriza\u00e7\u00e3o, identifica\u00e7\u00e3o e controle de sistemas. Como detalhado em Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models, modelos NARX provaram ser uma escolha vi\u00e1vel para descrever os loops de histerese. Veja o Cap\u00edtulo 8 para um background detalhado. No entanto, mesmo considerando as condi\u00e7\u00f5es suficientes para representa\u00e7\u00e3o de histerese independente de taxa, algoritmos cl\u00e1ssicos de sele\u00e7\u00e3o de estrutura falham em retornar um modelo com desempenho decente e o usu\u00e1rio precisa definir uma fun\u00e7\u00e3o multi-valorada para garantir a ocorr\u00eancia da estrutura limitante \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models).</p> <p>Embora algum progresso tenha sido feito, trabalhos anteriores foram limitados a modelos com um \u00fanico ponto de equil\u00edbrio. O presente estudo de caso visa apresentar novas perspectivas na sele\u00e7\u00e3o de estrutura de modelos de sistemas hister\u00e9ticos considerando os casos onde os modelos t\u00eam m\u00faltiplas entradas e n\u00e3o \u00e9 restrito quanto ao n\u00famero de pontos de equil\u00edbrio. Para isso, o algoritmo MetaMSS ser\u00e1 usado para construir um modelo para um amortecedor magneto-reol\u00f3gico (MRD) considerando as condi\u00e7\u00f5es suficientes mencionadas.</p>"},{"location":"pt/user-guide/tutorials/modeling-a-magneto-rheological-damper-device/#uma-breve-descricao-do-modelo-bouc-wen-de-dispositivo-amortecedor-magneto-reologico","title":"Uma Breve descri\u00e7\u00e3o do modelo Bouc-Wen de dispositivo amortecedor magneto-reol\u00f3gico","text":"<p>Os dados usados neste estudo de caso s\u00e3o do modelo Bouc-Wen (Bouc, R - Forced Vibrations of a Mechanical System with Hysteresis), (Wen, Y. X. - Method for Random Vibration of Hysteretic Systems) de um MRD cujo diagrama esquem\u00e1tico \u00e9 mostrado na figura abaixo.</p> <p></p> <p>O modelo para um amortecedor magneto-reol\u00f3gico proposto por Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback.</p> <p>A forma geral do modelo Bouc-Wen pode ser descrita como (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback):</p> \\[ \\begin{equation} \\dfrac{dz}{dt} = g\\left[x,z,sign\\left(\\dfrac{dx}{dt}\\right)\\right]\\dfrac{dx}{dt}, \\end{equation} \\] <p>onde \\(z\\) \u00e9 a sa\u00edda do modelo hister\u00e9tico, \\(x\\) a entrada e \\(g[\\cdot]\\) uma fun\u00e7\u00e3o n\u00e3o linear de \\(x\\), \\(z\\) e \\(sign (dx/dt)\\). (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback) propuseram o seguinte modelo fenomenol\u00f3gico para o dispositivo mencionado:</p> \\[ \\begin{align} f&amp;= c_1\\dot{\\rho}+k_1(x-x_0),\\nonumber\\\\ \\dot{\\rho}&amp;=\\dfrac{1}{c_0+c_1}[\\alpha z+c_0\\dot{x}+k_0(x-\\rho)],\\nonumber\\\\ \\dot{z}&amp;=-\\gamma|\\dot{x}-\\dot{\\rho}|z|z|^{n-1}-\\beta(\\dot{x}-\\dot{\\rho})|z|^n+A(\\dot{x}-\\dot{\\rho}),\\nonumber\\\\ \\alpha&amp;=\\alpha_a+\\alpha_bu_{bw},\\nonumber\\\\ c_1&amp;=c_{1a}+c_{1b}u_{bw},\\nonumber\\\\ c_0&amp;=c_{0a}+c_{0b}u_{bw},\\nonumber\\\\ \\dot{u}_{bw}&amp;=-\\eta(u_{bw}-E). \\end{align} \\] <p>onde \\(f\\) \u00e9 a for\u00e7a de amortecimento, \\(c_1\\) e \\(c_0\\) representam os coeficientes viscosos, \\(E\\) \u00e9 a tens\u00e3o de entrada, \\(x\\) \u00e9 o deslocamento e \\(\\dot{x}\\) \u00e9 a velocidade do modelo. Os par\u00e2metros do sistema (veja a tabela abaixo) foram retirados de Leva, A. and Piroddi, L. - NARX-based technique for the modelling of magneto-rheological damping devices.</p> Par\u00e2metro Valor Par\u00e2metro Valor \\(c_{0_a}\\) \\(20.2 \\, N \\, s/cm\\) \\(\\alpha_{a}\\) \\(44.9 \\, N/cm\\) \\(c_{0_b}\\) \\(2.68 \\, N \\, s/cm \\, V\\) \\(\\alpha_{b}\\) \\(638 \\, N/cm\\) \\(c_{1_a}\\) \\(350 \\, N \\, s/cm\\) \\(\\gamma\\) \\(39.3 \\, cm^{-2}\\) \\(c_{1_b}\\) \\(70.7 \\, N \\, s/cm \\, V\\) \\(\\beta\\) \\(39.3 \\, cm^{-2}\\) \\(k_{0}\\) \\(15 \\, N/cm\\) \\(n\\) \\(2\\) \\(k_{1}\\) \\(5.37 \\, N/cm\\) \\(\\eta\\) \\(251 \\, s^{-1}\\) \\(x_{0}\\) \\(0 \\, cm\\) \\(A\\) \\(47.2\\) <p>Para este estudo particular, tanto as entradas de deslocamento quanto de tens\u00e3o, \\(x\\) e \\(E\\), respectivamente, foram geradas filtrando uma sequ\u00eancia de ru\u00eddo gaussiano branco usando um filtro FIR Blackman-Harris com frequ\u00eancia de corte de \\(6\\)Hz. O tamanho do passo de integra\u00e7\u00e3o foi definido como \\(h = 0.002\\), seguindo os procedimentos descritos em Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models. Estes procedimentos s\u00e3o apenas para fins de identifica\u00e7\u00e3o, j\u00e1 que as entradas de um MRD podem ter v\u00e1rias caracter\u00edsticas diferentes.</p> <p>Os dados usados neste exemplo s\u00e3o fornecidos pelo Professor Samir Angelo Milani Martins.</p> <p>Os desafios s\u00e3o:</p> <ul> <li>possui uma n\u00e3o linearidade com mem\u00f3ria, ou seja, uma n\u00e3o linearidade din\u00e2mica;</li> <li>a n\u00e3o linearidade \u00e9 governada por uma vari\u00e1vel interna z(t), que n\u00e3o \u00e9 mensur\u00e1vel;</li> <li>a forma funcional n\u00e3o linear na equa\u00e7\u00e3o de Bouc Wen \u00e9 n\u00e3o linear no par\u00e2metro;</li> <li>a forma funcional n\u00e3o linear na equa\u00e7\u00e3o de Bouc Wen n\u00e3o admite uma expans\u00e3o de s\u00e9rie de Taylor finita devido \u00e0 presen\u00e7a de valores absolutos</li> </ul>"},{"location":"pt/user-guide/tutorials/modeling-a-magneto-rheological-damper-device/#pacotes-necessarios-e-versoes","title":"Pacotes Necess\u00e1rios e Vers\u00f5es","text":"<p>Para garantir que voc\u00ea possa replicar este estudo de caso, \u00e9 essencial usar vers\u00f5es espec\u00edficas dos pacotes necess\u00e1rios. Abaixo est\u00e1 uma lista dos pacotes junto com suas respectivas vers\u00f5es necess\u00e1rias para executar os estudos de caso efetivamente.</p> <p>Para instalar todos os pacotes necess\u00e1rios, voc\u00ea pode criar um arquivo <code>requirements.txt</code> com o seguinte conte\u00fado:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nscikit-learn==1.4.2\n</code></pre> <p>Ent\u00e3o, instale os pacotes usando: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Certifique-se de usar um ambiente virtual para evitar conflitos entre vers\u00f5es de pacotes.</li> <li>As vers\u00f5es especificadas s\u00e3o baseadas na compatibilidade com os exemplos de c\u00f3digo fornecidos. Se voc\u00ea estiver usando vers\u00f5es diferentes, alguns ajustes no c\u00f3digo podem ser necess\u00e1rios.</li> </ul>"},{"location":"pt/user-guide/tutorials/modeling-a-magneto-rheological-damper-device/#configuracao-do-sysidentpy","title":"Configura\u00e7\u00e3o do SysIdentPy","text":"<pre><code>import numpy as np\nfrom sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/bouc_wen/boucwen_histeretic_system.csv\"\n)\nscaler_x = MaxAbsScaler()\nscaler_y = MaxAbsScaler()\n\ninit = 400\nx_train = df[[\"E\", \"v\"]].iloc[init : df.shape[0] // 2, :]\nx_train[\"sign_v\"] = np.sign(df[\"v\"])\nx_train = scaler_x.fit_transform(x_train)\n\nx_test = df[[\"E\", \"v\"]].iloc[df.shape[0] // 2 + 1 : df.shape[0] - init, :]\nx_test[\"sign_v\"] = np.sign(df[\"v\"])\nx_test = scaler_x.transform(x_test)\n\ny_train = df[[\"f\"]].iloc[init : df.shape[0] // 2, :].values.reshape(-1, 1)\ny_train = scaler_y.fit_transform(y_train)\n\ny_test = (\n    df[[\"f\"]].iloc[df.shape[0] // 2 + 1 : df.shape[0] - init, :].values.reshape(-1, 1)\n)\ny_test = scaler_y.transform(y_test)\n\n# Plotando os dados\nplt.figure(figsize=(10, 8))\nplt.suptitle(\"Dados de identifica\u00e7\u00e3o (treinamento)\", fontsize=16)\n\nplt.subplot(221)\nplt.plot(y_train, \"k\")\nplt.ylabel(\"For\u00e7a - Sa\u00edda\")\nplt.xlabel(\"Amostras\")\nplt.title(\"y\")\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(222)\nplt.plot(x_train[:, 0], \"k\")\nplt.ylabel(\"Tens\u00e3o de Controle\")\nplt.xlabel(\"Amostras\")\nplt.title(\"x_1\")\nplt.grid()\nplt.axis([0, 1500, 0, 1])\n\nplt.subplot(223)\nplt.plot(x_train[:, 1], \"k\")\nplt.ylabel(\"Velocidade\")\nplt.xlabel(\"Amostras\")\nplt.title(\"x_2\")\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(224)\nplt.plot(x_train[:, 2], \"k\")\nplt.ylabel(\"sign(Velocidade)\")\nplt.xlabel(\"Amostras\")\nplt.title(\"x_3\")\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n</code></pre> <p>Vamos verificar como \u00e9 o comportamento hister\u00e9tico considerando cada entrada:</p> <pre><code>plt.figure()\nplt.plot(x_train[:, 0], y_train)\nplt.xlabel(\"x1 - Tens\u00e3o\")\nplt.ylabel(\"y - For\u00e7a\")\n\nplt.figure()\nplt.plot(x_train[:, 1], y_train)\nplt.xlabel(\"x2 - Velocidade\")\nplt.ylabel(\"y - For\u00e7a\")\n\nplt.figure()\nplt.plot(x_train[:, 2], y_train)\nplt.xlabel(\"u3 - sign(Velocidade)\")\nplt.ylabel(\"y - For\u00e7a\")\n</code></pre> <pre><code>Text(0, 0.5, 'y - For\u00e7a')\n</code></pre> <p></p> <p></p> <p></p> <p>Agora, podemos simplesmente construir um modelo NARX:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=[[1], [1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"FROLS: sign(v) e MaxAbsScaler\",\n)\n</code></pre> <pre><code>0.04510435472905795\n</code></pre> <p></p> <p>Se removermos a entrada <code>sign(v)</code> e tentarmos construir um modelo NARX usando a mesma configura\u00e7\u00e3o, o modelo diverge, como pode ser visto na figura a seguir:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=[[1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"FROLS: MaxAbsScaler, descartando sign(v)\",\n)\n</code></pre> <pre><code>nan\n\n\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\n</code></pre> <p></p> <p>Se usarmos o algoritmo <code>MetaMSS</code> em vez disso, os resultados s\u00e3o melhores.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\nbasis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n    xlag=[[1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    random_state=42,\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"MetaMSS: MaxAbsScaler, descartando sign(v)\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:453: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n\n\n0.24685651932553157\n</code></pre> <p></p> <p>No entanto, quando a sa\u00edda do sistema atinge seu valor m\u00ednimo, o modelo oscila</p> <pre><code>plot_results(\n    y=y_test[1100:1200], yhat=yhat[1100:1200], n=10000, title=\"Regi\u00e3o inst\u00e1vel\"\n)\n</code></pre> <p></p> <p>Se adicionarmos a entrada <code>sign(v)</code> novamente e usarmos <code>MetaMSS</code>, os resultados s\u00e3o muito pr\u00f3ximos do algoritmo <code>FROLS</code> com todas as entradas</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n    xlag=[[1], [1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"MetaMSS: sign(v) e MaxAbsScaler\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\parameter_estimation\\estimators.py:75: UserWarning: Psi matrix might have linearly dependent rows.Be careful and check your data\n  self._check_linear_dependence_rows(psi)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:453: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n\n\n0.055422497807759194\n</code></pre> <p></p> <p>Este caso tamb\u00e9m destacar\u00e1 a import\u00e2ncia do escalonamento de dados. Anteriormente, usamos o m\u00e9todo <code>MaxAbsScaler</code>, que resultou em \u00f3timos modelos ao usar as entradas <code>sign(v)</code>, mas tamb\u00e9m resultou em modelos inst\u00e1veis ao remover essa feature de entrada. Quando o escalonamento \u00e9 aplicado usando <code>MinMaxScaler</code>, no entanto, a estabilidade geral dos resultados melhora, e o modelo n\u00e3o diverge, mesmo quando a entrada <code>sign(v)</code> \u00e9 removida, usando o algoritmo <code>FROLS</code>.</p> <p>O usu\u00e1rio pode obter os resultados abaixo apenas alterando o m\u00e9todo de escalonamento de dados usando</p> <pre><code>scaler_x = MinMaxScaler()\nscaler_y = MinMaxScaler()\n</code></pre> <p>e executando cada modelo novamente. Essa \u00e9 a \u00fanica mudan\u00e7a para melhorar os resultados.</p> <p></p> <p>FROLS: com <code>sign(v)</code> e <code>MinMaxScaler</code>. RMSE: 0.1159</p> <p> FROLS: descartando <code>sign(v)</code> e usando <code>MinMaxScaler</code>. RMSE: 0.1639</p> <p></p> <p>MetaMSS: descartando <code>sign(v)</code> e usando <code>MinMaxScaler</code>. RMSE: 0.1762</p> <p></p> <p>MetaMSS: incluindo <code>sign(v)</code> e usando <code>MinMaxScaler</code>. RMSE: 0.0694</p> <p>Em contraste, o m\u00e9todo MetaMSS retornou o melhor modelo geral, mas n\u00e3o melhor que o melhor m\u00e9todo <code>FROLS</code> usando <code>MaxAbsScaler</code>.</p> <p>Aqui est\u00e1 o loop hister\u00e9tico predito:</p> <pre><code>plt.plot(x_test[:, 1], yhat)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x225ff4f8b00&gt;]\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/","title":"Estima\u00e7\u00e3o Multiobjetivo de Par\u00e2metros - Vis\u00e3o Geral","text":"<p>Exemplo criado por Gabriel Bueno Leandro, Samir Milani Martins e Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es abrangentes sobre modelos, m\u00e9todos e uma grande variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece uma orienta\u00e7\u00e3o aprofundada para apoiar o seu trabalho com o SysIdentPy.</p> <p>A estima\u00e7\u00e3o multiobjetivo de par\u00e2metros representa uma mudan\u00e7a de paradigma fundamental na forma como abordamos o problema de ajuste de par\u00e2metros para modelos NARMAX. Em vez de buscar um \u00fanico conjunto de valores de par\u00e2metros que ajuste o modelo de forma \u00f3tima aos dados, abordagens multiobjetivo visam identificar um conjunto de solu\u00e7\u00f5es de par\u00e2metros, conhecido como frente de Pareto, que fornece um compromisso entre objetivos conflitantes. Esses objetivos frequentemente abrangem um espectro de crit\u00e9rios de desempenho do modelo, como qualidade de ajuste, complexidade do modelo e robustez.</p>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#referencia","title":"Refer\u00eancia","text":"<p>Para mais informa\u00e7\u00f5es, consulte a refer\u00eancia: https://doi.org/10.1080/00207170601185053.</p>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#caso-de-uso-conversor-buck","title":"Caso de uso: Conversor Buck","text":"Um conversor buck \u00e9 um tipo de conversor CC/CC que reduz a tens\u00e3o (enquanto aumenta a corrente) de sua entrada (fonte) para sua sa\u00edda (carga). Ele \u00e9 semelhante a um conversor boost (elevador) e \u00e9 um tipo de fonte de alimenta\u00e7\u00e3o chaveada (SMPS) que tipicamente cont\u00e9m pelo menos dois semicondutores (um diodo e um transistor, embora conversores buck modernos substituam o diodo por um segundo transistor usado para retifica\u00e7\u00e3o s\u00edncrona) e pelo menos um elemento de armazenamento de energia, um capacitor, um indutor ou ambos combinados.  <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.multiobjective_parameter_estimation import AILS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.narmax_tools import set_weights\n</code></pre>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#comportamento-dinamico","title":"Comportamento Din\u00e2mico","text":"<pre><code>df_train = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/buck/buck_id.csv\"\n)\ndf_valid = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/buck/buck_valid.csv\"\n)\n\n# Plotando a sa\u00edda medida (dados de identifica\u00e7\u00e3o e valida\u00e7\u00e3o)\nplt.figure(1)\nplt.title(\"Output\")\nplt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5)\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Voltage\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Plotando a entrada medida (dados de identifica\u00e7\u00e3o e valida\u00e7\u00e3o)\nplt.figure(2)\nplt.title(\"Input\")\nplt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5)\nplt.ylim(2.1, 2.6)\nplt.ylabel(\"u\")\nplt.xlabel(\"Samples\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#funcao-estatica-do-conversor-buck","title":"Fun\u00e7\u00e3o Est\u00e1tica do Conversor Buck","text":"<p>O ciclo de trabalho, representado pelo s\u00edmbolo \\(D\\), \u00e9 definido como a raz\u00e3o entre o tempo em que o sistema permanece ligado (\\(T_{on}\\)) e o tempo total do ciclo de opera\u00e7\u00e3o (\\(T\\)). Matematicamente, isso pode ser expresso como \\(D=\\frac{T_{on}}{T}\\). O complemento do ciclo de trabalho, representado por \\(D'\\), \u00e9 definido como a raz\u00e3o entre o tempo em que o sistema permanece desligado (\\(T_{off}\\)) e o tempo total do ciclo de opera\u00e7\u00e3o (\\(T\\)), podendo ser expresso como \\(D'=\\frac{T_{off}}{T}\\).</p> <p>A tens\u00e3o de carga (\\(V_o\\)) est\u00e1 relacionada \u00e0 tens\u00e3o da fonte (\\(V_d\\)) pela equa\u00e7\u00e3o \\(V_o = D\\cdot V_d = (1 - D')\\cdot V_d\\). Para este conversor em particular, sabe-se que \\(D' = \\frac{\\bar{u} - 1}{3}\\), o que significa que a fun\u00e7\u00e3o est\u00e1tica deste sistema pode ser derivada teoricamente como:</p> <p>\\(V_o = \\frac{4V_d}{3} - \\frac{V_d}{3}\\cdot \\bar{u}\\)</p> <p>Se assumirmos que a tens\u00e3o da fonte \\(V_d\\) \u00e9 igual a 24 V, podemos reescrever a express\u00e3o acima como:</p> <p>\\(V_o = (4 - \\bar{u})\\cdot 8\\)</p> <pre><code># Dados est\u00e1ticos\nVd = 24\nUo = np.linspace(0, 4, 50)\nYo = (4 - Uo) * Vd / 3\nUo = Uo.reshape(-1, 1)\nYo = Yo.reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Curve\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{y}$\")\nplt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\")\nplt.show()\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#ganho-estatico-do-conversor-buck","title":"Ganho Est\u00e1tico do Conversor Buck","text":"<p>O ganho de um conversor Buck \u00e9 uma medida de como sua tens\u00e3o de sa\u00edda varia em resposta a altera\u00e7\u00f5es na tens\u00e3o de entrada. Matematicamente, o ganho pode ser calculado como a derivada da fun\u00e7\u00e3o est\u00e1tica do conversor, que descreve a rela\u00e7\u00e3o entre suas tens\u00f5es de entrada e sa\u00edda.</p> <p>Neste caso, a fun\u00e7\u00e3o est\u00e1tica do conversor Buck \u00e9 dada pela equa\u00e7\u00e3o:</p> <p>\\(V_o = (4 - \\bar{u})\\cdot 8\\)</p> <p>Tomando a derivada dessa equa\u00e7\u00e3o em rela\u00e7\u00e3o a \\(\\hat{u}\\), obtemos que o ganho do conversor Buck \u00e9 igual a \u22128. Em outras palavras, para cada unidade de aumento na tens\u00e3o de entrada \\(\\hat{u}\\), a tens\u00e3o de sa\u00edda \\(V_o\\) diminuir\u00e1 em 8 unidades.</p> <p>Assim, \\(gain = V_o' = -8\\).</p> <pre><code># Definindo o ganho\ngain = -8 * np.ones(len(Uo)).reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Gain\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{gain}$\")\nplt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#construindo-um-modelo-dinamico-usando-a-abordagem-mono-objetivo","title":"Construindo um modelo din\u00e2mico usando a abordagem mono-objetivo","text":"<pre><code>x_train = df_train.input.values.reshape(-1, 1)\ny_train = df_train.y.values.reshape(-1, 1)\nx_valid = df_valid.input.values.reshape(-1, 1)\ny_valid = df_valid.y.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=8,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#algoritmo-affine-information-least-squares-ails","title":"Algoritmo Affine Information Least Squares (AILS)","text":"<p>O AILS \u00e9 um algoritmo de estima\u00e7\u00e3o multiobjetivo de par\u00e2metros, baseado em um conjunto de pares de informa\u00e7\u00e3o afim. A abordagem multiobjetivo proposta no artigo mencionado e implementada no SysIdentPy leva a um problema de otimiza\u00e7\u00e3o multiobjetivo convexo, que pode ser resolvido pelo AILS. O AILS \u00e9 um esquema do tipo LeastSquares, n\u00e3o iterativo, para encontrar as solu\u00e7\u00f5es de conjunto de Pareto para o problema multiobjetivo.</p> <p>Assim, com a estrutura do modelo definida (usaremos a obtida acima com os dados din\u00e2micos), \u00e9 poss\u00edvel estimar os par\u00e2metros utilizando a abordagem multiobjetivo.</p> <p>As informa\u00e7\u00f5es sobre fun\u00e7\u00e3o est\u00e1tica e ganho est\u00e1tico, al\u00e9m dos habituais dados din\u00e2micos de entrada/sa\u00edda, podem ser usadas para construir o par de informa\u00e7\u00f5es afins para estimar os par\u00e2metros do modelo. Podemos modelar a fun\u00e7\u00e3o de custo como:</p> \\[ \\gamma(\\hat\\theta) = w_1\\cdot J_{LS}(\\hat{\\theta}) + w_2\\cdot J_{SF}(\\hat{\\theta}) + w_3\\cdot J_{SG}(\\hat{\\theta}) \\]"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#estimacao-multiobjetivo-de-parametros-considerando-3-objetivos-diferentes-erro-de-predicao-funcao-estatica-e-ganho-estatico","title":"Estima\u00e7\u00e3o multiobjetivo de par\u00e2metros considerando 3 objetivos diferentes: erro de predi\u00e7\u00e3o, fun\u00e7\u00e3o est\u00e1tica e ganho est\u00e1tico","text":"<pre><code># voc\u00ea pode usar qualquer estrutura de modelo no seu caso de uso, mas neste notebook usaremos a obtida acima para comparar com outros trabalhos\nmo_estimator = AILS(final_model=model.final_model)\n\n# definindo os pesos (log-spaced) de cada fun\u00e7\u00e3o objetivo\nw = set_weights(static_function=True, static_gain=True)\n\n# voc\u00ea tamb\u00e9m pode usar algo como\n\n# w = np.array(\n#     [\n#         [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01],\n#         [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01],\n#         [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98],\n#     ]\n# )\n\n# para definir os pesos. Cada linha corresponde a cada objetivo\n</code></pre> <p>O AILS possui um m\u00e9todo <code>estimate</code> que retorna as fun\u00e7\u00f5es custo (J), a norma Euclidiana das fun\u00e7\u00f5es custo (E), os par\u00e2metros estimados referentes a cada combina\u00e7\u00e3o de pesos (theta), a matriz de regressores do ganho e da fun\u00e7\u00e3o est\u00e1tica HR e QR, respectivamente.</p> <pre><code>J, E, theta, HR, QR, position = mo_estimator.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.006842 0.003078 0.990080 0.999970 1.095020e-05 0.000013 0.245244 1 0.007573 0.002347 0.990080 0.999938 2.294665e-05 0.000016 0.245236 2 0.008382 0.001538 0.990080 0.999885 6.504913e-05 0.000018 0.245223 3 0.009277 0.000642 0.990080 0.999717 4.505541e-04 0.000021 0.245182 4 0.006842 0.098663 0.894495 1.000000 7.393246e-08 0.000015 0.245251 ... ... ... ... ... ... ... ... 2290 0.659632 0.333527 0.006842 0.995896 3.965699e-04 1.000000 0.244489 2291 0.730119 0.263039 0.006842 0.995632 5.602981e-04 0.972842 0.244412 2292 0.808139 0.185020 0.006842 0.995364 8.321071e-04 0.868299 0.244300 2293 0.894495 0.098663 0.006842 0.995100 1.364999e-03 0.660486 0.244160 2294 0.990080 0.003078 0.006842 0.992584 9.825987e-02 0.305492 0.261455 <p>2295 rows \u00d7 7 columns</p> <p>Agora podemos definir \\(\\theta\\) associado a qualquer combina\u00e7\u00e3o de pesos.</p> <pre><code>model.theta = theta[-1, :].reshape(\n    -1, 1\n)  # definindo o theta estimado para a \u00faltima combina\u00e7\u00e3o de pesos\n# a estrutura do modelo \u00e9 exatamente a mesma, mas a ordem dos regressores \u00e9 alterada no m\u00e9todo estimate. Por isso voc\u00ea precisa alterar o model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 2.2930E+00 9.999E-01 1 y(k-1) 2.3307E-01 2.042E-05 2 y(k-2) 6.3209E-01 1.108E-06 3 x1(k-1) -5.9333E-01 4.688E-06 4 y(k-1)^2 2.7673E-01 3.922E-07 5 y(k-2)y(k-1) -5.3228E-01 8.389E-07 6 x1(k-1)y(k-1) 1.6667E-02 5.690E-07 7 y(k-2)^2 2.5766E-01 3.827E-06"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#os-resultados-dinamicos-para-esse-theta-escolhido-sao","title":"Os resultados din\u00e2micos para esse \\(\\theta\\) escolhido s\u00e3o","text":"<pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#o-resultado-do-ganho-estatico-e","title":"O resultado do ganho est\u00e1tico \u00e9","text":"<pre><code>plt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#o-resultado-da-funcao-estatica-e","title":"O resultado da fun\u00e7\u00e3o est\u00e1tica \u00e9","text":"<pre><code>plt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \\u200b\\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#obtendo-a-melhor-combinacao-de-pesos-com-base-na-norma-da-funcao-de-custo","title":"Obtendo a melhor combina\u00e7\u00e3o de pesos com base na norma da fun\u00e7\u00e3o de custo","text":"<pre><code># a vari\u00e1vel `position` retornada no m\u00e9todo `estimate` fornece a posi\u00e7\u00e3o da melhor combina\u00e7\u00e3o de pesos\nmodel.theta = theta[position, :].reshape(\n    -1, 1\n)  # definindo o theta estimado para a melhor combina\u00e7\u00e3o de pesos\n# a estrutura do modelo \u00e9 exatamente a mesma, mas a ordem dos regressores \u00e9 alterada no m\u00e9todo estimate. Por isso voc\u00ea precisa alterar o model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\n# Resultados din\u00e2micos para esse theta escolhido\nplot_results(y=y_valid, yhat=yhat, n=1000)\n# Resultado do ganho est\u00e1tico\nplt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n# Resultado da fun\u00e7\u00e3o est\u00e1tica\nplt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \\u200b\\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>      Regressors   Parameters        ERR\n0              1   1.5405E+00  9.999E-01\n1         y(k-1)   2.9687E-01  2.042E-05\n2         y(k-2)   6.4693E-01  1.108E-06\n3        x1(k-1)  -4.1302E-01  4.688E-06\n4       y(k-1)^2   2.7671E-01  3.922E-07\n5   y(k-2)y(k-1)  -5.3474E-01  8.389E-07\n6  x1(k-1)y(k-1)   4.0624E-03  5.690E-07\n7       y(k-2)^2   2.5832E-01  3.827E-06\n</code></pre> <p>Voc\u00ea tamb\u00e9m pode plotar as solu\u00e7\u00f5es de Pareto-set</p> <pre><code>plt.figure(6)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Pareto-set solutions\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#detalhando-o-ails","title":"Detalhando o AILS","text":"<p>O modelo NARX polinomial constru\u00eddo usando a abordagem mono-objetivo possui a seguinte estrutura:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>O objetivo ao usar a informa\u00e7\u00e3o de fun\u00e7\u00e3o est\u00e1tica e ganho est\u00e1tico no cen\u00e1rio multiobjetivo \u00e9 estimar o vetor \\(\\hat{\\theta}\\) com base em:</p> \\[ \\theta = [w_1\\Psi^T\\Psi + w_2(HR)^T(HR) + w_3(QR)(QR)^T]^{-1} [w_1\\Psi^T y + w_2(HR)^T\\overline{g}+w_3(QR)^T\\overline{y}] \\] <p>A matriz \\(\\Psi\\) \u00e9 constru\u00edda usando a abordagem usual de modelagem din\u00e2mica mono-objetivo no SysIdentPy. Entretanto, ainda \u00e9 necess\u00e1rio encontrar as matrizes Q, H e R. O AILS possui m\u00e9todos para calcular todas essas matrizes. Basicamente, para isso, \\(q_i^T\\) \u00e9 primeiro estimado:</p> \\[ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_1} &amp; \\overline{y_i}^2 &amp; \\cdots &amp; \\overline{y_i}^l &amp;  F_{yu} &amp; \\overline{u_i}^2 &amp; \\cdots &amp; \\overline{u_i}^l \\end{bmatrix} \\] <p>onde \\(F_{yu}\\) representa todos os mon\u00f4mios n\u00e3o lineares no modelo que est\u00e3o relacionados a \\(y(k)\\) e \\(u(k)\\), e \\(l\\) \u00e9 o maior grau de n\u00e3o linearidade no modelo para termos de entrada e sa\u00edda. Para um modelo com grau de n\u00e3o linearidade igual a 2, podemos obter:</p> \\[ q_i^T =  \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_i} &amp; \\overline{y_i}^2 &amp; \\overline{u_i}\\:\\overline{y_i} &amp; \\overline{u_i}^2  \\end{bmatrix} \\] <p>\u00c9 poss\u00edvel codificar a matriz \\(q_i^T\\) de forma que ela siga a codifica\u00e7\u00e3o de modelo definida no SysIdentPy. Para isso, 0 \u00e9 considerado como constante, \\(y_i\\) igual a 1 e \\(u_i\\) igual a 2. O n\u00famero de colunas indica o grau de n\u00e3o linearidade do sistema e o n\u00famero de linhas reflete o n\u00famero de termos:</p> \\[ q_i =  \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\ 2 &amp; 2\\\\ \\end{bmatrix} =  \\begin{bmatrix} 1 \\\\ \\overline{y_i}\\\\ \\overline{u_i}\\\\ \\overline{y_i}^2\\\\ \\overline{u_i}\\:\\overline{y_i}\\\\ \\overline{u_i}^2\\\\ \\end{bmatrix} \\] <p>Por fim, o resultado pode ser facilmente obtido usando o m\u00e9todo <code>regressor_space</code> do SysIdentPy.</p> <pre><code>from sysidentpy.narmax_base import RegressorDictionary\n\nobject_qit = RegressorDictionary(xlag=1, ylag=1)\nR_example = object_qit.regressor_space(n_inputs=1) // 1000\nprint(f\"R = {R_example}\")\n</code></pre> <p>assim:</p> \\[ \\overline{y_i} = q_i^T R\\theta \\] <p>e:</p> \\[ \\overline{g_i} = H R\\theta \\] <p>onde \\(R\\) \u00e9 o mapeamento linear dos regressores est\u00e1ticos representados por \\(q_i^T\\). Al\u00e9m disso, a matriz \\(H\\) cont\u00e9m informa\u00e7\u00e3o afim relativa a \\(\\overline{g_i}\\), que \u00e9 igual a \\(\\overline{g_i} = \\frac{d\\overline{y}}{d\\overline{u}}{\\big |}_{(\\overline{u_i}\\:\\overline{y_i})}\\).</p> <p>A partir de agora, come\u00e7aremos a aplicar a estima\u00e7\u00e3o de par\u00e2metros de forma multiobjetivo. Isso ser\u00e1 feito tendo em mente o modelo NARX polinomial do conversor BUCK. Nesse contexto, \\(q_i^T\\) ser\u00e1 gen\u00e9rico e assumir\u00e1 um formato espec\u00edfico para o problema em quest\u00e3o. Para essa tarefa, ser\u00e1 usado o m\u00e9todo <code>R_qit</code>, cujo objetivo \u00e9 retornar o \\(q_i^T\\) relacionado ao modelo e a matriz de mapeamento linear \\(R\\):</p> <pre><code>R, qit = mo_estimator.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> <pre><code>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 0 0 0 0]\n [0 0 0 0 1 1 0 1]\n [0 0 0 0 0 0 1 0]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [2 0]\n [1 1]]\n</code></pre> <p>Assim,</p> \\[ q_i =  \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\  \\end{bmatrix} =  \\begin{bmatrix} 1\\\\ \\overline{y}\\\\ \\overline{u}\\\\ \\overline{y^2}\\\\ \\overline{u}\\:\\overline{y}\\\\  \\end{bmatrix} \\] <p>Voc\u00ea pode notar que o m\u00e9todo produz sa\u00eddas consistentes com o esperado:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>e:</p> \\[ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ \\overline{y^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\\\ \\overline{y}\\:\\overline{u} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\end{bmatrix} \\]"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#validacao","title":"Valida\u00e7\u00e3o","text":"<p>A seguinte estrutura de modelo ser\u00e1 usada para validar a abordagem:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[ \\therefore \\] \\[ final\\_model =  \\begin{bmatrix} 1001 &amp; 0\\\\ 1002 &amp; 0\\\\ 0 &amp; 0\\\\ 2001 &amp; 0\\\\ 2001 &amp; 2001\\\\ 2002 &amp; 2001\\\\ 2002 &amp; 0\\\\ 2002 &amp; 2002 \\end{bmatrix} \\] <p>Definindo em c\u00f3digo:</p> <pre><code>final_model = np.array(\n    [\n        [1001, 0],\n        [1002, 0],\n        [0, 0],\n        [2001, 0],\n        [2001, 2001],\n        [2002, 2001],\n        [2002, 0],\n        [2002, 2002],\n    ]\n)\nfinal_model\n</code></pre> <pre><code>array([[1001,    0],\n       [1002,    0],\n       [   0,    0],\n       [2001,    0],\n       [2001, 2001],\n       [2002, 2001],\n       [2002,    0],\n       [2002, 2002]])\n</code></pre> <pre><code>mult2 = AILS(final_model=final_model)\n</code></pre> <pre><code>def psi(X, Y):\n    PSI = np.zeros((len(X), 8))\n    for k in range(2, len(Y)):\n        PSI[k, 0] = Y[k - 1]\n        PSI[k, 1] = Y[k - 2]\n        PSI[k, 2] = 1\n        PSI[k, 3] = X[k - 1]\n        PSI[k, 4] = X[k - 1] ** 2\n        PSI[k, 5] = X[k - 2] * X[k - 1]\n        PSI[k, 6] = X[k - 2]\n        PSI[k, 7] = X[k - 2] ** 2\n    return np.delete(PSI, [0, 1], axis=0)\n</code></pre> <p>O valor de \\(\\theta\\) com o menor erro quadr\u00e1tico m\u00e9dio obtido com o mesmo c\u00f3digo implementado em Scilab foi:</p> \\[ W_{LS} = 0.3612343 \\] <p>e:</p> \\[ W_{SG} = 0.3548699 \\] <p>e:</p> \\[ W_{SF} = 0.3548699 \\] <pre><code>PSI = psi(x_train, y_train)\nw = np.array([[0.3612343], [0.2838959], [0.3548699]])\n</code></pre> <pre><code>J, E, theta, HR, QR, position = mult2.estimate(\n    y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\n# a ordem dos pesos \u00e9 diferente por causa da forma como implementamos em Python, mas os resultados s\u00e3o muito pr\u00f3ximos, como esperado\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.361234 0.35487 0.283896 1.0 1.0 1.0 1.0 <p>Resultados din\u00e2micos</p> <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = mult2.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4287E+00 9.999E-01 1 y(k-1) 5.5147E-01 2.042E-05 2 y(k-2) 4.0449E-01 1.108E-06 3 x1(k-1) -1.2605E+01 4.688E-06 4 x1(k-2) 1.2257E+01 3.922E-07 5 x1(k-1)^2 8.3274E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1416E+01 5.690E-07 7 x1(k-2)^2 3.0846E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <p>Ganho est\u00e1tico</p> <pre><code>plt.figure(7)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Fun\u00e7\u00e3o est\u00e1tica</p> <pre><code>plt.figure(8)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \\u200b\\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Solu\u00e7\u00f5es Pareto-set</p> <pre><code>plt.figure(9)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Optimum pareto-curve\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p> <pre><code>theta[position, :]\n</code></pre> <pre><code>array([  1.42867821,   0.55147249,   0.40449005, -12.60549001,\n        12.25730092,   8.32739876, -11.41573694,   3.08460955])\n</code></pre> <p>A tabela a seguir mostra os resultados reportados em <code>IniciacaoCientifica2007</code> e aqueles obtidos com a implementa\u00e7\u00e3o do SysIdentPy:</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.5514725 0.549144 \\(\\theta_2\\) 0.40449005 0.408028 \\(\\theta_3\\) 1.42867821 1.45097 \\(\\theta_4\\) -12.60548863 -12.55788 \\(\\theta_5\\) 8.32740057 8.1516315 \\(\\theta_6\\) -11.41574116 -11.09728 \\(\\theta_7\\) 12.25729955 12.215782 \\(\\theta_8\\) 3.08461195 2.9319577 <p>onde:</p> \\[ E_{Scilab} = 17.426613 \\] <p>e:</p> \\[ E_{Python} = 17.474865 \\]"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#nota-como-mencionado-anteriormente-a-ordem-dos-regressores-no-modelo-muda-mas-a-estrutura-e-a-mesma-as-tabelas-mostram-o-respectivo-parametro-de-cada-regressor-em-sysidentpy-e-iniciacaocientifica2007-mas-a-ordem-theta_1-theta_2-e-assim-por-diante-nao-e-a-mesma-daquela-em-modelfinal_model","title":"Nota: como mencionado anteriormente, a ordem dos regressores no modelo muda, mas a estrutura \u00e9 a mesma. As tabelas mostram o respectivo par\u00e2metro de cada regressor em <code>SysIdentPy</code> e <code>IniciacaoCientifica2007</code>, mas a ordem \\(\\theta_1\\), \\(\\theta_2\\) e assim por diante n\u00e3o \u00e9 a mesma daquela em <code>model.final_model</code>.","text":"<pre><code>R, qit = mult2.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> <pre><code>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 1 0 0 0]\n [0 0 0 0 0 1 1 1]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [0 2]]\n</code></pre> <p>A estrutura do modelo que ser\u00e1 utilizada (<code>IniciacaoCientifica2007</code>):</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[ q_i =  \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 2 &amp; 2\\\\  \\end{bmatrix} =  \\begin{bmatrix} 1\\\\ \\overline{y}\\\\ \\overline{u}\\\\ \\overline{u^2} \\end{bmatrix} \\] <p>e:</p> \\[ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ \\overline{u^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} \\] <p>consistente com a matriz R:</p> <pre><code>R = [0 0 1 0 0 0 0 0;1 1 0 0 0 0 0 0;0 0 0 1 0 0 1 0;0 0 0 0 1 1 0 1]; // R\n</code></pre> <p>ou:</p> \\[ R =  \\begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} \\]"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#otimizacao-biobjetivo","title":"Otimiza\u00e7\u00e3o biobjetivo","text":""},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#caso-de-uso-aplicado-ao-conversor-buck-cc-cc-usando-como-objetivos-a-informacao-da-curva-estatica-e-o-erro-de-predicao-dinamico","title":"Caso de uso aplicado ao conversor Buck CC-CC usando como objetivos a informa\u00e7\u00e3o da curva est\u00e1tica e o erro de predi\u00e7\u00e3o (din\u00e2mico)","text":"<pre><code>bi_objective = AILS(\n    static_function=True, static_gain=False, final_model=final_model, normalize=True\n)\n</code></pre> <p>O valor de \\(\\theta\\) com o menor erro quadr\u00e1tico m\u00e9dio obtido atrav\u00e9s da rotina em Scilab foi:</p> \\[ W_{LS} = 0.9931126 \\] <p>e:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective.estimate(\n    y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg ||J||: 0 0.990080 0.009920 0.990863 1.000000 0.990939 1 0.987127 0.012873 0.990865 0.987032 0.990939 2 0.984182 0.015818 0.990867 0.974307 0.990939 3 0.981247 0.018753 0.990870 0.961803 0.990940 4 0.978320 0.021680 0.990873 0.949509 0.990941 ... ... ... ... ... ... 1995 0.002555 0.997445 0.999993 0.000072 0.999993 1996 0.002547 0.997453 0.999994 0.000072 0.999994 1997 0.002540 0.997460 0.999996 0.000071 0.999996 1998 0.002532 0.997468 0.999998 0.000071 0.999998 1999 0.002525 0.997475 1.000000 0.000070 1.000000 <p>2000 rows \u00d7 5 columns</p> <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 1.3873E+00 9.999E-01 1 y(k-1) 5.4941E-01 2.042E-05 2 y(k-2) 4.0804E-01 1.108E-06 3 x1(k-1) -1.2515E+01 4.688E-06 4 x1(k-2) 1.2227E+01 3.922E-07 5 x1(k-1)^2 8.1171E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1047E+01 5.690E-07 7 x1(k-2)^2 2.9043E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(10)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \\u200b\\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Static Curve Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>onde o melhor \\(\\theta\\) estimado \u00e9</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54940883 0.5494135 \\(\\theta_2\\) 0.40803995 0.4080312 \\(\\theta_3\\) 1.38725684 3.3857601 \\(\\theta_4\\) -12.51466378 -12.513688 \\(\\theta_5\\) 8.11712897 8.116575 \\(\\theta_6\\) -11.04664789 -11.04592 \\(\\theta_7\\) 12.22693907 12.227184 \\(\\theta_8\\) 2.90425844 2.9038468 <p>onde:</p> \\[ E_{Scilab} = 17.408934 \\] <p>e:</p> \\[ E_{Python} = 17.408947 \\]"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#estimacao-multiobjetivo-de-parametros","title":"Estima\u00e7\u00e3o multiobjetivo de par\u00e2metros","text":""},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#caso-de-uso-considerando-2-objetivos-diferentes-erro-de-predicao-e-ganho-estatico","title":"Caso de uso considerando 2 objetivos diferentes: erro de predi\u00e7\u00e3o e ganho est\u00e1tico","text":"<pre><code>bi_objective_gain = AILS(\n    static_function=False, static_gain=True, final_model=final_model, normalize=False\n)\n</code></pre> <p>O valor de \\(\\theta\\) com o menor erro quadr\u00e1tico m\u00e9dio obtido atrav\u00e9s da rotina em Scilab foi:</p> \\[ W_{LS} = 0.9931126 \\] <p>e:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(0, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective_gain.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg ||J||: 0 1.000000 0.000000 17.407256 3.579461e+01 39.802849 1 0.997012 0.002988 17.407528 2.109260e-01 17.408806 2 0.994033 0.005967 17.407540 2.082067e-01 17.408785 3 0.991063 0.008937 17.407559 2.056636e-01 17.408774 4 0.988102 0.011898 17.407585 2.031788e-01 17.408771 ... ... ... ... ... ... 1995 0.002555 0.997445 17.511596 3.340081e-07 17.511596 1996 0.002547 0.997453 17.511596 3.320125e-07 17.511596 1997 0.002540 0.997460 17.511597 3.300289e-07 17.511597 1998 0.002532 0.997468 17.511598 3.280571e-07 17.511598 1999 0.002525 0.997475 17.511599 3.260972e-07 17.511599 <p>2000 rows \u00d7 5 columns</p> <pre><code># Escrevendo os resultados\nmodel.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective_gain.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4853E+00 9.999E-01 1 y(k-1) 5.4940E-01 2.042E-05 2 y(k-2) 4.0806E-01 1.108E-06 3 x1(k-1) -1.2581E+01 4.688E-06 4 x1(k-2) 1.2210E+01 3.922E-07 5 x1(k-1)^2 8.1686E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1122E+01 5.690E-07 7 x1(k-2)^2 2.9455E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(12)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Gain Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>sendo o \\(\\theta\\) selecionado:</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54939785 0.54937289 \\(\\theta_2\\) 0.40805603 0.40810168 \\(\\theta_3\\) 1.48525190 1.48663719 \\(\\theta_4\\) -12.58066084 -12.58127183 \\(\\theta_5\\) 8.16862622 8.16780294 \\(\\theta_6\\) -11.12171897 -11.11998621 \\(\\theta_7\\) 12.20954849 12.20927355 \\(\\theta_8\\) 2.94548501 2.9446532 <p>onde:</p> \\[ E_{Scilab} =  17.408997 \\] <p>e:</p> \\[ E_{Python} = 17.408781 \\]"},{"location":"pt/user-guide/tutorials/multiobjective-parameter-estimation-overview/#informacoes-adicionais","title":"Informa\u00e7\u00f5es adicionais","text":"<p>Voc\u00ea tamb\u00e9m pode acessar as matrizes Q e H usando os seguintes m\u00e9todos.</p> <p>Matriz Q:</p> <pre><code>bi_objective_gain.build_static_function_information(Uo, Yo)[1]\n</code></pre> <pre><code>array([[   50.        ,   800.        ,   800.        ,   100.        ,\n          100.        ,   269.3877551 ,   269.3877551 ,   269.3877551 ],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407]])\n</code></pre> <p>Matriz H+R:</p> <pre><code>bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1]\n</code></pre> <pre><code>array([[    0.        ,     0.        ,     0.        ,     0.        ,\n            0.        ,     0.        ,     0.        ,     0.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041]])\n</code></pre>"},{"location":"pt/user-guide/tutorials/parameter-estimation-overview/","title":"Estima\u00e7\u00e3o de Par\u00e2metros - Vis\u00e3o Geral","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <p>Aqui importamos o modelo NARMAX, a m\u00e9trica para avalia\u00e7\u00e3o do modelo e os m\u00e9todos para gerar dados de amostra para testes. Tamb\u00e9m importamos o pandas para uso espec\u00edfico.</p> <pre><code>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import (\n    TotalLeastSquares,\n    RecursiveLeastSquares,\n    NonNegativeLeastSquares,\n    LeastMeanSquares,\n    AffineLeastMeanSquares,\n)\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n</code></pre>"},{"location":"pt/user-guide/tutorials/parameter-estimation-overview/#gerando-dados-de-amostra-com-1-entrada-e-1-saida","title":"Gerando dados de amostra com 1 entrada e 1 sa\u00edda","text":"<p>Os dados s\u00e3o gerados simulando o seguinte modelo:</p> <p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\)</p> <p>Se colored_noise for definido como True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>onde \\(x\\) \u00e9 uma vari\u00e1vel aleat\u00f3ria uniformemente distribu\u00edda e \\(\\nu\\) \u00e9 uma vari\u00e1vel com distribui\u00e7\u00e3o gaussiana com \\(\\mu=0\\) e \\(\\sigma=0.1\\)</p> <p>No pr\u00f3ximo exemplo, geraremos dados com 1000 amostras com ru\u00eddo branco e selecionando 90% dos dados para treinar o modelo.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre>"},{"location":"pt/user-guide/tutorials/parameter-estimation-overview/#existem-varios-metodos-para-estimacao-de-parametros","title":"Existem v\u00e1rios m\u00e9todos para estima\u00e7\u00e3o de par\u00e2metros.","text":"<ul> <li>Least Squares;</li> <li>Total Least Squares;</li> <li>Recursive Least Squares</li> <li>Ridge Regression</li> <li>NonNegative Least Squares</li> <li>Least Squares Minimal Residues</li> <li>Bounded Variable Least Squares</li> <li>Least Mean Squares</li> <li>Affine Least Mean Squares</li> <li>Least Mean Squares Sign Error</li> <li>Normalized Least Mean Squares</li> <li>Least Mean Squares Normalized Sign Error</li> <li>Least Mean Squares Sign Regressor</li> <li>Least Mean Squares Normalized Sign Regressor</li> <li>Least Mean Squares Sign Sign</li> <li>Least Mean Squares Normalized Sign Sign</li> <li>Least Mean Squares Normalized Leaky</li> <li>Least Mean Squares Leaky</li> <li>Least Mean Squares Fourth</li> <li>Least Mean Squares Mixed Norm</li> </ul> <p>Modelos NARMAX polinomiais s\u00e3o lineares nos par\u00e2metros, ent\u00e3o m\u00e9todos baseados em Least Squares funcionam bem para a maioria dos casos (usando com o algoritmo Extended Least Squares ao lidar com ru\u00eddo colorido).</p> <p>No entanto, o usu\u00e1rio pode escolher alguns m\u00e9todos recursivos e de gradiente descendente estoc\u00e1stico (neste caso, o algoritmo Least Mean Squares e suas variantes) para essa tarefa tamb\u00e9m.</p> <p>Escolher o m\u00e9todo \u00e9 simples: passe qualquer um dos m\u00e9todos mencionados acima no par\u00e2metro estimator.</p> <ul> <li>Nota: Cada algoritmo tem par\u00e2metros espec\u00edficos que precisam ser ajustados. Nos exemplos a seguir, usaremos os valores padr\u00e3o. Mais exemplos sobre ajuste de par\u00e2metros estar\u00e3o dispon\u00edveis em breve. Por enquanto, o usu\u00e1rio pode ler a documenta\u00e7\u00e3o do m\u00e9todo para mais informa\u00e7\u00f5es.</li> </ul>"},{"location":"pt/user-guide/tutorials/silver-box-system/","title":"Sistema Silver Box","text":"<p>Nota: O exemplo mostrado neste notebook \u00e9 retirado do livro complementar Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>O conte\u00fado da descri\u00e7\u00e3o deriva principalmente (copiar e colar) do artigo associado - Three free data sets for development and benchmarking in nonlinear system identification. Para uma descri\u00e7\u00e3o detalhada, os leitores s\u00e3o encaminhados \u00e0 refer\u00eancia vinculada.</p> <p>O sistema Silverbox pode ser visto como uma implementa\u00e7\u00e3o eletr\u00f4nica do oscilador de Duffing. \u00c9 constru\u00eddo como um sistema linear invariante no tempo de 2\u00aa ordem com uma n\u00e3o linearidade est\u00e1tica polinomial de 3\u00ba grau ao redor dele em feedback. Este tipo de din\u00e2mica \u00e9, por exemplo, frequentemente encontrado em sistemas mec\u00e2nicos Nonlinear Benchmark - Silverbox.</p> <p>Neste estudo de caso, criaremos um modelo NARX para o benchmark Silver box. O Silver box representa uma vers\u00e3o simplificada de processos oscilat\u00f3rios mec\u00e2nicos, que s\u00e3o uma categoria cr\u00edtica de sistemas din\u00e2micos n\u00e3o lineares. Exemplos incluem suspens\u00f5es de ve\u00edculos, onde amortecedores e molas progressivas desempenham pap\u00e9is vitais. Os dados gerados pelo Silver box fornecem uma representa\u00e7\u00e3o simplificada de tais componentes combinados. O circuito el\u00e9trico que gera esses dados aproxima de perto, mas n\u00e3o corresponde perfeitamente, aos modelos idealizados descritos abaixo.</p> <p>Conforme descrito no artigo original, o sistema foi excitado usando um gerador de forma de onda geral (HPE1445A). O sinal de entrada come\u00e7a como um sinal de tempo discreto \\(r(k)\\), que \u00e9 convertido para um sinal anal\u00f3gico \\(r_c(t)\\) usando reconstru\u00e7\u00e3o zero-order-hold. O sinal de excita\u00e7\u00e3o real \\(u_0(t)\\) \u00e9 ent\u00e3o obtido passando \\(r_c(t)\\) atrav\u00e9s de um filtro passa-baixa anal\u00f3gico \\(G(p)\\) para eliminar o conte\u00fado de alta frequ\u00eancia em torno de m\u00faltiplos da frequ\u00eancia de amostragem. Aqui, \\(p\\) denota o operador de diferencia\u00e7\u00e3o. Assim, a entrada \u00e9 dada por:</p> \\[ u_0(t) = G(p) r_c(t). \\] <p>Os sinais de entrada e sa\u00edda foram medidos usando placas de aquisi\u00e7\u00e3o de dados HP1430A, com rel\u00f3gios sincronizados para as placas de aquisi\u00e7\u00e3o e gerador. A frequ\u00eancia de amostragem foi:</p> \\[ f_s = \\frac{10^7}{2^{14}} = 610.35 \\, \\text{Hz}. \\] <p>O silver box usa circuitos el\u00e9tricos anal\u00f3gicos para gerar dados representando um sistema mec\u00e2nico ressonante n\u00e3o linear com uma massa m\u00f3vel \\(m\\), amortecimento viscoso \\(d\\), e uma mola n\u00e3o linear \\(k(y)\\). O circuito el\u00e9trico \u00e9 projetado para relacionar o deslocamento \\(y(t)\\) (a sa\u00edda) \u00e0 for\u00e7a \\(u(t)\\) (a entrada) pela seguinte equa\u00e7\u00e3o diferencial:</p> \\[ m \\frac{d^2 y(t)}{dt^2} + d \\frac{d y(t)}{dt} + k(y(t)) y(t) = u(t). \\] <p>A mola progressiva n\u00e3o linear \u00e9 descrita por uma rigidez est\u00e1tica dependente da posi\u00e7\u00e3o:</p> \\[ k(y(t)) = a + b y^2(t). \\] <p>A rela\u00e7\u00e3o sinal-ru\u00eddo \u00e9 suficientemente alta para modelar o sistema sem considerar o ru\u00eddo de medi\u00e7\u00e3o. No entanto, o ru\u00eddo de medi\u00e7\u00e3o pode ser inclu\u00eddo substituindo \\(y(t)\\) pela vari\u00e1vel artificial \\(x(t)\\) na equa\u00e7\u00e3o acima, e introduzindo perturba\u00e7\u00f5es \\(w(t)\\) e \\(e(t)\\) da seguinte forma:</p> \\[ \\begin{align} &amp; m \\frac{d^2 x(t)}{dt^2} + d \\frac{d x(t)}{dt} + k(x(t)) x(t) = u(t) + w(t), \\\\ &amp; k(x(t)) = a + b x^2(t), \\\\ &amp; y(t) = x(t) + e(t). \\end{align} \\]"},{"location":"pt/user-guide/tutorials/silver-box-system/#pacotes-necessarios-e-versoes","title":"Pacotes Necess\u00e1rios e Vers\u00f5es","text":"<p>Para garantir que voc\u00ea possa replicar este estudo de caso, \u00e9 essencial usar vers\u00f5es espec\u00edficas dos pacotes necess\u00e1rios. Abaixo est\u00e1 uma lista dos pacotes junto com suas respectivas vers\u00f5es necess\u00e1rias para executar os estudos de caso efetivamente.</p> <p>Para instalar todos os pacotes necess\u00e1rios, voc\u00ea pode criar um arquivo <code>requirements.txt</code> com o seguinte conte\u00fado:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Ent\u00e3o, instale os pacotes usando:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Certifique-se de usar um ambiente virtual para evitar conflitos entre vers\u00f5es de pacotes.</li> <li>As vers\u00f5es especificadas s\u00e3o baseadas na compatibilidade com os exemplos de c\u00f3digo fornecidos. Se voc\u00ea estiver usando vers\u00f5es diferentes, alguns ajustes no c\u00f3digo podem ser necess\u00e1rios.</li> </ul>"},{"location":"pt/user-guide/tutorials/silver-box-system/#configuracao-do-sysidentpy","title":"Configura\u00e7\u00e3o do SysIdentPy","text":"<p>Nesta se\u00e7\u00e3o, demonstraremos a aplica\u00e7\u00e3o do SysIdentPy ao dataset Silver box. O c\u00f3digo a seguir guiar\u00e1 voc\u00ea atrav\u00e9s do processo de carregamento do dataset, configura\u00e7\u00e3o dos par\u00e2metros do SysIdentPy e constru\u00e7\u00e3o de um modelo para o sistema mencionado.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.Silverbox(atleast_2d=True)\n\nx_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_multisine.u, test_multisine.y\n\nn = test_multisine.state_initialization_window_length\n</code></pre> <p>Usamos o pacote <code>nonlinear_benchmarks</code> para carregar os dados. O usu\u00e1rio \u00e9 encaminhado \u00e0 documenta\u00e7\u00e3o do pacote - GerbenBeintema/nonlinear_benchmarks: The official dataload for http://www.nonlinearbenchmark.org/ (github.com) para verificar os detalhes de como us\u00e1-lo.</p> <p>O gr\u00e1fico a seguir detalha os dados de treinamento e teste do experimento.</p> <pre><code>plt.plot(x_train)\nplt.plot(y_train, alpha=0.3)\nplt.title(\"Experimento 1: dados de treinamento\")\nplt.show()\n\nplt.plot(x_test)\nplt.plot(y_test, alpha=0.3)\nplt.title(\"Experimento 1: dados de teste\")\nplt.show()\n\nplt.plot(test_arrow_full.u)\nplt.plot(test_arrow_full.y, alpha=0.3)\nplt.title(\"Experimento 2: dados de treinamento\")\nplt.show()\n\nplt.plot(test_arrow_no_extrapolation.u)\nplt.plot(test_arrow_no_extrapolation.y, alpha=0.2)\nplt.title(\"Experimento 2: dados de teste\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Nota Importante</p> <p>O objetivo deste benchmark \u00e9 desenvolver um modelo que supere o modelo estado-da-arte (SOTA) apresentado no artigo de benchmarking. No entanto, os resultados no artigo diferem daqueles fornecidos no reposit\u00f3rio GitHub.</p> nx Conjunto NRMS RMS (mV) 2 Treino 0.10653 5.8103295 2 Valida\u00e7\u00e3o 0.11411 6.1938068 2 Teste 0.19151 10.2358533 2 Teste (no extra) 0.12284 5.2789727 4 Treino 0.03571 1.9478290 4 Valida\u00e7\u00e3o 0.03922 2.1286373 4 Teste 0.12712 6.7943448 4 Teste (no extra) 0.05204 2.2365904 8 Treino 0.03430 1.8707026 8 Valida\u00e7\u00e3o 0.03732 2.0254112 8 Teste 0.10826 5.7865255 8 Teste (no extra) 0.04743 2.0382715 &gt; Tabela: resultados apresentados no github. <p>Parece que os valores mostrados no artigo realmente representam o tempo de treinamento, n\u00e3o as m\u00e9tricas de erro. Entrarei em contato com os autores para confirmar esta informa\u00e7\u00e3o. De acordo com o site Nonlinear Benchmark, a informa\u00e7\u00e3o \u00e9 a seguinte:</p> <p></p> <p>onde os valores na coluna \"Training time\" correspondem aos apresentados como m\u00e9tricas de erro no artigo.</p> <p>Enquanto aguardamos a confirma\u00e7\u00e3o dos valores corretos para este benchmark, demonstraremos o desempenho do SysIdentPy. No entanto, nos absteremos de fazer compara\u00e7\u00f5es ou tentar melhorar o modelo nesta fase.</p>"},{"location":"pt/user-guide/tutorials/silver-box-system/#resultados","title":"Resultados","text":"<p>Come\u00e7aremos (como fizemos em todos os outros estudos de caso) com uma configura\u00e7\u00e3o b\u00e1sica do FROLS usando uma fun\u00e7\u00e3o de base polinomial com grau igual a 2. O <code>xlag</code> e <code>ylag</code> s\u00e3o definidos como \\(7\\) neste primeiro exemplo. Como o dataset \u00e9 consideravelmente grande, come\u00e7aremos com <code>n_info_values=40</code>. Como estamos lidando com um grande dataset de treinamento, usaremos o <code>err_tol</code> em vez de crit\u00e9rios de informa\u00e7\u00e3o para ter um desempenho mais r\u00e1pido. Tamb\u00e9m definiremos <code>n_terms=40</code>, o que significa que a busca parar\u00e1 se o <code>err_tol</code> for atingido ou 40 regressores forem testados no algoritmo <code>ERR</code>. Embora esta abordagem possa resultar em um modelo sub-\u00f3timo, \u00e9 um ponto de partida razo\u00e1vel para nossa primeira tentativa. Existem tr\u00eas experimentos diferentes: multisine, arrow (full) e arrow (no extrapolation).</p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.999,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nnrmse = rmse / y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Multisine. Modelo -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=300,\n    figsize=(15, 4),\n    title=f\"Multisine. Modelo -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n</code></pre> <pre><code>0.1423804033714937 7.727682109791501\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_full.u, test_arrow_full.y\n\nn = test_arrow_full.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9999,\n    n_terms=80,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\n# n\u00e3o concatenaremos os \u00faltimos valores dos dados de treino para usar como condi\u00e7\u00e3o inicial aqui porque\n# estes dados de teste t\u00eam um comportamento muito diferente.\n# No entanto, se voc\u00ea quiser, pode fazer isso e ver\u00e1 que o modelo ainda ter\u00e1\n# um \u00f3timo desempenho ap\u00f3s algumas itera\u00e7\u00f5es\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nnrmse = rmse / y_test.std()\nrmse_mv = 1000 * rmse\n\nprint(nrmse, rmse_mv)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Arrow (full). Modelo -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=300,\n    figsize=(15, 4),\n    title=f\"Arrow (full). Modelo -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n</code></pre> <pre><code>0.07762658947015803 4.14903534238172\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_no_extrapolation.u, test_arrow_no_extrapolation.y\n\nn = test_arrow_no_extrapolation.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9999,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nnrmse = rmse / y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Arrow (no extrapolation). Modelo -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=300,\n    figsize=(15, 4),\n    title=f\"Simula\u00e7\u00e3o Free Run. Modelo -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n</code></pre> <pre><code>0.05187400789723806 2.2293393254015776\n</code></pre> <p></p> <p></p>"},{"location":"pt/user-guide/tutorials/wiener-hammerstein-system/","title":"Sistema Wiener Hammerstein","text":"<p>Nota: O exemplo mostrado neste notebook \u00e9 retirado do livro complementar Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>O conte\u00fado da descri\u00e7\u00e3o deriva principalmente do site do benchmark - Nonlinear Benchmark e do artigo associado - Wiener-Hammerstein benchmark with process noise. Para uma descri\u00e7\u00e3o detalhada, os leitores s\u00e3o encaminhados \u00e0s refer\u00eancias vinculadas.</p> <p>O site de benchmarks n\u00e3o lineares representa uma contribui\u00e7\u00e3o significativa para a comunidade de identifica\u00e7\u00e3o de sistemas e aprendizado de m\u00e1quina. Os usu\u00e1rios s\u00e3o encorajados a explorar todos os artigos referenciados no site.</p> <p>Este benchmark foca em um circuito eletr\u00f4nico Wiener-Hammerstein onde o ru\u00eddo de processo desempenha um papel significativo na distor\u00e7\u00e3o do sinal de sa\u00edda.</p> <p>A estrutura Wiener-Hammerstein \u00e9 um sistema orientado a blocos bem conhecido que cont\u00e9m uma n\u00e3o linearidade est\u00e1tica intercalada entre dois blocos Lineares Invariantes no Tempo (LTI) (Figura 2). Este arranjo apresenta um problema de identifica\u00e7\u00e3o desafiador devido \u00e0 presen\u00e7a desses blocos LTI.</p> <p></p> <p>Figura 2: o sistema Wiener-Hammerstein</p> <p>Na Figura 2, o sistema Wiener-Hammerstein \u00e9 ilustrado com ru\u00eddo de processo \\(e_x(t)\\) entrando antes da n\u00e3o linearidade est\u00e1tica \\(f(x)\\), intercalado entre blocos LTI representados por \\(R(s)\\) e \\(S(s)\\) na entrada e sa\u00edda, respectivamente. Al\u00e9m disso, pequenas fontes de ru\u00eddo desprez\u00edveis \\(e_u(t)\\) e \\(e_y(t)\\) afetam os canais de medi\u00e7\u00e3o. Os sinais de entrada e sa\u00edda medidos s\u00e3o denotados como \\(u_m(t)\\) e \\(y_m(t)\\).</p> <p>O primeiro bloco LTI \\(R(s)\\) \u00e9 efetivamente modelado como um filtro passa-baixa de terceira ordem. O segundo subsistema LTI \\(S(s)\\) \u00e9 configurado como um filtro Chebyshev inverso com atenua\u00e7\u00e3o de banda de parada de \\(40 dB\\) e frequ\u00eancia de corte de \\(5 kHz\\). Notavelmente, \\(S(s)\\) inclui um zero de transmiss\u00e3o dentro da faixa de frequ\u00eancia operacional, complicando sua invers\u00e3o.</p> <p>A n\u00e3o linearidade est\u00e1tica \\(f(x)\\) \u00e9 implementada usando uma rede de diodo-resistor, resultando em n\u00e3o linearidade de satura\u00e7\u00e3o. O ru\u00eddo de processo \\(e_x(t)\\) \u00e9 introduzido como ru\u00eddo gaussiano branco filtrado, gerado a partir de um filtro Butterworth passa-baixa de terceira ordem em tempo discreto seguido por zero-order hold e filtragem de reconstru\u00e7\u00e3o passa-baixa anal\u00f3gica com corte de \\(20 kHz\\).</p> <p>As fontes de ru\u00eddo de medi\u00e7\u00e3o \\(e_u(t)\\) e \\(e_y(t)\\) s\u00e3o m\u00ednimas comparadas a \\(e_x(t)\\). As entradas do sistema e o ru\u00eddo de processo s\u00e3o gerados usando um Gerador de Forma de Onda Arbitr\u00e1ria (AWG), especificamente o Agilent/HP E1445A, amostrando a \\(78125 Hz\\), sincronizado com um sistema de aquisi\u00e7\u00e3o (Agilent/HP E1430A) para garantir coer\u00eancia de fase e prevenir erros de vazamento. O buffering entre as placas de aquisi\u00e7\u00e3o e as entradas e sa\u00eddas do sistema minimiza a distor\u00e7\u00e3o do equipamento de medi\u00e7\u00e3o.</p> <p>O benchmark fornece dois sinais de teste padr\u00e3o atrav\u00e9s do site de benchmarking: um multisine de fase aleat\u00f3ria e um sinal de varredura senoidal. Ambos os sinais t\u00eam um valor \\(rms\\) de \\(0.71 Vrms\\) e cobrem frequ\u00eancias de DC a \\(15 kHz\\) (excluindo DC). A varredura senoidal abrange esta faixa de frequ\u00eancia a uma taxa de \\(4.29 MHz/min\\). Estes conjuntos de teste servem como alvos para avaliar o desempenho do modelo, enfatizando representa\u00e7\u00e3o precisa sob condi\u00e7\u00f5es variadas.</p> <p>O benchmark Wiener-Hammerstein destaca tr\u00eas desafios principais de identifica\u00e7\u00e3o de sistemas n\u00e3o lineares:</p> <ol> <li>Ru\u00eddo de Processo: Significativo no sistema, influenciando a fidelidade da sa\u00edda.</li> <li>N\u00e3o Linearidade Est\u00e1tica: Indiretamente acess\u00edvel a partir de dados medidos, apresentando desafios de identifica\u00e7\u00e3o.</li> <li>Din\u00e2micas de Sa\u00edda: Invers\u00e3o complexa devido \u00e0 presen\u00e7a de zero de transmiss\u00e3o em \\(S(s)\\).</li> </ol> <p>O objetivo deste benchmark \u00e9 desenvolver e validar modelos robustos usando dados de estima\u00e7\u00e3o separados, garantindo caracteriza\u00e7\u00e3o precisa do comportamento do sistema Wiener-Hammerstein.</p>"},{"location":"pt/user-guide/tutorials/wiener-hammerstein-system/#pacotes-necessarios-e-versoes","title":"Pacotes Necess\u00e1rios e Vers\u00f5es","text":"<p>Para garantir que voc\u00ea possa replicar este estudo de caso, \u00e9 essencial usar vers\u00f5es espec\u00edficas dos pacotes necess\u00e1rios. Abaixo est\u00e1 uma lista dos pacotes junto com suas respectivas vers\u00f5es necess\u00e1rias para executar os estudos de caso efetivamente.</p> <p>Para instalar todos os pacotes necess\u00e1rios, voc\u00ea pode criar um arquivo <code>requirements.txt</code> com o seguinte conte\u00fado:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Ent\u00e3o, instale os pacotes usando: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Certifique-se de usar um ambiente virtual para evitar conflitos entre vers\u00f5es de pacotes.</li> <li>As vers\u00f5es especificadas s\u00e3o baseadas na compatibilidade com os exemplos de c\u00f3digo fornecidos. Se voc\u00ea estiver usando vers\u00f5es diferentes, alguns ajustes no c\u00f3digo podem ser necess\u00e1rios.</li> </ul>"},{"location":"pt/user-guide/tutorials/wiener-hammerstein-system/#configuracao-do-sysidentpy","title":"Configura\u00e7\u00e3o do SysIdentPy","text":"<p>Nesta se\u00e7\u00e3o, demonstraremos a aplica\u00e7\u00e3o do SysIdentPy ao dataset do sistema Wiener-Hammerstein. O c\u00f3digo a seguir guiar\u00e1 voc\u00ea atrav\u00e9s do processo de carregamento do dataset, configura\u00e7\u00e3o dos par\u00e2metros do SysIdentPy e constru\u00e7\u00e3o de um modelo para o sistema Wiener-Hammerstein.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import (\n    LeastSquares,\n    BoundedVariableLeastSquares,\n    NonNegativeLeastSquares,\n    LeastSquaresMinimalResidual,\n)\n\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.WienerHammerBenchMark(atleast_2d=True)\nx_train, y_train = train_val\nx_test, y_test = test\n</code></pre> <p>Usamos o pacote <code>nonlinear_benchmarks</code> para carregar os dados. O usu\u00e1rio \u00e9 encaminhado \u00e0 documenta\u00e7\u00e3o do pacote para verificar os detalhes de como us\u00e1-lo.</p> <p>O gr\u00e1fico a seguir detalha os dados de treinamento e teste do experimento.</p> <pre><code>plot_n = 800\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_train[:plot_n])\nplt.plot(y_train[:plot_n])\nplt.title(\"Experimento: dados de treinamento\")\nplt.legend([\"x_train\", \"y_train\"])\nplt.show()\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_test[:plot_n])\nplt.plot(y_test[:plot_n])\nplt.title(\"Experimento: dados de teste\")\nplt.legend([\"x_test\", \"y_test\"])\nplt.show()\n</code></pre> <p></p> <p></p> <p>O objetivo deste benchmark \u00e9 obter um modelo que tenha um desempenho melhor que o modelo SOTA fornecido no artigo de benchmarking.</p> <p></p> <p>Resultados estado-da-arte apresentados no artigo de benchmarking. Nesta se\u00e7\u00e3o estamos trabalhando apenas com os resultados Wiener-Hammerstein, que s\u00e3o apresentados na coluna \\(W-H\\).</p>"},{"location":"pt/user-guide/tutorials/wiener-hammerstein-system/#resultados","title":"Resultados","text":"<p>Come\u00e7aremos com uma configura\u00e7\u00e3o b\u00e1sica do FROLS usando uma fun\u00e7\u00e3o de base polinomial com grau igual a 2. O <code>xlag</code> e <code>ylag</code> s\u00e3o definidos como \\(7\\) neste primeiro exemplo. Como o dataset \u00e9 consideravelmente grande, come\u00e7aremos com <code>n_info_values=50</code>. Isso significa que o algoritmo FROLS n\u00e3o incluir\u00e1 todos os regressores ao calcular os crit\u00e9rios de informa\u00e7\u00e3o usados para determinar a ordem do modelo. Embora esta abordagem possa resultar em um modelo sub-\u00f3timo, \u00e9 um ponto de partida razo\u00e1vel para nossa primeira tentativa.</p> <pre><code># 3min para rodar na minha m\u00e1quina (amd 5600x, 32gb ram)\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nrmse_sota = rmse / y_test.std()\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=1000,\n    title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\",\n)\n</code></pre> <p></p> <p>A primeira configura\u00e7\u00e3o j\u00e1 \u00e9 melhor que os modelos SOTA mostrados na tabela de benchmark! Come\u00e7amos usando <code>xlag=ylag=7</code> para ter uma ideia de qu\u00e3o bem o SysIdentPy lidaria com este dataset, mas os resultados j\u00e1 s\u00e3o muito bons! No entanto, o artigo de benchmarking indica que eles usaram lags maiores para seus modelos. Vamos verificar o que acontece se definirmos <code>xlag=ylag=10</code>.</p> <pre><code># 7min para rodar na minha m\u00e1quina (amd 5600x, 32gb ram)\n\nx_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nrmse_sota = rmse / y_test.std()\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=1000,\n    title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\",\n)\n</code></pre> <p></p> <p>O desempenho \u00e9 ainda melhor agora! Por enquanto, n\u00e3o estamos preocupados com a complexidade do modelo (mesmo neste caso onde estamos comparando com uma rede neural de estado profundo...). No entanto, se verificarmos a ordem do modelo e o gr\u00e1fico <code>AIC</code>, vemos que o modelo tem 50 regressores, mas os valores de <code>AIC</code> n\u00e3o mudam muito ap\u00f3s cada regress\u00e3o adicionada.</p> <pre><code>plt.plot(model.info_values)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x28c0058a450&gt;]\n</code></pre> <p></p> <p>Ent\u00e3o, o que acontece se definirmos um modelo com metade dos regressores?</p> <pre><code># 14 segundos para rodar\n\nx_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    n_info_values=50,\n    n_terms=25,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nrmse_sota = rmse / y_test.std()\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=1000,\n    title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\",\n)\n</code></pre> <p></p> <p>Como mostrado na figura acima, os resultados ainda superam os modelos SOTA apresentados no artigo de benchmarking. Os resultados SOTA do artigo provavelmente tamb\u00e9m poderiam ser melhorados. Os usu\u00e1rios s\u00e3o encorajados a explorar o pacote deepsysid, que pode ser usado para construir redes neurais de estado profundo.</p> <p>Esta configura\u00e7\u00e3o b\u00e1sica pode servir como ponto de partida para os usu\u00e1rios desenvolverem modelos ainda melhores usando o SysIdentPy. Experimente!</p>"},{"location":"pt/user-guide/tutorials/your-first-model/","title":"Seu Primeiro Modelo","text":"<p>Exemplo criado por Wilson Rocha Lacerda Junior</p> <p>Procurando mais detalhes sobre modelos NARMAX? Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e uma ampla variedade de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>Este livro oferece orienta\u00e7\u00e3o aprofundada para apoiar seu trabalho com o SysIdentPy.</p> <p>Aqui importamos o modelo NARMAX, a m\u00e9trica para avalia\u00e7\u00e3o do modelo e os m\u00e9todos para gerar dados de amostra para testes. Tamb\u00e9m importamos o pandas para uso espec\u00edfico.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre>"},{"location":"pt/user-guide/tutorials/your-first-model/#gerando-dados-de-amostra-com-1-entrada-e-1-saida","title":"Gerando dados de amostra com 1 entrada e 1 sa\u00edda","text":"<p>Os dados s\u00e3o gerados simulando o seguinte modelo:</p> <p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\)</p> <p>Se colored_noise for definido como True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>onde \\(x\\) \u00e9 uma vari\u00e1vel aleat\u00f3ria uniformemente distribu\u00edda e \\(\\nu\\) \u00e9 uma vari\u00e1vel com distribui\u00e7\u00e3o gaussiana com \\(\\mu=0\\) e \\(\\sigma=0.1\\)</p> <p>No pr\u00f3ximo exemplo, geraremos dados com 1000 amostras com ru\u00eddo branco e selecionando 90% dos dados para treinar o modelo.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n</code></pre> <p>Para obter um modelo NARMAX, precisamos escolher alguns valores, e.g., o grau de n\u00e3o-linearidade (degree), o lag m\u00e1ximo para as entradas e sa\u00edda (xlag e ylag).</p> <p>Al\u00e9m disso, voc\u00ea pode selecionar o crit\u00e9rio de informa\u00e7\u00e3o a ser usado com o Error Reduction Ratio para selecionar a ordem do modelo e o m\u00e9todo para estimar os par\u00e2metros do modelo:</p> <ul> <li>Crit\u00e9rios de Informa\u00e7\u00e3o: aic, aicc, bic, lilc, fpe</li> <li>Estima\u00e7\u00e3o de Par\u00e2metros: LeastSquares, TotalLeastSquares, RecursiveLeastSquares, NonNegativeLeastSquares, LeastMeanSquares e muitos outros (veja a documenta\u00e7\u00e3o)</li> </ul> <p>O valor n_terms \u00e9 opcional. Ele se refere ao n\u00famero de termos a incluir no modelo final. Voc\u00ea pode definir este valor com base no crit\u00e9rio de informa\u00e7\u00e3o (veja abaixo) ou com base em informa\u00e7\u00e3o a priori sobre a estrutura do modelo. O valor padr\u00e3o \u00e9 n_terms=None, ent\u00e3o o algoritmo escolher\u00e1 o valor m\u00ednimo alcan\u00e7ado pelo crit\u00e9rio de informa\u00e7\u00e3o.</p> <p>Para usar crit\u00e9rios de informa\u00e7\u00e3o, voc\u00ea deve definir order_selection=True. Voc\u00ea tamb\u00e9m pode selecionar n_info_values (padr\u00e3o = 15).</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    err_tol=None,\n    basis_function=basis_function,\n)\n</code></pre>"},{"location":"pt/user-guide/tutorials/your-first-model/#selecao-de-estrutura-do-modelo","title":"Sele\u00e7\u00e3o de Estrutura do Modelo","text":"<p>O m\u00e9todo fit executa o algoritmo Error Reduction Ratio usando reflex\u00e3o de Householder para selecionar a estrutura do modelo.</p> <p>Argumentos keyword-only s\u00e3o obrigat\u00f3rios nos m\u00e9todos fit e predict. Este \u00e9 um esfor\u00e7o para promover o uso claro e n\u00e3o-amb\u00edguo da biblioteca.</p> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1db932f5090&gt;\n</code></pre>"},{"location":"pt/user-guide/tutorials/your-first-model/#simulacao-free-run","title":"Simula\u00e7\u00e3o Free Run","text":"<p>O m\u00e9todo predict \u00e9 usado para gerar as predi\u00e7\u00f5es. Por enquanto, suportamos apenas free run simulation (tamb\u00e9m conhecida como infinity steps ahead). Em breve permitiremos ao usu\u00e1rio definir uma predi\u00e7\u00e3o one-step ahead ou k-step ahead.</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"pt/user-guide/tutorials/your-first-model/#avaliando-o-modelo","title":"Avaliando o modelo","text":"<p>Neste exemplo usamos a m\u00e9trica root_relative_squared_error porque ela \u00e9 frequentemente usada em Identifica\u00e7\u00e3o de Sistemas. Mais m\u00e9tricas e informa\u00e7\u00f5es podem ser encontradas na documenta\u00e7\u00e3o.</p> <pre><code>rrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00017649882109753117\n</code></pre> <p>model_object.results retorna os regressores selecionados do modelo, os par\u00e2metros estimados e os valores de ERR. Como mostrado abaixo, o algoritmo detecta exatamente o modelo que foi usado para simular os dados.</p> <pre><code>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.57604864E-01\n1         y(k-1)  2.0000E-01  3.88976063E-02\n2  x1(k-1)y(k-1)  9.9992E-02  3.49749526E-03\n</code></pre> <p>Al\u00e9m disso, voc\u00ea pode acessar os m\u00e9todos residuals e plot_result para visualizar a predi\u00e7\u00e3o e duas an\u00e1lises de res\u00edduos. Os valores extras e lam abaixo cont\u00eam outra an\u00e1lise de res\u00edduos para que voc\u00ea possa plotar manualmente. Este m\u00e9todo ser\u00e1 melhorado em breve.</p> <pre><code>plt.style.available\n</code></pre> <pre><code>['Solarize_Light2',\n '_classic_test_patch',\n '_mpl-gallery',\n '_mpl-gallery-nogrid',\n 'bmh',\n 'classic',\n 'dark_background',\n 'fast',\n 'fivethirtyeight',\n 'ggplot',\n 'grayscale',\n 'seaborn-v0_8',\n 'seaborn-v0_8-bright',\n 'seaborn-v0_8-colorblind',\n 'seaborn-v0_8-dark',\n 'seaborn-v0_8-dark-palette',\n 'seaborn-v0_8-darkgrid',\n 'seaborn-v0_8-deep',\n 'seaborn-v0_8-muted',\n 'seaborn-v0_8-notebook',\n 'seaborn-v0_8-paper',\n 'seaborn-v0_8-pastel',\n 'seaborn-v0_8-poster',\n 'seaborn-v0_8-talk',\n 'seaborn-v0_8-ticks',\n 'seaborn-v0_8-white',\n 'seaborn-v0_8-whitegrid',\n 'tableau-colorblind10']\n</code></pre> <pre><code>plot_results(\n    y=y_valid,\n    yhat=yhat,\n    n=1000,\n    title=\"test\",\n    xlabel=\"Samples\",\n    ylabel=r\"y, $\\hat{y}$\",\n    data_color=\"#1f77b4\",\n    model_color=\"#ff7f0e\",\n    marker=\"o\",\n    model_marker=\"*\",\n    linewidth=1.5,\n    figsize=(10, 6),\n    style=\"seaborn-v0_8-notebook\",\n    facecolor=\"white\",\n)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(\n    data=ee, title=\"Residues\", ylabel=\"$e^2$\", style=\"seaborn-v0_8-notebook\"\n)\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(\n    data=x1e, title=\"Residues\", ylabel=\"$x_1e$\", style=\"seaborn-v0_8-notebook\"\n)\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"pt/user-guide/tutorials/your-first-model/#definindo-o-parametro-n_terms","title":"Definindo o par\u00e2metro n_terms","text":"<p>No exemplo acima, deixamos o n\u00famero de termos que comp\u00f5em o modelo final ser definido como o valor m\u00ednimo do crit\u00e9rio de informa\u00e7\u00e3o. Uma vez que voc\u00ea executou o algoritmo e escolheu o melhor n\u00famero de par\u00e2metros, pode definir order_selection como False e definir o valor de n_terms (3 neste exemplo). Aqui temos um pequeno dataset, mas em dados maiores isso pode ser cr\u00edtico porque executar o algoritmo de crit\u00e9rio de informa\u00e7\u00e3o \u00e9 mais custoso computacionalmente. Como j\u00e1 sabemos o melhor n\u00famero de regressores, definimos n_terms e obtemos o mesmo resultado.</p> <p>No entanto, isso n\u00e3o \u00e9 cr\u00edtico apenas por efici\u00eancia computacional. Em muitas situa\u00e7\u00f5es, o valor m\u00ednimo do crit\u00e9rio de informa\u00e7\u00e3o pode levar a overfitting. Em alguns casos, a diferen\u00e7a entre escolher um modelo com 30 regressores ou 10 \u00e9 m\u00ednima, ent\u00e3o voc\u00ea pode escolher o modelo com 10 termos sem perder precis\u00e3o.</p> <p>A seguir, usamos info_values para plotar os valores do crit\u00e9rio de informa\u00e7\u00e3o. Como voc\u00ea pode ver, o valor m\u00ednimo est\u00e1 onde \\(xaxis = 5\\)</p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <p></p> <pre><code> Aqui estamos criando amostras aleat\u00f3rias com ru\u00eddo branco e deixando o algoritmo escolher\n o n\u00famero de termos com base no valor m\u00ednimo do crit\u00e9rio de informa\u00e7\u00e3o.\n Esta n\u00e3o \u00e9 a melhor abordagem em Identifica\u00e7\u00e3o de Sistemas, mas serve como um exemplo simples.\n O crit\u00e9rio de informa\u00e7\u00e3o deve ser usado como uma __ferramenta auxiliar__ para selecionar *n_terms*.\n Plote os valores de informa\u00e7\u00e3o para ajud\u00e1-lo nisso!\n\n Se voc\u00ea executar o exemplo acima v\u00e1rias vezes, pode encontrar alguns casos onde o\n algoritmo escolhe apenas os dois primeiros regressores, ou quatro (dependendo do m\u00e9todo\n de crit\u00e9rio de informa\u00e7\u00e3o selecionado). Isso ocorre porque o valor m\u00ednimo do crit\u00e9rio de informa\u00e7\u00e3o\n depende da vari\u00e2ncia residual (afetada pelo ru\u00eddo) e tem algumas limita\u00e7\u00f5es em cen\u00e1rios n\u00e3o-lineares.\n No entanto, se voc\u00ea verificar os valores de ERR (robusto ao ru\u00eddo), ver\u00e1 que o\n ERR est\u00e1 ordenando os regressores da maneira correta!\n\n Temos alguns exemplos no notebook *information_criteria*!\n</code></pre> <p>O n_info_values limita o n\u00famero de regressores para aplicar o crit\u00e9rio de informa\u00e7\u00e3o. Escolhemos \\(n_y = n_x = \\ell = 2\\), ent\u00e3o o regressor candidato \u00e9 uma lista de 15 regressores. Podemos definir n_info_values = 15 e ver os valores de informa\u00e7\u00e3o para todos os regressores. Esta op\u00e7\u00e3o pode economizar alguns recursos computacionais ao lidar com m\u00faltiplas entradas e grandes datasets.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <p></p> <p>Agora executando sem executar m\u00e9todos de crit\u00e9rio de informa\u00e7\u00e3o (definindo o n_terms) porque j\u00e1 sabemos o n\u00famero \u00f3timo de regressores</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_info_values=15,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.00017649882109753117\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.57604864E-01\n1         y(k-1)  2.0000E-01  3.88976063E-02\n2  x1(k-1)y(k-1)  9.9992E-02  3.49749526E-03\n</code></pre>"},{"location":"pt/user-guide/tutorials/your-first-model/#metodo-predict","title":"M\u00e9todo predict","text":"<p>Voc\u00ea pode perguntar por que \u00e9 necess\u00e1rio passar os dados de teste no m\u00e9todo predict. A resposta \u00e9: voc\u00ea n\u00e3o precisa passar os dados de teste quando est\u00e1 executando uma predi\u00e7\u00e3o infinity-steps ahead, voc\u00ea s\u00f3 precisa passar as condi\u00e7\u00f5es iniciais. No entanto, se voc\u00ea quiser verificar como seu modelo se comporta em uma predi\u00e7\u00e3o 1-step ahead ou n-step ahead, voc\u00ea deve fornecer os dados de teste.</p> <p>Para mostrar que voc\u00ea s\u00f3 precisa da condi\u00e7\u00e3o inicial, considere o seguinte exemplo usando o modelo treinado anteriormente:</p> <pre><code>model.max_lag  # the number of initial conditions you should provide\n</code></pre> <pre><code>2\n</code></pre> <pre><code>yhat = model.predict(\n    X=x_valid, y=y_valid[: model.max_lag]\n)  # passing only the 2 initial values which will be used as initial conditions\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00017649882109753117\n</code></pre> <p>Como voc\u00ea pode ver, o rrse obtido \u00e9 o mesmo que o obtido quando inserimos os dados de teste completos. Isso ocorre porque, mesmo nos casos em que voc\u00ea fornece os dados de teste completos, o m\u00e9todo de predi\u00e7\u00e3o usa apenas os primeiros valores como condi\u00e7\u00f5es iniciais e descarta os outros valores internamente.</p> <p>Na predi\u00e7\u00e3o 1-step ahead ou n-steps ahead, voc\u00ea deve fornecer os dados de teste completos porque tratamos todos os c\u00e1lculos internamente para que os usu\u00e1rios n\u00e3o precisem se preocupar em implementar os loops por conta pr\u00f3pria.</p> <p>Se voc\u00ea quiser verificar como seu modelo se comporta em um cen\u00e1rio de 3-steps ahead usando 200 amostras nos dados de teste, isso significa que a cada 3 itera\u00e7\u00f5es o feedback do modelo usar\u00e1 os dados reais como condi\u00e7\u00f5es iniciais. \u00c9 por isso que os dados de teste completos s\u00e3o necess\u00e1rios, porque caso contr\u00e1rio o modelo n\u00e3o encontrar\u00e1 o valor real para usar como feedback na n-\u00e9sima itera\u00e7\u00e3o.</p> <p>Este \u00e9 o caso onde voc\u00ea tem acesso aos dados hist\u00f3ricos para verificar como seu modelo se comporta na predi\u00e7\u00e3o n-steps ahead. Se voc\u00ea optar por usar uma predi\u00e7\u00e3o de modelo 3-steps ahead na vida real, voc\u00ea precisa prever as pr\u00f3ximas 3 amostras, aguardar as 3 itera\u00e7\u00f5es, coletar os dados reais e usar os novos dados como condi\u00e7\u00e3o inicial para prever os pr\u00f3ximos 3 valores e assim por diante.</p> <p>No cen\u00e1rio de predi\u00e7\u00e3o infinity-steps ahead, se seu modelo tiver uma entrada, ele ser\u00e1 capaz de fazer predi\u00e7\u00f5es para todas as entradas fornecendo apenas as condi\u00e7\u00f5es iniciais. Se seu modelo n\u00e3o tiver entrada (um modelo NAR, por exemplo), voc\u00ea pode definir o horizonte de previs\u00e3o e o modelo far\u00e1 predi\u00e7\u00f5es usando apenas as condi\u00e7\u00f5es iniciais. Todo o feedback ser\u00e1 dos valores preditos (esta \u00e9, ali\u00e1s, uma das raz\u00f5es pelas quais modelos n-steps ahead geralmente s\u00e3o melhores que modelos infinity-ahead).</p> <p>Vale mencionar que mudar a condi\u00e7\u00e3o inicial n\u00e3o significa que voc\u00ea est\u00e1 mudando seu modelo. A \u00fanica coisa que muda \u00e9 a condi\u00e7\u00e3o inicial e isso pode fazer uma diferen\u00e7a real em muitos casos.</p>"},{"location":"pt/user-guide/tutorials/your-first-model/#informacoes-extras","title":"Informa\u00e7\u00f5es extras","text":"<p>Voc\u00ea pode acessar algumas informa\u00e7\u00f5es extras como a lista de todos os regressores candidatos</p> <pre><code># for now the list is returned as a codification. Here, $0$ is the constant term, $[1001]=y{k-1}, [100n]=y_{k-n}, [200n] = x1_{k-n}, [300n]=x2_{k-n}$ and so on\nmodel.regressor_code  # list of all possible regressors given non_degree, n_y and n_x values\n</code></pre> <pre><code>array([[   0,    0],\n       [1001,    0],\n       [1002,    0],\n       [2001,    0],\n       [2002,    0],\n       [1001, 1001],\n       [1002, 1001],\n       [2001, 1001],\n       [2002, 1001],\n       [1002, 1002],\n       [2001, 1002],\n       [2002, 1002],\n       [2001, 2001],\n       [2002, 2001],\n       [2002, 2002]])\n</code></pre> <pre><code>print(model.err, \"\\n\\n\")  # err values for the selected terms\nprint(model.theta)  # estimated parameters for the final model structure\n</code></pre> <pre><code>[9.57604864e-01 3.88976063e-02 3.49749526e-03 1.43420284e-10\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n\n\n[[0.90000582]\n [0.20000142]\n [0.0999919 ]]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"pt/user-guide/tutorials/chaotic-systems/logistic-map/","title":"Mapa Log\u00edstico","text":"<p>Tutorial criado por Wilson Rocha</p>"},{"location":"pt/user-guide/tutorials/chaotic-systems/logistic-map/#o-mapa-logistico","title":"O Mapa Log\u00edstico","text":"<p>Sistemas ca\u00f3ticos s\u00e3o processos determin\u00edsticos que exibem comportamento imprevis\u00edvel e aparentemente aleat\u00f3rio devido \u00e0 sua extrema sensibilidade \u00e0s condi\u00e7\u00f5es iniciais. Embora governados por regras simples, esses sistemas s\u00e3o notoriamente dif\u00edceis de modelar em escalas de tempo longas, tornando-os um desafio fascinante para identifica\u00e7\u00e3o de sistemas. O Mapa Log\u00edstico \u00e9 um exemplo cl\u00e1ssico de caos, primeiro popularizado na ecologia para modelar o crescimento populacional. Sua equa\u00e7\u00e3o simples,</p> \\[ x_{n+1} = r x_n (1 - x_n) \\] <p>captura uma rica variedade de comportamentos, desde equil\u00edbrios est\u00e1veis at\u00e9 oscila\u00e7\u00f5es peri\u00f3dicas e caos completo, dependendo do par\u00e2metro de crescimento \\(r\\). Estudando este sistema, obtemos insights sobre fen\u00f4menos universais como a rota de duplica\u00e7\u00e3o de per\u00edodo para o caos e o surgimento de estruturas fractais em diagramas de bifurca\u00e7\u00e3o. Neste tutorial, usaremos o SysIdentPy para modelar o Mapa Log\u00edstico e reconstruir seu diagrama de bifurca\u00e7\u00e3o, demonstrando como abordagens orientadas a dados podem capturar din\u00e2micas ca\u00f3ticas.</p>"},{"location":"pt/user-guide/tutorials/chaotic-systems/logistic-map/#visualizando-o-mapa-logistico","title":"Visualizando o Mapa Log\u00edstico","text":"<p>Para entender o Mapa Log\u00edstico, vamos primeiro visualizar seu comportamento em diferentes regimes de \\(r\\):</p> <ul> <li>Pontos Fixos Est\u00e1veis \\(r &lt; 3\\): Para \\(r\\) baixo, a popula\u00e7\u00e3o converge para um valor estacion\u00e1rio.</li> <li>Regimes Peri\u00f3dicos \\(3 &lt; r &lt; 3.57\\): \u00c0 medida que \\(r\\) aumenta, o sistema passa por bifurca\u00e7\u00f5es de duplica\u00e7\u00e3o de per\u00edodo, oscilando entre 2, 4, 8, \u2026 estados.</li> <li>Caos \\(r \\geq 3.57\\): Al\u00e9m do ponto de acumula\u00e7\u00e3o, o sistema se comporta caoticamente, com trajet\u00f3rias que nunca se repetem.</li> </ul> <p>Um gr\u00e1fico de teia de aranha ajuda a visualizar o processo de itera\u00e7\u00e3o: come\u00e7ando de um \\(x_0\\) inicial, alternamos entre avaliar o Mapa Log\u00edstico (saltos verticais) e atualizar \\(x_n\\) (movimentos horizontais para a diagonal). Regimes ca\u00f3ticos mostram padr\u00f5es err\u00e1ticos e n\u00e3o repetitivos de teia de aranha, enquanto regimes peri\u00f3dicos tra\u00e7am ciclos est\u00e1veis.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.display_results import results\n\n\ndef logistic_map(x, r):\n    \"\"\"Calcular uma itera\u00e7\u00e3o do mapa log\u00edstico.\"\"\"\n    return r * x * (1 - x)\n\n\n# Par\u00e2metros\nr = 3.7  # Taxa de crescimento (tente 2.8, 3.2, 3.5, 3.9)\nn_iter = 50  # N\u00famero de itera\u00e7\u00f5es\nx0 = 0.2  # Condi\u00e7\u00e3o inicial\n\n# Criar gr\u00e1fico de teia de aranha\nx = np.linspace(0, 1, 1000)\nf = logistic_map(x, r)\n\nplt.figure(figsize=(8, 6))\nplt.plot(x, f, \"b-\", label=f\"Mapa Log\u00edstico ($r={r}$)\")\nplt.plot(x, x, \"k--\", label=\"$y = x$\")\n\n# Simular itera\u00e7\u00f5es\nxt = np.zeros(n_iter)\nxt[0] = x0\nfor i in range(n_iter - 1):\n    y = logistic_map(xt[i], r)\n    plt.plot([xt[i], xt[i]], [xt[i], y], \"r\", lw=0.5)  # Linha vertical\n    plt.plot([xt[i], y], [y, y], \"r\", lw=0.5)  # Linha horizontal\n    xt[i + 1] = y\n\nplt.xlabel(\"$x_n$\", fontsize=12)\nplt.ylabel(\"$x_{n+1}$\", fontsize=12)\nplt.title(\"Gr\u00e1fico de Teia de Aranha para o Mapa Log\u00edstico\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.show()\n</code></pre> <p></p> <p>O gr\u00e1fico acima mostra a Curva Log\u00edstica (Azul). A linha diagonal (Preta Tracejada) representa \\(x_{n+1} = x_n\\) e mostra as interse\u00e7\u00f5es com a curva log\u00edstica que indicam pontos fixos. Finalmente, as linhas verticais da trajet\u00f3ria da Teia de Aranha (em vermelho) aplicam o mapa \\(x_n \\to x_{n+1}\\) e as linhas horizontais redefinem \\(x_n = x_{n+1}\\) para a pr\u00f3xima itera\u00e7\u00e3o.</p> <p>A interpreta\u00e7\u00e3o por regime pode ser feita da seguinte forma:</p> <ul> <li> <p>Ponto Fixo Est\u00e1vel (ex., \\(r = 2.8\\)):   A teia de aranha espirala para dentro at\u00e9 um \u00fanico ponto.</p> </li> <li> <p>Peri\u00f3dico (ex., \\(r = 3.5\\)):   A trajet\u00f3ria cicla entre 4 pontos (per\u00edodo-4).</p> </li> <li> <p>Ca\u00f3tico (ex., \\(r = 3.9\\)):   As linhas vermelhas nunca se repetem, preenchendo o espa\u00e7o erraticamente.</p> </li> </ul>"},{"location":"pt/user-guide/tutorials/chaotic-systems/logistic-map/#3-gerando-o-diagrama-de-bifurcacao","title":"3. Gerando o Diagrama de Bifurca\u00e7\u00e3o","text":"<p>O diagrama de bifurca\u00e7\u00e3o resume como o comportamento de longo prazo do Mapa Log\u00edstico muda com \\(r\\). Vamos cri\u00e1-lo considerando: 1. Variando \\(r\\) em um intervalo (\\(3.5 \\leq r \\leq 4.0\\)). 2. Iterar o mapa: Para cada \\(r\\), descartar itera\u00e7\u00f5es transientes (primeiros 500 passos) para focar no comportamento assint\u00f3tico. 3. Plotar \\(x\\) para as itera\u00e7\u00f5es restantes.</p> <p>O diagrama de bifurca\u00e7\u00e3o revela v\u00e1rias caracter\u00edsticas-chave. Cascatas de duplica\u00e7\u00e3o de per\u00edodo ocorrem quando bifurca\u00e7\u00f5es sucessivas dividem pontos est\u00e1veis em pares, levando a per\u00edodos progressivamente mais longos. Regi\u00f5es ca\u00f3ticas emergem como bandas densas de pontos, indicando alta sensibilidade \u00e0s condi\u00e7\u00f5es iniciais. Dentro dessas regi\u00f5es ca\u00f3ticas, janelas de ordem aparecem, como perto de \\(r \\approx 3.83\\), onde o comportamento peri\u00f3dico reemerge temporariamente, destacando a estrutura sutil subjacente ao caos.</p> <p>Este diagrama serve como uma \"impress\u00e3o digital\" do sistema, que reconstruiremos posteriormente usando modelos SysIdentPy.</p> <pre><code>num = 1000\nN = 1000\nN_drop = 500\nr0 = 3.5\n\nrs = r0 + np.arange(num) / num * (4 - r0)\nxss = []\n\n# Gerar dados de bifurca\u00e7\u00e3o\nfor r in rs:\n    x = 0.5  # Condi\u00e7\u00e3o inicial\n    xs = []\n\n    # Itera\u00e7\u00f5es de aquecimento (descartar transiente)\n    for _ in range(N_drop):\n        x = logistic_map(x, r)\n\n    # Armazenar itera\u00e7\u00f5es est\u00e1veis\n    for _ in range(N):\n        x = logistic_map(x, r)\n        xs.append(x)\n\n    xss.append(xs)\n\nplt.figure(figsize=(4, 4), dpi=100)\nfor r, xs in zip(rs, xss):\n    plt.plot([r] * len(xs), xs, \",\", alpha=0.1, c=\"black\", rasterized=True)\n\nplt.xlabel(\"$r$\")\nplt.ylabel(\"$x_n$\")\nplt.title(\"Diagrama de Bifurca\u00e7\u00e3o do Mapa Log\u00edstico\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Vamos come\u00e7ar construindo um modelo para o mapa log\u00edstico com \\(r = 3.5\\) para avaliar como o SysIdentPy se sai neste cen\u00e1rio peri\u00f3dico.</p> <pre><code>y = np.array(xss[0]).reshape(-1, 1)\ny_train = y[:800, :].copy()\ny_test = y[200:, :].copy()\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    ylag=1,\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\nmodel.fit(y=y_train)\nyhat = model.predict(y=y_test[:2].reshape(-1, 1), forecast_horizon=len(y_test))\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressores\", \"Par\u00e2metros\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :])\n</code></pre> <pre><code>  Regressores   Par\u00e2metros             ERR\n0          1  -2.4433E-14  9.05069788E-01\n1   y(k-1)^3   3.6415E-14  9.17450154E-02\n2     y(k-1)   3.5000E+00  3.13396079E-03\n3   y(k-1)^2  -3.5000E+00  5.12359326E-05\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/chaotic-systems/logistic-map/#5-reconstruindo-o-diagrama-de-bifurcacao-com-modelos","title":"5. Reconstruindo o Diagrama de Bifurca\u00e7\u00e3o com Modelos","text":"<p>Um modelo orientado a dados como NARMAX pode replicar o comportamento de bifurca\u00e7\u00e3o do Mapa Log\u00edstico? Vamos descobrir:</p> <p>Os seguintes passos ser\u00e3o executados:</p> <ol> <li>Loop sobre \\(r\\): Para cada valor no intervalo (ex., 2.5 a 4.0 em passos de 0.01).</li> <li>Treinar um modelo: Gerar dados sint\u00e9ticos para o \\(r\\) atual, dividir em conjuntos de treino/teste, e ajustar um modelo NARMAX usando o algoritmo <code>FROLS</code> do SysIdentPy.</li> <li>Previs\u00e3o: Usar o modelo treinado para prever estados futuros, come\u00e7ando de uma condi\u00e7\u00e3o inicial.</li> <li>Plotar: Coletar os estados assint\u00f3ticos e sobrepor em um diagrama de bifurca\u00e7\u00e3o.</li> </ol> <pre><code>def fit_model(y_train, y_test, degree=3, ylag=1, n_info_values=50):\n    \"\"\"Ajustar um modelo NARX e retornar previs\u00f5es.\"\"\"\n    basis_function = Polynomial(degree=degree)\n    model = FROLS(\n        ylag=ylag,\n        n_info_values=n_info_values,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n    )\n    model.fit(y=y_train)\n    return model.predict(\n        y=y_test[: model.max_lag].reshape(-1, 1), forecast_horizon=len(y_test)\n    )\n\n\ndef generate_bifurcation_data(rs, N, N_drop):\n    \"\"\"Gerar dados do mapa log\u00edstico e previs\u00f5es do modelo para cada r.\"\"\"\n    xss, yhat_bifurcation = [], []\n    for r in rs:\n        x = 0.5\n        # Itera\u00e7\u00f5es de aquecimento\n        for _ in range(N_drop):\n            x = logistic_map(x, r)\n\n        # Itera\u00e7\u00f5es est\u00e1veis (loop corrigido)\n        xs = []\n        for _ in range(N):\n            x = logistic_map(x, r)\n            xs.append(x)\n\n        xss.append(xs)\n\n        # Preparar dados para modelagem\n        y = np.array(xs).reshape(-1, 1)\n        y_train, y_test = y[:800, :], y[200:, :]\n        yhat = fit_model(y_train, y_test)\n        yhat_bifurcation.append(np.array(yhat))\n\n    return xss, yhat_bifurcation\n\n\n# Par\u00e2metros\nnum = 1000\nN, N_drop = 1000, 500\nr0 = 3.5\nrs = r0 + np.arange(num) / num * (4 - r0)\n\nxss, yhat_bifurcation = generate_bifurcation_data(rs, N, N_drop)\n\n# Plotar diagrama de bifurca\u00e7\u00e3o previsto\nplt.figure(figsize=(4, 4), dpi=100)\nfor ind, r in enumerate(rs):\n    plt.plot(\n        [r] * len(yhat_bifurcation[ind]),\n        yhat_bifurcation[ind],\n        \",\",\n        alpha=0.1,\n        c=\"black\",\n        rasterized=True,\n    )\n\nplt.title(\"Diagrama de Bifurca\u00e7\u00e3o com Previs\u00f5es FROLS\", fontsize=14)\nplt.xlabel(\"$r$\", fontsize=12)\nplt.ylabel(\"$x_n$\", fontsize=12)\nplt.grid(alpha=0.2)\nplt.show()\n</code></pre> <p></p> <p>Conforme mostrado nos gr\u00e1ficos acima, os modelos reproduzem com precis\u00e3o ciclos est\u00e1veis em regimes peri\u00f3dicos, com erros de previs\u00e3o m\u00ednimos. Em regimes ca\u00f3ticos, os modelos fornecem previs\u00f5es precisas a curto prazo, mas ocorre diverg\u00eancia a longo prazo devido \u00e0 sensibilidade \u00e0s condi\u00e7\u00f5es iniciais, destacando as limita\u00e7\u00f5es inerentes \u00e0 modelagem de sistemas ca\u00f3ticos. O diagrama de bifurca\u00e7\u00e3o gerado mant\u00e9m as caracter\u00edsticas-chave, como duplica\u00e7\u00e3o de per\u00edodo e transi\u00e7\u00f5es ca\u00f3ticas, embora detalhes mais finos possam requerer um modelo com maior complexidade. Este experimento demonstra a capacidade do SysIdentPy de capturar din\u00e2micas n\u00e3o lineares essenciais, mesmo em cen\u00e1rios ca\u00f3ticos.</p>"},{"location":"pt/user-guide/tutorials/chaotic-systems/lorenz-system/","title":"Sistema de Lorenz","text":"<p>Exemplo criado por Wilson Rocha</p> <p>Nota: Este exemplo \u00e9 baseado na refer\u00eancia de simula\u00e7\u00e3o do sistema de Lorenz de UF|Physics: Introduction to Biological Physics. Parte do c\u00f3digo usado para as simula\u00e7\u00f5es de Lorenz e o diagrama de bifurca\u00e7\u00e3o \u00e9 adaptado deste recurso.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.integrate import odeint\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\n</code></pre>"},{"location":"pt/user-guide/tutorials/chaotic-systems/lorenz-system/#o-sistema-de-lorenz","title":"O Sistema de Lorenz","text":"<p>O sistema de Lorenz \u00e9 um exemplo bem conhecido na teoria do caos, consistindo em tr\u00eas equa\u00e7\u00f5es diferenciais ordin\u00e1rias que descrevem o comportamento de correntes de convec\u00e7\u00e3o na atmosfera. Introduzido pela primeira vez por Edward Lorenz em 1963, este sistema \u00e9 famoso por exibir din\u00e2micas ca\u00f3ticas, onde pequenas mudan\u00e7as nas condi\u00e7\u00f5es iniciais levam a resultados drasticamente diferentes. Apesar de ser determin\u00edstico, o comportamento do sistema \u00e9 altamente imprevis\u00edvel, tornando-o um exemplo-chave de caos em matem\u00e1tica e ci\u00eancia.</p> <p>As equa\u00e7\u00f5es que definem o sistema de Lorenz s\u00e3o:</p> \\[ \\begin{aligned} \\frac{dx}{dt} &amp;= \\sigma (y - x), \\\\ \\frac{dy}{dt} &amp;= x (\\rho - z) - y, \\\\ \\frac{dz}{dt} &amp;= xy - \\beta z, \\end{aligned} \\] <p>Onde: - \\(x\\), \\(y\\) e \\(z\\) representam vari\u00e1veis como a taxa de convec\u00e7\u00e3o, a diferen\u00e7a de temperatura entre o ar ascendente e descendente, e o desvio da temperatura em rela\u00e7\u00e3o \u00e0 m\u00e9dia, - \\(\\sigma\\) \u00e9 o n\u00famero de Prandtl (viscosidade do fluido/difusividade t\u00e9rmica), \\(\\rho\\) \u00e9 o n\u00famero de Rayleigh (impulsiona a convec\u00e7\u00e3o), e \\(\\beta\\) \u00e9 um par\u00e2metro do sistema relacionado \u00e0 geometria do sistema. O caos emerge em \\(\\rho = 28\\), \\(\\sigma = 10\\), \\(\\beta = 8/3\\).</p> <p>Neste tutorial, usaremos o SysIdentPy para criar um modelo do sistema de Lorenz. O SysIdentPy nos permite identificar os par\u00e2metros do sistema a partir de dados e explorar seu comportamento ca\u00f3tico de forma estruturada. Primeiro geraremos dados resolvendo as equa\u00e7\u00f5es de Lorenz numericamente e ent\u00e3o aplicaremos o SysIdentPy para modelar o sistema, capturando a din\u00e2mica do atrator e estimando os principais par\u00e2metros do sistema.</p>"},{"location":"pt/user-guide/tutorials/chaotic-systems/lorenz-system/#visualizando-o-atrator-de-lorenz","title":"Visualizando o Atrator de Lorenz","text":"<p>O atrator de Lorenz, um gr\u00e1fico tridimensional das solu\u00e7\u00f5es do sistema, ilustra visualmente o comportamento ca\u00f3tico do sistema, muitas vezes lembrando uma forma de \"borboleta\". Esta estrutura ca\u00f3tica revela a depend\u00eancia sens\u00edvel do sistema \u00e0s condi\u00e7\u00f5es iniciais, significando que mesmo pequenas diferen\u00e7as nos pontos de partida podem levar a trajet\u00f3rias vastamente diferentes. Este \u00e9 um exemplo marcante de movimento n\u00e3o-peri\u00f3dico em sistemas din\u00e2micos.</p> <p>O par\u00e2metro \\(\\rho\\) desempenha um papel cr\u00edtico na forma\u00e7\u00e3o do comportamento do sistema. Variando \\(\\rho\\), \u00e9 poss\u00edvel observar transi\u00e7\u00f5es de estados estacion\u00e1rios para caos. Para valores mais baixos de \\(\\rho\\), o sistema permanece est\u00e1vel, frequentemente exibindo comportamento peri\u00f3dico. \u00c0 medida que \\(\\rho\\) aumenta, o sistema se torna cada vez mais sens\u00edvel e come\u00e7a a exibir din\u00e2micas complexas e ca\u00f3ticas. Explorar essas transi\u00e7\u00f5es permite uma compreens\u00e3o mais profunda de como sistemas determin\u00edsticos podem evoluir para sistemas ca\u00f3ticos e destaca o conceito de bifurca\u00e7\u00e3o dentro da din\u00e2mica n\u00e3o linear.</p> <pre><code>def lorenz(xyz, t, sigma, rho, beta):\n    x, y, z = xyz  # parse variables\n    dxdt = sigma * (y - x)\n    dydt = x * (rho - z) - y\n    dzdt = x * y - beta * z\n    return [dxdt, dydt, dzdt]\n\n\nsigma = 10.0\nbeta = 8 / 3\nrho = 28.0\n\nT = 50.0  # tempo total de execu\u00e7\u00e3o\ndt = 0.01  # passo de tempo\ntime_points = np.arange(0.0, T, dt)\n\n# Valores de estado estacion\u00e1rio n\u00e3o triviais para plotagem\nx2 = y2 = np.sqrt(beta * (rho - 1))  # estado estacion\u00e1rio n\u00e3o trivial\nz2 = rho - 1\n\ninit = np.random.rand(3) * 2 - 1  # condi\u00e7\u00f5es iniciais entre -1 e 1\nsol2 = odeint(lorenz, init, time_points, args=(sigma, rho, beta))\n\nplt.figure(figsize=(10, 6))\n# Plotar estados estacion\u00e1rios n\u00e3o triviais como linhas horizontais\nplt.axhline(x2, color=\"k\", lw=1, linestyle=\"--\", label=f\"Estado estacion\u00e1rio: x={x2:.2f}\")\nplt.axhline(-x2, color=\"k\", lw=1, linestyle=\"--\", label=f\"Estado estacion\u00e1rio: x={-x2:.2f}\")\n\n# Plotar a trajet\u00f3ria \u00fanica\nplt.plot(time_points, sol2[:, 0], lw=2, label=f\"Trajet\u00f3ria iniciando em {init}\")\n\nplt.xlabel(r\"$t$\")\nplt.ylabel(r\"$x$\")\nplt.title(\"Atrator de Lorenz: Trajet\u00f3ria \u00danica\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure()\nplt.plot(sol2[:, 0], sol2[:, 2])  # plotar trajet\u00f3rias projetadas no plano (x,z)\nplt.plot(\n    [sol2[0, 0]], [sol2[0, 2]], \"b^\"\n)  # tri\u00e2ngulo azul marca o ponto inicial de cada trajet\u00f3ria\nplt.plot([x2], [z2], \"rx\")  # estado estacion\u00e1rio\nplt.plot([-x2], [z2], \"rx\")  # estado estacion\u00e1rio\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$z$\")\nplt.show()\n</code></pre> <p></p> <pre><code># Vamos agora visualizar todas as solu\u00e7\u00f5es como trajet\u00f3rias em um gr\u00e1fico 3D\n\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\nax.plot(sol2[:, 0], sol2[:, 1], sol2[:, 2])\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$y$\")\nax.set_zlabel(r\"$z$\")\nplt.show()\n</code></pre> <p></p> <p>O diagrama de bifurca\u00e7\u00e3o mostra os poss\u00edveis estados estacion\u00e1rios ou \u00f3rbitas peri\u00f3dicas do sistema para diferentes valores de um par\u00e2metro do sistema (como \\(\\rho\\)). Esta \u00e9 uma forma compacta e poderosa de visualizar como pequenas mudan\u00e7as em um par\u00e2metro podem alterar drasticamente o comportamento do sistema.</p> <pre><code>sigma = 10.0\nbeta = 8 / 3.0\n\nt1 = 20.0  # tempo para adicionar perturba\u00e7\u00e3o\nT = 100.0\ndt = 0.01\ntime_points = np.arange(0.0, 50.0, 0.01)\n\nrho_list = np.geomspace(0.1, 1000.0, 401)  # lista de valores de rho espa\u00e7ados geometricamente\nsol_list = []  # lista de solu\u00e7\u00f5es das equa\u00e7\u00f5es de Lorenz\n\nfor rho in rho_list:\n    init = np.random.rand(3) * 2 - 1  # valores iniciais aleat\u00f3rios\n    sol = odeint(lorenz, init, time_points, args=(sigma, rho, beta))\n    sol_list.append(sol)\n\n\nrP = 1  # in\u00edcio da instabilidade na origem, bifurca\u00e7\u00e3o pitchfork\nrH = sigma * (sigma + beta + 3) / (sigma - beta - 1)  # in\u00edcio do caos, bifurca\u00e7\u00e3o de Hopf\n\nplt.figure(figsize=(8, 6))\nplt.axvline(rP, lw=1, ls=\":\", label=r\"$\\rho = 1$\")\nplt.axvline(rH, lw=1, ls=\"--\", label=r\"$\\rho = %.3f$\" % rH)\nplt.axvline(28, lw=1, ls=\"-\", label=r\"$\\rho = 28$\")\nfor i in range(len(rho_list)):\n    rho = rho_list[i]\n    sol = sol_list[i]\n    y = sol[int(t1 / dt) :, 0]\n    x = [rho] * len(y)\n    plt.scatter(x, y, s=0.1, c=\"k\", marker=\".\", edgecolor=\"none\")\n\nplt.xscale(\"log\")\nplt.xlabel(r\"$\\rho$\")\nplt.ylabel(r\"$x$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>O estado estacion\u00e1rio est\u00e1vel na origem sofre uma bifurca\u00e7\u00e3o pitchfork, dividindo-se em dois estados estacion\u00e1rios distintos. \u00c0 medida que o par\u00e2metro aumenta ainda mais, esses dois estados estacion\u00e1rios experimentam uma bifurca\u00e7\u00e3o de Hopf, onde um dos pontos fixos se torna um ponto de sela, marcando o in\u00edcio do comportamento ca\u00f3tico. Para \\(\\rho &lt; 24.74\\), o sistema exibe estados estacion\u00e1rios ou \u00f3rbitas peri\u00f3dicas, enquanto para \\(\\rho \\geq 24.74\\), o caos emerge, revelando uma estrutura fractal.</p>"},{"location":"pt/user-guide/tutorials/chaotic-systems/lorenz-system/#modelagem-com-sysidentpy","title":"Modelagem com SysIdentPy","text":"<p>Vamos visualizar os dados que precisamos modelar</p> <pre><code>plt.plot(sol2)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x27c7c2b5bb0&gt;,\n &lt;matplotlib.lines.Line2D at 0x27c7c177ce0&gt;,\n &lt;matplotlib.lines.Line2D at 0x27c75eb8da0&gt;]\n</code></pre> <p></p> <pre><code>def prepare_data(data, y_col, x_cols, train_size=4000):\n    \"\"\"Dividir e remodelar os dados para cada modelo.\"\"\"\n    train_data, test_data = data[:train_size], data[train_size:]\n\n    # Extrair y (alvo) e x (preditores)\n    y_train, y_test = train_data[:, y_col].reshape(-1, 1), test_data[:, y_col].reshape(\n        -1, 1\n    )\n    x_train, x_test = train_data[:, x_cols], test_data[:, x_cols]\n\n    return x_train, y_train, x_test, y_test\n\n\ndef train_and_predict(x_train, y_train, x_test, y_test):\n    \"\"\"Treinar o modelo e fazer previs\u00f5es.\"\"\"\n    basis_function = Polynomial(degree=2)\n    model = FROLS(\n        ylag=1,\n        xlag=[[1], [1]],\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n    )\n    model.fit(X=x_train, y=y_train)\n    yhat = model.predict(\n        X=x_test,\n        y=y_test[: model.max_lag].reshape(-1, 1),\n    )\n\n    r = pd.DataFrame(\n        results(\n            model.final_model,\n            model.theta,\n            model.err,\n            model.n_terms,\n            err_precision=8,\n            dtype=\"sci\",\n        ),\n        columns=[\"Regressores\", \"Par\u00e2metros\", \"ERR\"],\n    )\n    print(r)\n    return yhat\n\n\n# Primeiro modelo: Usando sol2[:, 0] para y e sol2[:, [1, 2]] para x\nx_train, y_train, x_test, y_test = prepare_data(\n    sol2, y_col=0, x_cols=[1, 2], train_size=4000\n)\nyhat_1 = train_and_predict(x_train, y_train, x_test, y_test)\n\n# definir 1 porque \u00e9 o lag m\u00e1ximo. Pode definir como model.max_lag tamb\u00e9m\nplot_results(y=y_test[1:], yhat=yhat_1[1:], n=1000)\n\n# Segundo modelo: Usando sol2[:, 1] para y e sol2[:, [0, 2]] para x\nx_train, y_train, x_test, y_test = prepare_data(\n    sol2, y_col=1, x_cols=[0, 2], train_size=4000\n)\nyhat_2 = train_and_predict(x_train, y_train, x_test, y_test)\n\nplot_results(y=y_test[1:], yhat=yhat_2[1:], n=1000)\n\n# Terceiro modelo: Usando sol2[:, 2] para y e sol2[:, [0, 1]] para x\nx_train, y_train, x_test, y_test = prepare_data(\n    sol2, y_col=2, x_cols=[0, 1], train_size=4000\n)\nyhat_3 = train_and_predict(x_train, y_train, x_test, y_test)\n\nplot_results(y=y_test[1:], yhat=yhat_3[1:], n=1000)\n</code></pre> <pre><code>       Regressores   Par\u00e2metros             ERR\n0          y(k-1)   9.1832E-01  9.98019711E-01\n1         x1(k-1)   9.4946E-02  1.97448493E-03\n2   x2(k-1)y(k-1)  -4.7263E-04  5.79130765E-06\n3  x2(k-1)x1(k-1)  -2.1239E-05  4.72613968E-09\n4         x2(k-1)   6.9084E-06  4.82674757E-10\n5       x1(k-1)^2  -1.1524E-06  4.27141670E-11\n6        y(k-1)^2  -8.8057E-06  3.31043247E-11\n7               1  -1.8509E-04  3.67821804E-11\n8   x1(k-1)y(k-1)   7.9252E-06  5.31217490E-11\n9       x2(k-1)^2   5.5901E-07  5.27580418E-12\n</code></pre> <p></p> <pre><code>       Regressores   Par\u00e2metros             ERR\n0          y(k-1)   9.9829E-01  9.96084591E-01\n1   x2(k-1)y(k-1)  -6.5230E-04  3.09653629E-03\n2  x2(k-1)x1(k-1)  -9.6452E-03  4.62214305E-04\n3         x1(k-1)   2.7760E-01  3.50549821E-04\n4         x2(k-1)   2.8138E-04  3.86911341E-07\n5        y(k-1)^2  -3.2352E-05  3.41517241E-08\n6       x1(k-1)^2  -2.6343E-04  2.79943269E-08\n7               1  -6.2559E-03  3.02963912E-08\n8   x1(k-1)y(k-1)   2.3472E-04  4.11556688E-08\n9       x2(k-1)^2   1.5355E-05  3.32074748E-09\n</code></pre> <p></p> <pre><code>       Regressores   Par\u00e2metros             ERR\n0          y(k-1)   9.7932E-01  9.99326652E-01\n1  x2(k-1)x1(k-1)   8.7471E-03  6.70396643E-04\n2       x2(k-1)^2   9.5637E-04  2.13515631E-06\n3       x1(k-1)^2  -1.1022E-04  4.45532422E-07\n4         x1(k-1)   4.1767E-03  3.37917913E-08\n5        y(k-1)^2  -1.5200E-04  1.35172660E-08\n6               1  -3.4932E-02  2.45525697E-08\n7         x2(k-1)  -2.9565E-03  1.41491234E-09\n8   x2(k-1)y(k-1)   8.8849E-05  3.12544165E-09\n9   x1(k-1)y(k-1)  -1.0276E-04  6.02297621E-09\n</code></pre> <p></p>"},{"location":"pt/user-guide/tutorials/chaotic-systems/lorenz-system/#resultados","title":"Resultados","text":"<p>Agora podemos visualizar os resultados de cada trajet\u00f3ria prevista</p> <pre><code># Primeiro modelo: Usando previs\u00f5es de yhat_1 (x previsto vs z previsto)\nplt.figure()\nplt.plot(yhat_1, yhat_3)  # plotar previs\u00f5es projetadas no plano (x, z)\nplt.plot(\n    [sol2[0, 0]], [sol2[0, 2]], \"b^\"\n)  # tri\u00e2ngulo azul marca o ponto inicial de cada trajet\u00f3ria\nplt.plot([x2], [z2], \"rx\")  # estado estacion\u00e1rio\nplt.plot([-x2], [z2], \"rx\")  # estado estacion\u00e1rio\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$z$\")\nplt.title(\"Gr\u00e1fico Borboleta - Previs\u00f5es do Modelo 1\")\nplt.show()\n\n# Segundo modelo: Usando previs\u00f5es de yhat_2 (y previsto vs z previsto)\nplt.figure()\nplt.plot(yhat_2, yhat_3)  # plotar previs\u00f5es projetadas no plano (y, z)\nplt.plot(\n    [sol2[0, 1]], [sol2[0, 2]], \"b^\"\n)  # tri\u00e2ngulo azul marca o ponto inicial de cada trajet\u00f3ria\nplt.plot([x2], [z2], \"rx\")  # estado estacion\u00e1rio\nplt.plot([-x2], [z2], \"rx\")  # estado estacion\u00e1rio\nplt.xlabel(r\"$y$\")\nplt.ylabel(r\"$z$\")\nplt.title(\"Gr\u00e1fico Borboleta - Previs\u00f5es do Modelo 2\")\nplt.show()\n\n# Terceiro modelo: Usando previs\u00f5es de yhat_3 (z previsto vs x previsto)\nplt.figure()\nplt.plot(yhat_3, yhat_1)  # plotar previs\u00f5es projetadas no plano (z, x)\nplt.plot(\n    [sol2[0, 2]], [sol2[0, 0]], \"b^\"\n)  # tri\u00e2ngulo azul marca o ponto inicial de cada trajet\u00f3ria\nplt.plot([z2], [x2], \"rx\")  # estado estacion\u00e1rio\nplt.plot([z2], [-x2], \"rx\")  # estado estacion\u00e1rio\nplt.xlabel(r\"$z$\")\nplt.ylabel(r\"$x$\")\nplt.title(\"Gr\u00e1fico Borboleta - Previs\u00f5es do Modelo 3\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <pre><code>fig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\nax.plot(yhat_1, yhat_2, yhat_3)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$y$\")\nax.set_zlabel(r\"$z$\")\nax.set_title(\"Gr\u00e1fico 3D Borboleta com Dados Previstos\")\nplt.show()\n</code></pre> <p></p> <p>Conforme mostrado nos gr\u00e1ficos acima, os modelos capturam efetivamente a din\u00e2mica do sistema de Lorenz, reproduzindo os ciclos est\u00e1veis em regimes peri\u00f3dicos com erros de previs\u00e3o m\u00ednimos. Em regimes ca\u00f3ticos, os modelos mant\u00eam previs\u00f5es precisas a curto prazo, mas podem exibir diverg\u00eancia a longo prazo, o que \u00e9 esperado devido \u00e0 sensibilidade \u00e0s condi\u00e7\u00f5es iniciais intr\u00ednseca aos sistemas ca\u00f3ticos. Embora detalhes mais finos possam requerer configura\u00e7\u00f5es de modelo mais sofisticadas, o SysIdentPy demonstra sua capacidade de modelar a din\u00e2mica n\u00e3o linear do sistema de Lorenz, mesmo no regime ca\u00f3tico. Este experimento destaca a versatilidade do SysIdentPy em capturar comportamentos est\u00e1veis e ca\u00f3ticos em sistemas complexos.</p>"},{"location":"es/developer-guide/how-to-add-a-translation/","title":"C\u00f3mo A\u00f1adir una Traducci\u00f3n","text":"<p>Gu\u00eda para crear o mejorar traducciones de la documentaci\u00f3n en cualquier idioma.</p> <p>Usamos MkDocs + Material + <code>mkdocs-static-i18n</code>. El ingl\u00e9s es el fallback. Cualquier idioma nuevo replica la estructura de carpetas. Si falta una p\u00e1gina traducida, se muestra la versi\u00f3n en ingl\u00e9s.</p>"},{"location":"es/developer-guide/how-to-add-a-translation/#1-resumen","title":"1. Resumen","text":"<p>Tres escenarios comunes:</p> <ol> <li>Mejorar una p\u00e1gina ya traducida.</li> <li>Traducir una p\u00e1gina que s\u00f3lo existe en ingl\u00e9s.</li> <li>A\u00f1adir un idioma completamente nuevo.</li> </ol> <p>Todo lo siguiente cubre esos casos.</p>"},{"location":"es/developer-guide/how-to-add-a-translation/#2-estructura-de-carpetas","title":"2. Estructura de carpetas","text":"<pre><code>/docs\n  en/\n    developer-guide/\n    getting-started/\n    user-guide/\n  &lt;locale&gt;/\n    (mismos caminos relativos)\n</code></pre> <p>Ejemplos de <code>&lt;locale&gt;</code>: <code>pt</code>, <code>es</code>, <code>fr</code>, <code>de</code>, <code>it</code>, <code>ja</code>, <code>zh</code>, <code>ru</code>. Use c\u00f3digos cortos (BCP47). Evite variantes regionales salvo necesidad (<code>pt-BR</code>, <code>pt-PT</code>).</p>"},{"location":"es/developer-guide/how-to-add-a-translation/#3-como-traducir-una-pagina-existente","title":"3. C\u00f3mo traducir una p\u00e1gina existente","text":"<ol> <li>Busca la p\u00e1gina en <code>docs/en/</code>.</li> <li>Copia la p\u00e1gina id\u00e9ntica a <code>docs/&lt;locale&gt;/</code>.</li> <li>Traduce el contenido entre <code>---</code> front-matter y footer sin tocar c\u00f3digo o rutas.</li> <li>Mant\u00e9n todos los <code>code fences</code>, URLs y nombres de paquetes sin traducir.</li> </ol>"},{"location":"es/developer-guide/how-to-add-a-translation/#4-recomendaciones-y-reglas","title":"4. Recomendaciones y reglas","text":"<ul> <li>Mant\u00e9n la propiedad t\u00e9cnica: no traduzcas c\u00f3digo, identificadores, par\u00e1metros o nombres de paquetes.</li> <li>Preserva front matter (delimitadores <code>---</code>).</li> <li>Traduce los t\u00edtulos de admoniciones (Nota, Atenci\u00f3n) y contenido textual.</li> <li>Usa el glosario del proyecto cuando exista.</li> <li>Si una cadena es ambigua, deja una nota tipo <code>&lt;!-- TODO: revisar traducci\u00f3n --&gt;</code>.</li> </ul>"},{"location":"es/developer-guide/how-to-add-a-translation/#5-pruebas-locales","title":"5. Pruebas locales","text":"<p>Para comprobar tu traducci\u00f3n localmente:</p> <pre><code>conda activate syspyenv\nmkdocs serve\n</code></pre> <p>Abre <code>http://127.0.0.1:8000</code> y selecciona el idioma en la esquina superior.</p>"},{"location":"es/developer-guide/how-to-add-a-translation/#6-pull-request","title":"6. Pull request","text":"<ol> <li>A\u00f1ade tu rama con s\u00f3lo los archivos traducidos.</li> <li>Incluye una nota en la PR describiendo las decisiones de traducci\u00f3n.</li> <li>Una revisi\u00f3n t\u00e9cnica y ling\u00fc\u00edstica es deseable antes de merge.</li> </ol> <p>Gracias por ayudar a traducir la documentaci\u00f3n.</p>"},{"location":"es/getting-started/getting-started/","title":"Primeros Pasos","text":"<p>\u00c2\u00a1Bienvenido a la documentaci\u00c3\u00b3n de SysIdentPy! Aprende c\u00c3\u00b3mo empezar a usar SysIdentPy en tu proyecto. Luego explora los conceptos principales y descubre recursos adicionales para modelar sistemas din\u00c3\u00a1micos y series temporales.</p>          \u00f0\u0178\u201c\u0161 \u00c2\u00bfBuscas m\u00c3\u00a1s detalles sobre modelos NARMAX? \u00e2\u2013\u00bc <p>             Para informaci\u00c3\u00b3n completa sobre modelos, m\u00c3\u00a9todos y un conjunto de ejemplos y benchmarks implementados en SysIdentPy, consulta nuestro libro:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>             Este libro ofrece una gu\u00c3\u00ada detallada para ayudarte en tu trabajo con SysIdentPy.         </p> <p>             \u00f0\u0178\u203a\u00a0\u00ef\u00b8\u008f Tambi\u00c3\u00a9n puedes explorar los tutoriales en la documentaci\u00c3\u00b3n para ejemplos pr\u00c3\u00a1cticos.         </p>"},{"location":"es/getting-started/getting-started/#aqua-es-sysidentpy","title":"\u00c2\u00bfQu\u00c3\u00a9 es SysIdentPy?","text":"<p>SysIdentPy es una biblioteca Python de c\u00c3\u00b3digo abierto para identificaci\u00c3\u00b3n de sistemas usando modelos NARMAX, construida sobre NumPy y distribuida bajo la licencia BSD de 3 cl\u00c3\u00a1usulas. SysIdentPy proporciona una estructura flexible y f\u00c3\u00a1cil de usar para construir modelos din\u00c3\u00a1micos no lineales para series temporales y sistemas din\u00c3\u00a1micos.</p> <p>Con SysIdentPy, puedes:</p> <ul> <li>Construir y personalizar modelos no lineales para predicci\u00c3\u00b3n de series temporales y sistemas din\u00c3\u00a1micos.</li> <li>Utilizar t\u00c3\u00a9cnicas innovadoras para selecci\u00c3\u00b3n de estructura y estimaci\u00c3\u00b3n de par\u00c3\u00a1metros del modelo.</li> <li>Experimentar modelos NARX neuronales y otros algoritmos avanzados.</li> </ul>"},{"location":"es/getting-started/getting-started/#instalacia3n","title":"Instalaci\u00c3\u00b3n","text":"<p>SysIdentPy se publica como un paquete Python y puede instalarse con <code>pip</code>, preferiblemente en un entorno virtual. Si no tienes experiencia, desplaza la p\u00c3\u00a1gina y ampl\u00c3\u00ada la caja de ayuda. Instala con:</p> \u00c3\u0161ltima Versi\u00c3\u00b3n <pre><code>pip install sysidentpy</code></pre> Soporte NARX Neural <pre><code>pip install sysidentpy[\"all\"]</code></pre> Versi\u00c3\u00b3n Espec\u00c3\u00adfica <pre><code>pip install sysidentpy==\"0.5.3\"</code></pre> Desde Git <pre><code>pip install git+https://github.com/wilsonrljr/sysidentpy.git</code></pre>          \u00e2\u009d\u201c \u00c2\u00bfC\u00c3\u00b3mo gestionar las dependencias de mi proyecto? \u00e2\u2013\u00bc <p>             Si no tienes experiencia previa con Python, recomendamos la lectura de                              Using Python's pip to Manage Your Projects' Dependencies             , que es una excelente introducci\u00c3\u00b3n a la gesti\u00c3\u00b3n de paquetes en Python y ayuda en la resoluci\u00c3\u00b3n de errores.         </p>"},{"location":"es/getting-started/getting-started/#acuales-son-las-principales-funcionalidades-de-sysidentpy","title":"\u00c2\u00bfCu\u00c3\u00a1les son las principales funcionalidades de SysIdentPy?","text":"\u00f0\u0178\u00a7\u00a9 Filosof\u00c3\u00ada NARMAX <p>Construye variaciones como NARX, NAR, ARMA, NFIR y otras.</p> \u00f0\u0178\u201c\u009d Selecci\u00c3\u00b3n de la Estructura <p>Usa m\u00c3\u00a9todos como FROLS, MetaMSS y combinaciones con t\u00c3\u00a9cnicas de estimaci\u00c3\u00b3n de par\u00c3\u00a1metros.</p> \u00f0\u0178\u201d\u2014 Funciones Base <p>Elige entre 8+ funciones base, combinando tipos lineales y no lineales para modelos NARMAX personalizados.</p> \u00f0\u0178\u017d\u00af Estimaci\u00c3\u00b3n de Par\u00c3\u00a1metros <p>M\u00c3\u00a1s de 15 m\u00c3\u00a9todos para explorar diferentes escenarios junto con t\u00c3\u00a9cnicas de selecci\u00c3\u00b3n de estructura.</p> \u00e2\u0161\u2013\u00ef\u00b8\u008f T\u00c3\u00a9cnicas Multiobjetivo <p>Minimiza diferentes funciones objetivo usando informaci\u00c3\u00b3n af\u00c3\u00adn para estimaci\u00c3\u00b3n de par\u00c3\u00a1metros.</p> \u00f0\u0178\u201d\u201e Simulaci\u00c3\u00b3n de Modelos <p>Reproduce resultados de art\u00c3\u00adculos con SimulateNARMAX. Prueba y compara modelos publicados en art\u00c3\u00adculos.</p> \u00f0\u0178\u00a4\u2013 NARX Neural (PyTorch) <p>Integra con PyTorch para arquitecturas NARX neuronales usando cualquier optimizador y funci\u00c3\u00b3n de coste.</p> \u00f0\u0178\u203a\u00a0\u00ef\u00b8\u008f Estimadores Generales <p>Compatible con scikit-learn, CatBoost y m\u00c3\u00a1s para crear modelos NARMAX.</p>"},{"location":"es/getting-started/getting-started/#recursos-adicionales","title":"Recursos adicionales","text":"<ul> <li> \u00f0\u0178\u00a4\u009d Contribuye con SysIdentPy </li> <li> \u00f0\u0178\u201c\u0153 Informaci\u00c3\u00b3n de Licencia </li> <li> \u00f0\u0178\u2020\u02dc Ayuda &amp; Soporte </li> <li> \u00f0\u0178\u201c\u2026 Charlas </li> <li> \u00f0\u0178\u2019\u2013 Hazte Patrocinador </li> <li> \u00f0\u0178\u00a7\u00a9 Explora el C\u00c3\u00b3digo Fuente </li> </ul>"},{"location":"es/getting-started/getting-started/#ate-gusta-sysidentpy","title":"\u00c2\u00bfTe gusta SysIdentPy?","text":"<p>\u00c2\u00bfTe gustar\u00c3\u00ada ayudar a SysIdentPy, a otros usuarios y al autor de la biblioteca? Puedes \u00c2\u00abestrellar\u00c2\u00bb el proyecto en GitHub haciendo clic en el bot\u00c3\u00b3n de estrella en la esquina superior derecha de la p\u00c3\u00a1gina: https://github.com/wilsonrljr/sysidentpy. \u00e2\u00ad\u0090\u00ef\u00b8\u008f</p> <p>Al marcar un repositorio con estrella, lo encontrar\u00c3\u00a1s m\u00c3\u00a1s f\u00c3\u00a1cilmente en el futuro, recibir\u00c3\u00a1s sugerencias de proyectos relacionados en GitHub y adem\u00c3\u00a1s valoras el trabajo del mantenedor.</p> <p>Considera tambi\u00c3\u00a9n apoyar el proyecto haci\u00c3\u00a9ndote sponsor. Tu apoyo ayuda a mantener el desarrollo activo y garantiza la evoluci\u00c3\u00b3n continua de SysIdentPy.</p> <p>[ \u00a0 S\u00c3\u00a9 un  Patrocinador en GitHub][wilsonrljr's sponsor profile]</p>"},{"location":"es/getting-started/quickstart-guide/","title":"Uso B\u00e1sico","text":""},{"location":"es/getting-started/quickstart-guide/#1-requisitos-previos","title":"1. Requisitos previos","text":"<p>Necesitas tener conocimientos b\u00e1sicos de Python.</p> <p>Para ejecutar los ejemplos, adem\u00e1s de NumPy necesitar\u00e1s <code>pandas</code> instalado.</p> <pre><code>pip install sysidentpy pandas\n# Opcional: Para redes neuronales y funcionalidades avanzadas\npip install sysidentpy[\"all\"]\n</code></pre>"},{"location":"es/getting-started/quickstart-guide/#2-principales-funcionalidades","title":"2. Principales funcionalidades","text":"<p>SysIdentPy ofrece una estructura flexible para construir, validar y visualizar modelos no lineales de serie temporal y sistemas din\u00e1micos. El proceso de modelado incluye algunos pasos: definir la representaci\u00f3n matem\u00e1tica, elegir el algoritmo de estimaci\u00f3n de par\u00e1metros, seleccionar la estructura del modelo y determinar el horizonte de predicci\u00f3n.</p> <p>Las siguientes funcionalidades est\u00e1n disponibles en SysIdentPy:</p>"},{"location":"es/getting-started/quickstart-guide/#clases-de-modelo","title":"Clases de Modelo","text":"<ul> <li>NARMAX, NARX, NARMA, NAR, NFIR, ARMAX, ARX, AR y sus variantes.</li> </ul>"},{"location":"es/getting-started/quickstart-guide/#representaciones-matematicas","title":"Representaciones Matem\u00e1ticas","text":"<ul> <li>Polynomial (Polinomial)</li> <li>Neural</li> <li>Fourier</li> <li>Laguerre</li> <li>Bernstein</li> <li>Bilinear</li> <li>Legendre</li> <li>Hermite</li> <li>HermiteNormalized</li> </ul> <p>Tambi\u00e9n puedes definir modelos NARX como Bayesian y Gradient Boosting usando la clase GeneralNARX, que ofrece integraci\u00f3n directa con varios algoritmos de aprendizaje autom\u00e1tico.</p>"},{"location":"es/getting-started/quickstart-guide/#algoritmos-de-seleccion-de-estructura","title":"Algoritmos de Selecci\u00f3n de Estructura","text":"<ul> <li>Forward Regression Orthogonal Least Squares (FROLS)</li> <li>Meta-model Structure Selection (MeMoSS / MetaMSS)</li> <li>Accelerated Orthogonal Least Squares (AOLS)</li> <li>Entropic Regression (ER)</li> <li>Ultra Orthogonal Least Squares (UOLS)</li> </ul>"},{"location":"es/getting-started/quickstart-guide/#metodos-de-estimacion-de-parametros","title":"M\u00e9todos de Estimaci\u00f3n de Par\u00e1metros","text":"<ul> <li>M\u00ednimos Cuadrados (MQ)</li> <li>Total Least Squares (TLS)</li> <li>M\u00ednimos Cuadrados Recursivos (MQR)</li> <li>Ridge Regression</li> <li>Non-Negative Least Squares (NNLS)</li> <li>Least Squares Minimal Residues (LSMR)</li> <li>Bounded Variable Least Squares (BVLS)</li> <li>Least Mean Squares (LMS) y sus variantes:</li> <li>Affine LMS</li> <li>LMS with Sign Error</li> <li>Normalized LMS</li> <li>LMS with Normalized Sign Error</li> <li>LMS with Sign Regressor</li> <li>Normalized LMS with Sign Sign</li> <li>Leaky LMS</li> <li>Fourth-Order LMS</li> <li>Mixed Norm LMS</li> </ul>"},{"location":"es/getting-started/quickstart-guide/#criterios-de-seleccion-de-orden","title":"Criterios de Selecci\u00f3n de Orden","text":"<ul> <li>Criterio de Informaci\u00f3n de Akaike (AIC)</li> <li>Criterio de Informaci\u00f3n de Akaike Corregido (AICc)</li> <li>Criterio de Informaci\u00f3n Bayesiano (BIC)</li> <li>Final Prediction Error (FPE)</li> <li>Khundrin's Law of Iterated Logarithm Criterion (LILC)</li> </ul>"},{"location":"es/getting-started/quickstart-guide/#metodos-de-prediccion","title":"M\u00e9todos de Predicci\u00f3n","text":"<ul> <li>Un paso adelante (one-step ahead)</li> <li>n pasos adelante (n-steps ahead)</li> <li>Paso infinito adelante / simulaci\u00f3n libre (infinity-steps / free run simulation)</li> </ul>"},{"location":"es/getting-started/quickstart-guide/#herramientas-de-visualizacion","title":"Herramientas de Visualizaci\u00f3n","text":"<ul> <li>Gr\u00e1ficos de predicci\u00f3n</li> <li>An\u00e1lisis de residuos</li> <li>Visualizaci\u00f3n de la estructura del modelo</li> <li>Visualizaci\u00f3n de par\u00e1metros</li> </ul> <p>Como puedes ver, SysIdentPy soporta diversas combinaciones de modelos. No te preocupes por elegir todas las configuraciones al principio. Comencemos con las configuraciones por defecto.</p> Buscas m\u00e1s detalles sobre modelos NARMAX? <p>             Para informaci\u00f3n completa sobre modelos, m\u00e9todos y un conjunto de ejemplos y benchmarks implementados en SysIdentPy, consulta nuestro libro:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>             Este libro ofrece una gu\u00eda detallada para ayudarte en tu trabajo con SysIdentPy.         </p> <p>              Tambi\u00e9n puedes explorar los tutoriales en la documentaci\u00f3n para ejemplos pr\u00e1cticos.         </p>"},{"location":"es/getting-started/quickstart-guide/#3-guia-rapida","title":"3. Gu\u00eda R\u00e1pida","text":"<p>Para mantener las cosas simples, cargaremos algunos datos simulados para los ejemplos.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\n\n# Genera un conjunto de datos de un sistema din\u00e1mico simulado.\nx_train, x_valid, y_train, y_valid = get_siso_data(\n        n=300,\n        colored_noise=False,\n        sigma=0.0001,\n        train_percentage=80\n)\n</code></pre>"},{"location":"es/getting-started/quickstart-guide/#construye-tu-primer-modelo-narx","title":"Construye tu primer modelo NARX","text":"<p>Con los datos cargados, vamos a construir un modelo NARX Polinomial. Usando la configuraci\u00f3n por defecto, necesitas definir al menos el m\u00e9todo de selecci\u00f3n de estructura y la representaci\u00f3n matem\u00e1tica (funci\u00f3n base).</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\n</code></pre> <p>El m\u00e9todo de selecci\u00f3n de estructura (MSS) habilita las operaciones de \"entrenamiento\" y predicci\u00f3n del modelo.</p> <p>Aunque distintos algoritmos tienen diferentes hiperpar\u00e1metros, ese no es el foco aqu\u00ed. Mostraremos c\u00f3mo modificarlos, pero no discutiremos las mejores configuraciones en esta gu\u00eda.</p> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Para evaluar el rendimiento, puedes usar cualquier m\u00e9trica disponible en la librer\u00eda. Ejemplo con Root Relative Squared Error (RRSE):</p> <pre><code>from sysidentpy.metrics import root_relative_squared_error\n\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00014\n</code></pre> <p>Para mostrar la ecuaci\u00f3n final del modelo polinomial, usa la funci\u00f3n <code>results</code>. Requiere la siguiente configuraci\u00f3n:</p> <ul> <li><code>final_model</code>: Regresores seleccionados tras el ajuste</li> <li><code>theta</code>: Par\u00e1metros estimados</li> <li><code>err</code>: Error Reduction Ratio (ERR)</li> </ul> <pre><code>from sysidentpy.utils.display_results import results\n\nr = pd.DataFrame(\n        results(\n                model.final_model, model.theta, model.err,\n                model.n_terms, err_precision=8, dtype='sci'\n        ),\n        columns=['Regressors', 'Parameters', 'ERR'])\nprint(r)\n</code></pre> <p>Resultado (ejemplo):</p> <pre><code>Regressors     Parameters        ERR\n0        x1(k-2)     0.9000  0.95556574\n1         y(k-1)     0.1999  0.04107943\n2  x1(k-1)y(k-1)     0.1000  0.00335113\n</code></pre>"},{"location":"es/user-guide/overview/","title":"Resumen","text":"\ud83e\udde9 Tutorials <p>Follow practical tutorials covering how to build models such as NARX, Neural NARX, NFIR and variants. Learn essential steps, from data preparation to model evaluation, using SysIdentPy\u2019s main features.</p> \ud83d\udcdd Companion Book <p>Looking for more details on NARMAX models? Our book, Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy, covers the theory behind models and methods, while showing how to implement them using SysIdentPy through a wide range of examples and benchmarks.</p> \ud83d\udd17 How To <p>Find practical \"how-to\" guides on a variety of topics, including selecting basis functions, configuring model parameters, performing predictions, and evaluating model performance. Get straightforward solutions for common modeling tasks.</p> \ud83c\udfaf API <p>Explore the complete API reference with detailed documentation of SysIdentPy\u2019s source code. Understand class structures, methods, and parameters to extend and customize functionalities for your projects.</p>"}]}
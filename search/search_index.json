{"config":{"lang":["en","pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"book/0-Preface/","title":"Preface","text":"<p>All the world is a nonlinear system</p> <p>He linearised to the right</p> <p>He linearised to the left</p> <p>Till nothing was right</p> <p>And nothing was left</p> <p>Stephen A. Billings - Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains</p>"},{"location":"book/0-Preface/#nonlinear-system-identification-and-forecasting-theory-and-practice-with-sysidentpy","title":"Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy","text":"<p>Welcome to our companion book on System Identification! This book is a comprehensive approach to learning about dynamic models and forecasting. The main aim of this book is to describe a comprehensive set of algorithms for the identification, forecasting and analysis of nonlinear systems.</p> <p>Our book is specifically designed for those who are interested in learning system identification and forecasting.  We will guide you through the process step-by-step using Python and the SysIdentPy package. With SysIdentPy, you will be able to apply a range of techniques for modeling dynamic systems, making predictions, and exploring different design schemes for dynamic models, from polynomial to neural networks. This book is for graduates, postgraduates, researchers, and for all people from different research areas who have data and want to find models to understand their systems better.</p> <p>The research literature is filled with books and papers covering various aspects of nonlinear system identification, including NARMAX methods. In this book, our objective isn't to replicate all the numerous algorithm variations available. Instead, we want to show you how to model your data using those algorithms with SysIdentPy. We'll mention all the specific details and different versions of the algorithms in the book, so if you're more interested in the theoretical aspects, you can explore those ideas further. We aim to focus on the fundamental techniques, explaining them in straightforward language and showing how to use them in real-world situations. While there will be some math and technical details involved, the aim is to keep it as easy to understand as possible. In essence, this book aims to be a resource that readers from various fields can use to learn how to model dynamic nonlinear systems.</p> <p>The best part about our book is that it is open source material, meaning that it is freely available for anyone to use and contribute to. We hope this brings together people who share interest for system identification and forecasting techniques, from linear to nonlinear models.</p> <p>So, whether you're a student, researcher, data scientist or practitioner, we invite you to share your knowledge and contribute with us. Let\u2019s explore system identification and forecasting with SysIdentPy!</p> <p>To follow along with the Python examples in the book, you\u2019ll need to have some packages installed. We\u2019ll cover the main ones here and let you know if any additional packages are required as we proceed.</p> <pre><code>import sysidentpy\nimport pandas as pd\nimport numpy as np\nimport torch\nimport matplotlib\nimport scipy\n</code></pre>"},{"location":"book/0-Preface/#about-the-author","title":"About the Author","text":"<p>Wilson Rocha is the Head of Data Science at RD Sa\u00fade and the creator of the SysIdentPy library. He holds a degree in Electrical Engineering and a Master's in Systems Modeling and Control, both from Federal University of S\u00e3o Jo\u00e3o del-Rei (UFSJ), Brazil. Wilson began his journey in Machine Learning by developing soccer-playing robots and continues to advance his research in the fields of Multi-objective Nonlinear System Identification and Time Series Forecasting.</p> <p>Connect with Wilson Rocha through the following social networks:</p> <ul> <li>LinkedIn</li> <li>ResearchGate</li> <li>Discord</li> </ul>"},{"location":"book/0-Preface/#referencing-this-book","title":"Referencing This Book","text":"<p>If you find this book useful, please cite it as follows:</p> <pre><code>Lacerda Junior, W.R. (2024). *Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy*. Web version. https://sysidentpy.org\n</code></pre> <p>If you use SysIdentPy on your project, please drop me a line.</p> <p>If you use SysIdentPy on your scientific publication, we would appreciate citations to the following paper: - Lacerda et al., (2020). SysIdentPy: A Python package for System Identification using NARMAX models. Journal of Open Source Software, 5(54), 2384, https://doi.org/10.21105/joss.02384</p> <pre><code>@article{Lacerda2020,\n  doi = {10.21105/joss.02384},\n  url = {https://doi.org/10.21105/joss.02384},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {54},\n  pages = {2384},\n  author = {Wilson Rocha Lacerda Junior and Luan Pascoal Costa da Andrade and Samuel Carlos Pessoa Oliveira and Samir Angelo Milani Martins},\n  title = {SysIdentPy: A Python package for System Identification using NARMAX models},\n  journal = {Journal of Open Source Software}\n}\n</code></pre>"},{"location":"book/0-Preface/#pdf-epub-and-mobi-version","title":"PDF, Epub and Mobi version","text":"<p>Download the pdf version of the book: pdf version</p> <p>Download the epub version of the book: epub version</p> <p>Download the mobi version of the book: mobi version</p>"},{"location":"book/0-Preface/#acknowledgments","title":"Acknowledgments","text":"<p>The System Identification class taught by Samir Martins  (in Portuguese) has been a great source of inspiration for this series. In this book, we will explore Dynamic Systems and learn how to master NARMAX models using Python and the SysIdentPy package. The Stephen A. Billings book, Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio - Temporal Domains, have been instrumental in showing us how powerful System Identification can be.</p> <p>In addition to these resources, we will also reference Luis Ant\u00f4nio Aguirre Introdu\u00e7\u00e3o \u00e0 Identifica\u00e7\u00e3o de Sistemas. T\u00e9cnicas Lineares e n\u00e3o Lineares Aplicadas a Sistemas. Teoria e Aplica\u00e7\u00e3o (in Portuguese), which has proven to be an invaluable tool in introducing complex dynamic modeling concepts in a straightforward way. As an open source material on System Identification and Forecasting, this book aims to provide a accessible yet rigorous approach to learning dynamic models and forecasting.</p>"},{"location":"book/0-Preface/#support-the-project","title":"Support the Project","text":"<p>The Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy is an extensive open-source resource dedicated to the science of System Identification. Our goal is to make this knowledge accessible to everyone, both financially and intellectually.</p> <p>If this book has been valuable to you, and you'd like to support our efforts, we welcome financial contributions through our Sponsor page.</p> <p>If you're not in a position to contribute financially, you can still support by helping us improve the book. We encourage you to report any typos, suggest edits, or provide feedback on sections that you found challenging. You can do this by visiting the book's repository and opening an issue. Additionally, if you enjoyed the content, please consider sharing it with others who might benefit from it, and give us a star on GitHub.</p> <p>Your support, in any form, helps us continue to enhance this project and maintain a high-quality resource for the community. Thank you for your contribution!</p>"},{"location":"book/0.1-Contents/","title":"Contents","text":"<p>Preface</p> <ol> <li>Introduction<ol> <li>Models</li> <li>System Identification</li> <li>Linear or Nonlinear System Identification<ol> <li>Linear Models</li> <li>Nonlinear Models</li> </ol> </li> <li>NARMAX Methods</li> <li>What is the Purpose of System Identification?</li> <li>Is System Identification Machine Learning?</li> <li>Nonlinear System Identification and Forecasting Applications: Case Studies</li> <li>Abbreviations</li> <li>Variables</li> <li>Book Organization</li> </ol> </li> <li>NARMAX Model Representation<ol> <li>Basis Function</li> <li>Linear Models<ol> <li>ARMAX</li> <li>ARX</li> <li>ARMA</li> <li>AR</li> <li>FIR</li> <li>Other Variants</li> </ol> </li> <li>Nonlinear Models<ol> <li>NARMAX</li> <li>NARMA</li> <li>NAR</li> <li>NFIR</li> <li>Mixed NARMAX Models</li> <li>Neural NARX Network</li> <li>General Model Set Representation</li> <li>MIMO Models</li> </ol> </li> </ol> </li> <li>Parameter Estimation<ol> <li>Least Squares</li> <li>Total Least Squares</li> <li>Recursive Least Squares</li> <li>Least Mean Squares</li> <li>Extended Least Squares Algorithms</li> </ol> </li> <li>Model Structure Selection<ol> <li>Introduction</li> <li>The Forward Regression Orthogonal Least Squares<ol> <li>Case Study</li> </ol> </li> <li>Information Criteria<ol> <li>Overview of the Information Criteria Methods<ol> <li>AIC</li> <li>AICc</li> <li>BIC</li> <li>LILC</li> <li>FPE</li> </ol> </li> </ol> </li> <li>Meta Model Structure Selection (MetaMSS)<ol> <li>Meta-heuristics</li> <li>Standard Particle Swarm Optimization (PSO)</li> <li>Standard Gravitational Search Algorithm (GSA)</li> <li>The Binary Hybrid Optimization Algorithm</li> <li>Meta-Model Structure Selection (MetaMSS): Building NARX for Regression</li> <li>Case Studies: Simulation Results</li> <li>MetaMSS vs FROLS</li> <li>Meta-MSS vs RJMCMC</li> <li>MetaMSS algorithm using SysIdentPy</li> </ol> </li> <li>Accelerated Orthogonal Least Squares</li> <li>Entropic Regression</li> </ol> </li> <li>Multiobjective Parameter Estimation<ol> <li>Introduction</li> <li>Multi-objective optimization problem</li> <li>Pareto Optimal Definition and Pareto Dominance</li> <li>Affine Information Least Squares Algorithm</li> <li>Case Study - Buck converter</li> </ol> </li> <li>Multiobjective Model Structure Selection<ol> <li>Introduction</li> <li>Multiobjective Error Reduction Ratio</li> <li>Multiobjective Meta Model Structure Selection</li> <li>Case Studies</li> <li>References</li> </ol> </li> <li>NARX Neural Network<ol> <li>Introduction</li> <li>NARX Neural Network</li> <li>NARX Neural Network vs. Recursive Neural Network</li> <li>Case Studies</li> <li>References</li> </ol> </li> <li>Severely Nonlinear Systems<ol> <li>Introduction</li> <li>Modeling Hysteresis With Polynomial NARX Model</li> <li>Continuous-time loading-unloading quasi-static signal</li> <li>Hysteresis loops in continuous time \\(\\mathcal{H}_t(\\omega)\\)</li> <li>Rate Independent Hysteresis  in polynomial NARX model</li> </ol> </li> <li>Validation<ol> <li>The <code>predict</code> Method in SysIdentPy</li> <li>Infinity-Step-Ahead Prediction</li> <li>One-step Ahead Prediction</li> <li>n-step Ahead Prediction</li> <li>Model Performance</li> <li>Metrics Available in SysIdentPy</li> <li>Case study</li> </ol> </li> <li>Case Studies: System Identification and Forecasting<ol> <li>M4 Dataset</li> <li>Coupled Eletric Device</li> <li>Wiener-Hammerstein</li> <li>Air Passenger Demand Forecasting</li> <li>System With Hysteresis - Modeling a Magneto-rheological Damper Device</li> <li>Silver box</li> <li>F-16 Ground Vibration Test Benchmark</li> <li>PV Forecasting</li> <li>Industrial Robot Identification Benchmark (coming soon)</li> <li>Two-Story Frame with Hysteretic Links (coming soon)</li> <li>Cortical Responses Evoked by Wrist Joint Manipulation (coming soon)</li> <li>Total quarterly beer production in Australia (coming soon)</li> <li>Australian Domestic Tourism Demand (coming soon)</li> <li>Electric Power Consumption (coming soon)</li> <li>Gas Rate CO2 (coming soon)</li> <li>Number of Patients Seen With Influenza-like Illness (coming soon)</li> <li>Monthly Sales of Heaters and Ice Cream (coming soon)</li> <li>Monthly Production of Milk (coming soon)</li> <li>Half-hourly Electricity Demand in England and Wales (coming soon)</li> <li>Daily Temperature in Melbourne (coming soon)</li> <li>Weekly U.S. Product Supplied of Finished Motor Gasoline (coming soon)</li> <li>Australian Total Wine Sales (coming soon)</li> <li>Quarterly Production of Woollen Yarn in Australia (coming soon)</li> <li>Hourly Nuclear Energy Generation (coming soon)</li> </ol> </li> </ol>"},{"location":"book/1-Introduction/","title":"Introduction","text":"<p>The concept of a mathematical model is fundamental in many fields of science. From engineering to sociology, models plays a central role to the study of complex systems as they allow to simulate what will happen in different scenarios and conditions, predict its output for a given input, analyse its properties and explore different design schemes. To accomplish these goals, however, it is crucial that the model is a proper representation of the system under study. The modeling of dynamic and steady-state behaviors is, therefore, fundamental to this type of analysis and depends on System Identification (SI) procedures.</p>"},{"location":"book/1-Introduction/#models","title":"Models","text":"<p>Mathematical modeling is a great way to understand and analyze different parts of our world. It gives us a clear framework to make sense of complex systems and how they behave. Whether it\u2019s for everyday tasks or big-picture issues like disease control, models are a key part of how we deal with various challenges.</p> <p>Typing efficiently on a conventional QWERTY keyboard layout is the result of a well-learned model of the QWERTY keyboard embedded in the individual cognitive processes. However, if you are faced with a different keyboard layout, such as the Dvorak or AZERTY, you will probably struggle to adapt to the new model. The system changed, so you will have to update you model.</p> <p></p> <p>QWERTY - Wikipedia - ANSI\u00a0QWERTY\u00a0keyboard layout\u00a0(US)</p> <p></p> <p>AZERTY layout used on a keyboard</p> <p>Mathematical modeling touches on many parts of our lives. Whether we're looking at economic trends, tracking how diseases spread, or figuring out consumer behavior, models are essential tools for gaining knowledge, making informed decisions, and take control over complex systems.</p> <p>In essence, mathematical models help us make sense of the world. They let us understand human behavior and the systems we deal with every day. By using these models, we can learn, adapt, and adjust our strategies to keep up with the surrounding changes.</p>"},{"location":"book/1-Introduction/#system-identification","title":"System Identification","text":"<p>System identification is a data-driven framework to model dynamical systems. Initially, scientists focused on linear system identification, but this has been changing over the past decades with more emphasis in nonlinear systems. Nonlinear system identification is widely considered to be one of the most important topics concerning the modeling of many different dynamical systems, from time-series to severally nonlinear dynamic behaviors.</p> <p>Extensive resources, including excellent textbooks covering linear system identification and time series forecasting are readily available. In this book, we revisit some known topics, but we also try to approach such subjects in a different and complementary way. We will explore the modeling of nonlinear dynamic systems  using NARMAX(Nonlinear AutoRegressive Moving Average model with eXogenous inputs) methods, which were introduced by [Stephen A. Billings] in 1981.</p>"},{"location":"book/1-Introduction/#linear-or-nonlinear-system-identification","title":"Linear or Nonlinear System Identification","text":""},{"location":"book/1-Introduction/#linear-models","title":"Linear Models","text":"<p>While most real world systems are nonlinear, you probably should give linear models a try first. Linear models usually serves as a strong baseline and can be good enough for your case, giving satisfactory performance. Astron and Murray and Glad and Ljung showed that many nonlinear systems can be well described by locally linear models. Besides, linear models are easy to fit, easy to interpret, and requires less computational resources than nonlinear models, allowing you to experiment fast and gather insights before thinking about gray box models or complex nonlinear models.</p> <p>Linear models can be very useful, even in the presence of strong nonlinearities, because it is much easier to deal with it. Moreover, the development of linear identification algorithms is still a very active and healthy research field, with many papers being released every year Sai Li, Linjun Zhang, T. Tony Cai &amp; Hongzhe Li, Maria Jaenada, Leandro Pardo, Xing Liu;\u00a0Lin Qiu, Youtong Fang;\u00a0Kui Wang;\u00a0Yongdong Li, Jose Rodr\u00edguez, Alessandro D\u2019Innocenzo and Francesco Smarra. Linear models work well most of the time and should be the first choice for many applications. However, when dealing with complex systems where linear assumptions don\u2019t hold, nonlinear models become essential.</p>"},{"location":"book/1-Introduction/#nonlinear-models","title":"Nonlinear Models","text":"<p>When linear models do not perform well enough, you should consider nonlinear models. It's important to notice, however, that changing from a linear to a nonlinear model is not always a simple task. For inexperienced users, it's common to build nonlinear models that performs worse than the linear ones. To work with nonlinear models, you must consider that characteristics such structural errors, noise, operation point, excitation signals and many others aspects of your system under study impact your modelling approach and strategy.</p> <p>As suggested by Johan Schoukens and Lennart Ljung in \"Nonlinear System Identi\ufb01cation - A User-Oriented Roadmap\", only start working with nonlinear models if there is enough evidence that linear models will not solve the problem.</p> <p>Nonlinear models are more flexible than linear models and can be built using many different mathematical representations, such as polynomial, generalized additive, neural networks, wavelet and many more (Billings, S. A.). Such flexibility, however, makes nonlinear system identification much more complex the linear ones, from the experiment design to the model selection. The user should consider that, besides the modeling complexity, moving to nonlinear models will require a revision in the road-map and computational resources defined when dealing with the linear models. In this respect, always ask yourself whether the potential benefits of nonlinear models are worth the effort.</p>"},{"location":"book/1-Introduction/#narmax-methods","title":"NARMAX Methods","text":"<p>NARMAX model is one of the most frequently employed nonlinear model representation and is widely used to represent a broad class of nonlinear systems. NARMAX methods were successfully applied in many scenarios, which include industrial processes, control systems, structural systems, economic and financial systems, biology, medicine, social systems, and much more. The NARMAX model representation and the class of systems that can be represented by it will be discussed later in the book.</p> <p>The main steps involved to build NARMAX models are (Billings, 2013):</p> <ol> <li>Model Representation: define the model mathematical representation.</li> <li>Model Structure Selection: define which terms are in the final model.</li> <li>Parameter Estimation: estimate the coefficients of each model term selected in step 1.</li> <li>Model validation: to make sure the model is unbiased and accurate;</li> <li>Model Prediction/Simulation: predict future outputs or simulate the behavior of the system given different inputs.</li> <li>Analysis: understanding the dynamical properties of the system under study</li> <li>Control: develop control design schemes based on the obtained model.</li> </ol> <p>Model Structure Selection (MSS) is the most important aspect of NARMAX methods and the most complex. Selecting the model terms is fundamental if the goal of the identification is to obtain models that can reproduce the dynamics of the original system and impacts every other aspect of the identification process. Problems related to over parameterization and numerical ill-conditioning are typical because of the limitations the identification algorithms in selecting the appropriate terms that should compose the final model (L. A. Aguirre e S. A. Billings, L. Piroddi e W. Spinelli).</p> <p>In SysIdentPy, you are allowed to interact directly with every item described in the 7 steps, except for the control one. SysIdentPy focuses on modeling, not on control design. You'll have to use some of the code bellow in every modeling task using SysIdentPy. You'll learn the details along the book, so don't worry if you are not familiar with those methods yet.</p> <pre><code>from sysidentpy.basis_function import Polynomial\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.general_estimators import NARX\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.simulation import SimulateNARMAX\n</code></pre>"},{"location":"book/1-Introduction/#what-is-the-purpose-of-system-identification","title":"What is the Purpose of System Identification?","text":"<p>Because of the Model Structure Selection problem, Billings, S. A. states that the goal of System Identification using NARMAX  methods is twofold: performance and parsimony.</p> <p>The first goal is often about approximation. Here, the main focus is to build a model that make predictions with with the lowest error possible. This approach is common in applications like weather forecasting, demand forecasting, predicting stock prices, speech recognition, target tracking, and pattern classification. In these cases, the specific form of the model isn't as critical. In other words, how the terms interact (in parametric models), the mathematical representation, the static behavior and so on are not that important; what matters most is finding a way to minimize prediction errors.</p> <p>But system identification isn't just about minimizing prediction errors. One of the main goals of System Identification is to build models that help the user to understand and interpret the system being modeled. Beyond just making accurate predictions, the goal is to develop models that truly capture the dynamic behavior of the system being studied, ideally in the simplest form possible. Science and engineering are all about understanding systems by breaking down complex behaviors into simpler ones that we can understand and control. For example, if the system's behavior can be described by a simple first-order dynamic model with a cubic nonlinear term in the input, system identification should help uncover that.</p>"},{"location":"book/1-Introduction/#is-system-identification-machine-learning","title":"Is System Identification Machine Learning?","text":"<p>First, let's take an overview of static and dynamic systems. Imagine you have an electric guitar connected to an effect processor that can apply various audio effects, such as reverb or distortion. The effect is controlled by a switch that toggles between \"on\" and \"off\". Let\u2019s consider this from the perspective of signals. The input signal represents the state of the effect switch: switch off (low level), switch on (high level). If we represent the guitar signal, we have a binary condition: effect off (original guitar sound), effect on (modified guitar sound). This is an example of a static system: the output (guitar sound) directly follows the input (state of the effect switch).</p> <p>When the effect switch is off, the output is just the clean, unaltered guitar signal. When the effect switch is on, the output is the guitar signal with the effect applied, such as amplification or distortion. In this system, the effect being on or off directly influences the guitar signal without any delay or additional processing.</p> <p>This example illustrates how a static system operates with binary control inputs, where the output directly reflects the input state, providing a straightforward mapping between the control signal and the system\u2019s response.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\nt = np.linspace(0, 30, 500, endpoint=False)\nu = signal.square(0.2*np.pi * t)\nu[u &lt; 0] = 0\n# In a static system, the output y directly follows the input u\ny = u\n\n# Plot the input and output\nplt.figure(figsize=(15, 3))\nplt.plot(t, u, label='Input (State of the Switch)', color=\"grey\", linewidth=10, alpha=0.5)\nplt.plot(t, y, label='Output (Static System Response)', color='k', linewidth=0.5)\nplt.title('Static System Response to the Input')\nplt.xlabel('Time [s]')\nplt.ylabel('y')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre> <p></p> <p>Static response representation. The input signal representing the state of the switch (switch off (low level), switch on (high level)), and the static response: original sound (low level), processed sound (high level).</p> <p>Now, let\u2019s consider a dynamic system: using an air conditioner to lower the room temperature. This example effectively illustrates the concepts of dynamic systems and how their output responds over time.</p> <p>Let\u2019s imagine this from the perspective of signals. The input signal represents the state of the air conditioner's control: turning the air conditioner on (high level) or turning it off (low level). When the air conditioner is turned on, it begins to cool the room. However, the room temperature does not drop instantaneously to the desired cooler level. It takes time for the air conditioner to affect the temperature, and the rate at which the temperature decreases can vary based on factors like the room size and insulation.</p> <p>Conversely, when the air conditioner is turned off, the room temperature does not immediately return to its original ambient temperature. Instead, it gradually warms up as the cooling effect diminishes.</p> <p></p> <p>Using an air conditioner to lower the room temperature as dynamic system representation.</p> <p>In this dynamic system, the output (room temperature) does not instantly follow the input (state of the air conditioner) because there is a time lag involved in both cooling and warming processes. The system has memory, meaning the current room temperature depends not only on the current state of the air conditioner but also on how long it has been running or off, and how much it has already cooled or allowed the room to warm up.</p> <p>This example highlights the nature of dynamic systems: the response to an input is gradual and affected by the system\u2019s internal dynamics. The air conditioner\u2019s effect on the room temperature exemplifies how dynamic systems have a time-dependent response, where the output changes over time and does not immediately match the input signal.</p> <p>For static systems, the output is a direct function of the input, represented by an algebraic equation:</p> \\[ y(t) = G \\cdot u(t) \\] <p>For dynamic systems, the output depends on the input and the rate of change of the input, represented by a differential equation. For example, the output \\(y(t)\\) can be modeled as:</p> \\[ y(t) = G \\cdot u(t) - \\tau \\cdot \\frac{dy(t)}{dt} \\] <p>Here, \\(G\\) is the gain, and \\(\\tau\\) is a constant that incorporates the system's memory. For discrete-time systems, we consider signals at specific and spaced intervals. The differential equation is discretized, and the derivative is approximated by a finite difference:</p> \\[ y[k] = \\alpha y[k-1] + \\beta u[k] \\] <p>where \\(\\alpha\\) and \\(\\beta\\) are constants that determine the system's response. The z-transform can be used to obtain the transfer function in the z-domain.</p> <p>In summary, static systems are modeled by algebraic equations, while dynamic systems are modeled by differential equations.</p> <p>As Luis Antonio Aguirre states in one of his classes on YouTube (in Portuguese), all physical systems are dynamic, but depending on the timescale, they can be modeled as static for simplification. For example, the transition between the effects on the guitar sound, if taken in seconds (as we did in the example), could be treated as static depending on your analysis. However, the pedal board have components like capacitors, which are dynamic electrical components, making it a dynamic system. The response, however, is so fast that we dealt with it like a static system. Therefore, representing a system as static is a modeling decision.</p> <p>Table 1 shows how this field can be categorized with respect to linear/nonlinear and static/dynamic systems.</p> System Characteristics Linear Model Nonlinear Model Static Linear Regression Machine Learning Dynamic Linear System Identification Nonlinear System Identification &gt; Table 1: Naming conventions in the System Identification field. Adapted from Oliver Nelles"},{"location":"book/1-Introduction/#nonlinear-system-identification-and-forecasting-applications-case-studies","title":"Nonlinear System Identification and Forecasting Applications: Case Studies","text":"<p>There\u2019s a lot of research out there on nonlinear system identification, including NARMAX methods. However, there are a relatively small number of books and papers showing how to apply these methods to real-life systems Instead in a way that\u2019s easy to understand. Our goal with this book is to change that. We want to make these methods practical and accessible. While we\u2019ll cover the necessary math and algorithms, we\u2019ll keep things as clear and simple as possible, making it easier for readers from all backgrounds to learn how to model dynamic nonlinear systems using SysIdentPy.</p> <p>Therefore, this book aims to fill a gap in the existing literature. In Chapter 10, we present real-world case studies to show how NARMAX methods can be applied to a variety of complex systems. Whether it\u2019s modeling a highly nonlinear system like the Bouc-Wen model, modeling a dynamic behavior in a full-scale F-16 aircraft, or working with the M4 dataset for benchmarking, we\u2019ll guide you through building NARMAX models using SysIdentPy.</p> <p>The case studies we've selected come from a wide range of fields, not just the typical timeseries or industrial examples you might expect from traditional system identification or timeseries books. Our aim is to showcase the versatility of NARMAX algorithms and SysIdentPy and illustrate the kind of in-depth analysis you can achieve with these tools.</p>"},{"location":"book/1-Introduction/#abbreviations","title":"Abbreviations","text":"Abbreviation Full Name AIC Akaike Information Criterion AICC Corrected Akaike Information Criterion AOLS Accelerated Orthogonal Least Squares ANN Artificial Neural Network AR AutoRegressive ARMAX AutoRegressive Moving Average with eXogenous Input ARARX AutoRegressive AutoRegressive with eXogenous Input ARX AutoRegressive with eXogenous Input BIC Bayesian Information Criterion ELS Extended Least Squares ER Entropic Regression ERR Error Reduction Ratio FIR Finite Impulse Response FPE Final Prediction Error FROLS Forward Regression Orthogonal Least Squares GLS Generalized Least Squares LMS Least Mean Square LS Least Squares LSTM Long Short-Term Memory MA Moving Average MetaMSS Meta Model Structure Selection MIMO Multiple Input Multiple Output MISO Multiple Input Single Output MLP Multilayer Perceptron MSE Mean Squared Error MSS Model Structure Selection NARMAX Nonlinear AutoRegressive Moving Average with eXogenous Input NARX Nonlinear AutoRegressive with eXogenous Input NFIR Nonlinear Finite Impulse Response NIIR Nonlinear Infinite Impulse Response NLS Nonlinear Least Squares NN Neural Network OBF Orthonormal Basis Function OE Output Error OLS Orthogonal Least Squares RBF Radial Basis Function RELS Recursive Extended Least Squares RLS Recursive Least Squares RMSE Root Mean Squared Error SI System Identification SISO Single Input Single Output SVD Singular Value Decomposition WLS Weighted Least Squares"},{"location":"book/1-Introduction/#variables","title":"Variables","text":"Variable Name Description \\(f(\\cdot)\\) function to be approximated \\(k\\) discrete time \\(m\\) dynamic order \\(x\\) system inputs \\(y\\) system output \\(\\hat{y}\\) model predicted output \\(\\lambda\\) regularization strength \\(\\sigma\\) standard deviation \\(\\theta\\) parameter vector \\(N\\) number of data points \\(\\Psi(\\cdot)\\) Information Matrix \\(n_{m^r}\\) Number of potential regressors for MIMO models \\(\\mathcal{F}\\) Arbitrary mathematical representation \\(\\Omega_{y^p x^m}\\) Term cluster of polynomial NARX \\(\\ell\\) nonlinearity degree of the model \\(\\hat{\\Theta}\\) Estimated Parameter Vector \\(\\hat{y}_k\\) model predicted output at discrete time \\(k\\) \\(\\mathbf{X}_k\\) Column vector of multiple system inputs at discrete-time \\(k\\) \\(\\mathbf{Y}_k\\) Column vector of multiple system outputs at discrete-time \\(k\\) \\(\\mathcal{H}_t(\\omega)\\) Hysteresis loop of the system in continuous-time \\(\\mathcal{H}\\) Bounding structure that delimits the system hysteresis loop \\(\\rho\\) Tolerance value \\(\\sum_{y^p x^m}\\) Cluster coefficients of polynomial NARX \\(e_k\\) error vector at discrete-time \\(k\\) \\(n_r\\) Number of potential regressors for SISO models \\(n_x\\) maximum lag of the input regressor \\(n_y\\) maximum lag of the output regressor \\(n\\) number of observations in a sample \\(x_k\\) system input at discrete-time \\(k\\) \\(y_k\\) system output at discrete-time \\(k\\)"},{"location":"book/1-Introduction/#book-organization","title":"Book Organization","text":"<p>This book focuses on making concepts easy to understand, emphasizing clear explanations and practical connections between different methods. We avoid excessive formalism and complex equations, opting instead to illustrate core ideas with plenty of hands-on examples. Written with a System Identification perspective, the book offers practical implementation details throughout the chapters.</p> <p>The goals of this book are to help you:</p> <ul> <li>Understand the advantages, drawbacks, and areas of application of different NARMAX models and algorithms.</li> <li>Choose the right approach for your specific problem.</li> <li>Adjust all hyperparameters properly.</li> <li>Interpret and comprehend the obtained results.</li> <li>valuate the reliability and limitations of your models.</li> </ul> <p>Many chapters include real-world examples and data, guiding you on how to apply these methods using SysIdentPy in practice.</p>"},{"location":"book/10-Case-Studies/","title":"10. Case Studies","text":""},{"location":"book/10-Case-Studies/#m4-dataset","title":"M4 Dataset","text":"<p>The M4 dataset is a well known resource for time series forecasting, offering a wide range of data series used to test and improve forecasting methods. Created for the M4 competition organized by Spyros Makridakis, this dataset has driven many advancements in forecasting techniques.</p> <p>The M4 dataset includes 100,000 time series from various fields such as demographics, finance, industry, macroeconomics, and microeconomics, which were selected randomly from the ForeDeCk database. The series come in different frequencies (yearly, quarterly, monthly, weekly, daily, and hourly), making it a comprehensive collection for testing forecasting methods.</p> <p>In this case study, we will focus on the hourly subset of the M4 dataset. This subset consists of time series data recorded hourly, providing a detailed and high-frequency look at changes over time. Hourly data presents unique challenges due to its granularity and the potential for capturing short-term fluctuations and patterns.</p> <p>The M4 dataset provides a standard benchmark to compare different forecasting methods, allowing researchers and practitioners to evaluate their models consistently. With series from various domains and frequencies, the M4 dataset represents real-world forecasting challenges, making it valuable for developing robust forecasting techniques. The competition and the dataset itself have led to the creation of new algorithms and methods, significantly improving forecasting accuracy and reliability.</p> <p>We will present a end to end walkthrough using the M4 hourly dataset to demonstrate the capabilities of SysIdentPy. SysIdentPy offers a range of tools and techniques designed to effectively handle the complexities of time series data, but we will focus on fast and easy setup for this case. We will cover model selection and evaluation metrics specific to the hourly dataset.</p> <p>By the end of this case study, you will have a solid understanding of how to use SysIdentPy for forecasting with the M4 hourly dataset, preparing you to tackle similar forecasting challenges in real-world scenarios.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\ndatasetsforecast==0.0.8\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\ns3fs==2024.6.1\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import SettingWithCopyWarning\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error, symmetric_mean_absolute_percentage_error\nfrom sysidentpy.utils.plotting import plot_results\n\nfrom datasetsforecast.m4 import M4, M4Evaluation\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n\ntrain = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv')\ntest = pd.read_csv('https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv').rename(columns={'y': 'y_test'})\n</code></pre> <p>The following plots provide a visualization of the training data for a small subset of the time series. The plot shows the raw data, giving you an insight into the patterns and behaviors inherent in each series.</p> <p>By observing the data, you can get a sense of the variety and complexity of the time series we are working with. The plots can reveal important characteristics such as trends, seasonal patterns, and potential anomalies within the time series. Understanding these elements is crucial for the development of accurate forecasting models.</p> <p>However, when dealing with a large number of different time series, it is common to start with broad assumptions rather than detailed individual analysis. In this context, we will adopt a similar approach. Instead of going into the specifics of each dataset, we will make some general assumptions and see how SysIdentPy handles them.</p> <p>This approach provides a practical starting point, demonstrating how SysIdentPy can manage different types of time series data without too much work. As you become more familiar with the tool, you can refine your models with more detailed insights. For now, let's focus on using SysIdentPy to create the forecasts based on these initial assumptions.</p> <p>Our first assumption is that there is a 24-hour seasonal pattern in the series. By examining the plots below, this seems reasonable. Therefore, we'll begin building our models with <code>ylag=24</code>.</p> <pre><code>ax = train[train[\"unique_id\"]==\"H10\"].reset_index(drop=True)[\"y\"].plot(figsize=(15, 2), title=\"H10\")\nxcoords = [a for a in range(24, 24*30, 24)]\n\nfor xc in xcoords:\n\u00a0 \u00a0 plt.axvline(x=xc, color='red', linestyle='--', alpha=0.5)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Let's check build a model for the <code>H20</code> group before we extrapolate the settings for every group. Because there are no input features, we will be using a <code>NAR</code> model type in SysIdentPy. To keep things simple and fast, we will start with Polynomial basis function with degree \\(1\\).</p> <pre><code>unique_id = \"H20\"\ny_id = train[train[\"unique_id\"]==unique_id][\"y\"].values.reshape(-1, 1)\ny_val = test[test[\"unique_id\"]==unique_id][\"y_test\"].values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    order_selection=True,\n    ylag=24,\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\n\nmodel.fit(y=y_id)\ny_val = np.concatenate([y_id[-model.max_lag :], y_val])\ny_hat = model.predict(y=y_val, forecast_horizon=48)\nsmape = symmetric_mean_absolute_percentage_error(y_val[model.max_lag::], y_hat[model.max_lag::])\n\nplot_results(y=y_val[model.max_lag :], yhat=y_hat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Group: {unique_id} - SMAPE {round(smape, 4)}\")\n</code></pre> <p></p> <p>Probably, the result are not optimal and will not work for every group. However, let's check how this setting performs against the winner model M4 time series competition: the Exponential Smoothing with Recurrent Neural Networks (ESRNN).</p> <pre><code>esrnn_url = 'https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-118.zip'\nesrnn_forecasts = M4Evaluation.load_benchmark('data', 'Hourly', esrnn_url)\nesrnn_evaluation = M4Evaluation.evaluate('data', 'Hourly', esrnn_forecasts)\n\nesrnn_evaluation\n</code></pre> SMAPE MASE OWA Hourly 9.328 0.893 0.440 &gt; Table 1. ESRNN SOTA results <p>The following code took only 49 seconds to run on my machine (AMD Ryzen 5 5600x processor, 32GB RAM at 3600MHz). Because of its efficiency, I didn't create a parallel version. By the end of this use case, you will see how SysIdentPy can be both fast and effective, delivering good results without too much optimization.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n\u00a0 \u00a0 y_id = data[\"y\"].values.reshape(-1, 1)\n\u00a0 \u00a0 basis_function = Polynomial(degree=1)\n\u00a0 \u00a0 model = FROLS(\n        ylag=24,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        n_info_values=25,\n    )\n\u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 model.fit(y=y_id)\n\u00a0 \u00a0 \u00a0 \u00a0 y_val = y_id[-model.max_lag :].reshape(-1, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 y_hat = model.predict(y=y_val, forecast_horizon=48)\n\u00a0 \u00a0 \u00a0 \u00a0 r.append(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u_id*len(y_hat[model.max_lag::]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ds_test,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y_hat[model.max_lag::].ravel()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 except Exception:\n\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(['unique_id', 'ds', 'NARMAX_1'])\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)#.clip(lower=10)\npivot_df = results_1.pivot(index='unique_id', columns='ds', values='NARMAX_1')\nresults = pivot_df.to_numpy()\n\nM4Evaluation.evaluate('data', 'Hourly', results)\n</code></pre> SMAPE MASE OWA Hourly 16.034196 0.958083 0.636132 Table 2. First test with SysIdentPy <p>The initial results are reasonable, but they don't quite match the performance of <code>ESRNN</code>. These results are based solely on our first assumption. To better understand the performance, let\u2019s examine the groups with the worst results.</p> <p></p> <p>The following plot illustrates two such groups, <code>H147</code> and <code>H136</code>. Both exhibit a 24-hour seasonal pattern.</p> <p></p> <p></p> <p>However, a closer look reveals an additional insight: in addition to the daily pattern, these series also show a weekly pattern. Observe how the data looks like when we split the series into weekly segments.</p> <p></p> <pre><code>xcoords = list(range(0, 168*5, 168))\nfiltered_train = train[train[\"unique_id\"] == \"H147\"].reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(10, 1.5 * len(xcoords[1:])))\nfor i, start in enumerate(xcoords[:-1]):\n\u00a0 \u00a0 end = xcoords[i + 1]\n\u00a0 \u00a0 ax = fig.add_subplot(len(xcoords[1:]), 1, i + 1)\n\u00a0 \u00a0 filtered_train[\"y\"].iloc[start:end].plot(ax=ax)\n\u00a0 \u00a0 ax.set_title(f'H147 -&gt; Slice {i+1}: Hour {start} to {end-1}')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Therefore, we will build models setting <code>ylag=168</code>.</p> <p>Note that this is a very high number for lags, so be careful if you want to try it with higher polynomial degrees because the time to run the models can increase significantly. I tried some configurations with polynomial degree equal to 2 and only took \\(6\\) minutes to run (even less, using <code>AOLS</code>), without making the code run in parallel. As you can see, SysIdentPy can be very fast, and you can make it faster by applying parallelization.</p> <pre><code># this took 2min to run on my computer.\nr = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n\u00a0 \u00a0 y_id = data[\"y\"].values.reshape(-1, 1)\n\u00a0 \u00a0 basis_function = Polynomial(degree=1)\n\u00a0 \u00a0 model = FROLS(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ylag=168,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_type=\"NAR\",\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 model.fit(y=y_id)\n\u00a0 \u00a0 \u00a0 \u00a0 y_val = y_id[-model.max_lag :].reshape(-1, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 y_hat = model.predict(y=y_val, forecast_horizon=48)\n\u00a0 \u00a0 \u00a0 \u00a0 r.append(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u_id*len(y_hat[model.max_lag::]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ds_test,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y_hat[model.max_lag::].ravel()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 except Exception:\n\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(['unique_id', 'ds', 'NARMAX_1'])\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)#.clip(lower=10)\npivot_df = results_1.pivot(index='unique_id', columns='ds', values='NARMAX_1')\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate('data', 'Hourly', results)\n</code></pre> SMAPE MASE OWA Hourly 10.475998 0.773749 0.446471 &gt; Table 3. Improved results using SysIdentPy <p>Now, the results are much closer to those of the <code>ESRNN</code> model! While the Symmetric Mean Absolute Percentage Error (<code>SMAPE</code>) is slightly worse, the Mean Absolute Scaled Error (<code>MASE</code>) is better when comparing against <code>ESRNN</code>, leading to a very similar Overall Weighted Average (<code>OWA</code>) metric. Remarkably, these results are achieved using only simple <code>AR</code> models. Next, let's see if the <code>AOLS</code> method can provide even better results.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n\u00a0 \u00a0 y_id = data[\"y\"].values.reshape(-1, 1)\n\u00a0 \u00a0 basis_function = Polynomial(degree=1)\n\u00a0 \u00a0 model = AOLS(\n        ylag=168,\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        # due to high lag settings, k was increased to 6 as an initial guess\n        k=6,\n    )\n\u00a0 \u00a0 try:\n\u00a0 \u00a0 \u00a0 \u00a0 model.fit(y=y_id)\n\u00a0 \u00a0 \u00a0 \u00a0 y_val = y_id[-model.max_lag :].reshape(-1, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 y_hat = model.predict(y=y_val, forecast_horizon=48)\n\u00a0 \u00a0 \u00a0 \u00a0 r.append(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 u_id*len(y_hat[model.max_lag::]),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ds_test,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y_hat[model.max_lag::].ravel()\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\n\u00a0 \u00a0 \u00a0 \u00a0 )\n\u00a0 \u00a0 except Exception:\n\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(['unique_id', 'ds', 'NARMAX_1'])\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)#.clip(lower=10)\npivot_df = results_1.pivot(index='unique_id', columns='ds', values='NARMAX_1')\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate('data', 'Hourly', results)\n</code></pre> SMAPE MASE OWA Hourly 9.951141 0.809965 0.439755 &gt; Table 4. SysIdentPy results using AOLS algorithm <p>The Overall Weighted Average (<code>OWA</code>) is even better than that of the <code>ESRNN</code> model! Additionally, the <code>AOLS</code> method was incredibly efficient, taking only 6 seconds to run. This combination of high performance and rapid execution makes <code>AOLS</code> a compelling alternative for time series forecasting in cases with multiple series.</p> <p>Before we finish, let's verify how the performance of the <code>H147</code> model has improved with the <code>ylag=168</code> setting.</p> <p></p> <p>Based on the M4 benchmark paper, we could also clip the predictions lower than 10 to 10 and the results would be slightly better. But this is left to the user.</p> <p>We could achieve even better performance with some fine-tuning of the model configuration. However, I\u2019ll leave exploring these alternative adjustments as an exercise for the user. However, keep in mind that experimenting with different settings does not always guarantee improved results. A deeper theoretical knowledge can often lead you to better configurations and, hence, better results.</p>"},{"location":"book/10-Case-Studies/#coupled-eletric-device","title":"Coupled Eletric Device","text":"<p>The CE8 coupled electric drives dataset - Nonlinear Benchmark presents a compelling use case for demonstrating the performance of SysIdentPy. This system involves two electric motors driving a pulley with a flexible belt, creating a dynamic environment ideal for testing system identification tools.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p>"},{"location":"book/10-Case-Studies/#system-overview","title":"System Overview","text":"<p>The CE8 system, illustrated in Figure 1, features: - Two Electric Motors: These motors independently control the tension and speed of the belt, providing symmetrical control around zero. This enables both clockwise and counterclockwise movements. - Pulley Mechanism: The pulley is supported by a spring, introducing a lightly damped dynamic mode that adds complexity to the system. - Speed Control Focus: The primary focus is on the speed control system. The pulley\u2019s angular speed is measured using a pulse counter, which is insensitive to the direction of the velocity.</p> <p></p> <p>Figure 1. CE8 system design.</p>"},{"location":"book/10-Case-Studies/#sensor-and-filtering","title":"Sensor and Filtering","text":"<p>The measurement process involves: - Pulse Counter: This sensor measures the angular speed of the pulley without regard to the direction. - Analogue Low Pass Filtering: This reduces high-frequency noise, followed by antialiasing filtering to prepare the signal for digital processing. The dynamic effects are mainly influenced by the electric drive time constants and the spring, with the low pass filtering having a minimal impact on the output.</p>"},{"location":"book/10-Case-Studies/#sota-results","title":"SOTA Results","text":"<p>SysIdentPy can be used to build robust models for identifying and modeling the complex dynamics of the CE8 system. The performance will be compared against a benchmark provided by Max D. Champneys, Gerben I. Beintema, Roland T\u00f3th, Maarten Schoukens, and Timothy J. Rogers -\u00a0Baselines for Nonlinear Benchmarks,\u00a0Workshop on Nonlinear System Identification Benchmarks, 2024.</p> <p></p> <p>The benchmark evaluate the average metric between the two experiments. That's why the SOTA method do not have the better metric for <code>test 1</code>, but it is still the best overall.  The goal of this case study is not only to showcase the robustness of SysIdentPy but also provides valuable insights into its practical applications in real-world dynamic systems.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_1","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_1","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the CE8 coupled electric drives dataset. This example showcases the robust performance of SysIdentPy in modeling and identifying complex dynamic systems. The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for CE8 system.</p> <p>This practical example will help users understand how to effectively utilize SysIdentPy for their own system identification tasks, leveraging its advanced features to handle the complexities of real-world dynamic systems. Let's dive into the code and explore the capabilities of SysIdentPy.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.CED(atleast_2d=True)\ndata_train_1, data_train_2 = train_val\ndata_test_1, data_test_2 = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation GerbenBeintema - nonlinear_benchmarks: The official data load for nonlinear benchmark datasets to check the details of how to use it.</p> <p>The following plot detail the training and testing data of both experiments. Here we are trying to get two models, one for each experiment, that have a better performance than the mentioned baselines.</p> <pre><code>plt.plot(data_train_1.u)\nplt.plot(data_train_1.y)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(data_test_1.u)\nplt.plot(data_test_1.y)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(data_train_2.u)\nplt.plot(data_train_2.y)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(data_test_2.u)\nplt.plot(data_test_2.y)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"book/10-Case-Studies/#results","title":"Results","text":"<p>First, we will set the exactly same configuration to built models for both experiments. We can have better models by optimizing the configurations individually, but we will start simple.</p> <p>A basic configuration of FROLS using a polynomial basis function with degree equal 2 is defined. The information criteria will be the default one, the <code>aic</code>. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example.</p> <p>Model for experiment 1:</p> <pre><code>y_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 n_info_values=120\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <p>Model for experiment 2: <pre><code>y_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 n_info_values=120\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre></p> <p></p> <p>The first configuration for experiment 1 is already better than the LTI ARX, LTI SS, GRU, LSTM, MLP NARX, MLP FIR, OLSTM, and the SOTA models shown in the benchmark table. Better than 8 out 11 models shown in the benchmark. For experiment 2, its better than LTI ARX, LTI SS, GRU, RNN, LSTM, OLSTM, and pNARX (7 out 11). It's a good start, but let's check if the performance improves if we set a higher lag for both <code>xlag</code> and <code>ylag</code>.</p> <p>The average metric is \\((0.1131 + 0.1059)/2 = 0.1095\\), which is very good, but worse than the SOTA (\\(0.0945\\)). We will now increase the lags for <code>x</code> and <code>y</code> to check if we get a better model. Before increasing the lags, the information criteria is shown:</p> <p><pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> </p> <p>It can be observed that after 22 regressors, adding new regressors do not improve the model performance (considering the configuration defined for that model). Because we want to try models with higher lags and higher nonlinearity degree, the stopping criteria will be changed to <code>err_tol</code> instead of information criteria. This will made the algorithm runs considerably faster.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=22,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aicc\",\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=22,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <p>In the first experiment, the model showed a slight improvement, while the performance of the second experiment experienced a minor decline. Increasing the lag settings with these configurations did not result in significant changes. Therefore, let's set the polynomial degree to \\(3\\) and increase the number of terms to build the model to <code>n_terms=40</code> if the <code>err_tol</code> is not reached. It's important to note that these values are chosen empirically. We could also adjust the parameter estimation technique, the <code>err_tol</code>, the model structure selection algorithm, and the basis function, among other factors. Users are encouraged to employ hyperparameter tuning techniques to find the optimal combinations of hyperparameters.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aicc\",\n\u00a0 \u00a0 err_tol=0.9996,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\")\n</code></pre> <p></p> <p>As shown in the plot, we have surpassed the state-of-the-art (SOTA) results with an average metric of \\((0.0969 + 0.0731)/2 = 0.0849\\). Additionally, the metric for the first experiment matches the best model in the benchmark, and the metric for the second experiment slightly exceeds the benchmark's best model. Using the same configuration for both models, we achieved the best overall results!</p>"},{"location":"book/10-Case-Studies/#wiener-hammerstein","title":"Wiener-Hammerstein","text":"<p>The description content primarily derives from the benchmark website - Nonlinear Benchmark and associated paper - Wiener-Hammerstein benchmark with process noise. For a detailed description, readers are referred to the linked references.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p> <p>This benchmark focuses on a Wiener-Hammerstein electronic circuit where process noise plays a significant role in distorting the output signal.</p> <p>The Wiener-Hammerstein structure is a well-known block-oriented system which contains a static nonlinearity sandwiched between two Linear Time-Invariant (LTI) blocks (Figure 2). This arrangement presents a challenging identification problem due to the presence of these LTI blocks.</p> <p></p> <p>Figure 2: the Wiener-Hammerstein system</p> <p>In Figure 2, the Wiener-Hammerstein system is illustrated with process noise \\(e_x(t)\\) entering before the static nonlinearity \\(f(x)\\), sandwiched between LTI blocks represented by \\(R(s)\\) and \\(S(s)\\) at the input and output, respectively. Additionally, small, negligible noise sources \\(e_u(t)\\) and \\(e_y(t)\\) affect the measurement channels. The measured input and output signals are denoted as \\(u_m(t)\\) and \\(y_m(t)\\).</p> <p>The first LTI block \\(R(s)\\) is effectively modeled as a third-order lowpass filter. The second LTI subsystem \\(S(s)\\) is configured as an inverse Chebyshev filter with a stop-band attenuation of \\(40 dB\\) and a cutoff frequency of \\(5 kHz\\). Notably, \\(S(s)\\) includes a transmission zero within the operational frequency range, complicating its inversion.</p> <p>The static nonlinearity \\(f(x)\\) is implemented using a diode-resistor network, resulting in saturation nonlinearity. Process noise \\(e_x(t)\\) is introduced as filtered white Gaussian noise, generated from a discrete-time third-order lowpass Butterworth filter followed by zero-order hold and analog low-pass reconstruction filtering with a cutoff of \\(20 kHz\\).</p> <p>Measurement noise sources \\(e_u(t)\\) and \\(e_y(t)\\) are minimal compared to \\(e_x(t)\\). The system's inputs and process noise are generated using an Arbitrary Waveform Generator (AWG), specifically the Agilent/HP E1445A, sampling at \\(78125 Hz\\), synchronized with an acquisition system (Agilent/HP E1430A) to ensure phase coherence and prevent leakage errors. Buffering between the acquisition cards and the system's inputs and outputs minimizes measurement equipment distortion.</p> <p>The benchmark provides two standard test signals through the benchmarking website: a random phase multi sine and a sine-sweep signal. Both signals have a \\(rms\\) value of \\(0.71 Vrms\\) and cover frequencies from DC to \\(15 kHz\\) (excluding DC). The sine-sweep spans this frequency range at a rate of \\(4.29 MHz/min\\). These test sets serve as targets for evaluating the model's performance, emphasizing accurate representation under varied conditions.</p> <p>The Wiener-Hammerstein benchmark highlights three primary nonlinear system identification challenges:</p> <ol> <li>Process Noise: Significant in the system, influencing output fidelity.</li> <li>Static Nonlinearity: Indirectly accessible from measured data, posing identification challenges.</li> <li>Output Dynamics: Complex inversion due to transmission zero presence in \\(S(s)\\).</li> </ol> <p>The goal of this benchmark is to develop and validate robust models using separate estimation data, ensuring accurate characterization of the Wiener-Hammerstein system's behavior.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_2","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_2","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Wiener-Hammerstein system dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for Wiener-Hammerstein system.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares, BoundedVariableLeastSquares, NonNegativeLeastSquares, LeastSquaresMinimalResidual\n\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.WienerHammerBenchMark(atleast_2d=True)\nx_train, y_train = train_val\nx_test, y_test = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plot_n = 800\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_train[:plot_n])\nplt.plot(y_train[:plot_n])\nplt.title(\"Experiment: training data\")\nplt.legend([\"x_train\", \"y_train\"])\nplt.show()\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_test[:plot_n])\nplt.plot(y_test[:plot_n])\nplt.title(\"Experiment: testing data\")\nplt.legend([\"x_test\", \"y_test\"])\nplt.show()\n</code></pre> <p></p> <p></p> <p>The goal of this benchmark it to get a model that have a better performance than the SOTA model provided in the benchmarking paper.</p> <p></p> <p>State of the art results presented in the benchmarking paper. In this section we are only working with the Wiener-Hammerstein results, which are presented in the \\(W-H\\)  column.</p>"},{"location":"book/10-Case-Studies/#results_1","title":"Results","text":"<p>We will start with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=50</code>. This means the FROLS algorithm will not include all regressors when calculating the information criteria used to determine the model order. While this approach might result in a suboptimal model, it is a reasonable starting point for our first attempt.</p> <pre><code>n = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nrmse_sota = rmse/y_test.std()\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=1000, title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\")\n</code></pre> <p></p> <p>The first configuration is already better than the SOTA models shown in the benchmark table! We started using <code>xlag=ylag=7</code> to have an idea of how well SysIdentPy would handle this dataset, but the results are pretty good already! However, the benchmarking paper indicates  that they used higher lags for their models. Let's check what happens if we set <code>xlag=ylag=10</code>.</p> <pre><code>x_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=10,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nrmse_sota = rmse/y_test.std()\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=1000, title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\")\n</code></pre> <p></p> <p>The performance is even better now! For now, we are not worried about the model complexity (even in this case where we are comparing to a deep state neural network...). However, if we check the model order and the <code>AIC</code> plot, we see that the model have 50 regressors , but the <code>AIC</code> values do not change much after each added regression.</p> <pre><code>plt.plot(model.info_values)\n</code></pre> <p></p> <p>So, what happens if we set a model with half of the regressors?</p> <pre><code>x_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=10,\n\u00a0 \u00a0 ylag=10,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 n_info_values=50,\n\u00a0 \u00a0 n_terms=25,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nrmse_sota = rmse/y_test.std()\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=1000, title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\")\n</code></pre> <p></p> <p>As shown in the figure above, the results still outperform the SOTA models presented in the benchmarking paper. The SOTA results from the paper could likely be improved as well. Users are encouraged to explore the deepsysid package, which can be used to build deep state neural networks.</p> <p>This basic configuration can serve as a starting point for users to develop even better models using SysIdentPy. Give it a try!</p>"},{"location":"book/10-Case-Studies/#air-passenger-demand-forecasting-a-benchmarking","title":"Air Passenger Demand Forecasting - A Benchmarking","text":"<p>In this case study, we explore the capabilities of SysIdentPy by applying it to the Air Passenger dataset, a classic time series dataset widely used for evaluating time series forecasting methods. The primary goal of this analysis is to demonstrate that SysIdentPy can serve as a strong alternative for time series modeling, rather than to assert that one library is superior to another.</p>"},{"location":"book/10-Case-Studies/#dataset-overview","title":"Dataset Overview","text":"<p>The Air Passenger dataset consists of monthly totals of international airline passengers from 1949 to 1960. This dataset is characterized by its strong seasonal patterns, trend components, and variability, making it an ideal benchmark for evaluating various time series forecasting methods. Specifically, the dataset includes:</p> <ul> <li>Total Monthly Passengers: The number of passengers (in thousands) for each month.</li> <li>Time Period: From January 1949 to December 1960, providing 144 data points.</li> </ul> <p>The dataset exhibits clear seasonal fluctuations and a trend, which poses a significant challenge for forecasting methods. It serves as a well-known benchmark for assessing the performance of different time series models due to its inherent complexity and well-documented behavior.</p>"},{"location":"book/10-Case-Studies/#comparison-with-other-libraries","title":"Comparison with Other Libraries","text":"<p>We will compare the performance of SysIdentPy with other popular time series modeling libraries, focusing on the following tools:</p> <ul> <li>sktime: An extensive library for time series analysis in Python, offering various modeling techniques. For this case study, we will use:</li> <li><code>AutoARIMA</code>: Automatically selects the best ARIMA model based on the data.</li> <li><code>BATS</code> (Bayesian Structural Time Series): A model that captures complex seasonal patterns and trends.</li> <li><code>TBATS</code> (Trigonometric, Box-Cox, ARMA, Trend, and Seasonal): A model designed to handle multiple seasonal patterns.</li> <li><code>Exponential Smoothing</code>: A method that applies weighted averages to forecast future values.</li> <li><code>Prophet</code>: Developed by Facebook, it is particularly effective for capturing seasonality and holiday effects.</li> <li> <p><code>AutoETS</code> (Automatic Exponential Smoothing): Selects the best exponential smoothing model for the data.</p> </li> <li> <p>SysIdentPy: A library designed for system identification and time series modeling. We will focus on:</p> </li> <li><code>MetaMSS</code> (Meta-heuristic Model Structure Selection): Uses metaheuristic algorithms to select the best model structure.</li> <li><code>AOLS</code> (Accelerated Orthogonal Least Squares): A method for selecting relevant regressors in a model.</li> <li><code>FROLS</code> (Forward Regression with Orthogonal Least Squares, using polynomial base functions): A regression technique for model structure selection with polynomial terms.</li> <li><code>NARXNN</code> (Nonlinear Auto-Regressive model with Exogenous Inputs using Neural Networks): A flexible method for modeling nonlinear time series with external inputs.</li> </ul>"},{"location":"book/10-Case-Studies/#objective","title":"Objective","text":"<p>The objective of this case study is to evaluate and compare the performance of these methods on the Air Passenger dataset. We aim to assess how well each library handles the complex seasonal and trend components of the data and to showcase SysIdentPy as a viable option for time series forecasting.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_3","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npystan==2.19.1.1\nholidays==0.11.2\nfbprophet==0.7.1\nneuralprophet==0.2.7\npandas==1.3.2\nnumpy==1.23.3\nmatplotlib==3.8.4\npmdarima==1.8.3\nscikit-learn==0.24.2\nscipy==1.9.1\nsktime==0.8.0\nstatsmodels==0.12.2\ntbats==1.1.0\ntorch==1.12.1\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions. This practice isolates your project\u2019s dependencies and prevents version conflicts with other projects or system-wide packages. Additionally, be aware that some packages, such as <code>sktime</code> and <code>neuralprophet</code>, may install several dependencies automatically during their installation. Setting up a virtual environment helps manage these dependencies more effectively and keeps your project environment clean and reproducible.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul> <p>Let's begin by importing the necessary packages and setting up the environment for this analysis.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.signal.signaltools\n\ndef _centered(arr, newsize):\n\u00a0 \u00a0 # Return the center newsize portion of the array.\n\u00a0 \u00a0 # this is needed due a conflict error using the versions of the packages defined\n\u00a0 \u00a0 # for this example\n\u00a0 \u00a0 newsize = np.asarray(newsize)\n\u00a0 \u00a0 currsize = np.array(arr.shape)\n\u00a0 \u00a0 startind = (currsize - newsize) // 2\n\u00a0 \u00a0 endind = startind + newsize\n\u00a0 \u00a0 myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n\u00a0 \u00a0 return arr[tuple(myslice)]\n\n\nscipy.signal.signaltools._centered = _centered\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom torch import nn\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.ets import AutoETS\nfrom sktime.forecasting.arima import ARIMA, AutoARIMA\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.fbprophet import Prophet\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.bats import BATS\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.performance_metrics.forecasting import mean_squared_error\nfrom sktime.utils.plotting import plot_series\n\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n%matplotlib inline\nloss = mean_squared_error\n</code></pre> <p>We use the <code>sktime</code> method to load the data. Besides, 23 samples is used as test data, following the definitions in the <code>sktime</code> examples.</p> <pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23) \u00a0# 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</code></pre> <p>The following image shows the data of the system to be modeled.</p> <p></p>"},{"location":"book/10-Case-Studies/#results_2","title":"Results","text":"<p>Because we have several different models to test, the results are summarized in the following table. The user you will see that no hyperparameter tuning was made for SysIdentPy model. The idea here is to show how simple it can be to build good models in SysIdentPy.</p> No. Package Mean Squared Error 1 SysIdentPy (Neural Model) 316.54 2 SysIdentPy (MetaMSS) 450.99 3 SysIdentPy (AOLS) 476.64 4 NeuralProphet 501.24 5 SysIdentPy (FROLS) 805.95 6 Exponential Smoothing 910.52 7 Prophet 1186.00 8 AutoArima 1714.47 9 Manual Arima 2085.42 10 ETS 2590.05 11 BATS 7286.64 12 TBATS 7448.43"},{"location":"book/10-Case-Studies/#sysidentpy-frols","title":"SysIdentPy: FROLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 ylag=13, \u00a0# the lags for all models will be 13\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NAR\",\n)\n\nsysidentpy.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\nyhat = sysidentpy.predict(y=y_test, forecast_horizon=23)\nfrols_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(frols_loss)\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n&gt;&gt;&gt; 805.95\n</code></pre>"},{"location":"book/10-Case-Studies/#sysidentpy-aols","title":"SysIdentPy: AOLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\ndf_train, df_test = temporal_train_test_split(y, test_size=23)\ndf_train = df_train.reset_index()\ndf_train.columns = [\"ds\", \"y\"]\ndf_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"].astype(str))\ndf_test = df_test.reset_index()\ndf_test.columns = [\"ds\", \"y\"]\ndf_test[\"ds\"] = pd.to_datetime(df_test[\"ds\"].astype(str))\n\nsysidentpy_AOLS = AOLS(\n\u00a0 \u00a0 ylag=13, k=2, L=1, model_type=\"NAR\", basis_function=basis_function\n)\n\nsysidentpy_AOLS.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\nyhat = sysidentpy_AOLS.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\n\naols_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\n\nprint(aols_loss)\nplot_results(y=y_test[sysidentpy_AOLS.max_lag :], yhat=yhat[sysidentpy_AOLS.max_lag :])\n&gt;&gt;&gt; 476.64\n</code></pre>"},{"location":"book/10-Case-Studies/#sysidentpy-metamss","title":"SysIdentPy: MetaMSS","text":"<pre><code>set_random_seed(42)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nsysidentpy_metamss = MetaMSS(\n\u00a0 \u00a0 basis_function=basis_function, ylag=13, model_type=\"NAR\", test_size=0.17\n)\n\nsysidentpy_metamss.fit(y=y_train)\n\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\nyhat = sysidentpy_metamss.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\n\nmetamss_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\n\nprint(metamss_loss)\nplot_results(\n\u00a0 \u00a0 y=y_test[sysidentpy_metamss.max_lag :], yhat=yhat[sysidentpy_metamss.max_lag :]\n)\n\n&gt;&gt;&gt; 450.99\n</code></pre>"},{"location":"book/10-Case-Studies/#sysidentpy-neural-narx","title":"SysIdentPy: Neural NARX","text":"<p>The network architecture is just the same as the one used in to show how to build a Neural NARX model in SysIdentPy docs.</p> <pre><code>import torch\ntorch.manual_seed(42)\n\ny = load_airline()\n# the split here will use 36 as test size just because the network will use the first values as initial conditions. It could be done like the others methods by concatenating the values\ny_train, y_test = temporal_train_test_split(y, test_size=36)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nx_train = np.zeros_like(y_train)\nx_test = np.zeros_like(y_test)\n\nclass NARX(nn.Module):\n\u00a0 \u00a0 def __init__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin = nn.Linear(13, 20)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin2 = nn.Linear(20, 20)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin3 = nn.Linear(20, 20)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin4 = nn.Linear(20, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 self.relu = nn.ReLU()\n\n\n\u00a0 \u00a0 def forward(self, xb):\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin(xb)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.relu(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin2(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.relu(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin3(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.relu(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin4(z)\n\u00a0 \u00a0 \u00a0 \u00a0 return z\n\nnarx_net = NARXNN(\n\u00a0 \u00a0 net=NARX(),\n\u00a0 \u00a0 ylag=13,\n\u00a0 \u00a0 model_type=\"NAR\",\n\u00a0 \u00a0 basis_function=Polynomial(degree=1),\n\u00a0 \u00a0 epochs=900,\n\u00a0 \u00a0 verbose=False,\n\u00a0 \u00a0 learning_rate=2.5e-02,\n\u00a0 \u00a0 optim_params={}, \u00a0# optional parameters of the optimizer\n)\n\nnarx_net.fit(y=y_train)\nyhat = narx_net.predict(y=y_test, forecast_horizon=23)\n\nnarxnet_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[narx_net.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[narx_net.max_lag :]),\n)\n\nprint(narxnet_loss)\nplot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :])\n</code></pre> <p></p>"},{"location":"book/10-Case-Studies/#sktime-models","title":"sktime models","text":"<p>The following models are the ones available in the sktime package.</p> <pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23) \u00a0# 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-exponential-smoothing","title":"sktime: Exponential Smoothing","text":"<pre><code>es = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nes.fit(y_train)\ny_pred_es = es.predict(fh)\nplot_series(y_test, y_pred_es, labels=[\"y_test\", \"y_pred\"])\nes_loss = loss(y_test, y_pred_es)\nes_loss\n&gt;&gt;&gt; 910.46\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-autoets","title":"sktime: AutoETS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nets = AutoETS(auto=True, sp=12, n_jobs=-1)\nets.fit(y_train)\ny_pred_ets = ets.predict(fh)\nplot_series(y_test, y_pred_ets, labels=[\"y_test\", \"y_pred\"])\nets_loss = loss(y_test, y_pred_ets)\nets_loss\n&gt;&gt;&gt; 1739.11\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-autoarima","title":"sktime: AutoArima","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\n\nauto_arima = AutoARIMA(sp=12, suppress_warnings=True)\nauto_arima.fit(y_train)\ny_pred_auto_arima = auto_arima.predict(fh)\nplot_series(y_test, y_pred_auto_arima, labels=[\"y_test\", \"y_pred\"])\nautoarima_loss = loss(y_test, y_pred_auto_arima)\nautoarima_loss\n&gt;&gt;&gt; 1714.47\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-arima","title":"sktime: Arima","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nmanual_arima = ARIMA(\n\u00a0 \u00a0 order=(13, 1, 0), suppress_warnings=True\n) \u00a0# seasonal_order=(0, 1, 0, 12)\nmanual_arima.fit(y_train)\ny_pred_manual_arima = manual_arima.predict(fh)\nplot_series(y_test, y_pred_manual_arima, labels=[\"y_test\", \"y_pred\"])\nmanualarima_loss = loss(y_test, y_pred_manual_arima)\nmanualarima_loss\n&gt;&gt;&gt; 2085.42\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-bats","title":"sktime: BATS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nbats = BATS(sp=12, use_trend=True, use_box_cox=False)\nbats.fit(y_train)\ny_pred_bats = bats.predict(fh)\nplot_series(y_test, y_pred_bats, labels=[\"y_test\", \"y_pred\"])\nbats_loss = loss(y_test, y_pred_bats)\nbats_loss\n&gt;&gt;&gt; 7286.64\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-tbats","title":"sktime: TBATS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ntbats = TBATS(sp=12, use_trend=True, use_box_cox=False)\ntbats.fit(y_train)\ny_pred_tbats = tbats.predict(fh)\nplot_series(y_test, y_pred_tbats, labels=[\"y_test\", \"y_pred\"])\ntbats_loss = loss(y_test, y_pred_tbats)\ntbats_loss\n&gt;&gt;&gt; 7448.43\n</code></pre>"},{"location":"book/10-Case-Studies/#sktime-prophet","title":"sktime: Prophet","text":"<pre><code>set_random_seed(42)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nz = y.copy()\nz = z.to_timestamp(freq=\"M\")\nz_train, z_test = temporal_train_test_split(z, test_size=23)\nprophet = Prophet(\n\u00a0 \u00a0 seasonality_mode=\"multiplicative\",\n\u00a0 \u00a0 n_changepoints=int(len(y_train) / 12),\n\u00a0 \u00a0 add_country_holidays={\"country_name\": \"Germany\"},\n\u00a0 \u00a0 yearly_seasonality=True,\n\u00a0 \u00a0 weekly_seasonality=False,\n\u00a0 \u00a0 daily_seasonality=False,\n)\nprophet.fit(z_train)\ny_pred_prophet = prophet.predict(fh.to_relative(cutoff=y_train.index[-1]))\ny_pred_prophet.index = y_test.index\nplot_series(y_test, y_pred_prophet, labels=[\"y_test\", \"y_pred\"])\nprophet_loss = loss(y_test, y_pred_prophet)\nprophet_loss\n&gt;&gt;&gt; 1186.00\n</code></pre>"},{"location":"book/10-Case-Studies/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\ndf = pd.read_csv(r\".\\datasets\\air_passengers.csv\")\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\ndf_train = df.iloc[:-23, :].copy()\ndf_test = df.iloc[-23:, :].copy()\n\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\nmetrics = m.fit(df_train, freq=\"MS\")\nfuture = m.make_future_dataframe(\n\u00a0 \u00a0 df_train, periods=23, n_historic_predictions=len(df_train)\n)\nforecast = m.predict(future)\nplt.plot(forecast[\"yhat1\"].values[-23:])\nplt.plot(df_test[\"y\"].values)\nneuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values)\nneuralprophet_loss\n&gt;&gt;&gt; 501.24\n</code></pre> <p>The final results can be summarized as follows, resulting in the table presented in the beginning of this case study:</p> <pre><code>results = {\n\u00a0 \u00a0 \"Exponential Smoothing\": es_loss,\n\u00a0 \u00a0 \"ETS\": ets_loss,\n\u00a0 \u00a0 \"AutoArima\": autoarima_loss,\n\u00a0 \u00a0 \"Manual Arima\": manualarima_loss,\n\u00a0 \u00a0 \"BATS\": bats_loss,\n\u00a0 \u00a0 \"TBATS\": tbats_loss,\n\u00a0 \u00a0 \"Prophet\": prophet_loss,\n\u00a0 \u00a0 \"SysIdentPy (Polynomial Model)\": frols_loss,\n\u00a0 \u00a0 \"SysIdentPy (Neural Model)\": narxnet_loss,\n\u00a0 \u00a0 \"SysIdentPy (AOLS)\": aols_loss,\n\u00a0 \u00a0 \"SysIdentPy (MetaMSS)\": metamss_loss,\n\u00a0 \u00a0 \"NeuralProphet\": neuralprophet_loss,\n}\nsorted(results.items(), key=lambda result: result[1])\n</code></pre>"},{"location":"book/10-Case-Studies/#system-with-hysteresis-modeling-a-magneto-rheological-damper-device","title":"System With Hysteresis - Modeling a Magneto-rheological Damper Device","text":"<p>The memory effects between quasi-static input and output make the modeling of hysteretic systems very difficult. Physics-based models are often used to describe the hysteresis loops, but these models usually lack the simplicity and efficiency required in practical applications involving system characterization, identification, and control. As detailed in Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models, NARX models have proven to be a feasible choice to describe the hysteresis loops. See Chapter 8 for a detailed background. However, even considering the sufficient conditions for rate independent hysteresis representation, classical structure selection algorithms fails to return a model with decent performance and the user needs to set a multi-valued function to ensure the occurrence of the bounding structure \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models).</p> <p>Even though some progress has been made, previous work has been limited to models with a single equilibrium point. The present case study aims to present new prospects in the model structure selection of hysteretic systems regarding the cases where the models have multiple inputs, and it is not restricted concerning the number of equilibrium points. For that, the MetaMSS algorithm will be used to build a model for a magneto-rheological damper (MRD) considering the mentioned sufficient conditions.</p>"},{"location":"book/10-Case-Studies/#a-brief-description-of-the-bouc-wen-model-of-magneto-rheological-damper-device","title":"A Brief description of the Bouc-Wen model of magneto-rheological damper device","text":"<p>The data used in this study-case is the Bouc-Wen model (Bouc, R - Forced Vibrations of a Mechanical System with Hysteresis), (Wen, Y. X. - Method for Random Vibration of Hysteretic Systems) of an MRD whose schematic diagram is shown in the figure below.</p> <p></p> <p>The model for a magneto-rheological damper proposed by Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback.</p> <p>The general form of the Bouc-Wen model can be described as (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback):</p> \\[ \\begin{equation} \\dfrac{dz}{dt} = g\\left[x,z,sign\\left(\\dfrac{dx}{dt}\\right)\\right]\\dfrac{dx}{dt}, \\end{equation} \\] <p>where \\(z\\) is the hysteretic model output, \\(x\\) the input and \\(g[\\cdot]\\) a nonlinear function of \\(x\\), \\(z\\) and \\(sign (dx/dt)\\). (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback) proposed the following phenomenological model for the aforementioned device:</p> \\[ \\begin{align} f&amp;= c_1\\dot{\\rho}+k_1(x-x_0),\\nonumber\\\\ \\dot{\\rho}&amp;=\\dfrac{1}{c_0+c_1}[\\alpha z+c_0\\dot{x}+k_0(x-\\rho)],\\nonumber\\\\ \\dot{z}&amp;=-\\gamma|\\dot{x}-\\dot{\\rho}|z|z|^{n-1}-\\beta(\\dot{x}-\\dot{\\rho})|z|^n+A(\\dot{x}-\\dot{\\rho}),\\nonumber\\\\ \\alpha&amp;=\\alpha_a+\\alpha_bu_{bw},\\nonumber\\\\ c_1&amp;=c_{1a}+c_{1b}u_{bw},\\nonumber\\\\ c_0&amp;=c_{0a}+c_{0b}u_{bw},\\nonumber\\\\ \\dot{u}_{bw}&amp;=-\\eta(u_{bw}-E). \\end{align} \\] <p>where \\(f\\) is the damping force, \\(c_1\\) and \\(c_0\\) represent the viscous coefficients, \\(E\\) is the input voltage, \\(x\\) is the displacement and \\(\\dot{x}\\) is the velocity of the model. The parameters of the system (see table below) were taken from Leva, A. and Piroddi, L. - NARX-based technique for the modelling of magneto-rheological damping devices.</p> Parameter Value Parameter Value \\(c_{0_a}\\) \\(20.2 \\, N \\, s/cm\\) \\(\\alpha_{a}\\) \\(44.9 \\, N/cm\\) \\(c_{0_b}\\) \\(2.68 \\, N \\, s/cm \\, V\\) \\(\\alpha_{b}\\) \\(638 \\, N/cm\\) \\(c_{1_a}\\) \\(350 \\, N \\, s/cm\\) \\(\\gamma\\) \\(39.3 \\, cm^{-2}\\) \\(c_{1_b}\\) \\(70.7 \\, N \\, s/cm \\, V\\) \\(\\beta\\) \\(39.3 \\, cm^{-2}\\) \\(k_{0}\\) \\(15 \\, N/cm\\) \\(n\\) \\(2\\) \\(k_{1}\\) \\(5.37 \\, N/cm\\) \\(\\eta\\) \\(251 \\, s^{-1}\\) \\(x_{0}\\) \\(0 \\, cm\\) \\(A\\) \\(47.2\\) <p>For this particular study, both displacement and voltage inputs, \\(x\\) and \\(E\\), respectively, were generated by filtering a white Gaussian noise sequence using a Blackman-Harris FIR filter with \\(6\\)Hz cutoff frequency. The integration step-size was set to \\(h = 0.002\\), following the procedures described in Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models. These procedures are for identification purposes only since the inputs of a MRD could have several different characteristics.</p> <p>The data used in this example is provided by the Professor Samir Angelo Milani Martins.</p> <p>The challenges are:</p> <ul> <li>it possesses a nonlinearity featuring memory, i.e. a dynamic nonlinearity;</li> <li>the nonlinearity is governed by an internal variable z(t), which is not measurable;</li> <li>the nonlinear functional form in the Bouc Wen equation is nonlinear in the parameter;</li> <li>the nonlinear functional form in the Bouc Wen equation does not admit a finite Taylor series expansion because of the presence of absolute values</li> </ul>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_4","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nscikit-learn==1.4.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_3","title":"SysIdentPy Configuration","text":"<pre><code>import numpy as np\nfrom sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\ndf = pd.read_csv(\"boucwen_histeretic_system.csv\")\nscaler_x = MaxAbsScaler()\nscaler_y = MaxAbsScaler()\n\ninit = 400\nx_train = df[[\"E\", \"v\"]].iloc[init:df.shape[0]//2, :]\nx_train[\"sign_v\"] = np.sign(df[\"v\"])\nx_train = scaler_x.fit_transform(x_train)\n\nx_test = df[[\"E\", \"v\"]].iloc[df.shape[0]//2 + 1:df.shape[0] - init, :]\nx_test[\"sign_v\"] = np.sign(df[\"v\"])\nx_test = scaler_x.transform(x_test)\n\ny_train = df[[\"f\"]].iloc[init:df.shape[0]//2, :].values.reshape(-1, 1)\ny_train = scaler_y.fit_transform(y_train)\n\ny_test = df[[\"f\"]].iloc[df.shape[0]//2 + 1:df.shape[0] - init, :].values.reshape(-1, 1)\ny_test = scaler_y.transform(y_test)\n\n# Plotting the data\nplt.figure(figsize=(10, 8))\nplt.suptitle('Identification (training) data', fontsize=16)\n\nplt.subplot(221)\nplt.plot(y_train, 'k')\nplt.ylabel('Force - Output')\nplt.xlabel('Samples')\nplt.title('y')\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(222)\nplt.plot(x_train[:, 0], 'k')\nplt.ylabel('Control Voltage')\nplt.xlabel('Samples')\nplt.title('x_1')\nplt.grid()\nplt.axis([0, 1500, 0, 1])\n\nplt.subplot(223)\nplt.plot(x_train[:, 1], 'k')\nplt.ylabel('Velocity')\nplt.xlabel('Samples')\nplt.title('x_2')\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(224)\nplt.plot(x_train[:, 2], 'k')\nplt.ylabel('sign(Velocity)')\nplt.xlabel('Samples')\nplt.title('x_3')\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n</code></pre> <p>Let's check how is the hysteretic behavior considering each input: <pre><code>plt.plot(x_train[:, 0], y_train)\nplt.xlabel(\"x1 - Voltage\")\nplt.ylabel(\"y - Force\")\n</code></pre></p> <p></p> <pre><code>plt.plot(x_train[:, 1], y_train)\nplt.xlabel(\"x2 - Velocity\")\nplt.ylabel(\"y - Force\")\n</code></pre> <p></p> <pre><code>plt.plot(x_train[:, 2], y_train)\nplt.xlabel(\"u3 - sign(Velocity)\")\nplt.ylabel(\"y - Force\")\n</code></pre> <p></p> <p>Now, we can just build a NARX model:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=[[1], [1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"FROLS: sign(v) and MaxAbsScaler\")\n&gt;&gt;&gt; 0.0450\n</code></pre> <p></p> <p>If we remove the <code>sign(v)</code> input and try to build a NARX model using the same configuration, the model diverge, as can be seen in the following figure:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=[[1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"FROLS: MaxAbsScaler, discarding sign(v)\")\n&gt;&gt;&gt; nan\n</code></pre> <p></p> <p>If we use the <code>MetaMSS</code> algorithm instead, the results are better.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\nbasis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n\u00a0 \u00a0 xlag=[[1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 random_state=42,\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"MetaMSS: MaxAbsScaler, discarding sign(v)\")\n&gt;&gt;&gt; 0.24\n</code></pre> <p></p> <p>However, when the output of the system reach its minimum value, the model oscillate</p> <pre><code>plot_results(y=y_test[1100 : 1200], yhat=yhat[1100 : 1200], n=10000, title=\"Unstable region\")\n</code></pre> <p></p> <p>If we add the <code>sign(v)</code> input again and use <code>MetaMSS</code>, the results are very close to the <code>FROLS</code> algorithm with all inputs</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n\u00a0 \u00a0 xlag=[[1], [1], [1]],\n\u00a0 \u00a0 ylag=1,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=10000, title=\"MetaMSS: sign(v) and MaxAbsScaler\")\n&gt;&gt;&gt; 0.0554\n</code></pre> <p></p> <p>This case will also highlight the significance of data scaling. Previously, we used the <code>MaxAbsScaler</code> method, which resulted in great models when using the <code>sign(v)</code> inputs, but also resulted in unstable models when removing that input feature. When scaling is applied using <code>MinMaxScaler</code>, however, the overall stability of the results improves, and the model does not diverge, even when the <code>sign(v)</code> input is removed, using the <code>FROLS</code> algorithm.</p> <p>The user can get the results bellow by just changing the data scaling method using</p> <pre><code>scaler_x = MinMaxScaler()\nscaler_y = MinMaxScaler()\n</code></pre> <p>and running each model again. That is the only change to improve the results.</p> <p></p> <p>FROLS: with <code>sign(v)</code> and <code>MinMaxScaler</code>. RMSE: 0.1159</p> <p> FROLS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1639</p> <p></p> <p>MetaMSS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1762</p> <p></p> <p>MetaMSS: including <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.0694</p> <p>In contrast, the MetaMSS method returned the best model overall, but not better than the best <code>FROLS</code> method using <code>MaxAbsScaler</code>.</p> <p>Here is the predicted hysteretic loop: <pre><code>plt.plot(x_test[:, 1], yhat)\n</code></pre></p> <p></p>"},{"location":"book/10-Case-Studies/#silver-box","title":"Silver box","text":"<p>The description content mainly derives (copy and paste) from the associated paper - Three free data sets for development and benchmarking in nonlinear system identification. For a detailed description, readers are referred to the linked reference.</p> <p>The Silverbox system can be seen as an electronic implementation of the Duffing oscillator. It is build as a 2<sup>nd</sup> order linear time-invariant system with a 3<sup>rd</sup> degree polynomial static nonlinearity around it in feedback. This type of dynamics are, for instance, often encountered in mechanical systems Nonlinear Benchmark - Silverbox.</p> <p>In this case study, we will create a NARX model for the Silver box benchmark. The Silver box represents a simplified version of mechanical oscillating processes, which are a critical category of nonlinear dynamic systems. Examples include vehicle suspensions, where shock absorbers and progressive springs play vital roles. The data generated by the Silver box provides a simplified representation of such combined components. The electrical circuit generating this data closely approximates, but does not perfectly match, the idealized models described below.</p> <p>As described in the original paper, the system was excited using a general waveform generator (HPE1445A). The input signal begins as a discrete-time signal \\(r(k)\\), which is converted to an analog signal \\(r_c(t)\\) using zero-order-hold reconstruction. The actual excitation signal \\(u_0(t)\\) is then obtained by passing \\(r_c(t)\\) through an analog low-pass filter \\(G(p)\\) to eliminate high-frequency content around multiples of the sampling frequency. Here, \\(p\\) denotes the differentiation operator. Thus, the input is given by:</p> \\[ u_0(t) = G(p) r_c(t). \\] <p>The input and output signals were measured using HP1430A data acquisition cards, with synchronized clocks for the acquisition and generator cards. The sampling frequency was:</p> \\[ f_s = \\frac{10^7}{2^{14}} = 610.35 \\, \\text{Hz}. \\] <p>The silver box uses analog electrical circuitry to generate data representing a nonlinear mechanical resonating system with a moving mass \\(m\\), viscous damping \\(d\\), and a nonlinear spring \\(k(y)\\). The electrical circuit is designed to relate the displacement \\(y(t)\\) (the output) to the force \\(u(t)\\) (the input) by the following differential equation:</p> \\[ m \\frac{d^2 y(t)}{dt^2} + d \\frac{d y(t)}{dt} + k(y(t)) y(t) = u(t). \\] <p>The nonlinear progressive spring is described by a static, position-dependent stiffness:</p> \\[ k(y(t)) = a + b y^2(t). \\] <p>The signal-to-noise ratio is sufficiently high to model the system without accounting for measurement noise. However, measurement noise can be included by replacing \\(y(t)\\) with the artificial variable \\(x(t)\\) in the equation above, and introducing disturbances \\(w(t)\\) and \\(e(t)\\) as follows:</p> \\[ \\begin{align} &amp; m \\frac{d^2 x(t)}{dt^2} + d \\frac{d x(t)}{dt} + k(x(t)) x(t) = u(t) + w(t), \\\\ &amp; k(x(t)) = a + b x^2(t), \\\\ &amp; y(t) = x(t) + e(t). \\end{align} \\]"},{"location":"book/10-Case-Studies/#required-packages-and-versions_5","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_4","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.Silverbox(atleast_2d=True)\n\nx_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_multisine.u, test_multisine.y\n\nn = test_multisine.state_initialization_window_length\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation - GerbenBeintema/nonlinear_benchmarks: The official data load for http://www.nonlinearbenchmark.org/ (github.com) to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plt.plot(x_train)\nplt.plot(y_train, alpha=0.3)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(x_test)\nplt.plot(y_test, alpha=0.3)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(test_arrow_full.u)\nplt.plot(test_arrow_full.y, alpha=0.3)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(test_arrow_no_extrapolation.u)\nplt.plot(test_arrow_no_extrapolation.y, alpha=0.2)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Important Note</p> <p>The goal of this benchmark is to develop a model that outperforms the state-of-the-art (SOTA) model presented in the benchmarking paper. However, the results in the paper differ from those provided in the  GitHub repository.</p> nx Set NRMS RMS (mV) 2 Train 0.10653 5.8103295 2 Validation 0.11411 6.1938068 2 Test 0.19151 10.2358533 2 Test (no extra) 0.12284 5.2789727 4 Train 0.03571 1.9478290 4 Validation 0.03922 2.1286373 4 Test 0.12712 6.7943448 4 Test (no extra) 0.05204 2.2365904 8 Train 0.03430 1.8707026 8 Validation 0.03732 2.0254112 8 Test 0.10826 5.7865255 8 Test (no extra) 0.04743 2.0382715 &gt; Table: results presented in the github. <p>It appears that the values shown in the paper actually represent the training time, not the error metrics. I will contact the authors to confirm this information. According to the Nonlinear Benchmark website, the information is as follows:</p> <p></p> <p>where the values in the \"Training time\" column matches the ones presented as error metrics in the paper.</p> <p>While we await confirmation of the correct values for this benchmark, we will demonstrate the performance of SysIdentPy. However, we will refrain from making any comparisons or attempting to improve the model at this stage.</p>"},{"location":"book/10-Case-Studies/#results_3","title":"Results","text":"<p>We will start (as we did in every other case study) with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=40</code>. Because we're dealing with a large training dataset, we will use the <code>err_tol</code> instead of information criteria to have a faster performance. We will also set <code>n_terms=40</code>, which means that the search will stop if the <code>err_tol</code> is reached or 40 regressors is tested in the <code>ERR</code> algorithm. While this approach might result in a suboptimal model, it is a reasonable starting point for our first attempt. There are three different experiments: multi sine, arrow (full), and arrow (no extrapolation).</p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_multisine.u, test_multisine.y\n\nn = test_multisine.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.999,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag:], y_test])\nx_test = np.concatenate([x_train[-model.max_lag:], x_test])\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nnrmse = rmse/y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=300, figsize=(15, 4), title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\n&gt; 0.1423804033714937\n&gt; 7.727682109791501\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_full.u, test_arrow_full.y\n\nn = test_arrow_full.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9999,\n\u00a0 \u00a0 n_terms=80,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\n# we will not concatente the last values from train data to use as initial condition here because\n# this test data have a very different behavior.\n# However, if you want you can do that and you will see that the model will still perform\n# great after a few iterations\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nnrmse = rmse/y_test.std()\nrmse_mv = 1000 * rmse\n\nprint(nrmse, rmse_mv)\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=300, figsize=(15, 4), title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_no_extrapolation.u, test_arrow_no_extrapolation.y\n\nn = test_arrow_no_extrapolation.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n\u00a0 \u00a0 xlag=14,\n\u00a0 \u00a0 ylag=14,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 estimator=LeastSquares(),\n\u00a0 \u00a0 err_tol=0.9999,\n\u00a0 \u00a0 n_terms=40,\n\u00a0 \u00a0 order_selection=False\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[:model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n:])\nnrmse = rmse/y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=30000, figsize=(15, 4), title=f\"Arrow (no extrapolation). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :], n=300, figsize=(15, 4), title=f\"Free Run simulation. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\")\n</code></pre> <p></p> <p></p>"},{"location":"book/10-Case-Studies/#f-16-ground-vibration-test-benchmark","title":"F-16 Ground Vibration Test Benchmark","text":"<p>The following examples are intended to demonstrate the application of SysIdentPy on a real-world dataset. Please note that these examples are not aimed at replicating the results presented in the cited manuscripts. The model parameters, such as <code>ylag</code> and <code>xlag</code>, as well as the size of the identification and validation data sets, differ from those used in the original studies. Additionally, adjustments related to sampling rates and other data preparation steps are not covered in this notebook.</p> <p>For a comprehensive reference regarding the F-16 Ground Vibration Test benchmark, please visit the nonlinear benchmark website.</p> <p>Note: This notebook serves as a preliminary demonstration of SysIdentPy's performance on the F-16 dataset. A more detailed analysis will be provided in a future publication. The nonlinear benchmark website offers valuable resources and references related to system identification and machine learning, and readers are encouraged to explore the papers and information available there.</p>"},{"location":"book/10-Case-Studies/#benchmark-overview","title":"Benchmark Overview","text":"<p>The F-16 Ground Vibration Test benchmark is a notable experiment in the field of system identification and nonlinear dynamics. It involves a high-order system with clearance and friction nonlinearities at the mounting interfaces of payloads on a full-scale F-16 aircraft.</p> <p>Experiment Details: - Event: Siemens LMS Ground Vibration Testing Master Class - Date: September 2014 - Location: Saffraanberg military base, Sint-Truiden, Belgium</p> <p>During the test, two dummy payloads were mounted on the wing tips of the F-16 to simulate the mass and inertia of real devices typically equipped on the aircraft during flight. Accelerometers were installed on the aircraft structure to capture vibration data. A shaker was placed under the right wing to apply input signals. The key source of nonlinearity in the system was identified as the mounting interfaces of the payloads, particularly the right-wing-to-payload interface, which exhibited significant nonlinear distortions.</p> <p>Data and Resources: - Data Availability: The dataset, including detailed system descriptions, estimation and test data sets, and setup images, is available for download in both .csv and .mat file formats. - Reference: For in-depth information on the F-16 benchmark, refer to: J.P. No\u00ebl and M. Schoukens, \"F-16 aircraft benchmark based on ground vibration test data,\" 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> <p>The goal of this notebook is to illustrate how SysIdentPy can be applied to such complex datasets, showcasing its capabilities in modeling and analysis. For a thorough exploration of the benchmark and its methodologies, please consult the provided resources and references.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_6","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#sysidentpy-configuration_5","title":"SysIdentPy Configuration","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n\u00a0 \u00a0 compute_residues_autocorrelation,\n\u00a0 \u00a0 compute_cross_correlation,\n)\n</code></pre>"},{"location":"book/10-Case-Studies/#procedure","title":"Procedure","text":"<p><pre><code>f_16 = pd.read_csv(r\"examples/datasets/f-16.txt\", header=None, names=[\"x1\", \"x2\", \"y\"])\nf_16.shape\nf_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n\n&gt;&gt;&gt; (32768, 3)\n</code></pre> </p> <pre><code>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</code></pre> <p></p> <p>The following code is to split the dataset into training and test sets</p> <pre><code>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n\u00a0 \u00a0 16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n\u00a0 \u00a0 16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n\u00a0 \u00a0 16384::\n].values.reshape(-1, 1)\n</code></pre> <p>We will set the lags for both inputs as</p> <pre><code>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\n</code></pre> <p>and build a NARX model as follows</p> <pre><code>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=39,\n\u00a0 \u00a0 ylag=20,\n\u00a0 \u00a0 xlag=[x1lag, x2lag],\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\ny_hat = model.predict(X=x_val, y=y_val)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> <p>The RRSE is \\(0.2910\\)</p> Regressors Parameters ERR y(k-1) 1.8387E+00 9.43378253E-01 y(k-2) -1.8938E+00 1.95167599E-02 y(k-3) 1.3337E+00 1.02432261E-02 y(k-6) -1.6038E+00 8.03485985E-03 y(k-9) 2.6776E-01 9.27874557E-04 x2(k-7) -2.2385E+01 3.76837313E-04 x1(k-1) 8.2709E+00 6.81508210E-04 x2(k-3) 1.0587E+02 1.57459800E-03 x1(k-8) -3.7975E+00 7.35086279E-04 x2(k-1) 8.5725E+01 4.85358786E-04 y(k-7) 1.3955E+00 2.77245281E-04 y(k-5) 1.3219E+00 8.64120037E-04 y(k-10) -2.9306E-01 8.51717688E-04 y(k-4) -9.5479E-01 7.23623116E-04 y(k-8) -7.1309E-01 4.44988077E-04 y(k-12) -3.0437E-01 1.49743148E-04 y(k-11) 4.8602E-01 3.34613282E-04 y(k-13) -8.2442E-02 1.43738964E-04 y(k-15) -1.6762E-01 1.25546584E-04 x1(k-2) -8.9698E+00 9.76699739E-05 y(k-17) 2.2036E-02 4.55983807E-05 y(k-14) 2.4900E-01 1.10314107E-04 y(k-19) -6.8239E-03 1.99734771E-05 x2(k-9) -9.6265E+01 2.98523208E-05 x2(k-8) 2.2620E+02 2.34402543E-04 x2(k-2) -2.3609E+02 1.04172323E-04 y(k-20) -5.4663E-02 5.37895336E-05 x2(k-6) -2.3651E+02 2.11392628E-05 x2(k-4) 1.7378E+02 2.18396315E-05 x1(k-7) 4.9862E+00 2.03811842E-05 <pre><code>plot_results(y=y_val, yhat=y_hat, n=1000)\nee = compute_residues_autocorrelation(y_val, y_hat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0])\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"book/10-Case-Studies/#pv-forecasting","title":"PV Forecasting","text":"<p>In this case study, we evaluate SysIdentPy's capabilities for forecasting solar irradiance data, which can serve as a proxy for solar photovoltaic (PV) production. The objective is to demonstrate that SysIdentPy provides a competitive alternative for time series modeling, rather than claiming superiority over other libraries.</p>"},{"location":"book/10-Case-Studies/#dataset-overview_1","title":"Dataset Overview","text":"<p>The dataset used in this analysis consists of solar irradiance measurements, which are crucial for predicting solar PV production. Solar irradiance refers to the power of solar radiation received per unit area at the Earth's surface, typically measured in watts per square meter (W/m\u00b2). Accurate forecasting of solar irradiance is essential for optimizing energy production and managing grid stability in solar power systems.</p> <p>Dataset Details: - Source: The dataset can be accessed from the NeuralProphet GitHub repository. - Time Frame: The dataset covers a continuous period with frequent measurements. - Variables: Solar irradiance values over time, which will be used to model and forecast future irradiance levels.</p>"},{"location":"book/10-Case-Studies/#comparison-with-other-libraries_1","title":"Comparison with Other Libraries","text":"<p>To assess the effectiveness of SysIdentPy, we will compare its performance with the NeuralProphet library. NeuralProphet is known for its flexibility and ability to capture complex seasonal patterns and trends, making it a suitable benchmark for this task.</p> <p>For the comparison, we will use the following methods:</p> <ul> <li>NeuralProphet:</li> <li> <p>The configuration for NeuralProphet models will be based on examples provided in the NeuralProphet documentation. This library employs advanced techniques for capturing temporal patterns and forecasting.</p> </li> <li> <p>SysIdentPy:</p> </li> <li>MetaMSS (Meta-heuristic Model Structure Selection): Utilizes metaheuristic algorithms to determine the optimal model structure.</li> <li>AOLS (Accelerated Orthogonal Least Squares): A method designed for selecting relevant regressors in a model.</li> <li>FROLS (Forward Regression with Orthogonal Least Squares, using polynomial base functions): A regression technique that incorporates polynomial terms to enhance model selection.</li> </ul>"},{"location":"book/10-Case-Studies/#objective_1","title":"Objective","text":"<p>The goal of this case study is to compare the performance of SysIdentPy's forecasting methods with NeuralProphet. We will specifically focus on:</p> <ul> <li>1-Step Ahead Forecasting: Evaluating the models' ability to predict the next time step in the series based on historical data.</li> </ul> <p>We will train our models on 80% of the dataset and reserve the remaining 20% for validation purposes. This setup ensures that we test the models' performance on unseen data.</p>"},{"location":"book/10-Case-Studies/#required-packages-and-versions_7","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npystan==2.19.1.1\nholidays==0.11.2\nfbprophet==0.7.1\nneuralprophet==0.2.7\npandas==1.3.2\nnumpy==1.23.3\nmatplotlib==3.8.4\npmdarima==1.8.3\nscikit-learn==0.24.2\nscipy==1.9.1\nsktime==0.8.0\nstatsmodels==0.12.2\ntbats==1.1.0\ntorch==1.12.1\n</code></pre> <p>Then, install the packages using:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions. This practice isolates your project\u2019s dependencies and prevents version conflicts with other projects or system-wide packages. Additionally, be aware that some packages, such as <code>sktime</code> and <code>neuralprophet</code>, may install several dependencies automatically during their installation. Setting up a virtual environment helps manage these dependencies more effectively and keeps your project environment clean and reproducible.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"book/10-Case-Studies/#procedure_1","title":"Procedure","text":"<ol> <li>Data Preparation: Load and preprocess the solar irradiance dataset.</li> <li>Model Training: Apply the chosen methods from SysIdentPy and NeuralProphet to the training data.</li> <li>Evaluation: Assess the forecasting accuracy of each model on the validation set.</li> </ol> <p>By comparing these approaches, we aim to showcase SysIdentPy as a viable option for time series forecasting, highlighting its strengths and versatility in practical applications.</p> <p>Let\u2019s start by importing the necessary libraries and setting up the environment for this analysis.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.metrics import mean_squared_error\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n%matplotlib inline\n\nloss = mean_squared_error\ndata_location = r\".\\datasets\"\n</code></pre>"},{"location":"book/10-Case-Studies/#neural-prophet_1","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n\u00a0 \u00a0 n_lags=24,\n\u00a0 \u00a0 ar_sparsity=0.5,\n)\n\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\n\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n\nplt.plot(forecast[\"y\"][-104:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-104:], \"k*-\")\n</code></pre> <p>The error is \\(MSE=4642.23\\) and will be used as baseline in this case. Let's check how SysIdentPy methods handle this data.</p>"},{"location":"book/10-Case-Studies/#frols","title":"FROLS","text":"<pre><code>files = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 ylag=24,\n\u00a0 \u00a0 xlag=24,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 estimator=LeastSquares(),\n)\n\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\n\nprint(sysidentpy_loss)\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <p>The \\(MSE=3869.34\\) for this case.</p> <p></p>"},{"location":"book/10-Case-Studies/#metamss","title":"MetaMSS","text":"<pre><code>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nsysidentpy_metamss = MetaMSS(\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 xlag=24,\n\u00a0 \u00a0 ylag=24,\n\u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 maxiter=10,\n\u00a0 \u00a0 steps_ahead=1,\n\u00a0 \u00a0 n_agents=15,\n\u00a0 \u00a0 loss_func=\"metamss_loss\",\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 random_state=42,\n)\n\nsysidentpy_metamss.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\nyhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1)\nmetamss_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\n\nprint(metamss_loss)\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <p>The MetaMSS algorithm was able to select a better model in this case, as can be observed in the error metric, \\(MSE=2157.77\\).</p> <p></p>"},{"location":"book/10-Case-Studies/#aols","title":"AOLS","text":"<pre><code>set_random_seed(42)\nfiles = [\"\\SanFrancisco_PV_GHI.csv\", \"\\SanFrancisco_Hospital.csv\"]\nraw = pd.read_csv(data_location + files[0])\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(\n\u00a0 \u00a0 ylag=24, xlag=24, k=2, L=1, model_type=\"NARMAX\", basis_function=basis_function\n)\n\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n\u00a0 \u00a0 pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n\u00a0 \u00a0 pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <p>The error now is \\(MSE=2361.56\\).</p> <p></p>"},{"location":"book/2-NARMAX-Model-Representation/","title":"2. NARMAX Model Representation","text":"<p>There are several NARMAX model representations, including polynomial, Fourier, generalized additive, neural networks, and wavelet (Billings, S. A, Aguirra, L. A). This book focuses on the model representations available in SysIdentPy and we\u2019ll keep things updated as new methods are added to the package. If a particular representation is mentioned but is not available in SysIdentPy, it will be explicitly mentioned.</p> <p>To reproduce the codes presented in this section, make sure you have these packages installed:</p> <pre><code>sysidentpy, scikit-learn, scipy, pytorch, matplotlib\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#basis-function","title":"Basis Function","text":"<p>In System Identification, understanding the concept of basis functions is crucial for effectively modeling complex systems. Basis functions are predefined mathematical functions used to transform the input data into a new space, where the relationships within the data can be more easily modeled. By expressing the original data in terms of these basis functions, we can build nonlinear models in respect to its structure while keeping it linear in the parameters, allowing the usage of straightforward parameter estimation methods.</p> <p>Basis functions commonly used in system identification:</p> <ol> <li> <p>Polynomial Basis Functions: These functions are powers of the input variables. They are useful for capturing simple nonlinear relationships.</p> </li> <li> <p>Fourier Basis Functions: These sinusoidal functions (sine and cosine) are ideal for representing periodic patterns within the data.</p> </li> <li> <p>Wavelet Basis Functions: These functions are localized in both time and frequency, making them suitable for analyzing data with varying frequency components. Not available in SysIdentPy yet.</p> </li> </ol> <p>In SysIdentPy you can define the basis function you want to use in your model by just import them:</p> <pre><code>from sysidentpy.basis_function import Polynomial, Fourier, Bernstein\n</code></pre> <p>To keep things simple for now, we will show simple examples of how basis function can be used in a modeling task. We will show a simple polynomial basis functions, a triangular basis function, a radial basis function and a rectangular basis function.</p> <p>SysIdentPy does not currently include Vandermonde or any of the other basis functions defined below. These functions are provided solely as examples to illustrate the significance of the basis functions. The examples are based on Fredrik Bagge Carlson's PhD thesis, which I highly recommended for anyone interested in Nonlinear System Identification.</p> <p>Although Vandermonde and Radial Basis Functions (RBF) are planned for inclusion as native basis functions in SysIdentPy version 1.0, users can already create and use their own custom basis functions with SysIdentPy. An example of how to do this is available on the SysIdentPy documentation page.</p>"},{"location":"book/2-NARMAX-Model-Representation/#example-vandermonde-matrix","title":"Example: Vandermonde Matrix","text":"<p>The polynomial basis functions used in this example is defined as:</p> \\[ \\phi_i(x) = x^i \\tag{2.1} \\] <p>where \\(i\\) is the degree of the polynomial and \\(x\\) is the input variable.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n\n# Polynomial basis function\ndef poly_basis(x, degree):\n\u00a0 \u00a0 return np.vander(x, degree + 1, increasing=True)\n\n# Create polynomial features\ndegree = 2\nX_poly = poly_basis(x, degree)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_poly, y)\ny_pred = model.predict(X_poly)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the polynomial approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the polynomial basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(degree + 1):\n\u00a0 \u00a0 plt.plot(x, poly_basis(x, degree)[:, i], linewidth=0.5, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 1. Approximation using Vandermode Matrix. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2-NARMAX-Model-Representation/#example-rectangular-basis-functions","title":"Example: Rectangular Basis Functions","text":"<p>The rectangular basis functions are defined as:</p> \\[ \\phi_{i}(x) = \\begin{cases} 1 &amp; \\text{if } c_i - \\frac{w}{2} \\leq x &lt; c_i + \\frac{w}{2} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\tag{2.2} \\] <p>where \\(c_i\\) represents the center of the basis function, \\(w\\) is the width, and \\(x\\) is the input variable.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n# Rectangular basis function\ndef rectangular_basis(x, centers, width):\n\u00a0 \u00a0 return np.column_stack([(np.abs(x - c) &lt; width).astype(float) for c in centers])\n\n# Create rectangular features\ncenters = np.linspace(-3, 3, 6)\nwidth = 3\nX_rect = rectangular_basis(x, centers, width)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_rect, y)\ny_pred = model.predict(X_rect)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the rectangular approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the rectangular basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(len(centers)):\n\u00a0 \u00a0 plt.plot(x, rectangular_basis(x, centers, width)[:, i], linewidth=1, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 2. Approximation using Rectangular Basis Function. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2-NARMAX-Model-Representation/#example-triangular-basis-functions","title":"Example: Triangular Basis Functions","text":"<p>The triangular basis functions are defined as:</p> \\[ \\phi_{i}(x) = \\max \\left(0, 1 - \\frac{|x - c_i|}{w} \\right) \\tag{2.3} \\] <p>where \\(c_i\\) is the center of the basis function, \\(w\\) is the width, and \\(x\\) is the input variable.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200)\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape)\n# Triangular basis function\ndef triangular_basis(x, centers, width):\n\u00a0 \u00a0 return np.column_stack([np.maximum(0, 1 - np.abs((x - c) / width)) for c in centers])\n\n# Create triangular features\ncenters = np.linspace(-3, 3, 6)\nwidth = 1.5\nX_tri = triangular_basis(x, centers, width)\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_tri, y)\ny_pred = model.predict(X_tri)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Plot the triangular approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\n# Plot the triangular basis functions\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nfor i in range(len(centers)):\n\u00a0 \u00a0 plt.plot(x, triangular_basis(x, centers, width)[:, i], linewidth=1, color=basis_colors[i % len(basis_colors)])\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 3. Approximation using a Triangular Basis Function. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2-NARMAX-Model-Representation/#example-radial-basis-function-rbf-gaussian","title":"Example: Radial Basis Function (RBF) - Gaussian","text":"<p>The Gaussian Radial Basis Function is defined as:</p> \\[ \\phi(x; c, \\sigma) = \\exp\\left(- \\frac{(x - c)^2}{2 \\sigma^2}\\right) \\tag{2.4} \\] <p>where: - \\(x\\) is the input variable. - \\(c\\) is the center of the RBF. - \\(\\sigma\\) is the spread (or scale) of the RBF.</p> <p>This function measures the distance between \\(x\\) and the center \\(c\\), and it decays exponentially based on the width \\(\\sigma\\). The smaller the \\(\\sigma\\), the more localized the basis function is around the center \\(c\\).</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate simulated quadratic polynomial data\nnp.random.seed(0)\nx = np.linspace(-3, 3, 200) \u00a0# More points for a smoother curve\ny = 0.2 * x**2 - 0.3 * x + 0.1 + np.random.normal(0, 0.1, size=x.shape) \u00a0# Quadratic polynomial with noise\n# RBF centers and sigma\ncenters = np.linspace(-3, 3, 6) \u00a0# More centers for better coverage\nsigma = 0.5 \u00a0# Spread of the RBF\n# RBF basis function\ndef rbf_basis(x, c, sigma):\n\u00a0 \u00a0 return np.exp(- (x - c) ** 2 / (2 * sigma ** 2))\n\n# Create RBF features\nX_rbf = np.column_stack([rbf_basis(x, c, sigma) for c in centers])\n# Fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_rbf, y)\ny_pred = model.predict(X_rbf)\n# Plot the original data (quadratic polynomial)\nplt.scatter(x, y, color='#ffc865', s=25)\n# Basis function colors\nbasis_colors = [\"#00b262\", \"#20007e\", \"#b20000\"]\nn_colors = len(basis_colors)\n# Plot the basis functions\nfor i, c in enumerate(centers):\n\u00a0 \u00a0 color = basis_colors[i % n_colors]\n\u00a0 \u00a0 plt.plot(x, rbf_basis(x, c, sigma), linewidth=1, color=color, label=f'RBF Center {c:.2f}')\n\n# Plot the approximation\nplt.plot(x, y_pred, color='#00008c', linewidth=5)\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\nplt.gca().spines['left'].set_visible(False)\nplt.gca().spines['bottom'].set_visible(True)\nplt.gca().xaxis.set_ticks_position('bottom')\nplt.gca().yaxis.set_ticks([])\nplt.show()\n</code></pre> <p></p> <p>Figure 4. Approximation using the Radial Basis Function. The yellow dots show the system data, the bold blue line represents the predicted values, and the other lines depict the basis functions.</p>"},{"location":"book/2-NARMAX-Model-Representation/#linear-models","title":"Linear Models","text":""},{"location":"book/2-NARMAX-Model-Representation/#armax","title":"ARMAX","text":"<p>You may have noticed the similarity between the acronym NARMAX with the well-known models ARX, ARMAX, etc., which are widely used for forecasting time series. And this resemblance is not by chance. The AutoRegressive models with Moving Average and Exogenous Input (ARMAX) and their variations AR, ARX, ARMA (to name just a few) are one of the most used mathematical representations for identifying linear systems. The ARMAX can be expressed as:</p> \\[ y_k= \\mathcal{\\phi}[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\tag{2.5} \\] <p>where \\(n_y\\in \\mathbb{N}\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) , are the maximum lags for the system output, input and noise regressors (representing the moving average part), respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{\\phi}\\) is some linear function of the input and output regressors and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>If \\(\\mathcal{F}\\) is a polynomial, we have a polynomial ARMAX model</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.6} \\] <p>where \\(\\sum\\nolimits_{0}\\), \\(\\Theta_{y}^{i}\\), \\(\\Theta_{e}^{j}\\), and \\(\\Theta_{x}^{m}\\) are constant parameters.</p> <p>The following example is a polynomial ARMAX model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139x_{k-1} -0.1691x_{k-1} + 0.2245e_{k-1} \\end{align} \\tag{2.7} \\] <p>You can easily build a polynomial ARMAX model using SysIdentPy: <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True)\n)\n</code></pre></p> <p>In the example above, we define the linear polynomial basis function by importing the Polynomial basis and setting the degree equal to 1 (this ensures that we do not have a nonlinear combination of the regressors). Don't worry about the <code>FROLS</code> and <code>LeastSquares</code> yet. We'll talk about them in chapters 3 and 4, respectively.</p> <p>For Figure 4, we conducted 10 separate simulations to analyse the effects of different noise process generation on the ARMAX system's behavior. Each simulation uses a unique sample of noise to observe how variations in this random component influence the overall system output. To illustrate this, we highlight one specific simulation while the others are displayed with less emphasis.</p> <p>It's important to notice that all simulations, whether highlighted or not, are governed by the same underlying model. The deterministic part of the model equation explains the behavior of all the signals shown. The noticeable differences among the signals arise solely from the distinct noise samples used in each simulation. Despite these variations, the core dynamics of the signal remain consistent and are described by the model's deterministic component.</p> <p>Most of the code presented in this chapter is intended to illustrate fundamental concepts rather than demonstrating how to use SysIdentPy specifically. Many examples are implemented using pure Python to help you better understand the underlying concepts, replicate the examples, and adapt them as needed. SysIdentPy itself will be introduced and be used in the examples in the following chapter.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, u, nu):\n\u00a0 \u00a0 yk = 0.9*y[0] - 0.24*y[1] + 0.92*u[0] + 0.92*nu[0] + nu[1]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nu = np.random.normal(size=(random_samples,), scale=1)\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [u[i - 1]], [nu[i - 1], nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot \"nicer\"\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARMAX model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 4. Simulations to show the effects of different noise process generation on the ARMAX model's behavior.</p>"},{"location":"book/2-NARMAX-Model-Representation/#arx","title":"ARX","text":"<p>If we do not include noise terms \\(e_{k-n_e}\\)  in equation (1), we have ARX models.</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.8} \\] <p>The following example is a polynomial ARX model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139x_{k-1} -0.1691x_{k-1} \\end{align} \\tag{2.9} \\] <p>The only difference in SysIdentPy is setting the <code>unbiased=False</code></p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False)\n)\n</code></pre> <p>The following example shows 10 separate simulations to analyse the effects of different noise process generation on the ARX system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, u, nu):\n\u00a0 \u00a0 yk = 0.9*y[0] - 0.24*y[1] + 0.92*u[0] + nu[0]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nu = np.random.normal(size=(random_samples,), scale=1)\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [u[i - 1]], [nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARX model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 5. Simulations to show the effects of different noise process generation on the ARX model's behavior.</p>"},{"location":"book/2-NARMAX-Model-Representation/#arma","title":"ARMA","text":"<p>if we do not include input terms in equation (1), it turns to ARMA model</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + e_k \\tag{2.10} \\] <p>The following example is a polynomial ARMA model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139y_{k-3} -0.1691y_{k-4} + 0.2245e_{k-1} \\end{align} \\tag{2.11} \\] <p>Since the model representation do not have inputs, we have to set the model type to <code>NAR</code> and set <code>unbiased=True</code> again in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True),\n    model_type=\"NAR\"\n)\n</code></pre> <p>The figure bellow shows 10 separate simulations to analyse the effects of different noise process generation on the ARX system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, nu):\n\u00a0 \u00a0 yk = 0.5*y[0] - 0.4*y[1] + 0.8*nu[0] + nu[1]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [nu[i - 1], nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an ARMA model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 6. Simulations to show the effects of different noise process generation on the ARMA model's behavior.</p>"},{"location":"book/2-NARMAX-Model-Representation/#ar","title":"AR","text":"<p>if we do not include input terms and noise terms in equation (1), it turns to AR model</p> \\[ y_k = \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + e_k \\tag{2.12} \\] <p>The following example is a polynomial AR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}+0.1139y_{k-3} -0.1691y_{k-4} \\end{align} \\tag{2.13} \\] <p>In this case, we have to set the model type to <code>NAR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NAR\"\n)\n</code></pre> <p>The figure bellow shows 10 separate simulations to analyse the effects of different noise process generation on the AR system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(y, nu):\n\u00a0 \u00a0 yk = 0.5*y[0] - 0.3*y[1] + nu[0]\n\u00a0 \u00a0 return yk\n\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([y[i - 1], y[i - 2]], [nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an AR model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 7. Simulations to show the effects of different noise process generation on the AR model's behavior.</p>"},{"location":"book/2-NARMAX-Model-Representation/#fir","title":"FIR","text":"<p>if we only keep input terms in equation (1), it turns to NFIR model</p> \\[ y_k = \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m} + e_k \\tag{2.14} \\] <p>The following example is a polynomial FIR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213x_{k-1}-0.5692x_{k-2}+0.1139x_{k-3} -0.1691x_{k-4} \\end{align} \\tag{2.15} \\] <p>In this case, we have to set the model type to <code>NFIR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NFIR\"\n)\n</code></pre> <p>The figure bellow shows 10 separate simulations to analyse the effects of different noise process generation on the FIR system's behavior.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\nrandom_samples = 50\nn = np.arange(random_samples)\ndef system_equation(u, nu):\n\u00a0 \u00a0 yk = 0.28*u[0] - 0.34*u[1] + nu[0]\n\u00a0 \u00a0 return yk\n\nu = np.random.normal(size=(random_samples,), scale=1)\n# Create a single figure and axis for all plots\nfig, ax = plt.subplots(figsize=(12, 6))\nfor k in range(10):\n\u00a0 \u00a0 nu = np.random.normal(size=(random_samples,), scale=0.9)\n\u00a0 \u00a0 y = np.empty_like(nu)\n\u00a0 \u00a0 # Initial Conditions\n\u00a0 \u00a0 y0 = [0.5, -0.1]\n\u00a0 \u00a0 y[0:2] = y0\n\u00a0 \u00a0 for i in range(2, len(y)):\n\u00a0 \u00a0 \u00a0 \u00a0 y[i] = system_equation([0.1*u[i - 1], u[i - 2]], [nu[i]])\n\n\u00a0 \u00a0 # Interpolate the data just to make the plot easier to understand\n\u00a0 \u00a0 interpolation_function = interp1d(n, y, kind='quadratic')\n\u00a0 \u00a0 n_fine = np.linspace(n.min(), n.max(), 10*len(n)) \u00a0# More points for a smoother curve\n\u00a0 \u00a0 y_interpolated = interpolation_function(n_fine)\n\u00a0 \u00a0 # Plotting the interpolated data\n\u00a0 \u00a0 if k == 0:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='k', alpha=1, linewidth=1.5)\n\u00a0 \u00a0 else:\n\u00a0 \u00a0 \u00a0 \u00a0 ax.plot(n_fine, y_interpolated, color='grey', linestyle=\":\", alpha=0.5, linewidth=1.5)\n\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.set_title(\"Simulation of an FIR model\")\nplt.show()\n</code></pre> <p></p> <p>Figure 8. Simulations to show the effects of different noise process generation on the FIR model's behavior.</p> <p>We didn't set the model_type for ARMAX and ARX because the default is <code>NARMAX</code>. SysIdentPy allows three different model types: <code>NARMAX</code>, <code>NAR</code>, and <code>NFIR</code>. Because ARMAX, ARX and others linear variants are subsets of NARMAX models, there is no need for specific <code>ARMAX</code> model type. The idea is to have model types for model with input and output regressors; models with only output regressors; and models with only input regressors.</p>"},{"location":"book/2-NARMAX-Model-Representation/#other-variants","title":"Other Variants","text":"<p>For the sake of simplicity, we defined Equation 2.5 and only approach the polynomial representations. However, you can extend the representations to other basis functions, like the Fourier. If you set \\(\\mathcal{F}\\) as the Fourier extension</p> \\[ \\mathcal{F}(x) = [\\cos(\\pi x), \\sin(\\pi x), \\cos(2\\pi x), \\sin(2\\pi x), \\ldots, \\cos(N\\pi x), \\sin(N\\pi x)] \\tag{2.16} \\] <p>In this case, the Fourier ARX representation will be:</p> \\[ \\begin{aligned} y_k = &amp;\\Big[ \\cos(\\pi y_{k-1}), \\sin(\\pi y_{k-1}), \\cos(2\\pi y_{k-1}), \\sin(2\\pi y_{k-1}), \\ldots, \\cos(N\\pi y_{k-1}), \\sin(N\\pi y_{k-1}), \\\\ &amp;\\ \\ \\cos(\\pi y_{k-2}), \\sin(\\pi y_{k-2}), \\ldots, \\cos(N\\pi y_{k-n_y}), \\sin(N\\pi y_{k-n_y}), \\\\ &amp;\\ \\ \\cos(\\pi x_{k-1}), \\sin(\\pi x_{k-1}), \\cos(2\\pi x_{k-1}), \\sin(2\\pi x_{k-1}), \\ldots, \\cos(N\\pi x_{k-n_x}), \\sin(N\\pi x_{k-n_x}) \\Big] \\\\ &amp;\\ \\ + e_k \\end{aligned} \\tag{2.17} \\] <p>To do that in SysIdentPy, just import the Fourier basis instead of the Polynomial</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Fourier(degree=1)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NARMAX\"\n)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#nonlinear-models","title":"Nonlinear Models","text":""},{"location":"book/2-NARMAX-Model-Representation/#narmax","title":"NARMAX","text":"<p>The NARMAX model was proposed by  Stephen A. Billings and I.J. Leontaritis in 1981, (Billings, S. A. - Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains), and can be described as</p> \\[ \\begin{equation} y_k= \\mathcal{F}[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k, \\end{equation} \\tag{2.18} \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) , are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) represents uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}\\) is some nonlinear function of the input and output regressors and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>You can notice that the difference between Equation 2.5 and Equation 2.18 if the function representing the system. For NARMAX models, \\(\\mathcal{F}\\) can be any nonlinear function, while for Equation 2.5 only linear functions are allowed. Although there are many possible approximations of \\(\\mathcal{F}(\\cdot)\\) (e.g., Neural Networks, Fuzzy, Wavelet, Radial Basis Function), the power-form Polynomial NARMAX model is the most commonly used (Billings, S. A.; Khandelwal, D. and Schoukens, M. and Toth, R.):</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j}\\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.19} \\] <p>where \\(p\\) is the number of regressors, \\(\\Theta_i\\) are the model parameters, and \\(a_i, m\\), \\(b_i, j\\) and \\(d_i, l \\in \\mathbb{N}\\) are the exponents of the output, input and noise terms, respectively.</p> <p>The Equation 2.20 describes a polynomial NARMAX model where the nonlinearity degree is equal to \\(2\\), identified from experimental data of a DC motor/generator with no prior knowledge of the model form, taken from Lacerda Junior, W. R., Almeida, V. M., &amp; Martins, S. A. M. (2017):</p> \\[ \\begin{align}   y_k =&amp; 1.7813y_{k-1}-0.7962y_{k-2}+0.0339x_{k-1} -0.1597x_{k-1} y_{k-1} +0.0338x_{k-2} + \\\\   &amp; + 0.1297x_{k-1}y_{k-2} - 0.1396x_{k-2}y_{k-1}+ 0.1086x_{k-2}y_{k-2}+0.0085y_{k-2}^2 + 0.0247e_{k-1}e_{k-2} \\end{align} \\tag{2.20} \\] <p>The \\(\\Theta\\) values are the coefficients of each term of the polynomial equation.</p> <p>Polynomial basis functions are one of the most used representations of NARMAX models due to several interesting attributes, such as (Billings, S. A.):</p> <ul> <li>All polynomial functions are smooth in \\(\\mathbb{R}\\).</li> <li>The Weierstrass approximation theorem states that any continuous real-valued function defined on a closed and bounded space \\([a,b]\\) can be uniformly approximated using a polynomial on that interval.</li> <li>They can describe several nonlinear dynamical systems, including industrial processes, control systems, structural systems, economic and financial systems, biology, medicine, and social systems (some examples are detailed in Lacerda Junior, W. R. and Martins, S. A. M. and Nepomuceno, E. G. and Lacerda, Marcio J. ; Fung, E. H. K. and Wong, Y. K. and Ho, H. F. and Mignolet, M. P.; Kukreja, S. L. and Galiana, H. L. and Kearney, R. E.; Billings, S. A; Aguirre, L. A.; and many others).</li> <li>Several algorithms have been developed for structure selection and parameter estimation of polynomial NARMAX models, and it remains an active area of research.</li> <li>Polynomial NARMAX models are versatile and can be used both for prediction and inference. The structure of polynomial NARMAX models are easy to interpret and can be related to the underlying system, which is much harder to achieve with neural networks or wavelet functions, for instance.</li> </ul> <p>You can easily build a polynomial NARMAX model using SysIdentPy. Note that the difference for ARMAX, in this case, is the degree of the polynomial function.</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True)\n)\n</code></pre> <p>One could think that is a simple change, but in nonlinear scenarios the course of dimensionality becomes a real problem. The number of candidate regressors, \\(n_r\\), of polynomial NARX can be defined as Korenberg, M. L., Billings, S. A., Liu, Y. P., and McIlroy, P. J. - Orthogonal parameter estimation algorithm for non-linear stochastic systems:</p> \\[ \\begin{equation}     n_r = M+1, \\end{equation} \\tag{2.21} \\] <p>where</p> \\[ \\begin{align}     M = &amp; \\sum_{i=1}^{\\ell}n_i \\\\     n_i = &amp; \\frac{n_{i-1}(n_y+n_x+i-1)}{i}, n_{0} = 1. \\end{align} \\tag{2.22} \\] <p>As we mentioned in the Introduction of the book, NARMAX methods aims to build the simplest models possible. The idea is to reproduce a wide range of behaviors using a small subset of terms from the vast search space formed by candidate regressors.</p> <p>Let's use SysIdentPy to see how the search space grows in the linear versus the nonlinear scenario. The <code>count_model_regressors</code> method available in <code>narmax_tools</code> can be used the check how many regressors exists in the search space given the number of inputs, the delays of <code>y</code> and <code>x</code> regressors and the basis function. We will use <code>xlag=ylag=10</code> and the polynomial basis function. The user can simulate different scenarios by setting different parameters.</p> <pre><code>from sysidentpy.utils.information_matrix import count_model_regressors\nfrom sysidentpy.basis_function import Polynomial\nimport numpy as np\n</code></pre> <p>For the linear case with 1 input we have 21 regressors: <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=1)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=10,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 21\n</code></pre></p> <p>For the linear case with 2 inputs, the number of regressors jumps to 31:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=1)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=xlag,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 31\n</code></pre> <p>If we consider a nonlinear case with 1 input by just changing the degree to 2, we have 231 regressors.</p> <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=2)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=10,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 231\n</code></pre> <p>If we set the degree to 3, the number of terms increases significantly to 1771 regressors.</p> <pre><code>x_train = np.random.rand(10, 1)  # simulating a case with 1 input\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=3)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=10,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 1771\n</code></pre> <p>If you have 2 inputs in the nonlinear scenario with <code>degree=2</code>, the number of regressors is 496:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=2)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=xlag,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 496\n</code></pre> <p>If you have 2 inputs in the nonlinear scenario with <code>degree=3</code>, the number jumps to 5456 regressors:</p> <pre><code>x_train = np.random.rand(10, 2)  # simulating a case with 2 inputs\ny_train = np.random.rand(10, 1)\nbasis_function = Polynomial(degree=3)\nxlag = [list(range(1, 11))] * x_train.shape[1]\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=xlag,\n    ylag=10,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=False,\n)\nn_regressors\n&gt;&gt;&gt; 5456\n</code></pre> <p>As you can notice, the number of regressors increases significantly as the degree of the polynomial and the number of inputs increases. That makes the model structure selection much more complex! In the linear case with 10 inputs we have <code>2^31=2.15e+09</code> possible model combinations. When <code>degree=2</code> with 2 inputs we have <code>2^496=2.05e+149</code> possible combinations! Try to get the number of possible model combinations when <code>degree=3</code> with 2 inputs. Moreover, try that with more inputs and higher nonlinear degree and see how the course of dimensionality is a big problem.</p> <p>As you can see, getting a simple model in such a large search space is complex model structure selection task. To select the most significant terms from a huge dictionary of possible terms is not an easy task. And it is hard not only because the complex combinatorial problem and the uncertainty concerning the model order. Identifying the most significant terms in a nonlinear scenario is very difficult because depends on the type of the nonlinearity (sparse singularity or near-singular behavior, memory or dumping effects and many others), dynamical response (spatial-temporal systems, time-dependent), the steady-state response,  frequency of the data, the noise and many more.</p> <p>Because of the model structure selection algorithms developed for NARMAX models, even linear models like ARMAX can have different performance when obtained using SysIdentPy when compared to other libraries, like Statsmodels. We have a case study showing exactly that in Chapter 10.</p>"},{"location":"book/2-NARMAX-Model-Representation/#narx","title":"NARX","text":"<p>If we do not include noise terms \\(e_{k-n_e}\\)  in Equation (2.19), we have NARX models.</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.23} \\] <p>The Equation 2.24 describes a simple polynomial NARX model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^2+0.1139y_{k-1}x_{k-1} \\end{align} \\tag{2.24} \\] <p>The only difference in SysIdentPy is setting the <code>unbiased=False</code></p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False)\n)\n</code></pre> <p>The user can use the codes provided for linear models to analyse the nonlinear models with different noise realizations.</p>"},{"location":"book/2-NARMAX-Model-Representation/#narma","title":"NARMA","text":"<p>if we do not include input terms in Equation 2.19, it turns to NARMA model</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times\\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.25} \\] <p>The following example is a polynomial NARMA model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^3+0.1139y_{k-3}y_{k-4} + 0.2245e_{k-1} \\end{align} \\tag{2.26} \\] <p>Since the model representation do not have inputs, we have to set the model type to <code>NAR</code> and set <code>unbiased=True</code> again in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=True),\n    model_type=\"NAR\"\n)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#nar","title":"NAR","text":"<p>if we do not include input terms and noise terms in Equation 2.19, it turns to AR model</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\end{align} \\tag{2.27} \\] <p>The following example is a polynomial NAR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213y_{k-1}-0.5692y_{k-2}^2+0.1139y_{k-3}^3 -0.1691y_{k-4}y_{k-5} \\end{align} \\tag{2.28} \\] <p>In this case, we have to set the model type to <code>NAR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NAR\"\n)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#nfir","title":"NFIR","text":"<p>If we only keep input terms in Equation 2.19, it becomes a NFIR model</p> \\[ \\begin{align}   y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}x_{k-j}^{b_i, j} \\end{align} \\tag{2.29} \\] <p>The following example is a polynomial NFIR model:</p> \\[ \\begin{align}   y_k =&amp; 0.7213x_{k-1}-0.5692x_{k-2}^2+0.1139x_{k-3}x_{k-4} -0.1691x_{k-4}^3 \\end{align} \\tag{2.30} \\] <p>In this case, we have to set the model type to <code>NFIR</code> and set <code>unbiased=False</code> in <code>LeastSquares</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    model_type=\"NFIR\"\n)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#mixed-narmax-models","title":"Mixed NARMAX Models","text":"<p>In some applications, using a single basis functions cannot provide a satisfactory description for the relationship between the input (or independent) variables and the output (or response) variable. In order to improve the performance of the model, it has been proposed to use a linear combination of a set of nonlinear functions to replace the linear counterparts.</p> <p>You can achieve that in SysIdentPy using ensembles in basis functions. You can build a Fourier model where terms have interactions. You can also build a model with mixed basis functions, using terms expanded by polynomial basis and Fourier basis or any other basis function available is the package.</p> <p>You can only mix a basis function with the polynomial basis for now in SysIdentPy. You can mix Fourier with Polynomial, but you can't mix Fourier with Bernstein.</p> <p>To mix Fourier or Bernstein basis with Polynomial, the user just have to set <code>ensamble=True</code> in the basis function definition</p> <pre><code>from sysidentpy.basis_function import Fourier\n\nbasis_function = Fourier(degree=2, ensemble=True)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#neural-narx-network","title":"Neural NARX Network","text":"<p>Neural networks are models composed of interconnected layers of nodes (neurons) designed for tasks like classification and regression. Each neuron is a basic unit within these networks. Mathematically, a neuron is represented by a function \\(f\\) that takes an input vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]\\) and generates an output \\(y\\). This function usually involves a weighted sum of the inputs, an optional bias term \\(b\\), and an activation function \\(\\phi\\):</p> \\[ y = \\phi \\left( \\sum_{i=1}^{n} w_i x_i + b \\right) \\tag{2.31} \\] <p>where \\(\\mathbf{w} = [w_1, w_2, \\ldots, w_n]\\) are the weights associated with the inputs. The activation function \\(\\phi\\) introduces nonlinearity into the model, allowing the network to learn complex patterns. Common activation functions include:</p> <ul> <li> <p>Sigmoid: \\(\\phi(z) = \\frac{1}{1 + e^{-z}}\\)   Produces outputs between 0 and 1, making it useful for binary classification.</p> </li> <li> <p>Hyperbolic Tangent (tanh): \\(\\phi(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\\)   Outputs values between -1 and 1, often used to center data around zero.</p> </li> <li> <p>Rectified Linear Unit (ReLU): \\(\\phi(z) = \\max(0, z)\\)   Outputs zero for negative values and the input value itself for positive values, helping to mitigate the vanishing gradient problem.</p> </li> <li> <p>Leaky ReLU: \\(\\phi(z) = \\max(0.01z, z)\\)   A variant of ReLU that allows a small, non-zero gradient when the input is negative, addressing the problem of dying neurons.</p> </li> <li> <p>Softmax: \\(\\phi(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\\)   Converts logits into probabilities for multi-class classification, ensuring that the outputs sum to 1.</p> </li> </ul> <p>Each activation function has its own advantages and is chosen based on the specific needs of the neural network and the task at hand.</p> <p>As mentioned, neural network is composed of multiple layers, each consisting of several neurons. In this respect, the layers can be categorized into:</p> <ul> <li>Input Layer: The layer that receives the input data.</li> <li>Hidden Layers: Intermediate layers that process the inputs through weighted connections and activation functions.</li> <li>Output Layer: The final layer that produces the output of the network.</li> </ul> <p>The network itself therefore has a very simple architecture. The terminology used in neural networks is also slightly different from the standard notation that is universal in system identification and statistics. So, instead of talking about model parameters, the term network weights is used, and instead of estimation, the term learning is used. This terminology was no doubt introduced to make it appear that something completely new was being discussed, whereas some of the problems addressed are quite traditional - Stephen A. Billings</p> <p>Notice that the network itself is simply a collection of nonlinear activation units \\(\\phi(\\cdot)\\)  that are simple static functions. There are no dynamics within the network. This is fine for applications such as pattern recognition, but to use the network in system identification lagged inputs and outputs are necessary and these have to be supplied as inputs either explicitly or through a recurrent procedure. In this respect, if we set \\(\\mathcal{F}\\) as a neural function, we can adapt it to create a neural NARX model by transforming the neural architecture into a NARX architecture. The neural NARX, however, is not linear in the parameters like the NARMAX models based on basis functions. So, algorithms like Orthogonal Least Squares are not adequate to estimate the weights of the model.</p> <p>SysIdentPy\u00a0support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier. We convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction.</p> <p>Series-Parallel allows us to use <code>pytorch</code> directly for training, so SysIdentPy uses <code>pytorch</code> in the backend for neural NARX along with auxiliary methods available only in SysIdentPy.</p> <p>A simple neural NARX model can be represented as a Multi-Layer Perceptron neural network with autoregressive component along with delayed inputs.</p> <p></p> <p>Figure 9. Parallel and series-parallel neural network architectures for modeling the dynamic system \\(\\mathbf{y}[k]=\\mathbf{F}(\\mathbf{y}[k-1], \\mathbf{y}[k-2], \\mathbf{u}[k-1], \\mathbf{u}[k-2])\\). The delay operator \\(q^{-1}\\) is such that \\(\\mathbf{y}[k-1]=q^{-1} \\mathbf{y}[k]\\). Reference: Antonio H. Ribeiro and Luis A. Aguirre</p> <p>Neural NARX is not the same model as Recurrent Neural Networks (RNN). The user is referred to the following paper for more details A Note on the Equivalence of NARX and RNN</p> <p>To build a Neural NARX network in SysIdentPy, the user must use <code>pytorch</code>. We use <code>pytorch</code> to make the definition of the network architecture flexible. However, this requires that the user have a better understanding of how a neural networks. See the script bellow of how to build a simple Neural NARX model in SysIdentPy</p> <pre><code>from torch import nn\nimport torch\n\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.narmax_tools import regressor_code\n\n# simulated data\nx_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre> <p>The user can use <code>cuda</code> following the same approach when build a neural network in pytorch</p> <pre><code>torch.cuda.is_available()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</code></pre> <p>The user can create a NARXNN object and choose the maximum lag of both input and output for building the regressor matrix to serve as input of the network. In addition, you can choose the loss function, the optimizer, the optional parameters of the optimizer, the number of epochs.</p> <p>Because we built this feature on top of Pytorch, you can choose any of the loss function of the torch.nn.functional. Click here for a list of the loss functions you can use. You just need to pass the name of the loss function you want.</p> <p>Similarly, you can choose any of the optimizer of the torch.optim. Click here for a list of optimizer available.</p> <pre><code>basis_function = Polynomial(degree=1)\nnarx_net = NARXNN(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 loss_func=\"mse_loss\",\n\u00a0 \u00a0 optimizer=\"Adam\",\n\u00a0 \u00a0 epochs=2000,\n\u00a0 \u00a0 verbose=False,\n\u00a0 \u00a0 device=device,\n\u00a0 \u00a0 optim_params={\n\u00a0 \u00a0 \u00a0 \u00a0 \"betas\": (0.9, 0.999),\n\u00a0 \u00a0 \u00a0 \u00a0 \"eps\": 1e-05,\n\u00a0 \u00a0 }, \u00a0# optional parameters of the optimizer\n)\n</code></pre> <p>Because the NARXNN model were defined using \\(ylag=2\\), \\(xlag=2\\) and a polynomial basis function with \\(degree=1\\), we have a regressor matrix with 4 features. We need the size of the regressor matrix to build the layers of our network. Our input data(<code>x_train</code>) have only one feature, but since we are creating an NARX network, a regressor matrix is built behind the scenes with new features based on the <code>xlag</code> and <code>ylag</code>.</p> <p>If you need help finding how many regressors are created behind the scenes you can use the <code>narmax_tools</code> function <code>regressor_code</code> and take the size of the regressor code generated:</p> <pre><code>basis_function = Polynomial(degree=1)\nn_regressors = count_model_regressors(\n    x=x_train,\n    y=y_train,\n    xlag=2,\n    ylag=2,\n    model_type=\"NARMAX\",\n    basis_function=basis_function,\n    is_neural_narx=True,\n)\nn_regressors\n&gt;&gt;&gt; 4\n</code></pre> <p>The configuration of your network follows exactly the same pattern of a network defined in Pytorch. The following representing our NARX neural network.</p> <pre><code>class NARX(nn.Module):\n\u00a0 \u00a0 def __init__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin = nn.Linear(n_regressors, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin2 = nn.Linear(30, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin3 = nn.Linear(30, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 self.tanh = nn.Tanh()\n\n\n\u00a0 \u00a0 def forward(self, xb):\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin(xb)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin2(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin3(z)\n\u00a0 \u00a0 \u00a0 \u00a0 return z\n</code></pre> <p>The user have to pass the defined network to our NARXNN estimator and set <code>cuda</code> if available (or needed):</p> <pre><code>narx_net.net = NARX()\n\nif device == \"cuda\":\n\u00a0 \u00a0 narx_net.net.to(torch.device(\"cuda\"))\n</code></pre> <p>Because we have a fit (for training) and predict function for Polynomial NARMAX, we create the same pattern for the NARX net. So, you only have to fit and predict using the following:</p> <pre><code>narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid)\nyhat = narx_net.predict(X=x_valid, y=y_valid)\n</code></pre> <p>If the net configuration is built before calling the NARXNN, just pass the model to the NARXNN as follows:</p> <pre><code>class NARX(nn.Module):\n\u00a0 \u00a0 def __init__(self):\n\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin = nn.Linear(n_regressors, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin2 = nn.Linear(30, 30)\n\u00a0 \u00a0 \u00a0 \u00a0 self.lin3 = nn.Linear(30, 1)\n\u00a0 \u00a0 \u00a0 \u00a0 self.tanh = nn.Tanh()\n\n\n\u00a0 \u00a0 def forward(self, xb):\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin(xb)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin2(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.tanh(z)\n\u00a0 \u00a0 \u00a0 \u00a0 z = self.lin3(z)\n\u00a0 \u00a0 \u00a0 \u00a0 return z\n\n\nnarx_net2 = NARXNN(\n\u00a0 \u00a0 net=NARX(),\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 loss_func=\"mse_loss\",\n\u00a0 \u00a0 optimizer=\"Adam\",\n\u00a0 \u00a0 epochs=2000,\n\u00a0 \u00a0 verbose=False,\n\u00a0 \u00a0 optim_params={\n\u00a0 \u00a0 \u00a0 \u00a0 \"betas\": (0.9, 0.999),\n\u00a0 \u00a0 \u00a0 \u00a0 \"eps\": 1e-05,\n\u00a0 \u00a0 }, \u00a0# optional parameters of the optimizer\n)\n\nnarx_net2.fit(X=x_train, y=y_train)\nyhat = narx_net2.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"book/2-NARMAX-Model-Representation/#general-model-set-representation","title":"General Model Set Representation","text":"<p>Based on the idea of transforming a static neural network in a neural NARX model, we can extend the method for basically any model class. SysIdentPy do not aim to implement every model class that exists in literature. However, we created a functionality that allows the usage of any other machine learning package that follows a <code>fit</code> and <code>predict</code> API inside SysIdentPy to convert such models to NARX versions of them.</p> <p>Let's take XGBoost (eXtreme Gradient Boosting) Algorithm as an example. XGBoost is a well known model class used for regression tasks. XGBoost, however, are not a common choice when you are dealing with a dynamical system identification task because they are originally made for modeling static systems. You can easily transform XGBoost into a NARX model using SysIdentPy.</p> <p>Scikit-learn, for example, is another great example. You can transform any Scikit-learn model into NARX models using SysIdentPy. We will see such applications in detail at Chapter 11, but you can see how easy it in the script bellow</p> <pre><code>from sysidentpy.general_estimators import NARX\nfrom sysidentpy.basis_function import Polynomial\nfrom sklearn.linear_model import BayesianRidge\nimport xgboost as xgb\n\nbasis_function = Fourier(degree=1)\n# define the scikit estimator\nscikit_estimator = BayesianRidge()\n# transform scikit_estimator into NARX model\ngb_narx = NARX(\n    base_estimator=scikit_estimator,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\ngb_narx.fit(X=x_train, y=y_train)\nyhat = gb_narx.predict(X=x_valid, y=y_valid)\n\n# XGboost examples\nxgb_estimator = xgb.XGBRegressor()\nxgb_narx = NARX(\n    base_estimator=xgb_estimator,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nxgb_narx.fit(X=x_train, y=y_train)\nyhat = xgb_narx.predict(X=x_valid, y=y_valid)\n</code></pre> <p>You can use any other model by just changing the model class and passing it to the <code>base_estimator</code> in <code>NARX</code> functionality.</p>"},{"location":"book/2-NARMAX-Model-Representation/#mimo-models","title":"MIMO Models","text":"<p>To keep things simple, only SISO models were represented in previous sections. However,  the NARMAX  models can effortlessly be extended to MIMO case (Billings, S. A. and Chen, S. and Korenberg, M. J.):</p> \\[ \\begin{align}      y_{{_i}k}=&amp; F_{{_i}}^\\ell \\bigl[y_{{_1}k-1},  \\dotsc, y_{{_1}k-n^i_{y{_1}}},\\dotsc, y_{{_s}k-1},  \\dotsc, y_{{_s}k-n^i_{y{_s}}}, x_{{_1}k-d}, \\\\      &amp; x_{{_1}k-d-1}, \\dotsc, x_{{_1}k-d-n^i_{x{_1}}}, \\dotsc, x_{{_r}k-d}, x_{{_r}k-d-1}, \\dotsc, x_{{_r}k-d-n^i_{x{_r}}}\\bigr] + \\xi_{{_i}k}, \\end{align} \\tag{2.32} \\] <p>where for \\(i = 1, \\dotsc, s\\), each linear in the parameter sub-model can change regarding different maximum lags. More generally, considering</p> \\[ \\begin{align}     Y_k = \\begin{bmatrix}     y_{{_1}k} \\\\     y_{{_2}k} \\\\     \\vdots \\\\     y_{{_s}k}     \\end{bmatrix},     X_k = \\begin{bmatrix}     x_{{_1}k} \\\\     x_{{_2}k} \\\\     \\vdots \\\\     x_{{_r}k}     \\end{bmatrix},     \\Xi_k = \\begin{bmatrix}     \\xi_{{_1}k} \\\\     \\xi_{{_2}k} \\\\     \\vdots \\\\     \\xi_{{_r}k}     \\end{bmatrix}, \\end{align} \\tag{2.33} \\] <p>the MIMO model can be denoted as</p> \\[ \\begin{equation}              Y_k= F^\\ell[Y_{k-1},  \\dotsc, Y_{k-n_y},X_{k-d}, X_{k-d-1}, \\dotsc, X_{k-d-n_x}] + \\Xi_k, \\end{equation} \\tag{2.34} \\] <p>where \\(Xk ~= \\{x_{{_1}k}, x_{{_2}k}, \\dotsc, x_{{_r}k}\\}\\in \\mathbb{R}^{n^i_{x{_r}}}\\) and \\(Yk~= \\{y_{{_1}k}, y_{{_2}k}, \\dotsc, y_{{_s}k}\\}\\in \\mathbb{R}^{n^i_{y{_s}}}\\). The number of possibles terms of MIMO NARX model given the \\(i\\)-th polynomial degree, \\(\\ell_i\\), is:</p> \\[ \\begin{equation}     n_{{_{m}}r} = \\sum_{j = 0}^{\\ell_i}n_{ij}, \\end{equation} \\tag{2.35} \\] <p>where</p> \\[ \\begin{align}     n_{ij} = \\frac{ n_{ij-1} \\biggl[ \\sum\\limits_{k=1}^{s} n^i_{y_k} + \\sum\\limits_{k=1}^{r} n^i_{x_k} + j - 1 \\biggr]}{j}, \\qquad n_{i0}=1, j=1, \\dotsc, \\ell_i. \\end{align} \\tag{2.36} \\] <p>If \\(s=1\\), we have a MISO model that can be represented by a single polynomial function. Additionally, a MIMO model can be decomposed into MISO models, as presented in the following figure:</p> <p></p> <p>Figure 10. A MIMO model split into individual MISO models.</p> <p>SysIdentPy do not support MIMO models yet, only MISO models. You can, however, decompose a MIMO system as presented in Figure 9 and use SysIdentPy to create models for each subsystem.</p>"},{"location":"book/3-Parameter-Estimation/","title":"3. Parameter Estimation","text":""},{"location":"book/3-Parameter-Estimation/#least-squares","title":"Least Squares","text":"<p>Consider the NARX model described in a generic form as</p> \\[ \\begin{equation}     y_k = \\psi^\\top_{k-1}\\hat{\\Theta} + \\xi_k, \\end{equation} \\tag{3.1} \\] <p>where \\(\\psi^\\top_{k-1} \\in \\mathbb{R}^{n_r \\times n}\\) is the information matrix, also known as the regressors' matrix. The information matrix is the input and output transformation based in a basis function and \\(\\hat{\\Theta}~\\in \\mathbb{R}^{n_{\\Theta}}\\) the vector of estimated parameters. The model above can also be represented in a matrix form as:</p> \\[ \\begin{equation}     y = \\Psi\\hat{\\Theta} + \\Xi, \\end{equation} \\tag{3.2} \\] <p>where</p> \\[ \\begin{align}     Y = \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_n     \\end{bmatrix},     \\Psi = \\begin{bmatrix}     \\psi_{{_1}} \\\\     \\psi_{{_2}} \\\\     \\vdots \\\\     \\psi_{{_{n_{\\Theta}}}}     \\end{bmatrix}^\\top=     \\begin{bmatrix}     \\psi_{{_1}1} &amp; \\psi_{{_2}1} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}1} \\\\     \\psi_{{_1}2} &amp; \\psi_{{_2}2} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}2} \\\\     \\vdots &amp; \\vdots &amp;       &amp; \\vdots \\\\     \\psi_{{_1}n} &amp; \\psi_{{_2}n} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}n} \\\\     \\end{bmatrix},     \\hat{\\Theta} = \\begin{bmatrix}     \\hat{\\Theta}_1 \\\\     \\hat{\\Theta}_2 \\\\     \\vdots \\\\     \\hat{\\Theta}_{n_\\Theta}     \\end{bmatrix},     \\Xi = \\begin{bmatrix}     \\xi_1 \\\\     \\xi_2 \\\\     \\vdots \\\\     \\xi_n     \\end{bmatrix}. \\end{align} \\tag{3.3} \\] <p>We will consider the polynomial basis function to keep the examples straightforward, but the methods here will work for any other basis function.</p> <p>The parametric NARX model is linear in the parameters \\(\\Theta\\), so we can use well known algorithms, like the linear Least Squares algorithm developed by Gauss in \\(1795\\), to estimate the model parameters. The idea is to find the parameter vector that minimizes the \\(l2\\)-norm, also known as the residual sum of squares, described as</p> \\[ \\begin{equation}     J_{\\hat{\\Theta}} = \\Xi^\\top \\Xi = (y - \\Psi\\hat{\\Theta})^\\top(y - \\Psi\\hat{\\Theta}) = \\lVert y - \\Psi\\hat{\\Theta} \\rVert^2. \\end{equation} \\tag{3.4} \\] <p>In Equation 3.4 , \\(\\Psi\\hat{\\Theta}\\) is the one-step ahead prediction of \\(y_k\\), expressed as</p> \\[ \\begin{equation}     \\hat{y}_{1_k} = g(y_{k-1}, u_{k-1}\\lvert ~\\Theta), \\end{equation} \\tag{3.5} \\] <p>where \\(g\\) is some unknown polynomial function. If the gradient of \\(J_{\\Theta}\\) with respect to \\(\\Theta\\) is equal to zero, then we have the normal equation and the Least Squares estimate is expressed as</p> \\[ \\begin{equation}     \\hat{\\Theta}  = (\\Psi^\\top\\Psi)^{-1}\\Psi^\\top y, \\end{equation} \\tag{3.6} \\] <p>where \\((\\Psi^\\top\\Psi)^{-1}\\Psi^\\top\\) is called the pseudo-inverse of the matrix \\(\\Psi\\), denoted \\(\\Psi^+ \\in \\mathbb{R}^{n \\times n_r}\\).</p> <p>In order to have a bias-free estimator, the following are the basic assumptions needed for the least-squares method: - A1 - There is no correlation between the error vector, \\(\\Xi\\), and the matrix of regressors, \\(\\Psi\\). Mathematically: - \\(\\mathrm{E}\\{[(\\Psi^\\top\\Psi)^{-1}\\Psi^\\top] \\Xi\\} = \\mathrm{E}[(\\Psi^\\top\\Psi)^{-1}\\Psi^\\top] \\mathrm{E}[\\Xi]; \\tag{3.7}\\) - A2 - The error vector \\(\\Xi\\) is a zero mean white noise sequence: - \\(\\mathrm{E}[\\Xi] = 0; \\tag{3.8}\\) - A3 - The covariance matrix of the error vector is - \\(\\mathrm{Cov}[\\hat{\\Theta}] = \\mathrm{E}[(\\Theta - \\hat{\\Theta})(\\Theta - \\hat{\\Theta})^\\top] = \\sigma^2(\\Psi^\\top\\Psi); \\tag{3.9}\\) - A4 - The matrix of regressors, \\(\\Psi\\), is full rank.</p> <p>The aforementioned assumptions are needed to guarantee that the Least Squares algorithm produce a unbiased final model.</p>"},{"location":"book/3-Parameter-Estimation/#example","title":"Example","text":"<p>Let's see a practical example. Consider the model</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k} \\tag{3.10} \\] <p>We can generate the input <code>X</code>  and output <code>y</code> using SysIdentPy. Before getting in the details, let run a simple model using SysIdentPy. Because we know a priori that the system we are trying to have is not linear (the simulated system have an interaction term \\(0.1y_{k-1}x_{k-1}\\)) and the order is 2 (the maximum lag of the input and output), we will set the hyperparameters accordingly. Note that this a simulated scenario, and you'll not have such information a priori in a real identification task. But don't worry, the idea, for now, is just show how things works and we will develop some real models along the book.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\n\nx_train, x_test, y_train, y_test = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    n_info_values=3,\n    ylag=1,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\n# print the identified model\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nRegressors   Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.56885108E-01\n1         y(k-1)  2.0000E-01  3.96313039E-02\n2  x1(k-1)y(k-1)  1.0001E-01  3.48355000E-03\n</code></pre> <p>As you can see, the final model have the same 3 regressors of the simulated system and the parameters are very close the ones used to simulate the system. This shows us that the Least Squares performed well for this data.</p> <p>In this example, however, we are applying a Model Structure Selection algorithm (FROLS), which we will see in chapter 6. That's why the final model have only 3 regressors. The parameter estimation algorithm do not choose which terms to include in the model, so if we have a expanded basis function with 6 regressors, it will estimate the parameter for each one of the regressors.</p> <p>To check how this work, we can use SysIdentPy without Model Structure Selection by generating the information matrix and applying the parameter estimation algorithm directly.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils import build_lagged_matrix\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\ntheta = LeastSquares().optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[-4.1511e-06]\n [ 2.0002e-01]\n [ 1.1237e-05]\n [ 1.0068e-05]\n [ 8.9997e-01]\n [-6.3216e-05]\n [ 1.3298e-04]\n [ 1.0008e-01]\n [ 6.3118e-05]\n [-5.6031e-05]\n [-1.9073e-05]\n [-1.8223e-04]\n [ 1.1307e-04]\n [-1.6601e-04]\n [-8.5068e-05]]\n</code></pre> <p>In this case, we have 15 model parameters. If we take a look in the basis function expansion where the degree of the polynomial is equal to 2 and the lags for <code>y</code> and <code>x</code> are set to 2, we have</p> <pre><code>from sysidentpy.utils.narmax_tools import regressor_code\nbasis_function = Polynomial(degree=2)\nregressors = regressor_code(\n\u00a0 \u00a0 X=x_train,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 model_type=\"NARMAX\",\n\u00a0 \u00a0 model_representation=\"Polynomial\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nregressors\n\narray([[ 0, 0],\n   [1001, 0],\n   [1002, 0],\n   [2001, 0],\n   [2002, 0],\n   [1001, 1001],\n   [1002, 1001],\n   [2001, 1001],\n   [2002, 1001],\n   [1002, 1002],\n   [2001, 1002],\n   [2002, 1002],\n   [2001, 2001],\n   [2002, 2001],\n   [2002, 2002]]\n   )\n</code></pre> <p>The regressors is how SysIdentPy encode the polynomial basis function following this  codification pattern:</p> <ul> <li>\\(0\\) is the constant term,\\n\",</li> <li>\\([1001] = y_{k-1}\\)</li> <li>\\([100n] = y_{k-n}\\)</li> <li>\\([200n] = x1_{k-n}\\)</li> <li>\\([300n] = x2_{k-n}\\)</li> <li>\\([1011, 1001] = y_{k-11} \\\\times y_{k-1}\\)</li> <li>\\([100n, 100m] = y_{k-n} \\times y_{k-m}\\)</li> <li>\\([12001, 1003, 1001] = x11_{k-1} \\times y_{k-3} \\times y_{k-1}\\),</li> <li>and so on</li> </ul> <p>So, if you take a look at the parameters, we can see that the Least Squares algorithm estimation for the terms that belongs to the simulated system are very close to the real values.</p> <pre><code>[1001, 0] -&gt; [ 2.00002486e-01]\n[2002, 0] -&gt; [ 8.99927332e-01]\n[2001, 1001] -&gt; [ 1.00062340e-01]\n</code></pre> <p>Moreover, the parameters estimated for the other regressors are considerably lower values than the ones estimated for the correct terms, indicating that the other might not be relevant to the model.</p> <p>You can start thinking that we only need to define a basis function and apply some parameter estimation technique to build NARMAX models. However, as mentioned before, the main goal of the NARMAX methods is to build the best model possible while keeping it simple. And that's true for the case where we applied the FROLS algorithm. Besides, when dealing with system identification we want to recover the dynamics of the system under study, so adding more terms than necessary can lead to unexpected behaviors, poor performance and unstable models. Remember, this is only a toy example, so in real cases the model structure selection is fundamental.</p> <p>You can implement Least Squares method as simple as</p> <pre><code>import numpy as np\n\ndef simple_least_squares(psi, y):\n\u00a0 \u00a0 return np.linalg.pinv(psi.T @ psi) @ psi.T @ y\n\n# use the psi and y data created in previous examples or\n# create them again here to run the example.\ntheta = simple_least_squares(psi, y_train[max_lag:, :])\n\ntheta\n\narray(\n    [\n       [-1.08377785e-05],\n       [ 2.00002486e-01],\n       [ 1.73422294e-05],\n       [-3.50957931e-06],\n       [ 8.99927332e-01],\n       [ 2.04427279e-05],\n       [-1.47542408e-04],\n       [ 1.00062340e-01],\n       [ 4.53379771e-05],\n       [ 8.90006341e-05],\n       [ 1.15234873e-04],\n       [ 1.57770755e-04],\n       [ 1.58414037e-04],\n       [-3.09236444e-05],\n       [-1.60377753e-04]\n    ]\n)\n</code></pre> <p>As you can see, the estimated parameters are very close. However, be careful when using such approach in under-, well-, or over-determined systems. We recommend to use the numpy or scipy <code>lstsq</code> methods.</p>"},{"location":"book/3-Parameter-Estimation/#total-least-squares","title":"Total Least Squares","text":"<p>This section is based on the Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least squares methods. Signal Processing..</p> <p>The Total Least Squares (TLS) algorithm, is a statistical method used to find the best-fitting linear relationship between variables when both the input and output signals present white noise perturbation. Unlike ordinary least squares (OLS), which assumes that only the dependent variable is subject to error, TLS considers errors in all measured variables, providing a more robust solution in many practical applications. The algorithm was proposed by Golub and Van Loan.</p> <p>In TLS, we assume errors in both \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\), denoted as \\(\\Delta \\mathbf{X}\\) and \\(\\Delta \\mathbf{Y}\\), respectively. The true model becomes:</p> \\[ \\mathbf{Y} + \\Delta \\mathbf{Y} = (\\mathbf{X} + \\Delta \\mathbf{X}) \\mathbf{B} \\tag{3.11} \\] <p>Rearranging, we get:</p> \\[ \\Delta \\mathbf{Y} = \\Delta \\mathbf{X} \\mathbf{B} \\tag{3.12} \\]"},{"location":"book/3-Parameter-Estimation/#objective-function","title":"Objective Function","text":"<p>The TLS solution minimizes the Frobenius norm of the total perturbations in \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\):</p> \\[ \\min_{\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}} \\|[\\Delta \\mathbf{X}, \\Delta \\mathbf{Y}]\\|_F \\tag{3.13} \\] <p>subject to:</p> \\[ (\\mathbf{X} + \\Delta \\mathbf{X}) \\mathbf{B} = \\mathbf{Y} + \\Delta \\mathbf{Y} \\tag{3.14} \\] <p>where \\(\\| \\cdot \\|_F\\) denotes the Frobenius norm.</p>"},{"location":"book/3-Parameter-Estimation/#classical-solution","title":"Classical Solution","text":"<p>The classical approach to solve the TLS problem is by using Singular Value Decomposition (SVD). The augmented matrix \\([\\mathbf{X}, \\mathbf{Y}]\\) is decomposed as:</p> \\[ [\\mathbf{X}, \\mathbf{Y}] = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\tag{3.15} \\] <p>where \\(\\mathbf{U}\\) is an \\(n \\times n\\) orthogonal matrix, \\(\\Sigma=\\operatorname{diag}\\left(\\sigma_1, \\ldots, \\sigma_{n+d}\\right)\\) is a diagonal matrix of singular values; and \\(\\mathbf{V}\\) is an orthogonal matrix defined as</p> \\[ V:=\\left[\\begin{array}{cc} V_{11} &amp; V_{12} \\\\ V_{21} &amp; V_{22} \\end{array}\\right] \\quad \\begin{aligned} \\end{aligned} \\quad \\text { and } \\quad \\Sigma:=\\left[\\begin{array}{cc} \\Sigma_1 &amp; 0 \\\\ 0 &amp; \\Sigma_2 \\end{array}\\right] \\begin{gathered} \\end{gathered} . \\tag{3.16} \\] <p>A total least squares solution exists if and only if \\(V_{22}\\) is non-singular. In addition, it is unique if and only if \\(\\sigma_n \\neq \\sigma_{n+1}\\). In the case when the total least squares solution exists and is unique, it is given by</p> \\[ \\widehat{X}_{\\mathrm{tls}}=-V_{12} V_{22}^{-1} \\tag{3.17} \\] <p>and the corresponding total least squares correction matrix is</p> \\[ \\Delta C_{\\mathrm{tls}}:=\\left[\\begin{array}{ll} \\Delta A_{\\mathrm{tls}} &amp; \\Delta B_{\\mathrm{tls}} \\end{array}\\right]=-U \\operatorname{diag}\\left(0, \\Sigma_2\\right) V^{\\top} . \\tag{3.18} \\] <p>This is implemented in SysIdentPy as follows:</p> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using Total Least Squares method.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    \"\"\"\n    check_linear_dependence_rows(psi)\n    full = np.hstack((psi, y))\n    n = psi.shape[1]\n    _, _, v = np.linalg.svd(full, full_matrices=True)\n    theta = -v.T[:n, n:] / v.T[n:, n:]\n    return theta.reshape(-1, 1)\n</code></pre> <p>To use it in the modeling task, just import it like we did in the Least Squares example.</p> <p>From now on the examples will not include the Model Structure Selection step. The goal here is to focus on the parameter estimation methods. However, we already provided an example including MSS in the Least Squares section, so you will not have any problem to test that with other parameter estimation algorithms.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import TotalLeastSquares\nfrom sysidentpy.utils import build_lagged_matrix\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\ntheta = TotalLeastSquares().optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[ 1.3321e-04]\n [ 2.0014e-01]\n [-1.1771e-04]\n [ 5.8085e-05]\n [ 9.0011e-01]\n [-1.5490e-04]\n [-1.3517e-05]\n [ 9.9824e-02]\n [ 8.2326e-05]\n [-2.2814e-04]\n [-7.0837e-05]\n [-5.4319e-05]\n [-1.7472e-04]\n [-2.0396e-04]\n [ 1.7416e-05]]\n</code></pre>"},{"location":"book/3-Parameter-Estimation/#recursive-least-squares","title":"Recursive Least Squares","text":"<p>Consider the regression model</p> \\[ y_k = \\mathbf{\\Psi}_k^T \\theta_k + \\epsilon_k \\tag{3.19}\\] <p>where: - \\(y_k\\) is the observed output at time $ k $. - \\(\\mathbf{\\Psi}_k\\) is the information matrix at time \\(k\\). - \\(\\theta_k\\) is the parameter vector to be estimated at time \\(k\\). - \\(\\epsilon_k\\) is the noise at time \\(k\\).</p> <p>The Recursive Least Squares (RLS) algorithm updates the parameter estimate \\(\\theta_k\\) recursively as new data points \\((\\mathbf{x}_k, y_k)\\) become available, minimizing a weighted linear least squares cost function relating to the information matrix in a sequential manner. RLS is particularly useful in real-time applications where the data arrives sequentially and the model needs continuous updating or for modeling time varying systems (if the forgetting factor is included).</p> <p>Because it's a recursive estimation, it is useful to relate \\(\\hat{\\Theta}_k\\) to \\(\\hat{\\Theta}_{k-1}\\). In other words, the new \\(\\hat{\\Theta}_k\\) depends on the last estimated value (k). Moreover, to estimate \\(\\hat{\\Theta}_k\\), we need to incorporate the current information present in \\(y_k\\).</p> <p>Aguirre BOOK defines the Recursive Least Squares estimator with forgetting factor \\(\\lambda\\) as</p> \\[ \\left\\{\\begin{array}{c} K_k= Q_k\\psi_k = \\frac{P_{k-1} \\psi_k}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda} \\\\ \\hat{\\theta}_k=\\hat{\\theta}_{k-1}+K_k\\left[y(k)-\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1}\\right] \\\\ P_k=\\frac{1}{\\lambda}\\left(P_{k-1}-\\frac{P_{k-1} \\psi_k \\psi_k^{\\mathrm{T}} P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda}\\right) \\end{array}\\right. \\tag{3.20} \\] <p>where \\(K_k\\) is the gain vector calculation (also known as Kalman gain), \\(P_k\\) is the covariance matrix update, and \\(y_k - \\mathbf{\\Psi}_k^T \\theta_{k-1}\\) is the a priori estimation error. The forgetting factor \\(\\lambda\\) (\\(0 &lt; \\lambda \\leq 1\\)) is usually defined between \\(0.94\\) and \\(0.99\\). If you set \\(\\lambda = 1\\) you will be using the traditional recursive algorithm. The equation above consider that the regressor vector \\(\\psi(k-1)\\) has been rewritten as \\(\\psi_k\\), since this vector is updated at iteration \\(k\\) and contains information up to time instant \\(k-1\\). We can  Initialize the parameter estimate \\(\\theta_0\\) as</p> \\[ \\theta_0 = \\mathbf{0} \\tag{3.21}\\] <p>and Initialize the inverse of the covariance matrix \\(\\mathbf{P}_0\\) with a large value:</p> \\[ \\mathbf{P}_0 = \\frac{\\mathbf{I}}{\\delta} \\tag{3.22}\\] <p>where \\(\\delta\\) is a small positive constant, and \\(\\mathbf{I}\\) is the identity matrix.</p> <p>The forgetting factor \\(\\lambda\\) controls how quickly the algorithm forgets past data: - \\(\\lambda = 1\\) means no forgetting, and all past data are equally weighted. - \\(\\lambda &lt; 1\\) means that when new data is available, all weights are multiplied by \\(\\lambda\\), which can be interpreted as the ratio between consecutive weights for the same data.</p> <p>You can access the source code to check how SysIdentPy implements the RLS algorithm. The following example present how you can use it in SysIdentPy.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.utils import build_lagged_matrix\nimport matplotlib.pyplot as plt\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n = 1000, colored_noise = False, sigma = 0.001, train_percentage = 90\n)\n\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\nestimator = RecursiveLeastSquares(lam=0.99)\ntheta = estimator.optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[-1.1778e-04]\n [ 1.9988e-01]\n [-9.3114e-05]\n [ 2.5119e-04]\n [ 9.0006e-01]\n [ 1.8339e-04]\n [-1.1943e-04]\n [ 9.9957e-02]\n [-4.6181e-05]\n [ 1.3155e-04]\n [ 3.4535e-04]\n [ 1.3843e-04]\n [-3.5454e-05]\n [ 1.5669e-04]\n [ 2.4311e-04]]\n</code></pre> <p>You can plot the evolution of the estimated parameters over time by accessing the <code>theta_evolution</code> values <pre><code># plotting only the first 50 values\nplt.plot(estimator.theta_evolution.T[:50, :])\nplt.xlabel(\"iterations\")\nplt.ylabel(\"theta\")\n</code></pre> </p> <p>Figure 1. Evolution of the estimated parameters over time using the RLS algorithm.</p>"},{"location":"book/3-Parameter-Estimation/#least-mean-squares","title":"Least Mean Squares","text":"<p>The Least Mean Squares (LMS) adaptive filter is a popular stochastic gradient algorithm developed by Widrow and Hoff in 1960. The LMS adaptive filter aims to adaptively change its filter coefficients to achieve the best possible filtering of a signal. This is done by minimizing the error between the desired signal \\(d(n)\\) and the filter output \\(y(n)\\). We can derive the LMS algorithm from the RLS formulation.</p> <p>In RLS, the \\(\\lambda\\) is related to the minimization of the sum of weighted squares of the innovation</p> \\[ J_k = \\sum^k_{j=1}\\lambda^{k-j}e^2_j. \\tag{3.23} \\] <p>The \\(Q_k\\) in Equation 3.20, defined as</p> \\[ Q_k = \\frac{P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+\\lambda} \\\\ \\tag{3.24} \\] <p>is derived from the general form the Kalman Filter (KF) algorithm.</p> \\[ Q_k = \\frac{P_{k-1}}{\\psi_k^{\\mathrm{T}} P_{k-1} \\psi_k+v_0} \\\\ \\tag{3.25} \\] <p>where \\(v_0\\) is the variance of the noise in the definition of the KF, in which the cost function is defined as the sum of squares of the innovation (noise). You can check the details in the Billings, S. A. - Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains.</p> <p>If we change \\(Q_k\\) in Equation 3.25 to scaled identity matrix</p> \\[ Q_k = \\frac{\\mu}{\\Vert \\psi_k \\Vert^2}I \\tag{3.26} \\] <p>where \\(\\mu \\in \\mathbb{R}^+\\), the \\(Q_k\\) and \\(\\hat{\\theta}_k\\) in Equation 3.20 becomes</p> \\[ \\hat{\\theta}_k=\\hat{\\theta}_{k-1}+\\frac{\\mu\\left[y(k)-\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1}\\right]}{\\Vert \\psi_k \\Vert^2}\\psi_k \\tag{3.27} \\] <p>where \\(\\psi_k^{\\mathrm{T}} \\hat{\\theta}_{k-1} = \\hat{y}_k\\), which is known as the LMS algorithm.</p>"},{"location":"book/3-Parameter-Estimation/#convergence-and-step-size","title":"Convergence and Step-Size","text":"<p>The step-size parameter \\(\\mu\\) plays a crucial role in the performance of the LMS algorithm. If \\(\\mu\\) is too large, the algorithm may become unstable and fail to converge. If \\(\\mu\\) is too small, the algorithm will converge slowly. The choice of \\(\\mu\\) is typically:</p> \\[ 0 &lt; \\mu &lt; \\frac{2}{\\lambda_{\\max}} \\tag{3.28} \\] <p>where \\(\\lambda_{\\max}\\) is the largest eigenvalue of the input signal\u2019s autocorrelation matrix.</p> <p>In SysIdentPy, you can use several variants of the LMS algorithm:</p> <ol> <li>LeastMeanSquareMixedNorm</li> <li>LeastMeanSquares</li> <li>LeastMeanSquaresFourth</li> <li>LeastMeanSquaresLeaky</li> <li>LeastMeanSquaresNormalizedLeaky</li> <li>LeastMeanSquaresNormalizedSignRegressor</li> <li>LeastMeanSquaresNormalizedSignSign</li> <li>LeastMeanSquaresSignError</li> <li>LeastMeanSquaresSignSign</li> <li>AffineLeastMeanSquares</li> <li>NormalizedLeastMeanSquares</li> <li>NormalizedLeastMeanSquaresSignError</li> <li>LeastMeanSquaresSignRegressor</li> </ol> <p>To use any one on the methods above, you just need to import it and set the <code>estimator</code> using the option you want:</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastMeanSquares\nfrom sysidentpy.utils import build_lagged_matrix\n\nx_train, x_test, y_train, y_test = get_siso_data(\n    n = 1000, colored_noise = False, sigma = 0.001, train_percentage = 90\n)\n\nxlag = 2\nylag = 2\nmax_lag = 2\nregressor_matrix = build_lagged_matrix(\n    x=x_train, y=y_train, xlag=xlag, ylag=ylag, model_type=\"NARMAX\",\n)\n# apply the basis function\npsi = Polynomial(degree=2).fit(regressor_matrix, max_lag=max_lag, xlag=xlag, ylag=ylag)\nestimator = LeastMeanSquares(mu=0.1)\ntheta = estimator.optimize(psi, y_train[max_lag:, :])\ntheta\n\n[[ 1.5924e-04]\n [ 1.9950e-01]\n [ 3.2137e-04]\n [ 1.7824e-04]\n [ 8.9951e-01]\n [ 2.7314e-04]\n [ 3.3538e-04]\n [ 1.0062e-01]\n [ 3.5219e-04]\n [ 1.3544e-04]\n [ 3.4149e-04]\n [ 5.6315e-04]\n [-4.6664e-04]\n [ 2.2849e-04]\n [ 1.0536e-04]]\n</code></pre>"},{"location":"book/3-Parameter-Estimation/#extended-least-squares-algorithm","title":"Extended Least Squares Algorithm","text":"<p>Let's show an example of the effect of a biased parameter estimation. To make things simple,The data is generated by simulating the following model:</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k} \\] <p>In this case, we know the values of the true parameters, so it will be easier to understand how they are affected by a biased estimation. The data is generated using a method from SysIdentPy. If colored_noise is set to True in the method, a colored noise is added to the data:</p> \\[e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\] <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma\\) is defined by the user.</p> <p>We will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n</code></pre> <p>First we will train a model without the Extended Least Squares Algorithm for comparison purpose.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=False)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=False,\n\u00a0 \u00a0 n_terms=3,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR x1(k-2) 9.0442E-01 7.55518391E-01 y(k-1) 2.7405E-01 7.57565084E-02 x1(k-1)y(k-1) 9.8757E-02 3.12896171E-03 <p>Clearly we have something wrong with the obtained model. The estimated parameters differs from the true one defined in the equation that generated the data. As we can observe above, the model structure is exact the same the one that generate the data. You can se that the ERR ordered the terms in the correct way. And this is an important note regarding the ERR algorithm: it is very robust to colored noise!!</p> <p>That is a great feature! However, although the structure is correct, the model parameters are not correct! Here we have a biased estimation! For instance, the real parameter for \\(y_{k-1}\\) is \\(0.2\\), not \\(0.274\\).</p> <p>In this case, we are actually modeling using a NARX model, not a NARMAX. The MA part exists to allow an unbiased estimation of the parameters. To achieve a unbiased estimation of the parameters we have the Extend Least Squares algorithm.</p> <p>Before applying the Extended Least Squares Algorithm we will run several NARX models to check how different the estimated parameters are from the real ones.</p> <pre><code>parameters = np.zeros([3, 50])\nfor i in range(50):\n\u00a0 \u00a0 x_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 \u00a0 \u00a0 n=3000, colored_noise=True, train_percentage=90\n\u00a0 \u00a0 )\n\u00a0 \u00a0 model.fit(X=x_train, y=y_train)\n\u00a0 \u00a0 parameters[:, i] = model.theta.flatten()\n\n# Set the theme for seaborn (optional)\nsns.set_theme()\nplt.figure(figsize=(14, 4))\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label='Parameter 1')\nsns.kdeplot(parameters.T[:, 1], label='Parameter 2')\nsns.kdeplot(parameters.T[:, 2], label='Parameter 3')\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color='k', linestyle='--', label='Real Value 0.1')\nplt.axvline(x=0.2, color='k', linestyle='--', label='Real Value 0.2')\nplt.axvline(x=0.9, color='k', linestyle='--', label='Real Value 0.9')\nplt.xlabel('Parameter Value')\nplt.ylabel('Density')\nplt.title('Kernel Density Estimate of Parameters')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Figure 2.: Kernel Density Estimates (KDEs) of the estimated parameters obtained from 50 NARX models realizations, each fitted to data with colored noise. The vertical dashed lines indicate the true parameter values used to generate the data. While the model structure is correctly identified, the estimated parameters are biased due to the omission of the Moving Average (MA) component, highlighting the need for the Extended Least Squares algorithm to achieve unbiased parameter estimation</p> <p>As shown in figure above, we have a problem to estimate the parameter for \\(y_{k-1}\\). Now we will use the Extended Least Squares Algorithm. In SysIdentPy, just set <code>unbiased=True</code> in the parameter estimation definition and the ELS algorithm will be applied.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\nfor i in range(50):\n\u00a0 \u00a0 x_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 \u00a0 \u00a0 n=3000, colored_noise=True, train_percentage=90\n\u00a0 \u00a0 )\n\u00a0 \u00a0 model = FROLS(\n\u00a0 \u00a0 \u00a0 \u00a0 order_selection=False,\n\u00a0 \u00a0 \u00a0 \u00a0 n_terms=3,\n\u00a0 \u00a0 \u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 \u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 \u00a0 \u00a0 elag=2,\n\u00a0 \u00a0 \u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 \u00a0 \u00a0 estimator=estimator,\n\u00a0 \u00a0 \u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 )\n\n\u00a0 \u00a0 model.fit(X=x_train, y=y_train)\n\u00a0 \u00a0 parameters[:, i] = model.theta.flatten()\n\nplt.figure(figsize=(14, 4))\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label='Parameter 1')\nsns.kdeplot(parameters.T[:, 1], label='Parameter 2')\nsns.kdeplot(parameters.T[:, 2], label='Parameter 3')\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color='k', linestyle='--', label='Real Value 0.1')\nplt.axvline(x=0.2, color='k', linestyle='--', label='Real Value 0.2')\nplt.axvline(x=0.9, color='k', linestyle='--', label='Real Value 0.9')\nplt.xlabel('Parameter Value')\nplt.ylabel('Density')\nplt.title('Kernel Density Estimate of Parameters')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Figure 3. Kernel Density Estimates (KDEs) of the estimated parameters obtained from 50 NARX models using the Extended Least Squares (ELS) algorithm with unbiased estimation. The vertical dashed lines indicate the true parameter values used to generate the data.</p> <p>Unlike the previous biased estimation, these KDEs in Figure 3 show that the estimated parameters are now closely aligned with the true values, demonstrating the effectiveness of the ELS algorithm in achieving unbiased parameter estimation, even in the presence of colored noise.</p> <p>The Extended Least Squares algorithm is iterative by nature. In SysIdentPy, the default number of iterations is set to 30 (<code>uiter=30</code>). However, the literature suggests that the algorithm typically converges quickly, often within 10 to 20 iterations. Therefore, you may want to test different numbers of iterations to find the optimal balance between convergence speed and computational efficiency.</p>"},{"location":"book/4-Model-Structure-Selection/","title":"4. Model Structure Selection","text":""},{"location":"book/4-Model-Structure-Selection/#introduction","title":"Introduction","text":"<p>This section is taken mainly from my master thesis, which was based on Billings, S. A.</p> <p>Selecting the model structure is crucial to develop models that can correctly reproduce the system behavior. If some prior information about the system are known, e.g., the dynamic order and degree of nonlinearity, determining the terms and then estimate the parameters is trivial. In real life scenarios, however, usually there is no information about what terms should be included in the model and the correct regressors has to be selected in the identification framework. If the MSS is not performed with the necessary concerns, the scientific law that describes the system may will not be revealed and resulting in misleading interpretations about the system. To illustrate this scenario, consider the following example.</p> <p>Let \\(\\mathcal{D}\\) denote an arbitrary dataset</p> \\[ \\begin{equation}     \\mathcal{D} = \\{(x_k, y_k), k = 1, 2, \\dotsc, n\\}, \\end{equation} \\tag{1} \\] <p>where \\(x_k \\in \\mathbb{R}^{n_x}\\) and \\(y_k\\in \\mathbb{R}^{n_y}\\) are the input and output of an unknown system and \\(n\\) is the number of samples in the dataset. The following are two polynomial NARX models built to describe that system:</p> \\[ \\begin{align}     y_{ak} &amp;= 0.7077y_{ak-1} + 0.1642u_{k-1} + 0.1280u_{k-2} \\end{align} \\tag{2} \\] \\[ \\begin{align}     y_{bk} &amp;= 0.7103y_{bk-1} + 0.1458u_{k-1} + 0.1631u_{k-2} \\\\            &amp;\\quad - 1467y_{bk-1}^3 + 0.0710y_{bk-2}^3 + 0.0554y_{bk-3}^2u_{k-3}. \\end{align} \\tag{3} \\] <p>Figure 1 shows the predicted values of each model and the real data. As can be observed, the nonlinear model 2 seems to fit the data better than the linear model 1. The original system under consideration is an RLC circuit, consisting of a resistor (R), inductor (L), and capacitor (C) connected in series with a voltage source. It is well known that the behavior of such an RLC series circuit can be accurately described by a linear second-order differential equation that relates the current \\(I(t)\\) and the applied voltage \\(V(t)\\):</p> \\[ L\\frac{d^2I(t)}{dt^2} + R\\frac{dI(t)}{dt} + \\frac{1}{C}I(t) = \\frac{dV(t)}{dt} \\tag{4} \\] <p>Given this linear relationship, an adequate model for the RLC circuit should reflect this second-order linearity. While Model 2, which includes nonlinear terms, may provide a closer fit to the data, it is clearly over-parameterized. Such over-parameterization can introduce spurious nonlinear effects, often referred to as \"ghost\" nonlinearities, which do not correspond to the actual dynamics of the system. Therefore, these models need to be interpreted with caution, as the use of an overly complex model could obscure the true linear nature of the system and lead to incorrect conclusions about its behavior.</p> <p></p> <p>Figure 1.Results for two polynomial NARX models fitted to data from an unknown system. Model 1 (left) is a linear model, while Model 2 (right) includes nonlinear terms. The figure illustrates that Model 2 provides a closer fit to the data compared to Model 1. However, since the original system is a linear RLC circuit known to have a second-order linear behavior, the improved fit of Model 2 may be misleading due to over-parameterization. This highlights the importance of considering the physical characteristics of the system when interpreting model results to avoid misinterpretation of artificial nonlinearities. Reference: Meta Model Structure Selection: An Algorithm For Building Polynomial NARX Models For Regression And Classification</p> <p>Correctly identifying the structure of a model is crucial for accurately analyzing the system's dynamics. A well-chosen model structure ensures that the model reflects the true behavior of the system, allowing for consistent and meaningful analysis. In this respect, several algorithms have been developed to select the appropriate terms for constructing a polynomial NARX model. The primary goal of model structure selection (MSS) algorithms is to reveal the system's characteristics by producing the simplest model that adequately describes the data. While some systems may indeed require more complex models, it is essential to strike a balance between simplicity and accuracy. As Einstein aptly put it:</p> <p>A model should be as simple as possible, but not simpler.</p> <p>This principle emphasizes the importance of avoiding unnecessary complexity while ensuring that the model still captures the essential dynamics of the system.</p> <p>We see at chapter 2 that regressors selection, however, is not a simple task. If the nonlinear degree, the order of the model and the number inputs increases, the number of candidate models becomes too large for brute force approach. Considering the MIMO case, this problem is far worse than the SISO one if many inputs and outputs are required. The number of all different models can be calculated as</p> \\[ \\begin{align}     n_m =     \\begin{cases}     2^{n_r} &amp; \\text{for SISO models}, \\\\     2^{n_{{_{m}}r}} &amp; \\text{for MIMO models},     \\end{cases} \\end{align} \\tag{5} \\] <p>where \\(n_r\\) and \\(n_{{_{m}}r}\\) are the values computed using the equations presented in Chapter 2.</p> <p>A classical solution to regressors selection problem is the Forward Regression Orthogonal Least Squares (FROLS) algorithm associated with Error Reduction Ratio (ERR) algorithm. This technique is based on the Prediction Error Minimization framework and, one at time, select the most relevant regressor by using a step-wise regression. The FROLS method adapt the set of regressors in the search space into a set of orthogonal vectors, which ERR evaluates the individual contribution to the desired output variance.</p>"},{"location":"book/4-Model-Structure-Selection/#the-forward-regression-orthogonal-least-squares-algorithm","title":"The Forward Regression Orthogonal Least Squares Algorithm","text":"<p>Consider the general NARMAX model defined in Equation 2.23 described in a generic form as</p> \\[ \\begin{equation}     y_k = \\psi^\\top_{k-1}\\hat{\\Theta} + \\xi_k, \\end{equation} \\tag{6} \\] <p>where \\(\\psi^\\top_{k-1} \\in \\mathbb{R}^{n_r \\times n}\\) is a vector of some combinations of the regressors and \\(\\hat{\\Theta} \\in \\mathbb{R}^{n_{\\Theta}}\\) the vector of estimated parameters. In a more compact form, the NARMAX model can be represented in a matrix form as:</p> \\[ \\begin{equation}     y = \\Psi\\hat{\\Theta} + \\Xi, \\end{equation} \\tag{7} \\] <p>where</p> \\[ \\begin{align}     Y = \\begin{bmatrix}     y_1 \\\\     y_2 \\\\     \\vdots \\\\     y_n     \\end{bmatrix},     \\Psi = \\begin{bmatrix}     \\psi_{{_1}} \\\\     \\psi_{{_2}} \\\\     \\vdots \\\\     \\psi_{{_{n_{\\Theta}}}}     \\end{bmatrix}^\\top=     \\begin{bmatrix}     \\psi_{{_1}1} &amp; \\psi_{{_2}1} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}1} \\\\     \\psi_{{_1}2} &amp; \\psi_{{_2}2} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}2} \\\\     \\vdots &amp; \\vdots &amp;       &amp; \\vdots \\\\     \\psi_{{_1}n} &amp; \\psi_{{_2}n} &amp; \\dots &amp; \\psi_{{_{n_{\\Theta}}}n} \\\\     \\end{bmatrix},     \\hat{\\Theta} = \\begin{bmatrix}     \\hat{\\Theta}_1 \\\\     \\hat{\\Theta}_2 \\\\     \\vdots \\\\     \\hat{\\Theta}_{n_\\Theta}     \\end{bmatrix},     \\Xi = \\begin{bmatrix}     \\xi_1 \\\\     \\xi_2 \\\\     \\vdots \\\\     \\xi_n     \\end{bmatrix}. \\end{align} \\tag{8} \\] <p>The parameters in equation above could be estimated as a result of a Least Squares-based algorithm, but this would require to optimize all parameters at the same time on account of the fact of interaction between regressors due to non-orthogonality characteristic. Consequently, the computational demand becomes impractical for high number of regressors. In this respect, the FROLS transforms the non-orthogonal model presented in the equation above into a orthogonal one.</p> <p>The regressor matrix \\(\\Psi\\) can be orthogonally decomposed as</p> \\[ \\begin{equation}     \\Psi = QA, \\end{equation} \\tag{9} \\] <p>where \\(A \\in \\mathbb{R}^{n_{\\Theta}\\times n_{\\Theta}}\\) is an unit upper triangular matrix according to</p> \\[ \\begin{align} A =     \\begin{bmatrix}     1       &amp; a_{12} &amp; a_{13} &amp; \\dotsc &amp; a_{1n_{\\Theta}} \\\\     0       &amp;   1    &amp; a_{23} &amp; \\dotsc &amp; a_{2n_{\\Theta}} \\\\     0       &amp;   0    &amp;   1    &amp; \\dotsc &amp;     \\vdots       \\\\     \\vdots  &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; a_{n_{\\Theta}-1n_{\\Theta}} \\\\     0       &amp;  0     &amp;  0     &amp;  0     &amp; 1     \\end{bmatrix}, \\end{align} \\tag{10} \\] <p>and \\(Q \\in \\mathbb{R}^{n\\times n_{\\Theta}}\\) is a matrix with orthogonal columns \\(q_i\\), described as</p> \\[ \\begin{equation}     Q =         \\begin{bmatrix}         q_{{_1}} &amp; q_{{_2}} &amp; q_{{_3}} &amp; \\dotsc &amp; q_{{_{n_{\\Theta}}}}         \\end{bmatrix}, \\end{equation} \\tag{11} \\] <p>such that \\(Q^\\top Q = \\Lambda\\) and \\(\\Lambda\\) is diagonal with entry \\(d_i\\) and can be expressed as:</p> \\[ \\begin{align}     d_i = q_i^\\top q_i = \\sum^{k=1}_{n}q_{{_i}k}q_{{_i}k}, \\qquad 1\\leq i \\leq n_{\\Theta}. \\end{align} \\] <p>Because the space spanned by the orthogonal basis \\(Q\\) (Equation 11) is the same as that spanned by the basis set \\(\\Psi\\) (Equation 8) (i.e, contains every linear combination of elements of such subspace), we can define the Equation 7 as</p> \\[ \\begin{equation}     Y = \\underbrace{(\\Psi A^{-1})}_{Q}\\underbrace{(A\\Theta)}_{g}+ \\Xi = Qg+\\Xi, \\end{equation} \\tag{12} \\] <p>where \\(g\\in \\mathbb{R}^{n_\\Theta}\\) is an auxiliary parameter vector. The solution of the model described in Equation 12 is given by</p> \\[ \\begin{equation}     g = \\left(Q^\\top Q\\right)^{-1}Q^\\top Y = \\Lambda^{-1}Q^\\top Y \\end{equation} \\tag{13} \\] <p>or</p> \\[ \\begin{equation}     g_{{_i}} = \\frac{q_{{_i}}^\\top Y}{q_{{_i}}^\\top q_{{_i}}}. \\end{equation} \\tag{14} \\] <p>Since the parameter \\(\\Theta\\) and \\(g\\) satisfies the triangular system \\(A\\Theta = g\\), any orthogonalization method like Householder, Gram-Schmidt, modified Gram-Schmidt or Givens transformations can be used to solve the equation and estimate the original parameters. Assuming that \\(E[\\Psi^\\top \\Xi] = 0\\), the output variance can be derived by multiplying Equation 12 with itself and dividing by \\(n\\), resulting in</p> \\[ \\begin{equation}     \\frac{1}{n}Y^\\top Y = \\underbrace{\\frac{1}{n}\\sum^{i = 1}_{n_{\\Theta}}g_{{_i}}^2q^\\top_{{_i}}q_{{_i}}}_{\\text{{output explained by the regressors}}} + \\underbrace{\\frac{1}{n}\\Xi^\\top \\Xi}_{\\text{{unexplained variance}}}. \\end{equation} \\tag{15} \\] <p>Thus, the ERR due to the inclusion of the regressor \\(q_{{_i}}\\) is expressed as:</p> \\[ [\\text{ERR}]_i = \\frac{g_{i}^2 \\cdot q_{i}^\\top q_{i}}{Y^\\top Y}, \\qquad \\text{for } i=1,2,\\dotsc, n_\\Theta. \\] <p>There are many ways to terminate the algorithm. An approach often used is stop the algorithm if the model output variance drops below some predetermined limit \\(\\varepsilon\\):</p> \\[ \\begin{equation}     1 - \\sum_{i = 1}^{n_{\\Theta}}\\text{ERR}_i \\leq \\varepsilon, \\end{equation} \\tag{17} \\]"},{"location":"book/4-Model-Structure-Selection/#keep-it-simple","title":"Keep it simple","text":"<p>For the sake of simplicity, let's present the FROLS along with simple examples to make the intuition clear. First, let define the ERR calculation and then explain the idea of the FROLS in simple terms.</p>"},{"location":"book/4-Model-Structure-Selection/#orthogonal-case","title":"Orthogonal case","text":"<p>Consider the case where we have a set of inputs defined as \\(x_1, x_2, \\ldots, x_n\\) and an output called \\(y\\). These inputs are orthogonal vectors.</p> <p>Lets suppose that we want to create a model to approximate \\(y\\)  using \\(x_1, x_2, \\ldots, x_n\\), as follows:</p> \\[ y=\\hat{\\theta}_1 x_1+\\hat{\\theta}_2 x_2+\\ldots+\\hat{\\theta}_n x_n+e \\tag{18} \\] <p>where \\(\\hat{\\theta}_1, \\hat{\\theta}_2, \\ldots, \\hat{\\theta}_n\\) are parameters and \\(e\\) is white noise and independent of \\(x\\) and \\(y\\) (remember the  \\(E[\\Psi^\\top \\Xi] = 0\\), in previous section). In this case, we can rewrite the equation above as</p> \\[ y = \\hat{\\theta} x \\tag{19} \\] <p>so</p> \\[ \\left\\langle x, y\\right\\rangle = \\left\\langle \\hat{\\theta} x, x\\right\\rangle = \\hat{\\theta} \\left\\langle x, x\\right\\rangle \\tag{20} \\] <p>Which implies that</p> \\[ \\hat{\\theta} = \\frac{\\left\\langle x, y\\right\\rangle}{\\left\\langle x, x\\right\\rangle} \\tag{21} \\] <p>Therefore we can show that</p> \\[ \\begin{align} &amp; \\left\\langle x_1, y\\right\\rangle=\\hat{\\theta}_1\\left\\langle x_1, x_1\\right\\rangle \\Rightarrow \\hat{\\theta}_1=\\frac{\\left\\langle x_1, y\\right\\rangle}{\\left\\langle x_1, x_1\\right\\rangle}=\\frac{x_1^T y}{x_1^T x_1} \\\\ &amp; \\left\\langle x_2, y\\right\\rangle=\\hat{\\theta}_2\\left\\langle x_2, x_2\\right\\rangle \\Rightarrow \\hat{\\theta}_2=\\frac{\\left\\langle x_2, y\\right\\rangle}{\\left\\langle x_2, x_2\\right\\rangle}=\\frac{x_2^T y}{x_2^T x_2}, \\ldots \\\\ &amp; \\left\\langle x_n, y\\right\\rangle=\\hat{\\theta}_n\\left\\langle x_n, x_n\\right\\rangle \\Rightarrow \\hat{\\theta}_n=\\frac{\\left\\langle x_n, y\\right\\rangle}{\\left\\langle x_n, x_n\\right\\rangle}=\\frac{x_n^T y}{x_n^T x_n}, \\end{align} \\tag{22} \\] <p>Following the same idea, we can also show that</p> \\[ \\langle y, y\\rangle=\\hat{\\theta}_1^2\\left\\langle x_1, x_1\\right\\rangle+\\hat{\\theta}_2^2\\left\\langle x_2, x_2\\right\\rangle+\\ldots+\\hat{\\theta}_n^2\\left\\langle x_n, x_n\\right\\rangle+\\langle e, e\\rangle \\tag{23} \\] <p>which can be described as</p> \\[ y^T y=\\hat{\\theta}_1^2 x_1^T x_1+\\hat{\\theta}_2^2 x_2^T x_2+\\ldots+\\hat{\\theta}_n^2 x_n^T x_n+e^T e \\tag{24} \\] <p>or</p> \\[ \\|y\\|^2=\\hat{\\theta}_1^2\\left\\|x_1\\right\\|^2+\\hat{\\theta}_2^2\\left\\|x_2\\right\\|^2+\\ldots+\\hat{\\theta}_n^2\\left\\|x_n\\right\\|^2+\\|e\\|^2 \\tag{25} \\] <p>So, dividing both sides of the equation by \\(y\\) and rearranging the equation, we have</p> \\[ \\frac{\\|e\\|^2}{\\|y\\|^2}=1-\\hat{\\theta}_1^2 \\frac{\\left\\|x_1\\right\\|^2}{\\|y\\|^2}-\\hat{\\theta}_2^2 \\frac{\\left\\|x_2\\right\\|^2}{\\|y\\|^2}-\\ldots-\\hat{\\theta}_n^2 \\frac{\\left\\|x_n\\right\\|^2}{\\|y\\|^2} \\tag{26} \\] <p>Because \\(\\hat{\\theta}_k=\\frac{x_k^T y}{x_k^T x_k}=\\frac{x_k^T y}{\\left\\|x_k\\right\\|^2}, k=1,2, . ., n\\), we have</p> \\[ \\begin{align} \\frac{\\|e\\|^2}{\\|y\\|^2} &amp; =1-\\left(\\frac{x_1^T y}{\\left\\|x_1\\right\\|^2}\\right)^2 \\frac{\\left\\|x_1\\right\\|^2}{\\|y\\|^2}-\\left(\\frac{x_2^T y}{\\left\\|x_2\\right\\|^2}\\right)^2 \\frac{\\left\\|x_2\\right\\|^2}{\\|y\\|^2}-\\ldots-\\left(\\frac{x_n^T y}{\\left\\|x_n\\right\\|^2}\\right)^2 \\frac{\\left\\|x_n\\right\\|^2}{\\|y\\|^2} \\\\ &amp; =1-\\frac{\\left(x_1^T y\\right)^2}{\\left\\|x_1\\right\\|\\left\\|^2\\right\\| y \\|^2}-\\frac{\\left(x_2^T y\\right)^2}{\\left\\|x_2\\right\\|^2\\|y\\|^2}-\\cdots-\\frac{\\left(x_n^T y\\right)^2}{\\left\\|x_n\\right\\|^2\\|y\\|^2} \\\\ &amp; =1-ERR_1 \\quad-ERR_2-\\cdots-ERR_n \\end{align} \\tag{27} \\] <p>where \\(\\operatorname{ERR}_k(k=1,2 \\ldots, n)\\) is the Error Reduction Ratio defined in previous section.</p> <p>Check the example bellow using the fundamental basis</p> <pre><code>import numpy as np\n\ny = np.array([3, 7, 8])\n# Orthogonal Basis\nx1 = np.array([1, 0, 0])\nx2 = np.array([0, 1, 0])\nx3 = np.array([0, 0, 1])\n\ntheta1 = (x1.T@y)/(x1.T@x1)\ntheta2 = (x2.T@y)/(x2.T@x2)\ntheta3 = (x3.T@y)/(x3.T@x3)\n\nsquared_y = y.T @ y\nerr1 = (x1.T@y)**2/((x1.T@x1) * squared_y)\nerr2 = (x2.T@y)**2/((x2.T@x2) * squared_y)\nerr3 = (x3.T@y)**2/((x3.T@x3) * squared_y)\n\nprint(f\"x1 represents {round(err1*100, 2)}% of the variation in y, \\n x2 represents {round(err2*100, 2)}% of the variation in y, \\n x3 represents {round(err3*100, 2)}% of the variation in y\")\n\nx1 represents 7.38% of the variation in y,\nx2 represents 40.16% of the variation in y,\nx3 represents 52.46% of the variation in y\n</code></pre> <p>Lets see what happens in a non-orthogonal scenario.</p> <p><pre><code>y = np.array([3, 7, 8])\nx1 = np.array([1, 2, 2])\nx2 = np.array([-1, 0, 2])\nx3 = np.array([0, 0, 1])\n\ntheta1 = (x1.T@y)/(x1.T@x1)\ntheta2 = (x2.T@y)/(x2.T@x2)\ntheta3 = (x3.T@y)/(x3.T@x3)\n\nsquared_y = y.T @ y\nerr1 = (x1.T@y)**2/((x1.T@x1) * squared_y)\nerr2 = (x2.T@y)/((x2.T@x2) * squared_y)\nerr3 = (x3.T@y)**2/((x3.T@x3) * squared_y)\n\nprint(f\"x1 represents {round(err1*100, 2)}% of the variation in y, \\n x2 represents {round(err2*100, 2)}% of the variation in y, \\n x3 represents {round(err3*100, 2)}% of the variation in y\")\n\n&gt;&gt;&gt; x1 represents 99.18% of the variation in y,\n&gt;&gt;&gt; x2 represents 2.13% of the variation in y,\n&gt;&gt;&gt; x3 represents 52.46% of the variation in y\n</code></pre> In this case, \\(x1\\) have the highest \\(err\\) value, so we have chosen it to be the first orthogonal vector.</p> <pre><code>q1 = x1.copy()\n\nv1 = x2 - (q1.T@x2)/(q1.T@q1)*q1\nerrv1 = (v1.T@y)**2/((v1.T@v1) * squared_y)\n\nv2 = x3 - (q1.T@x3)/(q1.T@q1)*q1\nerrv2 = (v2.T@y)**2/((v2.T@v2) * squared_y)\n\nprint(f\"v1 represents {round(errv1*100, 2)}% of the variation in y, \\n v2 represents {round(errv2*100, 2)}% of the variation in y\")\n\n&gt;&gt;&gt; v1 represents 0.82% of the variation in y,\n&gt;&gt;&gt; v2 represents 0.66% of the variation in y\n</code></pre> <p>So, in this case, when we sum the err values of the first two orthogonal vectors, \\(x1\\) and \\(v1\\), we get \\(err_3 + errv1 = 100\\%\\). Then there is no need to keep the iterations looking for more terms. The model with this two terms already explain all the variance in the data.</p> <p>That's the idea of the FROLS algorithm. We calculate the ERR, choose the vector with the highest ERR to be the first orthogonal vector, orthogonalize every vector but the one we choose in the first step, calculate the ERR for each one of them, choose the vector with the highest ERR value and keep doing that until we reach some criteria.</p> <p>In SysIdentPy, we have 2 hyperparameters called <code>n_terms</code> and <code>err_tol</code>. Both of them can be used to stop the iterations. The first one will iterate until <code>n_terms</code> are chosen. The second one iterate until the \\(\\sum ERR_i &gt; err_{tol}\\) . If you set both, the algorithm stop when any of the conditions is true.</p> <pre><code>model = FROLS(\n\u00a0 \u00a0 \u00a0 \u00a0 n_terms=50,\n\u00a0 \u00a0 \u00a0 \u00a0 ylag=7,\n\u00a0 \u00a0 \u00a0 \u00a0 xlag=7,\n\u00a0 \u00a0 \u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 \u00a0 \u00a0 err_tol=0.98\n\u00a0 \u00a0 )\n</code></pre> <p>SysIdentPy apply the Golub -Householder method for the orthogonal decomposition. A more detailed discussion about Householder and orthogonalization procedures in general can be found in Chen, S. and Billings, S. A. and Luo, W.</p>"},{"location":"book/4-Model-Structure-Selection/#case-study","title":"Case Study","text":"<p>An example using real data will be described using SysIdentPy. In this example, we will build models linear and nonlinear models to describe the behavior of a DC motor operating as generator. Details of the experiment used to generate this data can be found in the paper (in Portuguese) IDENTIFICA\u00c7\u00c3O DE UM MOTOR/GERADOR CC POR MEIO DE MODELOS POLINOMIAIS AUTORREGRESSIVOS E REDES NEURAIS ARTIFICIAIS</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n\ndf1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n\n# checking the ouput\ndf2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <p></p> <p>Figure 2. Output of the electromechanical system.</p> <p>In this example, we will decimate the data using \\(d = 500\\). The rationale behind decimation here is that the data is oversampled due to the experimental setup. A future section will provide a detailed explanation of how to handle oversampled data in the context of system identification. For now, consider this approach as the most appropriate solution.</p> <pre><code>x_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre> <p>In this case, we will build a NARX model. In SysIdentPy, this means setting <code>unbiased=False</code> in the <code>LeastSquares</code> definition. We'll use a <code>Polynomial</code> basis function and set the maximum lag for both input and output to 2. This configuration results in 15 terms in the information matrix, so we'll set <code>n_terms=15</code>. This specification is necessary because, in this example, <code>order_selection</code> is set to <code>False</code>. We will discuss <code>order_selection</code> in more detail in the Information Criteria section later on.</p> <p><code>order_selection</code> is <code>True</code> by default in SysIdentPy. When <code>order_selection=False</code> the user must pass a values to <code>n_terms</code> because it is an optional argument and its default value is <code>None</code>. If we set <code>n_terms=5</code>, for exemple, the FROLS will stop after choosing the first 5 regressors. We do not want that in this case because we want the FROLS stop only when <code>e_tol</code> is reached.</p> <pre><code>basis_function = Polynomial(degree=2)\n\nmodel = FROLS(\n    order_selection=False,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 e_tol=0.9999\n\u00a0 \u00a0 n_terms=15\n)\n</code></pre> <p>SysIdentPy aims to simplify the use of algorithms like <code>FROLS</code> for the user. Building, training, or fitting a model is made straightforward through a simple interface called <code>fit</code>. By using this method, the entire process is handled internally, requiring no further interaction from the user.</p> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <p>SysIdentPy also offers a method to retrieve detailed information about the fitted model. Users can check the terms included in the model, the estimated parameters, the Error Reduction Ratio (ERR) values, and more.</p> <p>We're using <code>pandas</code> here only to make the output more readable, but it's optional.</p> <pre><code>r = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR y(k-1) 1.0998E+00 9.86000384E-01 x1(k-1)^2 1.0165E+02 7.94805130E-03 y(k-2)^2 -1.9786E-05 2.50905908E-03 x1(k-1)y(k-1) -1.2138E-01 1.43301039E-03 y(k-2) -3.2621E-01 1.02781443E-03 x1(k-1)y(k-2) 5.3596E-02 5.35200312E-04 x1(k-2) 3.4655E+02 2.79648078E-04 x1(k-2)y(k-1) -5.1647E-02 1.12211942E-04 x1(k-2)x1(k-1) -8.2162E+00 4.54743448E-05 y(k-2)y(k-1) 4.0961E-05 3.25346101E-05 &gt;Table 1 <p>The table above shows that 10 regressors (out of the 15 available) were needed to reach the defined <code>e_tol</code>, with the sum of the ERR for the selected regressors being \\(0.99992\\).</p> <p>Next, let's evaluate the model's performance using the test data. Similar to the <code>fit</code> method, SysIdentPy provides a <code>predict</code> method. To obtain the predicted values and plot the results, simply follow these steps:</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n</code></pre> <p></p> <p>Figure 3. Free run simulation (or infinity-steps ahead prediction) of the fitted model.</p>"},{"location":"book/4-Model-Structure-Selection/#information-criteria","title":"Information Criteria","text":"<p>We said that there are many ways to terminate the algorithm and select the model terms, but only ERR criteria was defined in previous section. Different ways to terminate the algorithm is by using some information criteria, e.g, Akaike Information Criteria (AIC). For Least Squares based regression analysis, the AIC indicates the number of regressors by minimizing the objective function (Akaike, H. - A new look at the statistical model identification):</p> \\[ \\begin{equation}     J_{\\text{AIC}} = \\underbrace{n\\log\\left(Var[\\xi_k]\\right)}_{\\text{first component}}+\\underbrace{2n_{\\Theta}}_{\\text{{second component}}}. \\end{equation} \\tag{28} \\] <p>It is important to note that the equation above illustrates a trade-off between model fit and model complexity. Specifically, this trade-off involves balancing the model's ability to accurately fit the data (the first component) against its complexity, which is related to the number of parameters included (the second component). As additional terms are included in the model, the Akaike Information Criterion (AIC) value initially decreases, reaching a minimum that represents an optimal balance between model complexity and predictive accuracy. However, if the number of parameters becomes excessive, the penalty for complexity outweighs the benefit of a better fit, causing the AIC value to increase. The AIC and many others variants have been extensively used for linear and nonlinear system identification. Check Wei, H. and Zhu, D. and Billings, S. A. and Balikhin, M. A. - Forecasting the geomagnetic activity of the Dst index using multiscale radial basis function networks, Martins, S. A. M. and Nepomuceno, E. G. and Barroso, M. F. S. - Improved Structure Detection For Polynomial NARX Models Using a Multiobjective Error Reduction Ratio, Hafiz, F. and Swain, A. and Mendes, E. M. A. M. and Patel, N. - Structure Selection of Polynomial NARX Models Using Two Dimensional (2D) Particle Swarms, Gu, Y. and Wei, H. and Balikhin, M. M. - Nonlinear predictive model selection and model averaging using information criteria and references therein.</p> <p>Despite their effectiveness in many linear model selection scenarios, information criteria such as AIC can struggle to select an appropriate number of parameters when dealing with systems exhibiting significant nonlinear behavior. Additionally, these criteria may lead to suboptimal models if the search space does not encompass all the necessary terms required to accurately represent the true model. Consequently, in highly nonlinear systems or when critical model components are missing, information criteria might not provide reliable guidance, resulting in models that exhibit poor performance.</p> <p>Besides AIC, SysIdentPy provides other four different information criteria: Bayesian Information Criteria (BIC), Final Prediction Error (FPE), Low of Iterated Logarithm Criteria (LILC), and Corrected Akaike Information Criteria (AICc), which can be described respectively as</p> \\[ \\begin{align} \\operatorname{FPE}\\left(n_\\theta\\right) &amp; =N \\ln \\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]+N \\ln \\left[\\frac{N+n_\\theta}{N-n_\\theta}\\right] \\\\ B I C\\left(n_\\theta\\right) &amp; =N \\ln \\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]+n_\\theta \\ln N \\\\ A I C c &amp;=A I C+2 n_p * \\frac{n_p+1}{N-n_p-1} \\\\ LILC &amp;= 2n_{\\theta}\\ln(\\ln(N)) + N \\ln(\\left[\\sigma_{\\text {erro }}^2\\left(n_\\theta\\right)\\right]) \\end{align} \\tag{29} \\] <p>To use any information criteria in SysIdentPy, set <code>order_selection=True</code> (as said before, the default value is already <code>True</code>). Besides <code>order_selection</code>, you can define how many regressors you want to evaluate before stopping the algorithm by using the <code>n_info_values</code> hyperparameter. The default value is \\(15\\), but the user should increase it based on how many regressors exists given the <code>ylag</code>, <code>xlag</code> and the degree of the basis function.</p> <p>Using information Criteria can take a long time depending on how many regressors you are evaluating and the number of samples. To calculate the criteria, the ERR algorithm is executed <code>n</code> times, where <code>n</code> is the number defined in <code>n_info_values</code>. Make sure to understand how it works to define whether you have to use it or not.</p> <p>Running the same example, but now using the BIC information criteria to select the order of the model, we have</p> <pre><code>model = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function\n)\nmodel.fit(X=x_train, y=y_train)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n</code></pre> Regressors Parameters ERR y(k-1) 1.3666E+00 9.86000384E-01 x1(k-1)^2 1.0500E+02 7.94805130E-03 y(k-2)^2 -5.8577E-05 2.50905908E-03 x1(k-1)y(k-1) -1.2427E-01 1.43301039E-03 y(k-2) -5.1414E-01 1.02781443E-03 x1(k-1)y(k-2) 5.3001E-02 5.35200312E-04 x1(k-2) 3.1144E+02 2.79648078E-04 x1(k-2)y(k-1) -4.8013E-02 1.12211942E-04 x1(k-2)x1(k-1) -8.0561E+00 4.54743448E-05 x1(k-2)y(k-2) 4.1381E-03 3.25346101E-05 1 -5.6653E+01 7.54107553E-06 y(k-2)y(k-1) 1.5679E-04 3.52002717E-06 y(k-1)^2 -9.0164E-05 6.17373260E-06 &gt;Table 2 <p>In this case, instead of 8 regressors, the final model have 13 terms.</p> <p>Currently, the number of regressors is determined by identifying the index of the last value where the difference between the current and previous value is less than 0. To inspect these values, you can use the following approach:</p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <p></p> <p>Figure 4. The plot shows the Information Criterion values (BIC) as a function of the number of terms included in the model. The model selection process, using the BIC criterion, iteratively adds regressors until the BIC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the BIC value stops decreasing marks the optimal number of terms, resulting in a final model with 13 terms.</p> <p>The model prediction in this case is shown in Figure 5</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n</code></pre> <p></p> <p>Figure 5. Free run simulation (or infinity-steps ahead prediction) of the fitted model using BIC.</p>"},{"location":"book/4-Model-Structure-Selection/#overview-of-the-information-criteria-methods","title":"Overview of the Information Criteria Methods","text":"<p>In this section, simulated data are used to provide users with a clearer understanding of the information criteria available in SysIdentPy.</p> <p>Here, we're working with a known model structure, which allows us to focus on how different information criteria perform. When dealing with real data, the correct number of terms in the model is unknown, making these methods invaluable for guiding model selection.</p> <p>If you review the metrics below, you'll notice excellent performance across all models. However, it's crucial to remember that System Identification is about finding the optimal model structure. Model Structure Selection is at the heart of NARMAX methods!</p> <p>The data is generated by simulating the following model:</p> \\[ y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_k \\tag{30} \\] <p>If <code>colored_noise</code> is set to <code>True</code>, the noise term is defined as:</p> \\[ e_k = 0.8\\nu_{k-1} + \\nu_k \\tag{31} \\] <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a Gaussian-distributed variable with \\(\\mu = 0\\) and \\(\\sigma = 0.1\\).</p> <p>In the next example, we will generate data with 100 samples, using white noise, and select 70% of the data to train the model.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n\u00a0 \u00a0 n=100, colored_noise=False, sigma=0.1, train_percentage=70\n)\n</code></pre> <p>The idea is to show the impact of the information criteria to select the number of terms to compose the final model. You will se why it is an auxiliary tool and let the algorithm select the number of terms based on the minimum value is not always a good idea when dealing with data highly corrupted by noise (even white noise).</p>"},{"location":"book/4-Model-Structure-Selection/#aic","title":"AIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <p>The regressors, the free run simulation and the AIC values are detailed bellow.</p> Regressors Parameters ERR x1(k-2) 9.4236E-01 9.26094341E-01 y(k-1) 2.4933E-01 3.35898283E-02 x1(k-1)y(k-1) 1.3001E-01 2.35736200E-03 x1(k-1) 8.4024E-02 4.11741791E-03 x1(k-1)^2 7.0807E-02 2.54231877E-03 x1(k-2)^2 -9.1138E-02 1.39658893E-03 y(k-1)^2 1.1698E-01 1.70257419E-03 x1(k-2)y(k-2) 8.3745E-02 1.11056684E-03 y(k-2)^2 -4.1946E-02 1.01686239E-03 x1(k-2)x1(k-1) 5.9034E-02 7.47435512E-04 &gt;Table 3 <p></p> <p>Figure 5. Free run simulation (or infinity-steps ahead prediction) of the fitted model using AIC.</p> <p></p> <p>Figure 6. The plot shows the Information Criterion values (AIC) as a function of the number of terms included in the model. The model selection process, using the AIC criterion, iteratively adds regressors until the AIC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the AICc value stops decreasing marks the optimal number of terms, resulting in a final model with 10 terms.</p> <p>For this case, we have a model with 10 terms. We know that the correct number is 3 because of the simulated system we are using as example.</p>"},{"location":"book/4-Model-Structure-Selection/#aicc","title":"AICc","text":"<p>The only change we have to do to use AICc instead of AIC is changing the information criteria hyperparameter: <code>information_criteria=\"aicc\"</code></p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aicc\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.2282E-01 9.26094341E-01 y(k-1) 2.4294E-01 3.35898283E-02 x1(k-1)y(k-1) 1.2753E-01 2.35736200E-03 x1(k-1) 6.9597E-02 4.11741791E-03 x1(k-1)^2 7.0578E-02 2.54231877E-03 x1(k-2)^2 -1.0523E-01 1.39658893E-03 y(k-1)^2 1.0949E-01 1.70257419E-03 x1(k-2)y(k-2) 7.1821E-02 1.11056684E-03 y(k-2)^2 -3.9756E-02 1.01686239E-03 &gt;Table 4 <p></p> <p>Figure 7. Free run simulation (or infinity-steps ahead prediction) of the fitted model using AICc.</p> <p></p> <p>Figure 8. The plot shows the Information Criterion values (AICc) as a function of the number of terms included in the model. The model selection process, using the AIC criterion, iteratively adds regressors until the AICc reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the AICc value stops decreasing marks the optimal number of terms, resulting in a final model with 9 terms.</p> <p>This time we have a model with 9 regressors.</p>"},{"location":"book/4-Model-Structure-Selection/#bic","title":"BIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1726E-01 9.26094341E-01 y(k-1) 1.8670E-01 3.35898283E-02 &gt;Table 5 <p>Figure 9. Free run simulation (or infinity-steps ahead prediction) of the fitted model using BIC.</p> <p></p> <p>Figure 10. The plot shows the Information Criterion values (BIC) as a function of the number of terms included in the model. The model selection process, using the BIC criterion, iteratively adds regressors until the BIC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the BIC value stops decreasing marks the optimal number of terms, resulting in a final model with 2 terms.</p> <p>BIC returned a model with only 2 regressors!</p>"},{"location":"book/4-Model-Structure-Selection/#lilc","title":"LILC","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"lilc\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1160E-01 9.26094341E-01 y(k-1) 2.3178E-01 3.35898283E-02 x1(k-1)y(k-1) 1.2080E-01 2.35736200E-03 x1(k-1) 6.3113E-02 4.11741791E-03 x1(k-1)^2 5.4088E-02 2.54231877E-03 x1(k-2)^2 -9.0683E-02 1.39658893E-03 y(k-1)^2 8.2157E-02 1.70257419E-03 &gt;Table 6 <p>Figure 11. Free run simulation (or infinity-steps ahead prediction) of the fitted model using LILC.</p> <p></p> <p>Figure 12. The plot shows the Information Criterion values (LILC) as a function of the number of terms included in the model. The model selection process, using the LILC criterion, iteratively adds regressors until the LILC reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the LILC value stops decreasing marks the optimal number of terms, resulting in a final model with 7 terms.</p> <p>LILC returned a model with 7 regressors.</p>"},{"location":"book/4-Model-Structure-Selection/#fpe","title":"FPE","text":"<pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"fpe\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> Regressors Parameters ERR x1(k-2) 9.4236E-01 9.26094341E-01 y(k-1) 2.4933E-01 3.35898283E-02 x1(k-1)y(k-1) 1.3001E-01 2.35736200E-03 x1(k-1) 8.4024E-02 4.11741791E-03 x1(k-1)^2 7.0807E-02 2.54231877E-03 x1(k-2)^2 -9.1138E-02 1.39658893E-03 y(k-1)^2 1.1698E-01 1.70257419E-03 x1(k-2)y(k-2) 8.3745E-02 1.11056684E-03 y(k-2)^2 -4.1946E-02 1.01686239E-03 x1(k-2)x1(k-1) 5.9034E-02 7.47435512E-04 &gt;Table 7 <p>Figure 13. Free run simulation (or infinity-steps ahead prediction) of the fitted model using FPE.</p> <p></p> <p>Figure 14. The plot shows the Information Criterion values (FPE) as a function of the number of terms included in the model. The model selection process, using the FPE criterion, iteratively adds regressors until the FPE reaches a minimum, indicating the optimal balance between model complexity and fit. The point where the FPE value stops decreasing marks the optimal number of terms, resulting in a final model with 10 terms.</p> <p>FPE returned a model with 10 regressors.</p>"},{"location":"book/4-Model-Structure-Selection/#meta-model-structure-selection-metamss","title":"Meta Model Structure Selection (MetaMSS)","text":"<p>This section largely reflects content from a paper I published on ArXiv titled \"Meta-Model Structure Selection: Building Polynomial NARX Models for Regression and Classification.\" This paper was initially written for journal publication based on the results of my master's thesis. However, as I transitioned into a Data Scientist role and considering the lengthy journal submission process and academic delays, I decided not to pursue journal publication at this time. Thus, the paper remains available only on ArXiv.</p> <p>The work extends a previous paper I presented at a Brazilian conference (in Portuguese), where part of the results were initially shared.</p> <p>This section introduces a meta-heuristic approach for selecting the structure of polynomial NARX models in regression tasks. The proposed method considers both the complexity of the model and the contribution of each term to construct parsimonious models through a novel cost function formulation. The robustness of this new algorithm is evaluated using various simulated and experimental systems with different nonlinear characteristics. The results demonstrate that the algorithm effectively identifies the correct model when the true structure is known and produces parsimonious models for experimental data, even in cases where traditional and contemporary methods often fail. The new approach is compared against classical methods such as FROLS and recent randomized techniques.</p> <p>We mentioned that selecting the appropriate model terms is crucial for accurately capturing the dynamics of the original system. Challenges such as over-parameterization and numerical ill-conditioning often arise due to the limitations of existing identification algorithms in selecting the right terms for the final model. Check Aguirre, L. A. and Billings, S. A. - Dynamical effects of overparametrization in nonlinear models, Piroddi, L. and Spinelli, W. - An identification algorithm for polynomial NARX models based on simulation error minimization. We also mentioned that one of the most traditionally algorithms for structure selection of polynomial NARMAX is the ERR algorithm. Numerous variants of FROLS algorithm has been developed to improve the model selection performance such as Billings, S. A., Chen, S., and Korenberg, M. J. - Identification of MIMO non-linear systems using a forward-regression orthogonal estimator, Farina, M. and Piroddi, L. - Simulation Error Minimization\u2013Based Identification of Polynomial Input\u2013Output Recursive Models, Guo, Y., Guo, L. Z., Billings, S. A., and Wei, H. - A New Iterative Orthogonal Forward Regression Algorithm, Mao, K. Z. and Billings, S. A. - VARIABLE SELECTION IN NON-LINEAR SYSTEMS MODELLING. The drawbacks of the FROLS have been extensively reviewed in the literature, e.g., in Billings, S. A. and Aguirre, L. A., Palumbo, P. and Piroddi, L., Falsone, A., Piroddi, L., and Prandini, M.. Most of these weak points are related to (i) the Prediction Error Minimization (PEM) framework; (ii) the inadequacy of the ERR index in measuring the absolute importance of regressors; (iii) the use of information criteria such as AIC, FPE and the BIC, to select the model order. Regarding the information criteria, although these techniques work well for linear models, in a nonlinear context no simple relation between model size and accuracy can be established Falsone, A., Piroddi, L., and Prandini, M. - A randomized algorithm for nonlinear model structure selection, Chen, S., Hong, X., and Harris, C. J. - Sparse kernel regression modeling using combined locally regularized orthogonal least squares and D-optimality experimental design.</p> <p>Due to the limitations of Ordinary Least Squares (OLS)-based algorithms, recent research has presented solutions that diverged from the classical FROLS approach. New methods have reformulated the Model Structure Selection (MSS) process within a probabilistic framework and employed random sampling techniques Falsone, A., Piroddi, L., and Prandini, M. - A randomized algorithm for nonlinear model structure selection, Tempo, R., Calafiore, G., and Dabbene, F. - Randomized Algorithms for Analysis and Control of Uncertain Systems: With Applications, Baldacchino, T., Anderson, S. R., and Kadirkamanathan, V. - Computational system identification for Bayesian NARMAX modelling, Rodriguez-Vazquez, K., Fonseca, C. M., and Fleming, P. J. - Identifying the structure of nonlinear dynamic systems using multiobjective genetic programming, Severino, A. G. V. and Araujo, F. M. U. de. Despite their advancements, these meta-heuristic and probabilistic approaches exhibit certain shortcomings. In particular, these methods often rely on information criteria such as AIC, FPE, and BIC to define the cost function for optimization, which frequently leads to over-parameterized models.</p> <p>Consider \\(\\mathcal{F}\\) as a class of bounded functions \\(\\phi: \\mathbf{R} \\mapsto \\mathbf{R}\\). If the properties of \\(\\phi(x)\\) satisfy</p> \\[ \\begin{align}     &amp;\\lim\\limits_{x \\to \\infty} \\phi(x) = \\alpha \\nonumber \\\\     &amp;\\lim\\limits_{x \\to -\\infty} \\phi(x) = \\beta \\quad \\text{with } \\alpha &gt; \\beta,  \\nonumber \\end{align} \\tag{32} \\] <p>the function is called sigmoidal.</p> <p>In this particular case and following definition Equation 32 with \\(alpha = 0\\) and \\(\\beta = 1\\), we write a \"S\" shaped curve as</p> \\[ \\begin{equation}     \\varsigma(x) = \\frac{1}{1+e^{-a(x-c)}}. \\end{equation} \\tag{33} \\] <p>In that case, we can specify \\(a\\), the rate of change. If \\(a\\) is close to zero, the sigmoid function will be gradual. If \\(a\\) is large, the sigmoid function will have an abrupt or sharp transition. If \\(a\\) is negative, the sigmoid will go from \\(1\\) to zero. The parameter \\(c\\) corresponds to the x value where \\(y = 0.5\\).</p> <p>The Sigmoid Linear Unit Function (SiLU) is defined by the sigmoid function multiplied by its input</p> \\[ \\begin{equation}     \\text{silu}(x) = x \\varsigma(x), \\end{equation} \\tag{34} \\] <p>which can be viewed as a steeper sigmoid function with overshoot.</p>"},{"location":"book/4-Model-Structure-Selection/#meta-heuristics","title":"Meta-heuristics","text":"<p>Over the past two decades, nature-inspired optimization algorithms have gained prominence due to their flexibility, simplicity, versatility, and ability to avoid local optima in real-world applications.</p> <p>Meta-heuristic algorithms are characterized by two fundamental features: exploitation and exploration Blum, C. and Roli, A. - Metaheuristics in combinatorial optimization: Overview and conceptual comparison. Exploitation focuses on utilizing local information to refine the search around the current best solution, improving the quality of nearby solutions. Conversely, exploration aims to search a broader area of the solution space to discover potentially superior solutions and prevent the algorithm from getting trapped in local optima.</p> <p>Despite the lack of a universal consensus on the definitions of exploration and exploitation in evolutionary computing, as highlighted by Eiben, Agoston E and Schippers, Cornelis A, it is generally agreed that these concepts function as opposing forces that are challenging to balance. To address this challenge, hybrid metaheuristics combine multiple algorithms to leverage both exploitation and exploration, resulting in more robust optimization methods.</p>"},{"location":"book/4-Model-Structure-Selection/#the-binary-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-bpsogsa-algorithm","title":"The Binary hybrid Particle Swarm Optimization and Gravitational Search Algorithm (BPSOGSA) algorithm","text":"<p>Achieving a balance between exploration and exploitation is a significant challenge in most meta-heuristic algorithms. For this method, we enhance performance and flexibility in the search process by employing a hybrid approach that combines Binary Particle Swarm Optimization (BPSO) with Gravitational Search Algorithm (GSA), as proposed by Mirjalili, S. and Hashim, S. Z. M.. This hybrid method incorporates a low-level co-evolutionary heterogeneous technique originally introduced by Talbi, E. G..</p> <p>The BPSOGSA approach leverages the strengths of both algorithms: the Particle Swarm Optimization (PSO) component is known to be good at exploring the entire search space to identify the global optimum, while the Gravitational Search Algorithm (GSA) component effectively refines the search by focusing on local solutions within a binary space. This combination aims to provide a more comprehensive and effective optimization strategy, ensuring a better balance between exploration and exploitation.</p>"},{"location":"book/4-Model-Structure-Selection/#standard-particle-swarm-optimization-pso","title":"Standard Particle Swarm Optimization (PSO)","text":"<p>In Particle Swarm Optimization (PSO) Kennedy, J. and Eberhart, R. C., Kennedy, J., each particle represents a candidate solution and is characterized by two components: its position in the search space, denoted as \\(\\vec{x}_{\\,np,d} \\in \\mathbb{R}^{np \\times d}\\), and its velocity, \\(\\vec{v}_{\\,np,d} \\in \\mathbb{R}^{np \\times d}\\). Here, \\(np = 1, 2, \\ldots, n_a\\) where \\(n_a\\) is the size of the swarm, and \\(d\\) is the dimensionality of the problem. The initial population is represented as follows:</p> \\[ \\vec{x}_{\\,np,d} = \\begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,d} \\\\ x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,d} \\\\ \\vdots  &amp; \\vdots  &amp; \\ddots &amp; \\vdots \\\\ x_{n_a,1} &amp; x_{n_a,2} &amp; \\cdots &amp; x_{n_a,d} \\end{bmatrix} \\tag{35} \\] <p>At each iteration \\(t\\), the position and velocity of a particle are updated using the following equations:</p> \\[ v_{np,d}^{t+1} = \\zeta v_{np,d}^{t} + c_1 \\kappa_1 (pbest_{np}^{t} - x_{np,d}^{t}) + c_2 \\kappa_2 (gbest_{np}^{t} - x_{np,d}^{t}), \\tag{36} \\] <p>where \\(\\kappa_j \\in \\mathbb{R}\\) for \\(j = [1,2]\\) are continuous random variables in the interval \\([0,1]\\), \\(\\zeta \\in \\mathbb{R}\\) is the inertia factor that controls the influence of the previous velocity on the current one and represents a trade-off between exploration and exploitation, \\(c_1\\) is the cognitive factor associated with the personal best position \\(pbest\\), and \\(c_2\\) is the social factor associated with the global best position \\(gbest\\). The velocity \\(\\vec{v}_{\\,np,d}\\) is typically constrained within the range \\([v_{min}, v_{max}]\\) to prevent particles from moving outside the search space. The updated position is then computed as:</p> \\[ x_{np,d}^{t+1} = x_{np,d}^{t} + v_{np,d}^{t+1}. \\tag{37} \\]"},{"location":"book/4-Model-Structure-Selection/#standard-gravitational-search-algorithm-gsa","title":"Standard Gravitational Search Algorithm (GSA)","text":"<p>In the Gravitational Search Algorithm (GSA) Rashedi, Esmat, Nezamabadi-Pour, Hossein, and Saryazdi, Saeid - GSA: A Gravitational Search Algorithm, agents are represented by masses, where the magnitude of each mass is proportional to the fitness value of the agent. These masses interact through gravitational forces, attracting each other towards locations closer to the global optimum. Heavier masses (agents with better fitness) move more slowly, while lighter masses (agents with poorer fitness) move more rapidly. Each mass in GSA has four properties: position, inertial mass, active gravitational mass, and passive gravitational mass. The position of a mass represents a candidate solution to the problem, and its gravitational and inertial masses are derived from the fitness function.</p> <p>Consider a population of agents as described by the following equations. At a specific time \\(t\\), the velocity and position of each agent are updated as follows:</p> \\[ \\begin{align}     v_{i,d}^{t+1} &amp;= \\kappa_i \\times v_{i,d}^t + a_{i,d}^t, \\\\     x_{i,d}^{t+1} &amp;= x_{i,d}^t + v_{i,d}^{t+1}. \\end{align} \\tag{38} \\] <p>Here, \\(\\kappa_i\\) introduces stochastic characteristics to the search process. The acceleration \\(a_{i,d}^t\\) is computed according to the law of motion Rashedi, Esmat and Nezamabadi-Pour, Hossein and Saryazdi, Saeid:</p> \\[ \\begin{equation}     a_{i,d}^t = \\frac{F_{i,d}^t}{M_{ii}^{t}}, \\end{equation} \\tag{39} \\] <p>where \\(M_{ii}^{t}\\) is the inertial mass of agent \\(i\\) and \\(F_{i,d}^t\\) represents the gravitational force acting on agent \\(i\\) in the \\(d\\)-dimensional space. The detailed process for calculating and updating \\(F_{i,d}\\) and \\(M_{ii}\\) can be found in Rashedi, Esmat and Nezamabadi-Pour, Hossein and Saryazdi, Saeid.</p>"},{"location":"book/4-Model-Structure-Selection/#the-binary-hybrid-optimization-algorithm","title":"The Binary Hybrid Optimization Algorithm","text":"<p>The combination of algorithms follows the approach described in Mirjalili, S. and Hashim, S. Z. M. - A new hybrid PSOGSA algorithm for function optimization:</p> \\[ \\begin{align}     v_{i}^{t+1} = \\zeta \\times v_i^t + \\mathrm{c}'_{1} \\times \\kappa \\times a_i^t + \\mathrm{c}'_2 \\times \\kappa \\times (gbest - x_i^t), \\end{align} \\tag{40} \\] <p>where \\(\\mathrm{c}'_j \\in \\mathbb{R}\\) are acceleration coefficients. This formulation accelerates the exploitation phase by incorporating the best mass location found so far. However, this method may negatively impact the exploration phase. To address this issue, Mirjalili, S., Mirjalili, S. M., and Lewis, A. - Grey Wolf Optimizer proposed adaptive values for \\(\\mathrm{c}'_j\\), as described in Mirjalili, S., Wang, Gai-Ge, and Coelho, L. dos S. - Binary optimization using hybrid particle swarm optimization and gravitational search algorithm:</p> \\[ \\begin{align}     \\mathrm{c}_1' &amp;= -2 \\times \\frac{t^3}{\\max(t)^3} + 2, \\\\     \\mathrm{c}_2' &amp;= 2 \\times \\frac{t^3}{\\max(t)^3} + 2. \\end{align} \\tag{41} \\] <p>In each iteration, the positions of particles are updated according to the following rules, with continuous space mapped to discrete solutions using a transfer function Mirjalili, S. and Lewis, A. - S-shaped versus V-shaped transfer functions for binary Particle Swarm Optimization:</p> \\[ \\begin{equation}     S(v_{ik}) = \\left|\\frac{2}{\\pi}\\arctan\\left(\\frac{\\pi}{2}v_{ik}\\right)\\right|. \\end{equation} \\tag{42} \\] <p>With a uniformly distributed random number \\(\\kappa \\in (0,1)\\), the positions of the agents in the binary space are updated as follows:</p> \\[ \\begin{equation}     x_{np,d}^{t+1} =     \\begin{cases}         (x_{np,d}^{t})^{-1}, &amp; \\text{if } \\kappa &lt; S(v_{ik}^{t+1}), \\\\         x_{np,d}^{t}, &amp; \\text{if } \\kappa \\geq S(v_{ik}^{t+1}).     \\end{cases} \\end{equation} \\tag{43} \\]"},{"location":"book/4-Model-Structure-Selection/#meta-model-structure-selection-metamss-building-narx-for-regression","title":"Meta-Model Structure Selection (MetaMSS): Building NARX for Regression","text":"<p>In this section, we explore the meta-heuristic approach for selecting the structure of NARX models using BPSOGSA proposed in my master's thesis. This method searches for the optimal model structure within a decision space defined by a predefined dictionary of regressors. The objective function for this optimization problem is based on the root mean squared error (RMSE) of the free-run simulation output, augmented by a penalty factor that accounts for the model's complexity and the contribution of each regressor to the final model.</p>"},{"location":"book/4-Model-Structure-Selection/#encoding-scheme","title":"Encoding Scheme","text":"<p>The process of using BPSOGSA for model structure selection involves defining the dimensions of the test function. Specifically, \\(n_y\\), \\(n_x\\), and \\(\\ell\\) are set to cover all possible regressors, and a general matrix of regressors, \\(\\Psi\\), is constructed. The number of columns in \\(\\Psi\\) is denoted as \\(noV\\), and the number of agents, \\(N\\), is specified. A binary \\(noV \\times N\\) matrix, referred to as \\(\\mathcal{X}\\), is then randomly generated to represent the position of each agent in the search space. Each column of \\(\\mathcal{X}\\) represents a potential solution, which is essentially a candidate model structure to be evaluated in each iteration. In this matrix, a value of 1 indicates that the corresponding column of \\(\\Psi\\) is included in the reduced matrix of regressors, while a value of 0 indicates exclusion.</p> <p>For example, consider a case where all possible regressors are defined with \\(\\ell = 1\\) and \\(n_y = n_u = 2\\). The matrix \\(\\Psi\\) is:</p> \\[ \\begin{align} [ \\text{constant} \\quad y(k-1) \\quad y(k-2) \\quad u(k-1) \\quad u(k-2) ] \\end{align} \\tag{44} \\] <p>With 5 possible regressors, \\(noV = 5\\). Assuming \\(N = 5\\), \\(\\mathcal{X}\\) might be represented as:</p> \\[ \\begin{equation}     \\mathcal{X} =     \\begin{bmatrix}         0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\         1 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\         0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\\\         0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\\\         1 &amp; 0 &amp; 1 &amp; 1 &amp; 0     \\end{bmatrix} \\end{equation} \\tag{45} \\] <p>Each column of \\(\\mathcal{X}\\) is transposed to generate a candidate solution. For example, the first column of \\(\\mathcal{X}\\) yields:</p> \\[ \\begin{equation*}     \\mathcal{X} =     \\begin{bmatrix}         \\text{constant} &amp; y(k-1) &amp; y(k-2) &amp; u(k-1) &amp; u(k-2) \\\\         1 &amp; 1 &amp; 1 &amp; 0 &amp; 1     \\end{bmatrix} \\end{equation*} \\tag{46} \\] <p>In this scenario, the first model to be evaluated is \\(\\alpha y(k-1) + \\beta u(k-2)\\), where \\(\\alpha\\) and \\(\\beta\\) are parameters estimated using some parameter estimation method. The process is repeated for each subsequent column of \\(\\mathcal{X}\\).</p>"},{"location":"book/4-Model-Structure-Selection/#formulation-of-the-objective-function","title":"Formulation of the objective function","text":"<p>For each candidate model structure randomly defined, the linear-in-the-parameters system can be solved directly using the Least Squares algorithm or any other method available in SysIdentPy. The variance of the estimated parameters can be calculated as:</p> \\[ \\hat{\\sigma}^2 = \\hat{\\sigma}_e^2 V_{jj}, \\tag{47} \\] <p>where \\(\\hat{\\sigma}_e^2\\) is the estimated noise variance, given by:</p> \\[ \\hat{\\sigma}_e^2 = \\frac{1}{N-m} \\sum_{k=1}^{N} (y_k - \\psi_{k-1}^\\top \\hat{\\Theta}), \\tag{48} \\] <p>and \\(V_{jj}\\) is the \\(j\\)th diagonal element of \\((\\Psi^\\top \\Psi)^{-1}\\).</p> <p>The estimated standard error of the \\(j\\)th regression coefficient \\(\\hat{\\Theta}_j\\) is the positive square root of the diagonal elements of \\(\\hat{\\sigma}^2\\):</p> \\[ \\mathrm{se}(\\hat{\\Theta}_j) = \\sqrt{\\hat{\\sigma}^2_{jj}}. \\tag{49} \\] <p>To assess the statistical relevance of each regressor, a penalty test considers the standard error of the regression coefficients. In this case, the \\(t\\)-test is used to perform a hypothesis test on the coefficients, evaluating the significance of individual regressors in the multiple linear regression model. The hypothesis statements are:</p> \\[ \\begin{align*}    H_0 &amp;: \\Theta_j = 0, \\\\    H_a &amp;: \\Theta_j \\neq 0. \\end{align*} \\tag{50} \\] <p>The \\(t\\)-statistic is computed as:</p> \\[ T_0 = \\frac{\\hat{\\Theta}_j}{\\mathrm{se}(\\hat{\\Theta}_j)}, \\tag{51} \\] <p>which measures how many standard deviations \\(\\hat{\\Theta}_j\\) is from zero. More precisely, if:</p> \\[ -t_{\\alpha/2, N-m} &lt; T_0 &lt; t_{\\alpha/2, N-m}, \\tag{52} \\] <p>where \\(t_{\\alpha/2, N-m}\\) is the \\(t\\) value obtained considering \\(\\alpha\\) as the significance level and \\(N-m\\) as the degrees of freedom, then if \\(T_0\\) falls outside this acceptance region, the null hypothesis \\(H_0: \\Theta_j = 0\\) is rejected. This implies that \\(\\Theta_j\\) is significant at the \\(\\alpha\\) level. Otherwise, if \\(T_0\\) lies within the acceptance region, \\(\\Theta_j\\) is not significantly different from zero, and the null hypothesis cannot be rejected.</p>"},{"location":"book/4-Model-Structure-Selection/#penalty-value-based-on-the-derivative-of-the-sigmoid-linear-unit-function","title":"Penalty value based on the Derivative of the Sigmoid Linear Unit function","text":"<p>We propose a penalty value based on the derivative of the sigmoid function, defined as:</p> \\[ \\dot{\\varsigma}(x(\\varrho)) = \\varsigma(x) [1 + (a(x - c))(1 - \\varsigma(x))]. \\tag{53} \\] <p>In this formulation, the parameters are defined as follows: \\(x\\) has the dimension of \\(noV\\); \\(c = noV / 2\\); and \\(a\\) is set as the ratio of the number of regressors in the current test model to \\(c\\). This approach results in a distinct curve for each model, with the slope of the sigmoid curve becoming steeper as the number of regressors increases. The penalty value, \\(\\varrho\\), corresponds to the \\(y\\) value of the sigmoid curve for the given number of regressors in \\(x\\). Since the derivative of the sigmoid function can return negative values, we normalize \\(\\varsigma\\) as:</p> \\[ \\varrho = \\varsigma - \\mathrm{min}(\\varsigma), \\tag{54} \\] <p>ensuring that \\(\\varrho \\in \\mathbb{R}^{+}\\).</p> <p>However, two different models with the same number of regressors can yield significantly different results due to the varying importance of each regressor. To address this, we use the \\(t\\)-student test to assess the statistical relevance of each regressor and incorporate this information into the penalty function. Specifically, we calculate \\(n_{\\Theta, H_{0}}\\), the number of regressors that are not significant for the model. The penalty value is then adjusted based on the model size:</p> \\[ \\mathrm{model\\_size} = n_{\\Theta} + n_{\\Theta, H_{0}}. \\tag{55} \\] <p>The objective function, which integrates the relative root mean squared error of the model with \\(\\varrho\\), is defined as:</p> \\[ \\mathcal{F} = \\frac{\\sqrt{\\sum_{k=1}^{n} (y_k - \\hat{y}_k)^2}}{\\sqrt{\\sum_{k=1}^{n} (y_k - \\bar{y})^2}} \\times \\varrho. \\tag{56} \\] <p>This approach ensures that even if models have the same number of regressors, those with redundant regressors are penalized more heavily.</p>"},{"location":"book/4-Model-Structure-Selection/#case-studies-simulation-results","title":"Case Studies: Simulation Results","text":"<p>In this section, six simulation examples are presented to illustrate the effectiveness of the MetaMSS algorithm. An analysis of the algorithm's performance has been conducted, considering various tuning parameters. The selected systems are generally used as benchmarks for model structure algorithms and were taken from the following sources: Wei, H. and Billings, S. A., \"Model structure selection using an integrated forward orthogonal search algorithm assisted by squared correlation and mutual information\", Falsone, A. and Piroddi, L. and Prandini, M., \"A randomized algorithm for nonlinear model structure selection\", Baldacchino, T. and Anderson, S. R. and Kadirkamanathan, V., \"Computational system identification for Bayesian NARMAX modelling\", Piroddi, L. and Spinelli, W., \"An identification algorithm for polynomial NARX models based on simulation error minimization\", Guo, Y. and Guo, L. Z. and Billings, S. A. and Wei, H., \"A New Iterative Orthogonal Forward Regression Algorithm\", Bonin, M. and Seghezza, V. and Piroddi, L., \"NARX model selection based on simulation error minimization and LASSO\", and Aguirre, L. A. and Barbosa, B. H. G. and Braga, A. P., \"Prediction and simulation errors in parameter estimation for nonlinear systems\". Finally, a comparative analysis has been performed with respect to the Randomized Model Structure Selection (RaMSS), \"A randomized algorithm for nonlinear model structure selection\", the FROLS, and the Reversible-jump Markov chain Monte Carlo (RJMCMC), \"Computational system identification for Bayesian NARMAX modelling\" algorithms to evaluate the effectiveness of the proposed method.</p> <p>The simulation models are described as:</p> \\[ \\begin{align}     &amp; S_1: \\quad y_k = -1.7y_{k-1} - 0.8y_{k-2} + x_{k-1} + 0.81x_{k-2} + e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-2, 2) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.01^2); \\nonumber \\\\     &amp; S_2: \\quad y_k = 0.8y_{k-1} + 0.4x_{k-1} + 0.4x_{k-1}^2 + 0.4x_{k-1}^3 + e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{N}(0, 0.3^2) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.01^2). \\nonumber \\\\     &amp; S_3: \\quad y_k = 0.2y_{k-1}^3 + 0.7y_{k-1}x_{k-1} + 0.6x_{k-2}^2 \\nonumber \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 -0.5y_{k-2}+ e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-1, 1) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.01^2). \\nonumber \\\\     &amp; S_4: \\quad y_k = 0.7y_{k-1}x_{k-1} - 0.5y_{k-2} + 0.6x_{k-2}^2 \\nonumber \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 + e_k, \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-1, 1) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.04^2). \\nonumber \\\\     &amp; S_5: \\quad y_k = 0.7y_{k-1}x_{k-1} - 0.5y_{k-2} + 0.6x_{k-2}^2 \\nonumber \\\\     &amp;- 0.7y_{k-2}x_{k-2}^2 + 0.2e_{k-1} \\nonumber \\\\     &amp; \\qquad \\quad - 0.3x_{k-1}e_{k-2} + e_k,\\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{U}(-1, 1) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.02^2); \\nonumber \\\\     &amp; S_6: \\quad y_k = 0.75y_{k-2} + 0.25x_{k-2} - 0.2y_{k-2}x_{k-2} + e_k \\nonumber \\\\     &amp; \\qquad \\quad \\text{with } x_k \\sim \\mathcal{N}(0, 0.25^2) \\text{ and } e_k \\sim \\mathcal{N}(0, 0.02^2); \\nonumber \\end{align} \\tag{57} \\] <p>where \\(\\mathcal{U}(a, b)\\) are samples evenly distributed over~\\([a, b]\\), and \\(\\mathcal{N}(\\eta, \\sigma^2)\\) are samples with a Gaussian distribution with mean \\(\\eta\\) and standard deviation \\(\\sigma\\). All realizations of the systems are composed of a total of \\(500\\) input-output data samples. Also, the same random seed is used to reproducibility purpose.</p> <p>All tests shown in this section are based on the original implementation and are took from the results of my master thesis. At the time, the algorithm was performed in Matlab \\(2018\\)a environment, on a Dell Inspiron \\(5448\\) Core i\\(5-5200\\)U CPU \\(2.20\\)GHz with \\(12\\)GB of RAM. However, it is not a hard task to adapt them to SysIdentPy.</p> <p>Following the aforementioned studies, the maximum lags for the input and output are chosen to be, respectively, \\(n_u=n_y=4\\) and the nonlinear degree is \\(\\ell = 3\\). The parameters related to the BPSOGSA are detailed on Table 8.</p> Parameters \\(n_u\\) \\(n_y\\) \\(\\ell\\) p-value max_iter n_agents \\(\\alpha\\) \\(G_0\\) Values \\(4\\) \\(4\\) \\(3\\) \\(0.05\\) \\(30\\) \\(10\\) \\(23\\) \\(100\\) &gt;Table 8. Parameters used in MetaMSS <p>\\(300\\) runs of the Meta-MSS algorithm have been executed for each model, aiming to compare some statistics about the algorithm performance. The elapsed time, the time required to obtain the final model, and correctness, the percentage of exact model selections, are analyzed.</p> <p>The results in Table 9 are obtained with the parameters configured accordingly to Table 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_5\\) \\(S_6\\) Correct model 100\\% 100\\% 100\\% 100\\% 100\\% 100\\% Elapsed time (mean) 5.16s 3.90s 3.40s 2.37s 1.40s 3.80s &gt;Table 9. Overall performance of the MetaMSS <p>Table 9 shows that all the model terms are correctly selected using the Meta-MSS. It is worth to notice that even the model \\(S_5\\), which have an autoregressive noise, was correctly selected using the proposed algorithm. This result resides in the evaluation of all regressors individually, and the ones considered redundant are removed from the model.</p> <p>Figure 15 presents the convergence of each execution of Meta-MSS. It is noticeable that the majority of executions converges to the correct model structures with \\(10\\) or fewer iterations. The reason for this relies on the maximum number of iterations and the number of search agents. The first one is related to the acceleration coefficient, which boosts the exploration phase of the algorithm, while the latter increases the number of candidate models to be evaluated. Intuitively, one can see that both parameters influence the elapsed time and, more importantly, the model structure selected to compose the final model. Consequently, an inappropriate choice of one of them may results in sub/over-parameterized models, since the algorithm can converge to a local optimum. The next subsection presents an analysis of the max_iter and n_agents influence in the algorithm performance.</p> <p></p> <p>Figure 15. Convergence of Meta-MSS for different model structures. The figure illustrates the convergence behavior of the Meta-MSS algorithm across multiple executions. Each curve represents the convergence trajectory for a specific model structure from \\(S_1\\) to \\(S_6\\) over a maximum of 30 iterations.</p>"},{"location":"book/4-Model-Structure-Selection/#influence-of-the-max_iter-and-n_agents-parameters","title":"Influence of the \\(max\\_iter\\) and \\(n\\_agents\\) parameters","text":"<p>The simulation models are used to show the performance of the Meta-MSS considering different tuning for <code>max_iter</code> and <code>n_agents</code> parameters. First, we set and uphold the <code>max_iter=30</code> while the <code>n_agents</code> are changed. Then, we set and uphold the <code>n_agents</code> while the <code>max_iter</code> is modified. The results detailed in this section have been obtained by setting the remaining parameters according to Table 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_5\\) \\(S_6\\) max_iter = 30, n_agents = 1 Correct model \\(65\\%\\) \\(55.66\\%\\) \\(14\\%\\) \\(14\\%\\) \\(7.3\\%\\) \\(20.66\\%\\) Elapsed time (mean) \\(0.26\\)s \\(0.19\\)s \\(0.15\\)s \\(0.11\\)s \\(0.13\\)s \\(0.13\\)s max_iter = 30, n_agents = 5 Correct model \\(100\\%\\) \\(100\\%\\) \\(99\\%\\) \\(98\\%\\) \\(91.66\\%\\) \\(98.33\\%\\) Elapsed time (mean) \\(2.08\\)s \\(1.51\\)s \\(1.41\\)s \\(0.99\\)s \\(0.59\\)s \\(1.13\\)s max_iter = 30, n_agents = 20 Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(12.88\\)s \\(9.10\\)s \\(8.77\\)s \\(5.70\\)s \\(3.37\\)s \\(9.50\\)s max_iter = 5, n_agents = 10 Correct model \\(96.33\\%\\) \\(99\\%\\) \\(86\\%\\) \\(93.66\\%\\) \\(93\\%\\) \\(97.33\\%\\) Elapsed time (mean) \\(0.92\\)s \\(0.73\\)s \\(0.72\\)s \\(0.52\\)s \\(0.29\\)s \\(0.64\\)s max_iter = 15, n_agents = 10 Correct model \\(100\\%\\) \\(100\\%\\) \\(99\\%\\) \\(99\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(2.80\\)s \\(2.33\\)s \\(2.25\\)s \\(1.60\\)s \\(0.90\\)s \\(2.30\\)s max_iter = 50, n_agents = 10 Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(7.38\\)s \\(5.44\\)s \\(4.56\\)s \\(3.01\\)s \\(2.10\\)s \\(4.52\\)s &gt;Table 10. <p>The aggregated results in Table 10 confirms the expected performance regarding the elapsed time and percentage of correct models. Indeed, both metrics increases significantly as the number of agents and the maximum number of iteration increases. The number of agents is very relevant because it yields a broader exploration of the search space. All system are affected by the increase in the number of agents and the maximum number of iterations.</p> <p>Regarding all tested systems, it is straightforward to notice that the more extensive exploration dramatically impacts on the exactitude of the selection procedure. If only a few agents are assigned, the performance of Meta-MSS algorithm deteriorates significantly, especially for systems \\(S_3, S_4\\) and \\(S_5\\). The maximum number of iteration empowers agents to explore, globally and locally, the space around the candidate models tested so far. In this sense, as the number of iterations increases, more the agents can explore the search space and examine different regressors.</p> <p>If these parameters are improperly chosen, the algorithm might fail to select the best model structure. In this respect, the results presented here concerns only the selected systems. The larger the search space, the larger the number of agents and iterations should be. Although the computational effort increases with larger values for n_agents and max_iteration, the algorithm remains very efficient regarding the elapsed time for all tuning configurations that ensured the selection of the exact model structures.</p>"},{"location":"book/4-Model-Structure-Selection/#selection-of-over-and-sub-parameterized-models","title":"Selection of over and sub-parameterized models","text":"<p>Regardless of the successful selection of all models structures by the Meta-Structure Selection Algorithm, one can ask how the models differs from the true ones in the cases presented in Table 10 where the algorithm failed to ensure \\(100\\%\\) of correctness. Figure 16 depicts the distribution of terms number selected in each case. It is evident that the number of over-parameterized models selected is higher than the sub-parameterized in overall. Regarding the cases where the number of search agents are low, due to low exploration and exploitation capacity, the algorithm converged early and resulted in models with a high number of spurious regressors. In respect to \\(S_2\\) and \\(S_5\\), for example, with n_agents\\(=1\\), the algorithm ends up selecting models with more than \\(20\\) terms. One can say this was a extreme scenario for comparison purpose. However, a suitable choice for the parameters is intrinsically related to the dimension of the search space. Referring to cases where n_agents\\(\\geq 5\\), the number of spurious terms decreased significantly where the algorithm failed to select the true models.</p> <p>Furthermore, it is interesting to point out the importance of tuning the parameters properly because since the exploration and exploitation phase of the algorithm are strongly dependent on them. A premature convergence of the algorithm may result in models with the factual number of terms, but with wrong ones. This happened with all cases with <code>n_agents=1</code>. For example, the algorithm generates models with correct number of terms in \\(33.33\\%\\) of the cases analyzed regarding \\(S_3\\). However, Table 10 shows that only \\(14\\%\\) are, in fact, equivalent to the true model.</p> <p></p> <p>Figure 16. The distribution of terms number selected for each simulated models concerning the variation of the <code>max_iter</code> and <code>n_agents</code>.</p>"},{"location":"book/4-Model-Structure-Selection/#selection-of-over-and-sub-parameterized-models_1","title":"Selection of over and sub-parameterized models","text":"<p>Regardless of the successful selection of all models structures by the MetaMSS, one can ask how the models differ from the true ones in the cases presented in Table 10 where the algorithm failed to ensure \\(100\\%\\) of correctness. Figure 16 depicts the distribution of terms number selected in each case. It is evident that the number of over-parameterized models selected is higher than the sub-parameterized in overall. Regarding the cases where the number of search agents is low, due to low exploration and exploitation capacity, the algorithm converged early and resulted in models with a high number of spurious regressors. In respect to \\(S_2\\) and \\(S_5\\), for example, with <code>n_agents=1</code>, the algorithm ends up selecting models with more than \\(20\\) terms. One can say this was an extreme scenario for comparison purpose. However, a suitable choice for the parameters is intrinsically related to the dimension of the search space. By referring to cases where <code>n_agents</code>\\(\\geq 5\\), the number of spurious terms decreased significantly where the algorithm failed to select the true models.</p> <p>Furthermore, it is interesting to point out the importance of tuning the parameters properly because since the exploration and exploitation phase of the algorithm is strongly dependent on them. A premature convergence of the algorithm may result in models with the actual number of terms, but with wrong ones. This issue happened with all cases with <code>n_agents=1</code>. For example, the algorithm generates models with the correct number of terms in \\(33.33\\%\\) of the cases analyzed regarding \\(S_3\\). However, Table 10 shows that only \\(14\\%\\) are, in fact, equivalent to the true model.</p> <p>The systems \\(S_1\\), \\(S_2\\), \\(S_3\\), \\(S_4\\) and \\(S_6\\) has been used as benchmark by Bianchi, F., Falsone, A., Prandini, M. and Piroddi, L., so we can compare directly our results with those reported by the author in his thesis. All techniques used \\(n_y=n_u=4\\) and \\(\\ell = 3\\). The RaMSS and the RaMSS with Conditional Linear Family (C-RaMSS) used the following configuration for the tuning parameters: \\(K=1\\), \\(\\alpha = 0.997\\), \\(NP = 200\\) and \\(v=0.1\\). The Meta-Structure Selection Algorithm was tuned according to Table 8.</p> \\(S_1\\) \\(S_2\\) \\(S_3\\) \\(S_4\\) \\(S_6\\) Meta-MSS Correct model \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(5.16\\)s \\(3.90\\)s \\(3.40\\)s \\(2.37\\)s \\(3.80\\)s RaMSS- \\(NP=100\\) Correct model \\(90.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(66\\%\\) Elapsed time (mean) \\(3.27\\)s \\(1.24\\)s \\(2.59\\)s \\(1.67\\)s \\(6.66\\)s RaMSS- \\(NP=200\\) Correct model \\(78.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(82\\%\\) Elapsed time (mean) \\(6.25\\)s \\(2.07\\)s \\(4.42\\)s \\(2.77\\)s \\(9.16\\)s C-RaMSS Correct model \\(93.33\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) \\(100\\%\\) Elapsed time (mean) \\(18\\)s \\(10.50\\)s \\(16.96\\)s \\(10.56\\)s \\(48.52\\)s &gt; Table 11. Comparative analysis between MetaMSS, RaMSS, and C-RaMSS <p>In terms of correctness, the MetaMSS outperforms (or at least equals) the RaMSS and C-RaMSS for all analyzed systems as shown in Table 11. Regarding \\(S_6\\), the correctness rate increased by \\(18\\%\\) when compared with RaMSS and the elapsed time required for C-RaMSS obtain \\(100\\%\\) of correctness is \\(1276.84\\%\\) higher than the MetaMSS. Furthermore, the MetaMSS is notably more computationally efficient than C-RaMSS and similar to RaMSS.</p>"},{"location":"book/4-Model-Structure-Selection/#metamss-vs-frols","title":"MetaMSS vs FROLS","text":"<p>The FROLS algorithm was applied to all tested systems, with the results summarized in Table 12. The algorithm successfully selected the correct model terms for \\(S_2\\) and \\(S_6\\). However, it failed to identify two out of four regressors for \\(S_1\\). For \\(S_3\\), FROLS included \\(y_{k-1}\\) instead of the correct term \\(y_{k-1}^3\\). Similarly, \\(S_4\\) incorrectly included \\(y_{k-4}\\) rather than the required term \\(y_{k-2}\\). Additionally, for \\(S_5\\), the algorithm produced an incorrect model structure by including the spurious term \\(y_{k-4}\\).</p> Meta-MSS Regressor Correct FROLS Regressor Correct \\(S_1\\) \\(y_{k-1}\\) yes \\(y_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-2}\\) yes \\(x_{k-4}\\) no \\(S_2\\) \\(y_{k-1}\\) yes \\(y_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}^2\\) yes \\(x_{k-1}^2\\) yes \\(x_{k-1}^3\\) yes \\(x_{k-1}^3\\) yes \\(S_3\\) \\(y_{k-1}^3\\) yes \\(y_{k-1}\\) no \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}\\) yes \\(y_{k-2}\\) yes \\(S_4\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(S_5\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-4}\\) no \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(S_6\\) \\(y_{k-2}\\) yes \\(y_{k-2}\\) yes \\(x_{k-1}\\) yes \\(x_{k-1}\\) yes \\(y_{k-2}x_{k-2}\\) yes \\(y_{k-2}x_{k-1}\\) yes &gt; Table 12. Comparative analysis between MetaMSS and FROLS"},{"location":"book/4-Model-Structure-Selection/#meta-mss-vs-rjmcmc","title":"Meta-MSS vs RJMCMC","text":"<p>The \\(S_4\\) model is taken from Baldacchino, Anderson, and Kadirkamanathan's work (Computational System Identification for Bayesian NARMAX Modelling). In their study, the maximum lags for the input and output are \\(n_y = n_u = 4\\), and the nonlinear degree is \\(\\ell = 3\\). The authors ran the RJMCMC algorithm 10 times on the same input-output data. The RJMCMC method successfully identified the true model structure 7 out of 10 times. In contrast, the MetaMSS algorithm consistently identified the true model structure in all runs. These results are summarized in Table 13.</p> <p>Additionally, the RJMCMC method has notable drawbacks that are addressed by the MetaMSS algorithm. Specifically, RJMCMC is computationally intensive, requiring \\(30,000\\) iterations to achieve results. Furthermore, it relies on various probability distributions to simplify the parameter estimation process, which can complicate the computations. In contrast, MetaMSS offers a more efficient and straightforward approach, avoiding these issues.</p> Meta-MSS Model Correct RJMCMC Model 1 (\\(7\\times\\)) RJMCMC Model 2 RJMCMC Model 3 RJMCMC Model 4 Correct \\(S_4\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) \\(y_{k-1}x_{k-1}\\) yes \\(y_{k-2}\\) yes \\(y_{k-2}\\) \\(y_{k-2}\\) \\(y_{k-2}\\) \\(y_{k-2}\\) yes \\(x_{k-2}^2\\) yes \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) \\(x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) yes \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) \\(y_{k-2}x_{k-2}^2\\) yes - - - \\(y_{k-3}x_{k-3}\\) \\(x_{k-4}^2\\) \\(x_{k-1}x_{k-3}^2\\) no &gt; Table 13. Comparative analysis between MetaMSS and RJMCMC."},{"location":"book/4-Model-Structure-Selection/#metamss-algorithm-using-sysidentpy","title":"MetaMSS algorithm using SysIdentPy","text":"<p>Consider the same data used in the Overview of the Information Criteria Methods.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\n\nbasis_function = Polynomial(degree=2)\nmodel = MetaMSS(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 random_state=42,\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p>The MetaMSS algorithm does not rely on information criteria methods such as ERR for model structure selection, which is why it does not involve those hyperparameters. This is also true for the AOLS and ER algorithms. For more details on how to use these methods and their associated hyperparameters, please refer to the documentation.</p> <p>When it comes to parameter estimation, SysIdentPy allows the use of any available method, regardless of the model structure selection algorithm. Users can select from a range of parameter estimation methods to apply to their chosen model structure. This flexibility enables users to explore various modeling approaches and customize their system identification process. While the examples provided use the default parameter estimation method, users are encouraged to experiment with different options to find the best fit for their needs.</p> <p>The results of the MetaMSS are</p> Regressors Parameters ERR y(k-1) 1.8004E-01 0.00000000E+00 x1(k-2) 8.9747E-01 0.00000000E+00 <p></p> <p>Figure 17. Free Run Simulation for the model fitted using MetaMSS.</p> <p>The <code>results</code> method brings ERR as 0 for every regressor because, as mentioned, ERR algorithm is not executed in this case.</p>"},{"location":"book/4-Model-Structure-Selection/#accelerated-orthogonal-least-squares-aols-and-entropic-regression-er","title":"Accelerated Orthogonal Least Squares (AOLS) and Entropic Regression (ER)","text":"<p>In addition to FROLS and MetaMSS, SysIdentPy includes two other methods for model structure selection: Accelerated Orthogonal Least Squares (AOLS) and Entropic Regression (ER). While I won't delve into the details of these methods in this section as I have with FROLS and MetaMSS, I will provide an overview and references for further reading:</p> <ul> <li>Accelerated Orthogonal Least Squares (AOLS): For an in-depth exploration of AOLS, refer to the original paper here.</li> <li>Entropic Regression (ER): Detailed information about ER can be found in the original paper here.</li> </ul> <p>For now, I will demonstrate how to use these methods within SysIdentPy.</p>"},{"location":"book/4-Model-Structure-Selection/#accelerated-orthogonal-least-squares","title":"Accelerated Orthogonal Least Squares","text":"<pre><code>from sysidentpy.model_structure_selection import AOLS\n\nbasis_function = Polynomial(degree=2)\nmodel = AOLS(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> Regressors Parameters ERR x1(k-2) 9.1542E-01 0.00000000E+00 <p>Figure 18. Free Run Simulation for the model fitted using AOLS algorithm.</p>"},{"location":"book/4-Model-Structure-Selection/#entropic-regression","title":"Entropic Regression","text":"<pre><code>from sysidentpy.model_structure_selection import ER\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=8,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> Regressors Parameters ERR 1 -2.4554E-02 0.00000000E+00 x1(k-2) 9.0273E-01 0.00000000E+00 <p>Figure 19. Free Run Simulation for the model fitted using Entropic Regression algorithm.</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/","title":"5. Multiobjective Parameter Estimation","text":"<p>Multiobjective parameter estimation represents a fundamental paradigm shift in the way we approach the parameter tuning problem for NARMAX models. Instead of seeking a single set of parameter values that optimally fits the model to the data, multiobjective approaches aim to identify a set of parameter solutions, known as the Pareto front, that provide a trade-off between competing objectives. These objectives often encompass a spectrum of model performance criteria, such as goodness-of-fit, model complexity, and robustness.</p> <p>What does that mean? It means that when we are modeling a dynamical system we are, most of the time, building models that are only good to represent the dynamical behavior of the system under study. Well, that is valid most of the time because we are building dynamical models, so if it doesn't perform well in static scenarios, it won't be a problem. However, that's not always the case and we might build a model that is good in both dynamical and static behavior. In such cases, the methods for purely dynamical systems are not adequate and multiobjective algorithms can help us in such task.</p> <p>The main idea in multiobjective parameter estimation is the inclusion of the affine information. The affine information is auxiliary information that can be defined a priori such as the static gain and the static function of the system. Formally, the affine information can be defined as follows:</p> <p>Let the parameter vector \\(\\Theta \\in \\mathbb{R}^{n_{\\Theta}}\\), a vector \\(\\mathrm{v}\\in \\mathbb{R}^p\\) and a matrix \\(\\mathrm{G}\\in \\mathbb{R}^{n_{\\Theta}\\times p}\\) where \\(\\mathrm{v}\\) and \\(\\mathrm{G}\\) are assumed to be accessible. Suppose \\(\\mathrm{G}\\Theta\\) be an estimate of \\(\\mathrm{v}\\). Hence, \\(\\mathrm{v} = \\mathrm{G}\\Theta + \\xi\\). Then, \\([\\mathrm{v}, \\mathrm{G}]\\) is a pair of affine information of the system.</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#multi-objective-optimization-problem","title":"Multi-objective optimization problem","text":"<p>Let's define what is a multiobjective problem. Given \\(m\\) objective functions</p> \\[ \\begin{equation}     \\mathrm{J}(\\hat{\\Theta}) = [J_1(\\hat{\\Theta}), J_2(\\hat{\\Theta}), \\cdots, J_m(\\hat{\\Theta})]^\\top, \\end{equation} \\tag{5.1} \\] <p>where \\(\\mathrm{J}(\\cdot):\\mathbb{R}^n \\mapsto \\mathbb{R}^m\\), a multi-objective optimization problem can be generally stated as (A. Baykasoglu, S. Owen, e N. Gindy)</p> \\[ \\begin{equation}     \\begin{aligned}         &amp; \\underset{\\Theta}{\\text{minimize}} &amp; &amp; \\mathrm{J}(\\Theta) \\\\ &amp; \\text{subject to} &amp; &amp; \\Theta \\in \\mathrm{S} = \\left\\{\\Theta \\mid \\Theta \\in \\mathrm{A}^n, g_i(\\Theta) \\leq a_i, h_j(\\Theta) = b_j \\right\\}, \\\\ &amp; &amp; &amp; i = 1, \\ldots, m, \\quad j = 1, \\ldots, n     \\end{aligned} \\end{equation} \\tag{5.2} \\] <p>where \\(\\Theta\\) is an \\(n\\)-dimensional vector of the decision variables, \\(\\mathrm{S}\\) is the set of feasible solutions bounded by \\(m\\) inequality constraints (\\(g_i\\)) and \\(n\\) equality constraints (\\(h_j\\)), and \\(a_i\\) and \\(b_j\\) are constants. For continuous variables \\(A = \\mathbb{R}\\) while \\(A\\) contains the set of permissible values for discrete variables.</p> <p>Usually problems with \\(1 &lt; m &lt; 4\\) are called multiobjective optimization problems. When there are more objectives (\\(m\\geq 4\\)), it is referred as many-objective optimization problems, an emergence class of multi-objective problems for solving complex modern real-world tasks. More details can be found in (Fleming, P. J., Purshouse, R. C., and Lygoe, R. J., \"Many-Objective Optimization: An Engineering Design Perspective\"), (Li, B., Li, J., Tang, K., and Yao, X., \"A survey on multi-objective evolutionary algorithms for many-objective problems\").</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#pareto-optimal-definition-and-pareto-dominance","title":"Pareto Optimal Definition and Pareto Dominance","text":"<p>Consider \\([y^{(1)}, y^{(2)}] \\in \\mathbb{R}^m\\) two vectors in the objective space. If and only if \\(\\forall \\in \\{1, \\ldots, m \\}: y_i^{(1)}\\leq y_i^{(2)}\\) and \\(\\exists j \\in \\{1, \\ldots, m \\}: y_j^{(1)} &lt; y_j^{(2)}\\) one can say \\(y^{(1)} \\prec y^{(2)}\\) (P. L. Yu, \"Cone convexity, cone extreme points, and non dominated solutions in decision problems with multiobjectives\").</p> <p>The concept of Pareto optimality is generally used to describe the trade-off among the minimization of different objectives. Following the pareto definition: the pareto optimal is any parameter vector representing an efficient solution  where no objective function can be improved without  making at least one objective function worse off will be referred to as a Pareto-model.</p> <p>In the system identification field, that means to find a model where you can't get a better dynamic performance without making the static performance worse.</p> <p>A hypothetical Pareto set is shown in Figure 1.</p> <p></p> <p>Figure 1. The figure illustrates the concept of Pareto optimality, where each point in the objective space represents a solution. The Pareto front is depicted as a curve, demonstrating the trade-off between two conflicting objectives. Points on the front cannot be improved in one objective without worsening the other, highlighting the balance in optimal solutions.</p> <p>In this case the model structure is assumed to be known and therefore there is a one-to-one correspondence between each parameter vector on the Pareto optimal solution and a model (Nepomuceno, E. G., Takahashi, R. H. C., and Aguirre, L. A., \"Multiobjective parameter estimation for non-linear systems: affine information and least-squares formulation\"). One can build a Pareto set by applying the Weighted Sum Method, where a set of objectives are scalarized into a single objective by adding each objective multiplied by a user supplied weight. Consider</p> \\[ \\begin{equation}     \\mathrm{W} = \\Bigg\\{ w|w \\in \\mathbb{R}^m, w_j\\geq 0 \\quad \\textrm{and} \\quad \\sum^{m}_{j=1}w_j=1 \\Bigg\\} \\end{equation} \\tag{5.3} \\] <p>as non-negative weights. Then, the convex optimization problem can be stated as</p> \\[ \\begin{equation} \\begin{aligned} \\Theta^* &amp;= \\underset{\\Theta}{\\text{argmin}} \\, \\langle w, \\mathrm{J}(\\Theta) \\rangle \\end{aligned} \\end{equation} \\tag{5.4} \\] <p>where \\(w\\) is a combination of weights to the different objectives functions. Therefore, the Pareto-set is associated to the set of realizations of \\(w \\in \\mathrm{W}\\). An efficient single-step computational strategy was presented by (Nepomuceno, E. G., Takahashi, R. H. C., and Aguirre, L. A., \"Multiobjective parameter estimation for non-linear systems: affine information and least-squares formulation\") for solving Equation 5.4 by means of a Least Squares formulation, which is presented in the following section.</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#affine-information-least-squares-algorithm","title":"Affine Information Least Squares Algorithm","text":"<p>Consider the \\(m\\) affine information pairs \\([\\mathrm{v}_i \\in \\mathbb{R}^{pi}, \\mathrm{G}_i \\mathbb{R}^{pi\\times n}]\\) with \\(i = 1, \\ldots, m\\). Assume there is exist a full column rank \\(\\mathrm{G}_i\\) and let \\(M\\) be a model of the form</p> \\[ y = \\Psi\\Theta + \\epsilon. \\tag{5.5} \\] <p>Then the \\(m\\) affine information pairs can be considered in the parameter estimation by solving</p> \\[ \\begin{equation} \\begin{aligned} \\Theta^* &amp;= \\underset{\\Theta}{\\text{argmin}} \\sum_{i=1}^{m} w_i (\\mathrm{v}_i - \\mathrm{G}_i \\Theta)^\\top (\\mathrm{v}_i - \\mathrm{G}_i \\Theta) \\end{aligned} \\end{equation} \\tag{5.6} \\] <p>with \\(w = [w_i, \\ldots, w_m]^\\top \\in \\mathrm{W}\\). The solution of equation above is given by</p> \\[ \\begin{equation}     \\Theta^* = \\left[\\sum^{m}_{i=1}w_i\\mathrm{G}_i^\\top\\mathrm{G}_i\\right]^{-1}  \\left[\\sum^{m}_{i=1}w_i\\mathrm{G}_i^\\top\\mathrm{v}_i\\right]. \\end{equation} \\tag{5.7} \\] <p>If there exists only one information, the problem reduces to the mono-objective Least Squares solution.</p> <p>To make things straightforward, lets check a detailed case study.</p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#case-study-buck-converter","title":"Case Study - Buck converter","text":"<p>A buck converter is a type of DC/DC converter that decreases the voltage (while increasing the current) from its input (power supply) to its output (load). It is similar to a boost converter (elevator) and is a type of switched-mode power supply (SMPS) that typically contains at least two semiconductors (a diode and a transistor, although modern buck converters replace the diode with a second transistor used for synchronous rectification) and at least one energy storage element, a capacitor, inductor or both combined.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.multiobjective_parameter_estimation import AILS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.narmax_tools import set_weights\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#dynamic-behavior","title":"Dynamic Behavior","text":"<pre><code>df_train = pd.read_csv(r\"datasets/buck_id.csv\")\ndf_valid = pd.read_csv(r\"datasets/buck_valid.csv\")\n\n# Plotting the measured output (identification and validation data)\nplt.figure(1)\nplt.title(\"Output\")\nplt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5)\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Voltage\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Plotting the measured input (identification and validation data)\nplt.figure(2)\nplt.title(\"Input\")\nplt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5)\nplt.ylim(2.1, 2.6)\nplt.ylabel(\"u\")\nplt.xlabel(\"Samples\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#buck-converter-static-function","title":"Buck Converter Static Function","text":"<p>The duty cycle, represented by the symbol \\(D\\), is defined as the ratio of the time the system is on (\\(T_{on}\\)) to the total operation cycle time (\\(T\\)). Mathematically, this can be expressed as \\(D=\\frac{T_{on}}{T}\\). The complement of the duty cycle, represented by \\(D'\\), is defined as the ratio of the time the system is off (\\(T_{off}\\)) to the total operation cycle time (\\(T\\)) and can be expressed as \\(D'=\\frac{T_{off}}{T}\\).</p> <p>The load voltage (\\(V_o\\)) is related to the source voltage (\\(V_d\\)) by the equation \\(V_o = D\u22c5V_d = (1\u2212D')\u22c5V_d\\). For this particular converter, it is known that \\(D\u2032=\\frac{\\bar{u}-1}{3}\\), which means that the static function of this system can be derived from theory to be:</p> \\[ V_o = \\frac{4V_d}{3} - \\frac{V_d}{3}\\cdot \\bar{u} \\] <p>If we assume that the source voltage \\(V_d\\) is equal to 24 V, then we can rewrite the above expression as follows:</p> \\[ V_o = (4 - \\bar{u})\\cdot 8 \\] <pre><code># Static data\nVd = 24\nUo = np.linspace(0, 4, 50)\nYo = (4 - Uo) * Vd / 3\nUo = Uo.reshape(-1, 1)\nYo = Yo.reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Curve\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{y}$\")\nplt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\")\nplt.show()\n</code></pre> <p></p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#buck-converter-static-gain","title":"Buck converter Static Gain","text":"<p>The gain of a Buck converter is a measure of how its output voltage changes in response to changes in its input voltage. Mathematically, the gain can be calculated as the derivative of the converter\u2019s static function, which describes the relationship between its input and output voltages.</p> <p>In this case, the static function of the Buck converter is given by the equation:</p> \\[ V_o = (4 - \\bar{u})\\cdot 8 \\] <p>Taking the derivative of this equation with respect to \\(\\hat{u}\\), we find that the gain of the Buck converter is equal to \u22128. In other words, for every unit increase in the input voltage \\(\\hat{u}\\), the output voltage Vo will decrease by 8 units, so</p> \\[ gain=V_o'=-8 \\] <pre><code># Defining the gain\ngain = -8 * np.ones(len(Uo)).reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Gain\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{gain}$\")\nplt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#building-a-dynamic-model-using-the-mono-objective-approach","title":"Building a dynamic model using the mono-objective approach","text":"<pre><code>x_train = df_train.input.values.reshape(-1, 1)\ny_train = df_train.y.values.reshape(-1, 1)\nx_valid = df_valid.input.values.reshape(-1, 1)\ny_valid = df_valid.y.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=8,\n\u00a0 \u00a0 extended_least_squares=False,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"aic\",\n\u00a0 \u00a0 estimator=\"least_squares\",\n\u00a0 \u00a0 basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#affine-information-least-squares-algorithm-ails","title":"Affine Information Least Squares Algorithm (AILS)","text":"<p>AILS is a multiobjective parameter estimation algorithm, based on a set of affine information pairs. The multiobjective approach proposed in the mentioned paper and implemented in SysIdentPy leads to a convex multiobjective optimization problem, which can be solved by AILS. AILS is a LeastSquares-type non-iterative scheme for finding the Pareto-set solutions for the multiobjective problem.</p> <p>So, with the model structure defined (we will be using the one built using the dynamic data above), one can estimate the parameters using the multiobjective approach.</p> <p>The information about static function and static gain, besides the usual dynamic input/output data, can be used to build the pair of affine information to estimate the parameters of the model. We can model the cost function as:</p> \\[ \\gamma(\\hat\\theta) = w_1\\cdot J_{LS}(\\hat{\\theta})+w_2\\cdot J_{SF}(\\hat{\\theta})+w_3\\cdot J_{SG}(\\hat{\\theta}) \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#multiobjective-parameter-estimation-considering-3-different-objectives-the-prediction-error-the-static-function-and-the-static-gain","title":"Multiobjective parameter estimation considering 3 different objectives: the prediction error, the static function and the static gain","text":"<pre><code># you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work\nmo_estimator = AILS(final_model=model.final_model)\n# setting the log-spaced weights of each objective function\nw = set_weights(static_function=True, static_gain=True)\n# you can also use something like\n# w = np.array(\n# \u00a0 \u00a0 [\n# \u00a0 \u00a0 \u00a0 \u00a0 [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01],\n# \u00a0 \u00a0 \u00a0 \u00a0 [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01],\n# \u00a0 \u00a0 \u00a0 \u00a0 [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98],\n# \u00a0 \u00a0 ]\n# )\n\n# to set the weights. Each row correspond to each objective\n</code></pre> <p>AILS has an <code>estimate</code> method that returns the cost functions (J), the Euclidean norm of the cost functions (E), the estimated parameters referring to each weight (theta), the regressor matrix of the gain and static_function affine information HR and QR, respectively.</p> <pre><code>J, E, theta, HR, QR, position = mo_estimator.estimate(\n\u00a0 \u00a0 X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n\u00a0 \u00a0 \"w1\": w[0, :],\n\u00a0 \u00a0 \"w2\": w[2, :],\n\u00a0 \u00a0 \"w3\": w[1, :],\n\u00a0 \u00a0 \"J_ls\": J[0, :],\n\u00a0 \u00a0 \"J_sg\": J[1, :],\n\u00a0 \u00a0 \"J_sf\": J[2, :],\n\u00a0 \u00a0 \"||J||:\": E,\n}\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf \\(\\lVert J \\rVert\\) 0.006842 0.003078 0.990080 0.999970 1.095020e-05 0.000013 0.245244 0.007573 0.002347 0.990080 0.999938 2.294665e-05 0.000016 0.245236 0.008382 0.001538 0.990080 0.999885 6.504913e-05 0.000018 0.245223 0.009277 0.000642 0.990080 0.999717 4.505541e-04 0.000021 0.245182 0.006842 0.098663 0.894495 1.000000 7.393246e-08 0.000015 0.245251 ... ... ... ... ... ... ... 0.659632 0.333527 0.006842 0.995896 3.965699e-04 1.000000 0.244489 0.730119 0.263039 0.006842 0.995632 5.602981e-04 0.972842 0.244412 0.808139 0.185020 0.006842 0.995364 8.321071e-04 0.868299 0.244300 0.894495 0.098663 0.006842 0.995100 1.364999e-03 0.660486 0.244160 0.990080 0.003078 0.006842 0.992584 9.825987e-02 0.305492 0.261455 <p>Now we can set theta related to any weight results</p> <pre><code>model.theta = theta[-1, :].reshape(\n\u00a0 \u00a0 -1, 1\n) \u00a0# setting the theta estimated for the last combination of the weights\n\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\n\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 1 2.2930E+00 9.999E-01 y(k-1) 2.3307E-01 2.042E-05 y(k-2) 6.3209E-01 1.108E-06 x1(k-1) -5.9333E-01 4.688E-06 y(k-1)^2 2.7673E-01 3.922E-07 y(k-2)y(k-1) -5.3228E-01 8.389E-07 x1(k-1)y(k-1) 1.6667E-02 5.690E-07 y(k-2)^2 2.5766E-01 3.827E-06"},{"location":"book/5-Multiobjective-Parameter-Estimation/#the-dynamic-results-for-that-chosen-theta-is","title":"The dynamic results for that chosen theta is","text":"<pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#the-static-gain-result-is","title":"The static gain result is","text":"<pre><code>plt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#the-static-function-result-is","title":"The static function result is","text":"<pre><code>plt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#getting-the-best-weight-combination-based-on-the-norm-of-the-cost-function","title":"Getting the best weight combination based on the norm of the cost function","text":"<p>The variable <code>position</code> returned in <code>estimate</code> method give the position of the best weight combination. The model structure is exactly the same, but the order of the regressors is changed in estimate method. That's why you have to change the model.final_model. The dynamic, static gain, and the static function  results for that chosen theta is shown below.</p> <pre><code>model.theta = theta[position, :].reshape(\n\u00a0 \u00a0 -1, 1\n) \u00a0# setting the theta estimated for the best combination of the weights\n\n# changing the model.final_model\n\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\n# dynamic results\nplot_results(y=y_valid, yhat=yhat, n=1000)\n\n# static gain\nplt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n\n# static function\nplt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> Regressors Parameters ERR 1 1.5405E+00 9.999E-01 y(k-1) 2.9687E-01 2.042E-05 y(k-2) 6.4693E-01 1.108E-06 x1(k-1) -4.1302E-01 4.688E-06 y(k-1)^2 2.7671E-01 3.922E-07 y(k-2)y(k-1) -5.3474E-01 8.389E-07 x1(k-1)y(k-1) 4.0624E-03 5.690E-07 y(k-2)^2 2.5832E-01 3.827E-06 <p></p> <p></p> <p></p> <p>You can also plot the pareto-set solutions</p> <pre><code>plt.figure(6)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Pareto-set solutions\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#detailing-ails","title":"Detailing AILS","text":"<p>The polynomial NARX model built using the mono-objective approach has the following structure:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>The, the goal when using the static function and static gain information in the multiobjective scenario is to estimate the vector \\(\\hat{\\theta}\\) based on:</p> \\[ \\theta = [w_1\\Psi^T\\Psi + w_2(HR)^T(HR) + w_3(QR)(QR)^T]^{-1} [w_1\\Psi^T y + w_2(HR)^T\\overline{g}+w_3(QR)^T\\overline{y}] \\] <p>The \\(\\Psi\\) matrix is built using the usual mono-objective dynamic modeling approach in SysIdentPy. However, it is still necessary to find the Q, H and R matrices. AILS have the methods to compute all of those matrices. Basically, to do that, \\(q_i^T\\) is first estimated:</p> \\[ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_1} &amp; \\overline{y_i}^2 &amp; \\cdots &amp; \\overline{y_i}^l &amp; F_{yu} &amp; \\overline{u_i}^2 &amp; \\cdots &amp; \\overline{u_i}^l \\end{bmatrix} \\] <p>where \\(F_{yu}\\) stands for all non-linear monomials in the model that are related to \\(y(k)\\) and \\(u(k)\\), \\(l\\) is the largest non-linearity in the model for input and output terms. For a model with a degree of nonlinearity equal to 2, we can obtain:</p> \\[ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_i} &amp; \\overline{y_i}^2 &amp; \\overline{u_i}\\:\\overline{y_i} &amp; \\overline{u_i}^2 \\end{bmatrix} \\] <p>It is possible to encode the \\(q_i^T\\) matrix so that it follows the model encoding defined in SysIdentPy. To do this, 0 is considered as a constant, \\(y_i\\) equal to 1 and \\(u_i\\) equal to 2. The number of columns indicates the degree of nonlinearity of the system and the number of rows reflects the number of terms:</p> \\[ q_i = \\begin{bmatrix} 0 &amp; 0\\\\ 1 &amp; 0\\\\ 2 &amp; 0\\\\ 1 &amp; 1\\\\ 2 &amp; 1\\\\ 2 &amp; 2\\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y_i}\\\\ \\overline{u_i}\\\\ \\overline{y_i}^2\\\\ \\overline{u_i}\\:\\overline{y_i}\\\\ \\overline{u_i}^2\\\\ \\end{bmatrix} \\] <p>Finally, the result can be easily obtained using the \u2018regressor_space\u2019 method of SysIdentPy</p> <pre><code>from sysidentpy.narmax_base import RegressorDictionary\n\nobject_qit = RegressorDictionary(xlag=1, ylag=1)\nR_example = object_qit.regressor_space(n_inputs=1) // 1000\nprint(f\"R = {R_example}\")\n</code></pre> \\[ R = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ 2 &amp; 2 \\end{bmatrix} \\] <p>such that:</p> \\[ \\overline{y_i} = q_i^T R\\theta \\] <p>and:</p> \\[ \\overline{g_i} = H R\\theta \\] <p>where \\(R\\) is the linear mapping of the static regressors represented by \\(q_i^T\\). In addition, the \\(H\\) matrix holds affine information regarding \\(\\overline{g_i}\\), which is equal to \\(\\overline{g_i} = \\frac{d\\overline{y}}{d\\overline{u}}{\\big |}_{(\\overline{u_i}\\:\\overline{y_i})}\\).</p> <p>From now on, we will begin to apply the parameter estimation in a multiobjective manner. This will be done with the NARX polynomial model of the BUCK converter in mind. In this context, \\(q_i^T\\) will be generic and will assume a specific format for the problem at hand. For this task, the \\(R_qit\\) method will be used, whose objective is to return the \\(q_i^T\\) related to the model and the matrix of the linear mapping \\(R\\):</p> <pre><code>R, qit = mo_estimator.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> \\[ R = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{bmatrix} \\] <p>and</p> \\[ qit = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>So</p> \\[ q_i = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 1 &amp; 1 \\\\ 2 &amp; 1 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y} \\\\ \\overline{u} \\\\ \\overline{y^2} \\\\ \\overline{u} \\cdot \\overline{y} \\\\ \\end{bmatrix} \\] <p>You can notice that the method produces outputs consistent with what is expected:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 \\] <p>and:</p> \\[ R = \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ \\overline{y^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\\\ \\overline{y}\\:\\overline{u} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ \\end{bmatrix} \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#validation","title":"Validation","text":"<p>The following model structure will be used to validate the approach:</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[ \\therefore \\] \\[ final\\_model = \\begin{bmatrix} 1001 &amp; 0\\\\ 1002 &amp; 0\\\\ 0 &amp; 0\\\\ 2001 &amp; 0\\\\ 2001 &amp; 2001\\\\ 2002 &amp; 2001\\\\ 2002 &amp; 0\\\\ 2002 &amp; 2002 \\end{bmatrix} \\] <p>defining in code:</p> <pre><code>final_model = np.array(\n\u00a0 \u00a0 [\n\u00a0 \u00a0 \u00a0 \u00a0 [1001, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [1002, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [0, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [2001, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [2001, 2001],\n\u00a0 \u00a0 \u00a0 \u00a0 [2002, 2001],\n\u00a0 \u00a0 \u00a0 \u00a0 [2002, 0],\n\u00a0 \u00a0 \u00a0 \u00a0 [2002, 2002],\n\u00a0 \u00a0 ]\n)\nfinal_model\n</code></pre> 1001 0 1002 0 0 0 2001 0 2001 2001 2002 2001 2002 0 2002 2002 <pre><code>mult2 = AILS(final_model=final_model)\n\ndef psi(X, Y):\n\u00a0 \u00a0 PSI = np.zeros((len(X), 8))\n\u00a0 \u00a0 for k in range(2, len(Y)):\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 0] = Y[k - 1]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 1] = Y[k - 2]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 2] = 1\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 3] = X[k - 1]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 4] = X[k - 1] ** 2\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 5] = X[k - 2] * X[k - 1]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 6] = X[k - 2]\n\u00a0 \u00a0 \u00a0 \u00a0 PSI[k, 7] = X[k - 2] ** 2\n\u00a0 \u00a0 return np.delete(PSI, [0, 1], axis=0)\n</code></pre> <p>The value of theta with the lowest mean squared error obtained with the same code implemented in Scilab was:</p> \\[ W_{LS} = 0.3612343 \\] <p>and:</p> \\[ W_{SG} = 0.3548699 \\] <p>and:</p> \\[ W_{SF} = 0.3548699 \\] <pre><code>PSI = psi(x_train, y_train)\nw = np.array([[0.3612343], [0.2838959], [0.3548699]])\nJ, E, theta, HR, QR, position = mult2.estimate(\n\u00a0 \u00a0 y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n\u00a0 \u00a0 \"w1\": w[0, :],\n\u00a0 \u00a0 \"w2\": w[2, :],\n\u00a0 \u00a0 \"w3\": w[1, :],\n\u00a0 \u00a0 \"J_ls\": J[0, :],\n\u00a0 \u00a0 \"J_sg\": J[1, :],\n\u00a0 \u00a0 \"J_sf\": J[2, :],\n\u00a0 \u00a0 \"||J||:\": E,\n}\n\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf \\(\\lVert J \\rVert\\) 0.361234 0.35487 0.283896 1.0 1.0 1.0 1.0 The order of the weights is different because the way we implemented in Python, but the results are very close as expected."},{"location":"book/5-Multiobjective-Parameter-Estimation/#dynamic-results","title":"Dynamic results","text":"<pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = mult2.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 1 1.4287E+00 9.999E-01 y(k-1) 5.5147E-01 2.042E-05 y(k-2) 4.0449E-01 1.108E-06 x1(k-1) -1.2605E+01 4.688E-06 x1(k-2) 1.2257E+01 3.922E-07 x1(k-1)^2 8.3274E+00 8.389E-07 x1(k-2)x1(k-1) -1.1416E+01 5.690E-07 x1(k-2)^2 3.0846E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#static-gain","title":"Static gain","text":"<pre><code>plt.figure(7)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\n\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#static-function","title":"Static function","text":"<pre><code>plt.figure(8)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"book/5-Multiobjective-Parameter-Estimation/#pareto-set-solutions","title":"Pareto-set solutions","text":"<pre><code>plt.figure(9)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Optimum pareto-curve\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p>The following table show the results reported in \u2018IniciacaoCientifica2007\u2019 and the ones obtained with SysIdentPy implementation</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.5514725 0.549144 \\(\\theta_2\\) 0.40449005 0.408028 \\(\\theta_3\\) 1.42867821 1.45097 \\(\\theta_4\\) -12.60548863 -12.55788 \\(\\theta_5\\) 8.32740057 8.1516315 \\(\\theta_6\\) -11.41574116 -11.09728 \\(\\theta_7\\) 12.25729955 12.215782 \\(\\theta_8\\) 3.08461195 2.9319577 <p>where:</p> \\[ E_{Scilab} = \u00a0 \u00a017.426613 \\] <p>and:</p> \\[ E_{Python} = 17.474865 \\] <p>Note: as mentioned before, the order of the regressors in the model change, but it is the same structure. The tables shows the respective regressor parameter concerning <code>SysIdentPy</code> and <code>IniciacaoCientifica2007</code>, but the order \\(\\Theta_1\\), \\(\\Theta_2\\) and so on are not the same of the ones in <code>model.final_model</code></p> <pre><code>R, qit = mult2.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> \\[ R = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} \\] <p>and</p> \\[ qit = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 2 \\\\ \\end{bmatrix} \\] <p>model's structure that will be utilized (\u2018IniciacaoCientifica2007\u2019):</p> \\[ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 \\] \\[ q_i = \\begin{bmatrix} 0 &amp; 0 \\\\ 1 &amp; 0 \\\\ 2 &amp; 0 \\\\ 2 &amp; 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\overline{y} \\\\ \\overline{u} \\\\ \\overline{u^2} \\end{bmatrix} \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#biobjective-optimization","title":"Biobjective optimization","text":""},{"location":"book/5-Multiobjective-Parameter-Estimation/#an-use-case-applied-to-buck-converter-cc-cc-using-as-objectives-the-static-curve-information-and-the-prediction-error-dynamic","title":"An use case applied to Buck converter CC-CC using as objectives the static curve information and the prediction error (dynamic)","text":"<pre><code>bi_objective = AILS(\n\u00a0 \u00a0 static_function=True, static_gain=False, final_model=final_model, normalize=True\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> \\[ W_{LS} = 0.9931126 \\] <p>and:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective.estimate(\n\u00a0 \u00a0 y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\n\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\n\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg \\(\\lVert J \\rVert\\) 0.990080 0.009920 0.990863 1.000000 0.990939 0.987127 0.012873 0.990865 0.987032 0.990939 0.984182 0.015818 0.990867 0.974307 0.990939 0.981247 0.018753 0.990870 0.961803 0.990940 0.978320 0.021680 0.990873 0.949509 0.990941 ... ... ... ... ... 0.002555 0.997445 0.999993 0.000072 0.999993 0.002547 0.997453 0.999994 0.000072 0.999994 0.002540 0.997460 0.999996 0.000071 0.999996 0.002532 0.997468 0.999998 0.000071 0.999998 0.002525 0.997475 1.000000 0.000070 1.000000 <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nr\n</code></pre> Regressors Parameters ERR 0 1 1.3873E+00 9.999E-01 1 y(k-1) 5.4941E-01 2.042E-05 2 y(k-2) 4.0804E-01 1.108E-06 3 x1(k-1) -1.2515E+01 4.688E-06 4 x1(k-2) 1.2227E+01 3.922E-07 5 x1(k-1)^2 8.1171E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1047E+01 5.690E-07 7 x1(k-2)^2 2.9043E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(10)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 QR.dot(model.theta),\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX \u200b\u200bstatic representation\",\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n)\n\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Static Curve Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>where the best estimated \\(\\Theta\\) is</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54940883 0.5494135 \\(\\theta_2\\) 0.40803995 0.4080312 \\(\\theta_3\\) 1.38725684 3.3857601 \\(\\theta_4\\) -12.51466378 -12.513688 \\(\\theta_5\\) 8.11712897 8.116575 \\(\\theta_6\\) -11.04664789 -11.04592 \\(\\theta_7\\) 12.22693907 12.227184 \\(\\theta_8\\) 2.90425844 2.9038468 <p>where:</p> \\[ E_{Scilab} = 17.408934 \\] <p>and:</p> \\[ E_{Python} = 17.408947 \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#multiobjective-parameter-estimation","title":"Multiobjective parameter estimation","text":""},{"location":"book/5-Multiobjective-Parameter-Estimation/#use-case-considering-2-different-objectives-the-prediction-error-and-the-static-gain","title":"Use case considering 2 different objectives: the prediction error and the static gain","text":"<pre><code>bi_objective_gain = AILS(\n\u00a0 \u00a0 static_function=False, static_gain=True, final_model=final_model, normalize=False\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> \\[ W_{LS} = 0.9931126 \\] <p>and:</p> \\[ W_{SF} = 0.0068874 \\] <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(0, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective_gain.estimate(\n\u00a0 \u00a0 X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\n\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\n\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg \\(\\lVert J \\rVert\\) 1.000000 0.000000 17.407256 3.579461e+01 39.802849 0.997012 0.002988 17.407528 2.109260e-01 17.408806 0.994033 0.005967 17.407540 2.082067e-01 17.408785 0.991063 0.008937 17.407559 2.056636e-01 17.408774 0.988102 0.011898 17.407585 2.031788e-01 17.408771 ... ... ... ... ... 0.002555 0.997445 17.511596 3.340081e-07 17.511596 0.002547 0.997453 17.511596 3.320125e-07 17.511596 0.002540 0.997460 17.511597 3.300289e-07 17.511597 0.002532 0.997468 17.511598 3.280571e-07 17.511598 0.002525 0.997475 17.511599 3.260972e-07 17.511599 <pre><code># Writing the results\nmodel.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective_gain.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n\u00a0 \u00a0 results(\n\u00a0 \u00a0 \u00a0 \u00a0 model.final_model,\n\u00a0 \u00a0 \u00a0 \u00a0 model.theta,\n\u00a0 \u00a0 \u00a0 \u00a0 model.err,\n\u00a0 \u00a0 \u00a0 \u00a0 model.n_terms,\n\u00a0 \u00a0 \u00a0 \u00a0 err_precision=3,\n\u00a0 \u00a0 \u00a0 \u00a0 dtype=\"sci\",\n\u00a0 \u00a0 ),\n\u00a0 \u00a0 columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4853E+00 9.999E-01 1 y(k-1) 5.4940E-01 2.042E-05 2 y(k-2) 4.0806E-01 1.108E-06 3 x1(k-1) -1.2581E+01 4.688E-06 4 x1(k-2) 1.2210E+01 3.922E-07 5 x1(k-1)^2 8.1686E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1122E+01 5.690E-07 7 x1(k-2)^2 2.9455E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(12)\nplt.title(\"Gain\")\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 gain,\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"o\",\n\u00a0 \u00a0 label=\"Buck converter static gain\",\n)\n\nplt.plot(\n\u00a0 \u00a0 Uo,\n\u00a0 \u00a0 HR.dot(model.theta),\n\u00a0 \u00a0 linestyle=\"-\",\n\u00a0 \u00a0 marker=\"^\",\n\u00a0 \u00a0 linewidth=1.5,\n\u00a0 \u00a0 label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Gain Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>being the selected \\(\\theta\\):</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54939785 0.54937289 \\(\\theta_2\\) 0.40805603 0.40810168 \\(\\theta_3\\) 1.48525190 1.48663719 \\(\\theta_4\\) -12.58066084 -12.58127183 \\(\\theta_5\\) 8.16862622 8.16780294 \\(\\theta_6\\) -11.12171897 -11.11998621 \\(\\theta_7\\) 12.20954849 12.20927355 \\(\\theta_8\\) 2.94548501 2.9446532 <p>where:</p> \\[ E_{Scilab} = \u00a017.408997 \\] <p>and:</p> \\[ E_{Python} = 17.408781 \\]"},{"location":"book/5-Multiobjective-Parameter-Estimation/#additional-information","title":"Additional Information","text":"<p>You can also access the matrix Q and H using the following methods</p> <p>Matrix Q:</p> <pre><code>bi_objective_gain.build_static_function_information(Uo, Yo)[1]\n</code></pre> <p>Matrix H+R:</p> <pre><code>bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1]\n</code></pre>"},{"location":"book/6-Multiobjective-Model-Structure-Selection/","title":"6. Multiobjective Model Structure Selection","text":"<p>Coming soon</p>"},{"location":"book/7-NARX-Neural-Network/","title":"7. NARX Neural Network","text":"<p>Coming soon</p>"},{"location":"book/8-Severely-Nonlinear-System-Identification/","title":"8. Severely Nonlinear System Identification","text":"<p>We have categorized systems into two different classes for now: linear systems and nonlinear systems. As mentioned, linear systems has been extensively studied with several different well-established methods available, while nonlinear systems is a very active field with several problems that are still open for research. Besides linear and nonlinear systems, there are the ones called Severely Nonlinear Systems. Severely Nonlinear Systems are the ones that exhibit highly complex and exotic dynamic behaviors like sub-harmonics, chaotic behavior and hysteresis. For now, we will focus on system with hysteresis.</p>"},{"location":"book/8-Severely-Nonlinear-System-Identification/#modeling-hysteresis-with-polynomial-narx-model","title":"Modeling Hysteresis With Polynomial NARX Model","text":"<p>Hysteresis nonlinearity is a severely nonlinear behavior commonly found in electromagnetic devices, sensors, semiconductors, intelligent materials, and many more, which have memory effects between quasi-static input and output (Visintin, A., \"Differential Models of Hysteresis\"), (Ahmad, I., \"Two Degree-of-Freedom Robust Digital Controller Design With Bouc-Wen Hysteresis Compensator for Piezoelectric Positioning Stage\"). A hysteretic system is one that exhibits a path-dependent behavior, meaning its response depends not only on its current state but also on its history.  In a hysteretic system, when you apply an input, the system's response (like displacement or stress) doesn't follow the same path to the starting point when you remove the input. Instead, it forms a loop-like pattern called a hysteresis loop. This is because the system have the ability to preserve a deformation caused by an input, characterizing a memory effect.</p> <p>The identification of hysteretic systems using polynomial NARX models is typically an intriguing task because the traditional Model Structure Selection algorithms do not work properly (Martins, S. A. M. and Aguirre, L. A., \"Sufficient conditions for rate-independent hysteresis in autoregressive identified models\", Leva, A. and Piroddi, L., \"NARX-based technique for the modelling of magneto-rheological damping devices\"). Martins, S. A. M. and Aguirre, L. A. presented the sufficient conditions to describe hysteresis using polynomial models by providing the concept of bounding structure \\(\\mathcal{H}\\). Polynomial NARX models with a single equilibrium can be used in a full characterization of the hysteresis behavior adopting the bounding structure concept.</p> <p>The following are some of the essential concepts and formal definitions for understanding how NARX model can be used to describe systems with hysteresis.</p>"},{"location":"book/8-Severely-Nonlinear-System-Identification/#continuous-time-loading-unloading-quasi-static-signal","title":"Continuous-time loading-unloading quasi-static signal","text":"<p>One important characteristic to model hysteretic systems is the input signal. A loading-unloading quasi-static signal is a periodic continuous time signal \\(x_t\\) with period \\(T = (t_f - t_i)\\) and frequency \\(\\omega = 2\\pi f\\) where \\(x_t\\) increases monotonically from \\(x_{min}\\) to \\(x_{max}\\), considering \\(t_i \\leq t \\leq t_m\\) (loading) and decreases monotonically from \\(x_{max}\\) to \\(x_{min}\\), considering \\(t_m \\leq t \\leq t_f\\) (unloading). If the loading-unloading signal changes with \\(\\omega \\rightarrow 0\\), the signal is also called a quasi-static signal. Visually, this is much more simple to understand. The following image shows a continuous-time loading-unloading quasi-static signal.</p> <p></p> <p>Figure 1. Continuous-time loading-unloading quasi-static signal, demonstrating the periodic increase and decrease of the input signal.</p> <p>In this respect, Martins, S. A. M. and Aguirre, L. A. also presented the idea of transforming the inputs of the system using multi-valued functions.</p> <p>Multivalued functions - Let \\(\\phi (\\Delta x_{k}): \\mathbb{R} \\rightarrow \\mathbb{R}\\). If~\\(\\Delta x_{k}=x_k-x_{k-1}\\), \\(\\phi (\\Delta x_{k})\\) is a multivalued function if:</p> \\[ \\begin{equation}     \\phi (\\Delta x_{k})=     \\begin{cases}         \\phi_1, &amp; if \\ \\Delta x_{k} &gt; \\epsilon; \\\\         \\phi_2, &amp; if \\ \\Delta x_{k} &lt; \\epsilon; \\\\         \\phi_3, &amp; if \\ \\Delta x_{k} = \\epsilon; \\\\     \\end{cases} \\end{equation} \\tag{1} \\] <p>where \\(\\epsilon \\in \\mathbb{R}\\), \\(\\phi_1 \\neq \\phi_2 \\neq \\phi_3\\). For some inputs  \\(\\Delta x_{k}\\neq \\epsilon, \\ \\forall{k} \\in \\mathbb{N}\\) , and the last value in equation above is not used.</p> <p>A frequently used multivalued function is the sign\\((\\cdot): \\mathbb{R} \\rightarrow \\mathbb{R}\\):</p> \\[  \\begin{equation}  sign(x)=     \\begin{cases}         1, &amp; if \\ x &gt; 0; \\\\         -1, &amp; if \\ x &lt; 0; \\\\         0, &amp; if \\ x = 0. \\\\     \\end{cases} \\end{equation} \\tag{2} \\]"},{"location":"book/8-Severely-Nonlinear-System-Identification/#hysteresis-loops-in-continuous-time-mathcalh_tomega","title":"Hysteresis loops in continuous time \\(\\mathcal{H}_t(\\omega)\\)","text":"<p>Let \\(x_t\\) be a continuous-time loading-unloading quasi-static signal applied to a continuous-time system and \\(y_t\\) is the system output. \\(\\mathcal{H}_t(\\omega)\\) denotes a closed loop in the \\(x_t - y_t\\) plane, which shape depend on \\(\\omega\\). If the system presents hysteretic nonlinearity, \\(\\mathcal{H}_t(\\omega)\\) is denoted as:</p> \\[ \\begin{equation} \\mathcal{H}_t(\\omega) =     \\begin{cases}         \\mathcal{H}_t(\\omega)^{+}, \\ for \\ t_i \\ \\leq \\ t \\ \\leq \\ t_m, \\\\         \\mathcal{H}_t(\\omega)^{-}; \\ for \\ t_m \\ \\leq \\ t \\ \\leq \\ t_f, \\\\     \\end{cases} \\end{equation} \\tag{3} \\] <p>where \\(\\mathcal{H}_t(\\omega)^{+} \\neq \\mathcal{H}_t(\\omega)^{-}\\), \\(\\forall t \\neq t_m\\). \\(t_i \\leq t \\leq t_m\\) and~\\(t_m \\leq t \\leq t_f\\) correspond to the regime when \\(x_t\\) is loading and unloading, respectively. \\(\\mathcal{H}_t(\\omega)^{+}\\) corresponds to the part of the loop formed in the \\(x_t - y_t\\) plane, while \\(t_i \\leq t \\leq t_m\\) (when \\(x_t\\) is loading) whereas \\(\\mathcal{H}_t(\\omega)^{-}\\) is the part of the loop formed in the~\\(x_t - y_t\\) plane for~\\(t_m \\leq t \\leq t_f\\) (when \\(x_t\\) is unloading), as shown in the Figure 2:</p> <p></p> <p>Figure 2. Example of a hysteresis curve.</p> <p>Rate Independent Hysteresis (RIH) (Visintin, A., \"Differential Models of Hysteresis\") - The hysteresis behavior is called to be rate independent if the path \\(ABCD\\), which depends on pair \\(x(t), y(t)\\), is invariant with respect to any increasing diffeomorphism~\\(\\varphi : [0,T] \\rightarrow [0,T]\\), i.e.:</p> \\[ \\begin{align}         F(u \\ o \\ \\varphi, y^{0}) = F(u,y^0)\\ o \\ \\varphi &amp; \\ em \\ [0,T]. \\end{align} \\tag{4} \\] <p>This means that at any instant \\(t\\), \\(y(t)\\) depends only on \\(u:[0,T] \\rightarrow \\mathbb{R}\\) and on the order in which values have been attained before \\(t\\). In other words, the memory effect is not affected by the frequency of the input.</p>"},{"location":"book/8-Severely-Nonlinear-System-Identification/#rate-independent-hysteresis-in-polynomial-narx-model","title":"Rate Independent Hysteresis  in polynomial NARX model","text":"<p>Martins, S. A. M. and Aguirre, L. A. presented the sufficient conditions for NARX model to represent hysteresis. One of the developed concepts in the Bounding Structure \\(\\mathcal{H}\\).</p> <p>Bounding Structure \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A.) - Let \\(\\mathcal{H}_t(\\omega)\\) be the system hysteresis. \\(\\mathcal{H}= \\lim_{\\omega \\to 0} \\mathcal{H}_t(\\omega)\\) is defined as the bounding structure that delimits \\(\\mathcal{H}_t(\\omega)\\).</p> <p>Now, consider a polynomial NARX excited by a loading-unloading quasi-static signal. If the model has one real and stable equilibrium point whose location depends on input and loading/unloading regime, the polynomial will exhibit a Rate Independent Hysteresis loop \\(\\mathcal{H}_t(\\omega)\\) in the \\(x-y\\) plane.</p> <p>Here is an example. Let \\(y_k  =  0.8y_{k-1} + 0.4\\phi_{k-1} + 0.2x_{k-1}\\), where \\(\\phi_{k} = \\rm{sign}(\\Delta(x_{k}))\\) and \\(x_{k} = sin(\\omega k)\\) and \\(\\omega\\) is the frequency of the input signal \\(x\\). The equilibria of this model is given by:</p> \\[ \\begin{equation}     \\overline{y}(\\overline{\\phi},\\overline{x})=     \\begin{cases}         \\frac{0.6+0.2\\overline{x}}{1-0.8} \\ = 3 \\ + \\ \\overline{x} \\ , &amp; for \\ loading; \\\\         \\frac{-0.6+0.2\\overline{x}}{1-0.8} \\ = -3 \\ + \\ \\overline{x} \\ , &amp; for \\ unloading; \\\\     \\end{cases} \\end{equation} \\tag{5} \\] <p>where \\(\\overline {x}\\) is a loading-unloading quasi-static input signal. Since the equilibrium points are asymptotically stable, the output converges to \\(\\mathcal{H}_k (w)\\) in the \\(x-y\\) plane. Note that for a constant input value \\(x ~ = ~ 1 ~ = ~ \\overline{x}\\), the equilibrium lies in \\(\\overline{y} ~ = ~ 3\\) for loading regime and \\(\\overline {y} ~ = ~ -1\\) for unloading regime. Analogously, for \\(\\overline {x} ~ = ~ -1\\), the equilibrium lies in \\(\\overline {y} ~ = ~ 1\\) for loading regime and \\(\\overline {y} ~ = ~ -3\\) for unloading regime, as shown in the figure below:</p> <p></p> <p>Figure 3. Example of a bounding structure \\(\\mathcal{H}\\). The black dots are on \\(\\mathcal{H}_{k}(\\omega)\\) for model \\(y_k  =  0.8y_{k-1} + 0.4\\phi_{k-1} + 0.2x_{k-1}\\). The bounding structure \\(\\mathcal{H}\\), in red, confines \\(\\mathcal{H}_{k}(\\omega)\\).}</p> <p>As can be observed in the Figure 3, in we guarantee the sufficient conditions proposed by Martins, S. A. M. and Aguirre, L. A., a NARX model can reproduce a hysteretic behavior. Chapter 10 presents a case study of a system with hysteresis.</p> <p>The following code can be used to reproduce the behavior shown in Figure 3. Change <code>w</code> from \\(1\\) to \\(0.1\\) to see how the bounded structure \\(\\mathcal{H}\\) converge to the equilibria of the system.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nw = 1\nt = np.arange(0, 60.1, 0.1)\ny = np.zeros(len(t))\nx = np.sin(w * t)\n\n# Initialize y and fi\nfi = np.zeros(len(t))\n# Iterate over the time array to calculate y\nfor k in range(1, len(t)):\n\u00a0 \u00a0 fi[k] = np.sign(x[k] - x[k-1])\n\u00a0 \u00a0 y[k] = 0.8 * y[k-1] + 0.2 * x[k-1] + 0.4 * fi[k-1]\n\nplt.plot(x, y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Example')\nplt.show()\n</code></pre> <p></p> <p>Figure 4.  Reproduction of a bounding structure \\(\\mathcal{H}\\) using python.</p>"},{"location":"book/9-Validation/","title":"9. Validation","text":""},{"location":"book/9-Validation/#the-predict-method-in-sysidentpy","title":"The <code>predict</code> Method in SysIdentPy","text":"<p>Before getting into the validation process in System Identification, it's essential to understand how the <code>predict</code> method works in SysIdentPy.</p>"},{"location":"book/9-Validation/#using-the-predict-method","title":"Using the <code>predict</code> Method","text":"<p>A typical usage of the <code>predict</code> method in SysIdentPy looks like this:</p> <pre><code>yhat = model.predict(X=x_test, y=y_test)\n</code></pre> <p>SysIdentPy users often have two common questions about this method:</p> <ol> <li>Why do we need to pass the test data, <code>y_test</code>, as an argument in the <code>predict</code> method?</li> <li>Why are the initial predicted values identical to the values in the test data?</li> </ol> <p>To address these questions, let\u2019s first explain the concepts of infinity-step-ahead prediction, n-step-ahead prediction, and one-step-ahead prediction in dynamic systems.</p>"},{"location":"book/9-Validation/#infinity-step-ahead-prediction","title":"Infinity-Step-Ahead Prediction","text":"<p>Infinity-step-ahead prediction, also known as free run simulation, refers to making predictions using previously predicted values, \\(\\hat{y}_{k-n_y}\\), in the prediction loop.</p> <p>For example, consider the following test input and output data:</p> \\[ x_{test} = [1, 2, 3, 4, 5, 6, 7] \\] \\[ y_{test} = [8, 9, 10, 11, 12, 13, 14] \\] <p>Suppose we want to validate a model \\(m\\) defined by:</p> \\[ m \\rightarrow y_k = 1*y_{k-1} + 2*x_{k-1} \\] <p>To predict the first value, we need access to both \\(y_{k-1}\\) and \\(x_{k-1}\\). This requirement explains why you need to pass <code>y_test</code> as an argument in the <code>predict</code> method. It also answers the second question: SysIdentPy requires the user to provide the initial conditions explicitly. The <code>y_test</code> data passed in the <code>predict</code> method is not used entirely; only the initial values needed for the model\u2019s lag structure are used.</p> <p>In this example, the model's maximum lag is 1, so we need only 1 initial condition. The predicted values, <code>yhat</code>, are then calculated as follows:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*10 + 2*2 = 14\nyhat(3) = 1*14 + 2*3 = 20\nyhat(4) = 1*20 + 2*4 = 28\n</code></pre> <p>As shown, the first value of <code>yhat</code> matches the first value of <code>y_test</code> because it serves as the initial condition. Another key point is that the prediction loop uses the previously predicted values, not the actual <code>y_test</code> values, which is why it's called infinity-step-ahead or free run simulation.</p> <p>In system identification, we often aim for models that perform well in infinity-step-ahead predictions. Since the prediction error propagates over time, a model that shows good performance in free run simulation is considered a robust model.</p> <p>In SysIdentPy, users only need to pass the initial conditions when performing an infinity-step-ahead prediction. If you pass only the initial conditions, the results will be the same! Therefore</p> <pre><code>yhat = model.predict(X=x_test, y=y_test)\n</code></pre> <p>is actually the same as</p> <pre><code>yhat = model.predict(X=x_test, y=y_test[:model.max_lag].reshape(-1, 1))\n</code></pre> <p><code>model.max_lag</code> can be accessed after we fit the model using the code below.</p> <pre><code>model = FROLS(\n    order_selection=False,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function,\n\u00a0 \u00a0 e_tol=0.9999\n\u00a0 \u00a0 n_terms=15\n)\nmodel.fit(X=x, y=y)\nmodel.max_lag\n</code></pre> <p>Its important to mention that, in current version of SysIdentPy, the maximum lag considered is actually the maximum lag between <code>xlag</code> and <code>ylag</code> definition. This is important because you can pass <code>ylag = xlag = 10</code> and the final model, after the model structure selection, select terms where the maximum lag is 3. You have to pass 10 initial conditions, but internally the calculations are done using the correct regressors. This is necessary due the way the regressors are created after that the model is fitted. Therefore, is recommended to use the <code>model.max_lag</code> to be sure.</p>"},{"location":"book/9-Validation/#1-step-ahead-prediction","title":"1-step ahead prediction","text":"<p>The difference between 1 step-ahead prediction and infinity-steps ahead prediction is that the model take the previous real <code>y_test</code> test values in the loop instead of the predicted <code>yhat</code> values. And that is a huge and important difference. Let's do prediction using 1-step ahead method:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*9 + 2*2 = 13\nyhat(3) = 1*10 + 2*3 = 16\nyhat(4) = 1*11 + 2*4 = 19\nand so on\n</code></pre> <p>The model uses real values in the loop and only predict the next value. The prediction error, in this case, is always corrected because we are not propagating the error using the predicted values in the loop.</p> <p>SysIdentPy's <code>predict</code> method allow the user to perform a 1-step ahead prediction by setting <code>steps_ahead=1</code></p> <pre><code>yhat = model.predict(X=x_test, y=y_test, steps_ahead=1)\n</code></pre> <p>In this case, as you can imagine, we need to pass the all the <code>y_test</code> data because the method have to access the real values at each iteration. If you pass only the initial conditions, <code>yhat</code> will have only the initial conditions plus 1 more sample, that is the 1-step ahead prediction. To predict another point, you would need to pass the new initial conditions again and so on. SysIdentPy already to everything for you, so just pass all the data you want to validate using the 1-step ahead method.</p>"},{"location":"book/9-Validation/#n-steps-ahead-prediction","title":"n-steps ahead prediction","text":"<p>The n-steps ahead prediction is almost the same as the 1-step ahead, but here you can define the number of steps ahead you want to test your model. If you set <code>steps_ahead=5</code>, for example, it means that the first 5 values will be predicted using <code>yhat</code> in the loop, but then the process is restarted by feeding the real values in <code>y_test</code> in the next iteration, then performing other 5 predictions using the <code>yhat</code> and so on. Let's check the example considering <code>steps_ahead=2</code>:</p> <pre><code>y_initial = yhat(0) = 8\nyhat(1) = 1*8 + 2*1 = 10\nyhat(2) = 1*10 + 2*2 = 14\nyhat(3) = 1*10 + 2*3 = 16\nyhat(4) = 1*16 + 2*4 = 24\nand so on\n</code></pre>"},{"location":"book/9-Validation/#model-performance","title":"Model Performance","text":"<p>Model validation is one of the most crucial part in system identification. As we mentioned before, in system identification we are trying the model the dynamic of the process without for task like control design. In such cases, we can not only rely on regression metrics, but also ensuring that residuals are unpredictable across various combinations of past inputs and outputs (Billings, S. A. and Voon, W. S. F., \"Structure detection and model validity tests in the identification of nonlinear systems\"). One often used statistical tests is the normalized RMSE, called RRSE, which can be expressed by</p> \\[ \\begin{equation}         \\textrm{RRSE}= \\frac{\\sqrt{\\sum\\limits_{k=1}^{n}(y_k-\\hat{y}_k)^2}}{\\sqrt{\\sum\\limits_{k=1}^{n}(y_k-\\bar{y})^2}}, \\end{equation} \\tag{1} \\] <p>where \\(\\hat{y}_k \\in \\mathbb{R}\\) the model predicted output and \\(\\bar{y} \\in \\mathbb{R}\\) the mean of the measured output \\(y_k\\). The RRSE gives some indication regarding the quality of the model, but concluding about the best model by evaluating only this quantity may lead to an incorrect interpretation, as shown in following example.</p> <p>Consider the models</p> \\[ y_{{_a}k} = 0.7077y_{{_a}k-1} + 0.1642u_{k-1} + 0.1280u_{k-2} \\] <p>and</p> \\[y_{{_b}k}=0.7103y_{{_b}k-1} + 0.1458u_{k-1} + 0.1631u_{k-2} -1467y^3_{{_b}k-1} + 0.0710y^3_{{_b}k-2} +0.0554y^2_{{_b}k-3}u_{k-3}\\] <p>defined in the Meta Model Structure Selection: An Algorithm For Building Polynomial NARX Models For Regression And Classification. The former results in a \\(RRSE = 0.1202\\) while the latter gives \\(RRSE~=0.0857\\). Although the model \\(y_{{_b}k}\\) fits the data better, it is only a biased representation to one piece of data and not a good description of the entire system.</p> <p>The RRSE (or any other metric) shows that validations test might be performed carefully. Another traditional practice is split the data set in two parts. In this respect, one can test the models obtained from the estimation part of the data using a specific data for validation. However, the one-step-ahead performance of NARX models generally results in misleading interpretations because even strongly biased models may fit the data well. Therefore, a free run simulation approach usually allows a better interpretation if the model is adequate or not (Billings, S. A.).</p> <p>Statistical tests for SISO models based on the correlation functions were proposed in (Billings, S. A. and Voon, W. S. F., \"A prediction-error and stepwise-regression estimation algorithm for non-linear systems\"), (Model validity tests for non-linear signal processing applications). The tests are:</p> \\[ \\begin{align}     \\phi_{_{\\xi \\xi}\\tau} &amp;= E\\{\\xi_k \\xi_{k-\\tau}\\} = \\delta_{\\tau}, \\\\     \\phi_{_{\\xi x}\\tau} &amp;= E\\{\\xi_k x_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{\\xi \\xi x}\\tau} &amp;= E\\{\\xi_k \\xi_{k-\\tau} x_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{x^2 \\xi}\\tau} &amp;= E\\{(u^2_k - E\\{x^2_k\\})\\xi_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{x^2 \\xi^2}\\tau} &amp;= E\\{(u^2_k - E\\{x^2_k\\})\\xi^2_{k-\\tau}\\} = 0 \\forall \\tau, \\\\     \\phi_{_{(y\\xi) x^2}\\tau} &amp;= E\\{(y_k\\xi_k - E\\{y_k\\xi_k\\})(x^2_{k-\\tau} - E\\{x^2_k\\})\\} = 0 \\forall \\tau, \\end{align} \\tag{2} \\] <p>where \\(\\delta\\) is the Dirac delta function and the cross-correlation function \\(\\phi\\) is denoted by (Billings, S. A. and Voon, W. S. F.):</p> \\[ \\begin{equation} \\phi_{{_{ab}}\\tau} = \\frac{\\frac{1}{n}\\sum\\limits_{k=1}^{n-\\tau}(a_k - \\hat{a})(b_{k+\\tau}-\\hat{b})}{\\sqrt{\\frac{1}{n}\\sum\\limits_{k=1}^{n}(a_k-\\hat{a})^2} \\sqrt{\\frac{1}{n}\\sum\\limits_{k=1}^{n}(b_k-\\hat{b})^2}} = \\frac{\\sum\\limits_{k=1}^{n-\\tau}(a_k - \\hat{a})(b_{k+\\tau}-\\hat{b})}{\\sqrt{\\sum\\limits_{k=1}^{n}(a_k-\\hat{a})^2} \\sqrt{\\sum\\limits_{k=1}^{n}(b_k-\\hat{b})^2}}, \\end{equation} \\tag{3} \\] <p>where \\(a\\) and \\(b\\) are two signal sequences. If the tests are true, then the model residues can be considered as white noise.</p>"},{"location":"book/9-Validation/#metrics-available-in-sysidentpy","title":"Metrics Available in SysIdentPy","text":"<p>SysIdentPy provides the following regression metrics out of the box:</p> <ul> <li>forecast_error</li> <li>mean_forecast_error</li> <li>mean_squared_error</li> <li>root_mean_squared_error</li> <li>normalized_root_mean_squared_error</li> <li>root_relative_squared_error</li> <li>mean_absolute_error</li> <li>mean_squared_log_error</li> <li>median_absolute_error</li> <li>explained_variance_score</li> <li>r2_score</li> <li>symmetric_mean_absolute_percentage_error</li> </ul> <p>To use them, the user only need to import the desired metric using, for example</p> <pre><code>from sysidentpy.metrics import root_relative_squared_error\n</code></pre> <p>SysIdentPy also provides methods for calculate and analyse the residues correlation</p> <pre><code>from sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n\u00a0 \u00a0 compute_residues_autocorrelation,\n\u00a0 \u00a0 compute_cross_correlation,\n)\n</code></pre> <p>Lets check the metrics of the eletro mechanical system modeled in Chapter 4.</p> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n\u00a0 \u00a0 compute_residues_autocorrelation,\n\u00a0 \u00a0 compute_cross_correlation,\n)\nfrom sysidentpy.metrics import root_relative_squared_error\n\ndf1 = pd.read_csv(\"examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"examples/datasets/y_cc.csv\")\n\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=15,\n\u00a0 \u00a0 ylag=2,\n\u00a0 \u00a0 xlag=2,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=LeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\n\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p> <p>The RRSE is 0.0800, which is a very good metric. However, we can see that the residues have somo high auto-correlations anda with the input. This mean that our model maybe is not good enough as it could be.</p> <p>Lets check what happens if we increase <code>xlag</code>, <code>ylag</code> and change the parameter estimation algorithm from Least Squares to the Recursive Least Squares</p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n\u00a0 \u00a0 order_selection=True,\n\u00a0 \u00a0 n_info_values=50,\n\u00a0 \u00a0 ylag=5,\n\u00a0 \u00a0 xlag=5,\n\u00a0 \u00a0 info_criteria=\"bic\",\n\u00a0 \u00a0 estimator=RecursiveLeastSquares(unbiased=False),\n\u00a0 \u00a0 basis_function=basis_function\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>Now the RRSE is 0.0568 and we have a better residual correlation!</p> <p></p> <p></p> <p></p> <p>In the end of day, the best model will be the model that satisfy the user needs. However, it's important to understand how to analyse the models so you can have an idea if you can get some improvements without too much work.</p> <p>For the sake of curiosity, lets check how the model perform if we run a 1-step ahead prediction. We don't need to fit the model again, just make another prediction using the 1-step option.</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid, steps_ahead=1)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n# plot only the first 100 samples (n=100)\nplot_results(y=y_valid, yhat=yhat, n=100)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>The same model, but evaluating the 1-step ahead prediction, now return a RRSE\\(= 0.02044\\) and the residues are even better. But remember, that is expected, as explained in the previous section.</p> <p></p> <p></p> <p></p>"},{"location":"changelog/changelog/","title":"Changes in SysIdentPy","text":""},{"location":"changelog/changelog/#v060","title":"v0.6.0","text":""},{"location":"changelog/changelog/#contributors","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>oliveira-mark</li> </ul>"},{"location":"changelog/changelog/#changes","title":"CHANGES","text":"<p>This release introduces significant improvements focused on improving code organization, readability, and compliance with PEP8 standards. It also includes a new foundational class for Error Reduction Ratio (ERR) based algorithms, expanded testing, and the discontinuation of Python 3.7 support.</p>"},{"location":"changelog/changelog/#new-features","title":"New Features","text":"<ul> <li>Introduced the <code>OFRBase</code> class, encapsulating common methods essential for ERR-based algorithms.</li> <li>Refactored the <code>FROLS</code> class to inherit from <code>OFRBase</code>, focusing it solely on the ERR algorithm.</li> <li>Implemented the Ultra Orthogonal Forward Regression (UOFR) algorithm, also inheriting from <code>OFRBase</code>.</li> </ul>"},{"location":"changelog/changelog/#api-changes","title":"API Changes","text":"<ul> <li>BREAKING CHANGE: Fixed a typo in Bernstein Basis Function. Previously it was defined as Bersntein.</li> <li>Refactoring and Modularization:<ul> <li>Extracted the <code>InformationMatrix</code> class from <code>narmax_base</code> into a new module: <code>utils.information_matrix</code>.</li> <li>Moved specific methods to newly created modules: <code>utils.lags</code> and <code>utils.simulation</code>, promoting better separation of concerns.</li> <li>Add deprecation message for RidgeRegression <code>solver</code> argument.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#code-quality-improvements","title":"Code Quality Improvements","text":"<ul> <li>Renamed variables and methods for better readability and PEP8 compliance, including changing uppercase variable names to lowercase.</li> <li>Updated imports to use new utility modules, reducing redundancy and improving maintainability.</li> <li>Removed unused imports and redundant parentheses, streamlining the codebase.</li> <li>Change python version in deploy action.</li> </ul>"},{"location":"changelog/changelog/#testing-enhancements","title":"Testing Enhancements","text":"<ul> <li>Added comprehensive tests for basis functions: Bernstein, Bilinear, Hermite (normalized), Laguerre, and Legendre.</li> <li>Introduced tests for utility methods, including <code>narmax_tools</code>, <code>save_load</code>, and the new simulation utilities.</li> <li>Increased test coverage to 92%, ensuring robustness and reliability.</li> </ul>"},{"location":"changelog/changelog/#validation-and-error-handling","title":"Validation and Error Handling","text":"<ul> <li>Implemented a validation check for <code>train_percentage</code>, raising an error for values exceeding 100%.</li> <li>Adapted methods and tests following the removal of the <code>InformationMatrix</code> class to ensure consistency across the codebase.</li> </ul>"},{"location":"changelog/changelog/#documentation-updates","title":"Documentation Updates","text":"<ul> <li>Launched a redesigned frontend page featuring a modern UI and improved responsiveness.</li> <li>Restructured multiple sections for better organization and clarity.</li> <li>Overhauled key guides, including <code>quick_start</code>, <code>developer_guide</code>, and <code>user_guide</code>.</li> <li>Added new examples, including the Lorenz System and Chaotic Map.</li> <li>Enhanced grammar and readability across documentation.</li> <li>Updated dependencies related to <code>mkdocs</code> for better performance and compatibility.</li> <li>Improved Google Analytics integration.</li> <li>Fixed broken links for the Nubank and Estatidados blogs, and refined link formatting in the book.</li> <li>Updated class docstrings to align with the latest changes.</li> <li>Standardized docstrings and method signatures to use lowercase variable names, following PEP8 guidelines.</li> <li>Revised contribution examples to reflect the latest <code>sysidentpy</code> version.</li> <li>Integrated book examples into traditional documentation, with direct links to the book section.</li> <li>Adjusted structure, titles, and links across various docs and examples for better navigation.</li> <li>Removed dataset files; datasets are now hosted in a dedicated repository, <code>sysidentpy-data</code>.</li> <li>Acknowledged JetBrains support and collaboration in the README and sponsor page.</li> <li>Fix edit uri when clicking to edit doc page in doc website.</li> <li>Now every example loads the data from <code>sysidentpy-data</code> repository.</li> </ul>"},{"location":"changelog/changelog/#python-version-support-update","title":"Python Version Support Update","text":"<ul> <li>Support for Python 3.7 has been discontinued. This aligns with the official end of support for Python 3.7 and resolves compatibility issues with newer dependencies.</li> <li>Certain parameter estimation algorithms, such as Bounded Variable Least Squares, require newer versions of SciPy that no longer support Python 3.7.</li> <li>Users can still run SysIdentPy on Python 3.7, but some features, including some parameter estimation functionalities, will be unavailable.</li> </ul>"},{"location":"changelog/changelog/#impact","title":"IMPACT","text":"<ul> <li>These changes improve the modularity, readability, and maintainability of the codebase. The introduction of the <code>OFRBase</code> class simplifies the implementation of ERR-based algorithms, facilitating future extensions. Comprehensive testing ensures the reliability of both new and existing functionalities.</li> </ul>"},{"location":"changelog/changelog/#testing","title":"TESTING","text":"<ul> <li>All new and existing tests were executed, achieving 92% test coverage.</li> </ul>"},{"location":"changelog/changelog/#v053","title":"v0.5.3","text":""},{"location":"changelog/changelog/#contributors_1","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_1","title":"CHANGES","text":"<p>IMPORTANT! This update addresses a bug related to the Bilinear basis function for models with more the 2 inputs. This release keep providing crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#api-changes_1","title":"API Changes","text":"<ul> <li>Fix Bilinear basis function issue for models with more than 2 inputs. This fix the <code>get_max_xlag</code> method in <code>basis_function_base</code> and also fix how <code>combination_xlag</code> is created.</li> </ul>"},{"location":"changelog/changelog/#v052","title":"v0.5.2","text":""},{"location":"changelog/changelog/#contributors_2","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_2","title":"CHANGES","text":"<p>IMPORTANT! This update addresses a critical bug related to the Polynomial and Bilinear basis function for models with more the 3 inputs. The issue raised due to the changes in basis function for v0.5.0, but has now been resolved. This release keep providing crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#api-changes_2","title":"API Changes","text":"<ul> <li>Fix Polynomial and Bilinear basis function issue for models with more than 3 inputs.</li> </ul>"},{"location":"changelog/changelog/#v051","title":"v0.5.1","text":""},{"location":"changelog/changelog/#contributors_3","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_3","title":"CHANGES","text":"<p>This update addresses a critical bug related to the unbiased estimator. The issue previously impacted all basis functions but has now been resolved. This release keep providing crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#documentation","title":"Documentation","text":"<ul> <li>Remove unnecessary code when importing basis functions in many examples.</li> </ul>"},{"location":"changelog/changelog/#api-changes_3","title":"API Changes","text":"<ul> <li>Fix unbiased estimator for every basis function.</li> </ul>"},{"location":"changelog/changelog/#v050","title":"v0.5.0","text":""},{"location":"changelog/changelog/#contributors_4","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>nataliakeles</li> <li>LeWerner42</li> <li>Suyash Gaikwad</li> </ul>"},{"location":"changelog/changelog/#changes_4","title":"CHANGES","text":"<p>This update introduces major new features and important bug fixes. This release provides crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#new-features_1","title":"New Features","text":"<ul> <li>MAJOR: Add Bilinear Basis Function (thanks nataliakeles). Now the user can use Bilinear NARX models for forecasting.</li> <li>MAJOR: Add Legendre polynomial basis function. Now the user can use Legendre NARX models for forecasting.</li> <li>MAJOR: Add Hermite polynomial basis function. Now the user can use Hermite NARX models for forecasting. MAJOR: Add Hermite Normalized polynomial basis function. Now the user can use Hermite Normalized NARX models for forecasting. MAJOR: Add Laguerre polynomial basis function. Now the user can use Laguerre NARX models for forecasting.</li> </ul>"},{"location":"changelog/changelog/#documentation_1","title":"Documentation","text":"<ul> <li>Add basis function overview.</li> <li>Files related to v.3.* doc removed.</li> <li>Improved formatting in mathematical equations.</li> <li>Fixed typos and grammatical errors in README.md (thanks Suyash Gaikwad and LeWerner42)</li> <li>Minor additions and grammar fixes.</li> <li>Remove book assets from main repository. The assets were moved to sysidentpy-data repository to keep main repository cleaner and lighter.</li> <li>Fixed link in the book cover to ensure it correctly redirects to the book details. Also change x2_val to x_valid in examples of how to use in readme.</li> <li>Add Pix method as an alternative for brazilian sponsors.</li> <li>Fix code documentation for basis function (it was not showing up in the docs before).</li> <li>Remove <code>pip install</code> from the list of the dependencies needed in the chapter.</li> </ul>"},{"location":"changelog/changelog/#datasets","title":"Datasets","text":"<ul> <li>Datasets are now available in a separate repository.</li> </ul>"},{"location":"changelog/changelog/#api-changes_4","title":"API Changes","text":"<ul> <li>add deprecated messages for bias and n in Bersntein basis function. Both parameters will be removed in v0.6.0. Use <code>include_bias</code> and <code>degree</code>, respectively, instead.</li> <li>Deploy-docs.yml: Change option to make a clean build of the documentation.</li> <li>Deploy-docs.yml: Change python version to deploy docs.</li> <li>Support for Python 3.13 depending on the release of the Pytorch 2.6. Every method in sysidentpy works in python 3.13 excluding neural narx.</li> <li>Update mkdocstrings dependency version</li> <li>Change Polynomial check from class name to isinstance method in every class.</li> <li>Remove support for torch==2.4.0 due to pip error in pytorch side. I'll check if it was solved before allow newer versions of pytorch.</li> <li>Make \"main\" the new default branch. Master branch removed.</li> <li>Change actions from master to main branch.</li> <li>Split basis function classes into multiples files (one for each basis).</li> <li>Fix redundant bias check on bersntein basis.</li> <li>Fix docstring math notation in basis functions docstring.</li> <li>Remove requirements.txt file.</li> <li>Extensive code refactoring, including type hint improvements, docstring enhancements, removal of unused code, and other behind-the-scenes changes to support new features.</li> <li>Add model_type in basis function base fit and predict method.</li> <li>Change variable name from <code>combinations</code> to <code>combination_list</code> to avoid any issue with itertools <code>combination</code> method in case I want to use it in the future.</li> <li>Remove requirements.txt file.</li> </ul>"},{"location":"changelog/changelog/#v040","title":"v0.4.0","text":""},{"location":"changelog/changelog/#contributors_5","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_5","title":"CHANGES","text":"<p>This update introduces several major features and changes, including some breaking changes. There is a guide to help you update your code to the new version. Depending on your model definition, you might not need to change anything. I decided to go directly to version v0.4.0 instead of providing incremental updates (0.3.5, 0.3.6, etc.) because the breaking changes are easy to fix and the new features are highly beneficial. This release provides crucial groundwork for the future development of SysIdentPy, making easier to add new features and improve the code, setting the stage for a robust and feature-complete 1.0.0 release in the feature.</p>"},{"location":"changelog/changelog/#new-features_2","title":"New Features","text":"<ul> <li>MAJOR: NonNegative Least Squares algorithm for parameter estimation.</li> <li>MAJOR: Bounded Variables Least Squares algorithm for parameter estimation.</li> <li>MAJOR: Least Squares Minimal Residual algorithm for parameter estimation.</li> <li>MAJOR: Error Reduction Ratio algorithm enhancement for FROLS model structure selection. Users can now set an <code>err_tol</code> value to stop the algorithm when the sum of the ERR values reaches this threshold, offering a faster alternative to Information Criteria algorithms. A new example is available in the documentation.</li> <li>MAJOR: New Bernstein basis function available, allowing users to choose between Polynomial, Fourier, and Bernstein.</li> <li>MAJOR: v0.1 of the companion book \"Nonlinear System Identification: Theory and Practice With SysIdentPy.\" This open-source book serves as robust documentation for the SysIdentPy package and a friendly introduction to Nonlinear System Identification and Timeseries Forecasting. There are case studies in the book that were not included in the documentation at the time of the update release. The book will always feature more in-depth studies and will be updated regularly with additional case studies.</li> </ul>"},{"location":"changelog/changelog/#documentation_2","title":"Documentation","text":"<ul> <li>All examples updated to reflect changes in v0.4.0.</li> <li>Added guide on defining a custom parameter estimation method and integrating it with SysIdentPy.</li> <li>Documentation moved to the <code>gh-pages</code> branch.</li> <li>Defined a GitHub Action to automatically build the docs when changes are pushed to the main branch.</li> <li>Removal of unused code in general</li> </ul>"},{"location":"changelog/changelog/#datasets_1","title":"Datasets","text":"<ul> <li>Datasets are now available in a separate repository.</li> </ul>"},{"location":"changelog/changelog/#api-changes_5","title":"API Changes","text":"<ul> <li>BREAKING CHANGE: Parameter estimation method must now be imported and passed to the model definition, replacing the previous string method. For example, use <code>from sysidentpy.parameter_estimation import LeastSquares</code> instead of <code>\"least_squares\"</code>. This change enhances code flexibility, organization, readability, and facilitates easier integration of custom methods. A specific doc page is available to guide migration from v0.3.4 to v0.4.0.</li> <li>BREAKING CHANGE: The <code>fit</code> method in MetaMSS now requires only <code>X</code> and <code>y</code> values, omitting the need to pass <code>fit(X=, y=, X_test=, y_test=)</code>.</li> <li>Added support for Python 3.12.</li> <li>Introduced <code>test_size</code> hyperparameter to set the proportion of training data used in the fitting process.</li> <li>Extensive code refactoring, including type hint improvements, docstring enhancements, removal of unused code, and other behind-the-scenes changes to support new features.</li> </ul>"},{"location":"changelog/changelog/#v034","title":"v0.3.4","text":""},{"location":"changelog/changelog/#contributors_6","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>dj-gauthier</li> <li>mtsousa</li> </ul>"},{"location":"changelog/changelog/#changes_6","title":"CHANGES","text":""},{"location":"changelog/changelog/#new-features_3","title":"New Features","text":"<ul> <li>MAJOR: Ridge Regression Parameter Estimation:</li> <li>Introducing Ridge algorithm for model parameter estimation (Issue #104). Set <code>estimator=\"ridge_regression\"</code> and control regularization with the <code>alpha</code> parameter. Special thanks to @dj-gauthier and @mtsousa for their contribution. Users are encouraged to visit https://www.researchgate.net/publication/380429918_Controlling_chaos_using_edge_computing_hardware to explore how @dj-gauthier used SysIdentPy in his research.</li> </ul>"},{"location":"changelog/changelog/#api-changes_6","title":"API Changes","text":"<ul> <li>Improved <code>plotting.py</code> code with type hints and new options for plotting results.</li> <li>Refactored methods to resolve future warnings from numpy.</li> <li>Code refactoring following PEP8 guidelines.</li> <li>Set \"default\" as the default style for plotting to avoid errors in new versions of matplotlib.</li> </ul>"},{"location":"changelog/changelog/#datasets_2","title":"Datasets","text":"<ul> <li>Added <code>buck_id.csv</code> and <code>buck_valid.csv</code> datasets to the SysIdentPy repository.</li> </ul>"},{"location":"changelog/changelog/#documentation_3","title":"Documentation","text":"<ul> <li>Add NFIR example (Issue #103). The notebook show how to build models without past output regressors (using only input regressors).</li> <li>Enhanced usage example for MetaMSS.</li> <li>Continued adding type hints to methods.</li> <li>Improved docstrings throughout the codebase.</li> <li>Minor additions and grammar fixes in documentation.</li> <li>@dj-gauthier provided valuable suggestions for enhancing the documentation, which are currently undergoing refinement and will soon be accessible.</li> </ul>"},{"location":"changelog/changelog/#development-tools","title":"Development Tools","text":"<ul> <li>Added pre-commit hooks to the repository.</li> <li>Enhanced <code>pyproject.toml</code> to assist contributors in setting up their own environment.</li> </ul>"},{"location":"changelog/changelog/#v033","title":"v0.3.3","text":""},{"location":"changelog/changelog/#contributors_7","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>GabrielBuenoLeandro</li> <li>samirmartins</li> </ul>"},{"location":"changelog/changelog/#changes_7","title":"CHANGES","text":"<ul> <li>The update v0.3.3  has been released with additional features, API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_7","title":"API Changes","text":"<ul> <li>MAJOR: Multiobjective Framework: Affine Information Least Squares Algorithm (AILS)<ul> <li>Now you can use AILS to estimate parameters of NARMAX models (and variants) using a multiobjective approach.</li> <li>AILS can be accessed using <code>from sysidentpy.multiobjective_parameter_estimation import AILS</code></li> <li>See the docs for a more in depth explanation of how to use AILS.</li> <li>This feature is related to Issue #101. This work is the result of an undergraduate research conducted by Gabriel Bueno Leandro under the supervision of Samir Milani Martins and Wilson Rocha Lacerda Junior.</li> <li>Several new methods were implemented to get the new feature and you can check all of it in sysidentpy -&gt; multiobjective_parameter_estimation.</li> </ul> </li> <li>API Change: <code>regressor_code</code> variable was renamed as <code>enconding</code> to avoid using the same name as the method in <code>narmax_tool</code> <code>regressor_code</code> method.</li> </ul>"},{"location":"changelog/changelog/#datasets_3","title":"Datasets","text":"<ul> <li>DATASET: Added buck_id.csv and buck_valid.csv dataset to SysIdentPy repository.</li> </ul>"},{"location":"changelog/changelog/#documentation_4","title":"Documentation","text":"<ul> <li>DOC: Add a Multiobjetive Parameter Optimization Notebook showing how to use the new AILS method</li> <li>DOC: Minor additions and grammar fixes.</li> </ul>"},{"location":"changelog/changelog/#v032","title":"v0.3.2","text":""},{"location":"changelog/changelog/#contributors_8","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_8","title":"CHANGES","text":"<ul> <li>The update v0.3.2  has been released with API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_8","title":"API Changes","text":"<ul> <li> <p>Major:</p> <ul> <li>Added Akaike Information Criteria corrected in FROLS. Now the user can use aicc as the information criteria to select the model order when using FROLS algorithm.</li> </ul> </li> <li> <p>FIX: Issue #114. Replace yhat with y in root relative squared error. Thanks @miroder</p> </li> <li>TESTS: Minor changes in tests by removing unnecessary data load.</li> <li>Remove unused code and comments.</li> </ul>"},{"location":"changelog/changelog/#documentation_5","title":"Documentation","text":"<ul> <li>Docs: Minor changes in notebooks. Added AICc method in the information criteria example.</li> </ul>"},{"location":"changelog/changelog/#v031","title":"v0.3.1","text":""},{"location":"changelog/changelog/#contributors_9","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_9","title":"CHANGES","text":"<ul> <li>The update v0.3.1  has been released with API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_9","title":"API Changes","text":"<ul> <li>MetaMSS was returning the max lag of the final model instead of the maximum lag related to the xlag and ylag. This is not wrong (it is related to the issue #55), but this change will be made for all methods at the same time. In this respect, I'm reverted this to return the maximum lag of the xlag and ylag.</li> <li>API Change: Added build_matrix method in BaseMSS. This change improved overall code readability by rewriting if/elif/else clauses in every model structure selection algorithm.</li> <li>API Change: Added bic, aic, fpe, and lilc methods in FROLS. Now the method is selected by using a predefined dictionary with the available options. This change improved overall code readability by rewriting if/elif/else clauses in the FROLS algorithm.</li> <li>TESTS: Added tests for Neural NARX class. The issue with pytorch was fixed and now we have the tests for every model class.</li> <li>Remove unused code and comments.</li> </ul>"},{"location":"changelog/changelog/#v030","title":"v0.3.0","text":""},{"location":"changelog/changelog/#contributors_10","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>gamcorn</li> <li>Gabo-Tor</li> </ul>"},{"location":"changelog/changelog/#changes_10","title":"CHANGES","text":"<ul> <li>The update v0.3.0  has been released with additional features, API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_10","title":"API Changes","text":"<ul> <li> <p>MAJOR: Estimators support in AOLS</p> <ul> <li>Now you can use any SysIdentPy estimator in AOLS model structure selection.</li> </ul> </li> <li> <p>Refactored base class for model structure selection. A refactored base class for model structure selection has been introduced in SysIdentPy. This update aims to enhance the system identification process by preparing the package for new features that are currently in development, like multiobjective parameter estimation, new basis functions and more.</p> </li> </ul> <p>Several methods within the base class have undergone significant restructuring to improve their functionality and optimize their performance. This reorganization will facilitate the incorporation of advanced model selection techniques in the future, which will enable users to obtain dynamic models with robust dynamic and static performance.   - Avoid unnecessary inheritance in every MSS method and improve the readability with better structured classes.   - Rewritten methods to avoid code duplication.   - Improve overall code readability by rewriting if/elif/else clauses.</p> <ul> <li> <p>Breaking Change: <code>X_train</code> and <code>y_train</code> were replaced respectively by <code>X</code> and <code>y</code> in <code>fit</code> method in MetaMSS model structure selection algorithm.  <code>X_test</code> and <code>y_test</code> were replaced by <code>X</code> and <code>y</code> in <code>predict</code> method in MetaMSS.</p> </li> <li> <p>API Change: Added BaseBasisFunction class, an abstract base class for implementing basis functions.</p> </li> <li>Enhancement: Added support for python 3.11.</li> <li>Future Deprecation Warning: The user will have to define the estimator and pass it to every model structure selection algorithm instead of using a string to define the Estimator. Currently the estimator is defined like \"estimator='least_squares'\". In version 0.4.0 the definition will be like \"estimator=LeastSquares()\"</li> <li>FIX: Issue #96. Fix issue with numpy 1.24.* version. Thanks for the contribution @gamcorn.</li> <li>FIX: Issue #91. Fix r2_score metric issue with 2 dimensional arrays.</li> <li>FIX: Issue #90.</li> <li>FIX: Issue #88 .Fix one step ahead prediction error in SimulateNARMAX class (thanks for pointing out, Lalith).</li> <li>FIX: Fix error in selecting the correct regressors in AOLS.</li> <li>Fix: Fix n step ahead prediction method not returning all values of the defined steps-ahead value when passing only the initial condition.</li> <li>FIX: Fix Visible Deprecation Warning raised in get_max_lag method.</li> <li>FIX: Fix deprecation warning in Extended Least Squares Example</li> </ul>"},{"location":"changelog/changelog/#datasets_4","title":"Datasets","text":"<ul> <li>DATASET: Added air passengers dataset to SysIdentPy repository.</li> <li>DATASET: Added San Francisco Hospital Load dataset to SysIdentPy repository.</li> <li>DATASET: Added San Francisco PV GHI dataset to SysIdentPy repository.</li> </ul>"},{"location":"changelog/changelog/#documentation_6","title":"Documentation","text":"<ul> <li>DOC: Improved documentation in Setting Specif Lags page. Now we bring an example of how to set specific lags for MISO models.</li> <li>DOC: Minor additions and grammar fixes.</li> <li>DOC: Improve image visualization using mkdocs-glightbox.</li> <li>Update dev packages versions</li> </ul>"},{"location":"changelog/changelog/#v021","title":"v0.2.1","text":""},{"location":"changelog/changelog/#contributors_11","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_11","title":"CHANGES","text":"<ul> <li>The update v0.2.1  has been released with additional feature, minor API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_11","title":"API Changes","text":"<ul> <li>MAJOR: Neural NARX now support CUDA<ul> <li>Now the user can build Neural NARX models with CUDA support. Just add <code>device='cuda'</code> to use the GPU benefits.</li> <li>Updated docs to show how to use the new feature.</li> </ul> </li> <li>Tests:<ul> <li>Now there are test for almost every function.</li> <li>Neural NARX tests are raising numpy issues. It'll be fixed til next update.</li> </ul> </li> <li>FIX: NFIR models in General Estimators<ul> <li>Fix support for NFIR models using sklearn estimators.</li> </ul> </li> <li>The setup is now handled by the pyproject.toml file.</li> <li>Remove unused code.</li> </ul>"},{"location":"changelog/changelog/#documentation_7","title":"Documentation","text":"<ul> <li> <p>MAJOR: New documentation website</p> <ul> <li>The documentation is now entirely based on Markdown (no rst anymore).</li> <li>We use MkDocs and Material for MkDocs theme now.</li> <li>Dark theme option.</li> <li>The Contribute page have more details to help those who wants to contribute with SysIdentPy.</li> <li>New sections (e.g., Blog, Sponsors, etc.)</li> <li>Many improvements under the hood.</li> </ul> </li> <li> <p>MAJOR: Github Sponsor</p> <ul> <li>Now you can support SysIdentPy by becoming a Sponsor! Details: https://github.com/sponsors/wilsonrljr</li> </ul> </li> <li> <p>Fix docstring variables.</p> </li> <li>Fix code format issues.</li> <li>Fix minor grammatical and spelling mistakes.</li> <li>Fix issues related to html on Jupyter notebooks examples on documentation.</li> <li>Updated Readme.</li> </ul>"},{"location":"changelog/changelog/#v020","title":"v0.2.0","text":""},{"location":"changelog/changelog/#contributors_12","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_12","title":"CHANGES","text":"<ul> <li>The update v0.2.0  has been released with additional feature, minor API changes and fixes.</li> </ul>"},{"location":"changelog/changelog/#api-changes_12","title":"API Changes","text":"<ul> <li> <p>MAJOR: Many new features for General Estimators</p> <ul> <li>Now the user can build General NARX models with Fourier basis function.</li> <li>The user can choose which basis they want by importing it from sysidentpy.basis_function. Check the notebooks with examples of how to use it.</li> <li>Now it is possible to build General NAR models. The user just need to pass model_type=\"NAR\" to build NAR models.</li> <li>Now it is possible to build General NFIR models. The user just need to pass model_type=\"NFIR\" to build NAR models.</li> <li>Now it is possible to run n-steps ahead prediction using General Estimators. Until now only infinity-steps ahead were allowed. Now the users can set any steps they want.</li> <li>Polynomial and Fourier are supported for now. New basis functions will be added in next releases.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>MAJOR: Many new features for NARX Neural Network</p> <ul> <li>Now the user can build Neural NARX models with Fourier basis function.</li> <li>The user can choose which basis they want by importing it from sysidentpy.basis_function. Check the notebooks with examples of how to use it.</li> <li>Now it is possible to build Neural NAR models. The user just need to pass model_type=\"NAR\" to build NAR models.</li> <li>Now it is possible to build Neural NFIR models. The user just need to pass model_type=\"NFIR\" to build NAR models.</li> <li>Now it is possible to run n-steps ahead prediction using Neural NARX. Until now only infinity-steps ahead were allowed. Now the users can set any steps they want.</li> <li>Polynomial and Fourier are supported for now. New basis functions will be added in next releases.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>many under the hood changes.</li> </ul> </li> <li> <p>Major: Support for old methods removed.</p> <ul> <li>Now the old sysidentpy.PolynomialNarmax is not available anymore. All the old features are included in the new API with a lot of new features and performance improvements.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.general_estimators.ModelPrediction</p> <ul> <li>ModelPrediction class was adapted to support General Estimators as a stand-alone class.</li> <li>predict: base method for prediction. Support infinity_steps ahead, one-step ahead and n-steps ahead prediction and any basis function.</li> <li>_one_step_ahead_prediction: Perform the 1-step-ahead prediction for any basis function.</li> <li>_n_step_ahead_prediction: Perform the n-step-ahead prediction for polynomial basis.</li> <li>_model_prediction: Perform the infinity-step-ahead prediction for polynomial basis.</li> <li>_narmax_predict: wrapper for NARMAX and NAR models.</li> <li>_nfir_predict: wrapper for NFIR models.</li> <li>_basis_function_predict: Perform the infinity-step-ahead prediction for basis functions other than polynomial.</li> <li>basis_function_n_step_prediction: Perform the n-step-ahead prediction for basis functions other than polynomial.</li> </ul> </li> <li> <p>API Change (new): sysidentpy.neural_network.ModelPrediction</p> <ul> <li>ModelPrediction class was adapted to support Neural NARX as a stand-alone class.</li> <li>predict: base method for prediction. Support infinity_steps ahead, one-step ahead and n-steps ahead prediction and any basis function.</li> <li>_one_step_ahead_prediction: Perform the 1-step-ahead prediction for any basis function.</li> <li>_n_step_ahead_prediction: Perform the n-step-ahead prediction for polynomial basis.</li> <li>_model_prediction: Perform the infinity-step-ahead prediction for polynomial basis.</li> <li>_narmax_predict: wrapper for NARMAX and NAR models.</li> <li>_nfir_predict: wrapper for NFIR models.</li> <li>_basis_function_predict: Perform the infinity-step-ahead prediction for basis functions other than polynomial.</li> <li>basis_function_n_step_prediction: Perform the n-step-ahead prediction for basis functions other than polynomial.</li> </ul> </li> <li> <p>API Change: Fit method for Neural NARX revamped.</p> <ul> <li>No need to convert the data to tensor before calling Fit method anymore.</li> </ul> </li> </ul> <p>API Change: Keyword and positional arguments     - Now users have to provide parameters with their names, as keyword arguments, instead of positional arguments. This is valid for every model class now.</p> <ul> <li>API Change (new): sysidentpy.utils.narmax_tools<ul> <li>New functions to help user getting useful information to build model. Now we have the regressor_code helper function to help to build neural NARX models.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#documentation_8","title":"Documentation","text":"<ul> <li>DOC: Improved Basic Steps notebook with new details about the prediction function.</li> <li>DOC: NARX Neural Network notebook was updated following the new api and showing new features.</li> <li>DOC: General Estimators notebook was updated following the new api and showing new features.</li> <li>DOC: Fixed minor grammatical and spelling mistakes, including Issues #77 and #78.</li> <li>DOC: Fix issues related to html on Jupyter notebooks examples on documentation.</li> </ul>"},{"location":"changelog/changelog/#v019","title":"v0.1.9","text":""},{"location":"changelog/changelog/#contributors_13","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>samirmartins</li> </ul>"},{"location":"changelog/changelog/#changes_13","title":"CHANGES","text":"<ul> <li>The update v0.1.9  has been released with additional feature, minor API changes and fixes of the new features added in v0.1.7.</li> </ul>"},{"location":"changelog/changelog/#api-changes_13","title":"API Changes","text":"<ul> <li>MAJOR: Entropic Regression Algorithm<ul> <li>Added the new class ER to build NARX models using the Entropic Regression algorithm.</li> <li>Only the Mutual Information KNN is implemented in this version and it may take too long to run on a high number of regressor, so the user should be careful regarding the number of candidates to put in the model.</li> </ul> </li> <li>API: save_load<ul> <li>Added a function to save and load models from file.</li> </ul> </li> <li>API: Added tests for python 3.9</li> <li>Fix : Change condition for n_info_values in FROLS. Now the value defined by the user is compared against X matrix shape instead of regressor space shape. This fix the Fourier basis function usage with more the 15 regressors in FROLS.</li> </ul>"},{"location":"changelog/changelog/#documentation_9","title":"Documentation","text":"<ul> <li>DOC: Save and Load models<ul> <li>Added a notebook showing how to use the save_load method.</li> </ul> </li> <li>DOC: Entropic Regression example<ul> <li>Added notebook with a simple example of how to use AOLS</li> </ul> </li> <li>DOC: Fourier Basis Function Example<ul> <li>Added notebook with a simple example of how to use Fourier Basis Function</li> </ul> </li> <li>DOC: PV forecasting benchmark<ul> <li>FIX AOLS prediction. The example was using the meta_mss model in prediction, so the results for AOLS were wrong.</li> </ul> </li> <li>DOC: Fixed minor grammatical and spelling mistakes.</li> <li>DOC: Fix issues related to html on Jupyter notebooks examples on documentation.</li> </ul>"},{"location":"changelog/changelog/#v018","title":"v0.1.8","text":""},{"location":"changelog/changelog/#contributors_14","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_14","title":"CHANGES","text":"<ul> <li>The update v0.1.8  has been released with additional feature, minor API changes and fixes of the new features added in v0.1.7.</li> </ul>"},{"location":"changelog/changelog/#api-changes_14","title":"API Changes","text":"<ul> <li>MAJOR: Ensemble Basis Functions<ul> <li>Now you can use different basis function together. For now we allow to use Fourier combined with Polynomial of different degrees.</li> </ul> </li> <li>API change: Add \"ensemble\" parameter in basis function to combine the features of different basis function.</li> <li>Fix: N-steps ahead prediction for model_type=\"NAR\" is working properly now with different forecast horizon.</li> </ul>"},{"location":"changelog/changelog/#documentation_10","title":"Documentation","text":"<ul> <li>DOC: Air passenger benchmark<ul> <li>Remove unused code.</li> <li>Use default hyperparameter in SysIdentPy models.</li> </ul> </li> <li>DOC: Load forecasting benchmark<ul> <li>Remove unused code.</li> <li>Use default hyperparameter in SysIdentPy models.</li> </ul> </li> <li>DOC: PV forecasting benchmark<ul> <li>Remove unused code.</li> <li>Use default hyperparameter in SysIdentPy models.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#v017","title":"v0.1.7","text":""},{"location":"changelog/changelog/#contributors_15","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_15","title":"CHANGES","text":"<ul> <li>The update v0.1.7  has been released with major changes and additional features. There are several API modifications, and you will need to change your code to have the new (and upcoming) features. All modifications are meant to make future expansion easier.</li> <li>On the user's side, the changes are not that disruptive, but in the background there are many changes that allowed the inclusion of new features and bug fixes that would be complex to solve without the changes. Check the <code>documentation page &lt;http://sysidentpy.org/notebooks.html&gt;</code>__</li> <li>Many classes were basically rebuild it from scratch, so I suggest to look at the new examples of how to use the new version.</li> <li>I will present the main updates below in order to highlight features and usability and then all API changes will be reported.</li> </ul>"},{"location":"changelog/changelog/#api-changes_15","title":"API Changes","text":"<ul> <li>MAJOR: NARX models with Fourier basis function <code>Issue63 &lt;https://github.com/wilsonrljr/sysidentpy/issues/63&gt;</code>, <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code><ul> <li>The user can choose which basis they want by importing it from sysidentpy.basis_function. Check the notebooks with examples of how to use it.</li> <li>Polynomial and Fourier are supported for now. New basis functions will be added in next releases.</li> </ul> </li> <li>MAJOR: NAR models <code>Issue58 &lt;https://github.com/wilsonrljr/sysidentpy/issues/58&gt;</code>__<ul> <li>It was already possible to build Polynomial NAR models, but with some hacks. Now the user just need to pass model_type=\"NAR\" to build NAR models.</li> <li>The user doesn't need to pass a vector of zeros as input anymore.</li> <li>Works for any model structure selection algorithm (FROLS, AOLS, MetaMSS)</li> </ul> </li> <li>Major: NFIR models <code>Issue59 &lt;https://github.com/wilsonrljr/sysidentpy/issues/59&gt;</code>__<ul> <li>NFIR models are models where the output depends only on past inputs. It was already possible to build Polynomial NFIR models, but with a lot of code on the user's side (much more than NAR, btw). Now the user just need to pass model_type=\"NFIR\" to build NFIR models.</li> <li>Works for any model structure selection algorithm (FROLS, AOLS, MetaMSS)</li> </ul> </li> <li>Major: Select the order for the residues lags to use in Extended Least Squares - elag<ul> <li>The user can select the maximum lag of the residues to be used in the Extended Least Squares algorithm. In previous versions sysidentpy used a predefined subset of residual lags.</li> <li>The degree of the lags follows the degree of the basis function</li> </ul> </li> <li>Major: Residual analysis methods <code>Issue60 &lt;https://github.com/wilsonrljr/sysidentpy/issues/60&gt;</code>__<ul> <li>There are now specific functions to calculate the autocorrelation of the residuals and cross-correlation for the analysis of the residuals. In previous versions the calculation was limited to just two inputs, for example, limiting user usability.</li> </ul> </li> <li>Major: Plotting methods <code>Issue61 &lt;https://github.com/wilsonrljr/sysidentpy/issues/61&gt;</code>__<ul> <li>The plotting functions are now separated from the models objects, so there are more flexibility regarding what to plot.</li> <li>Residual plots were separated from the forecast plot</li> </ul> </li> <li>API Change: sysidentpy.polynomial_basis.PolynomialNarmax is deprecated. Use sysidentpy.model_structure_selection.FROLS instead. <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/62&gt;</code>__<ul> <li>Now the user doesn't need to pass the number of inputs as a parameter.</li> <li>Added the elag parameter for unbiased_estimator. Now the user can define the number of lags of the residues for parameter estimation using the Extended Least Squares algorithm.</li> <li>model_type parameter: now the user can select the model type to be built. The options are \"NARMAX\", \"NAR\" and \"NFIR\". \"NARMAX\" is the default. If you want to build a NAR model without any \"hack\", just set model_type=\"NAR\". The same for \"NFIR\" models.</li> </ul> </li> <li>API Change: sysidentpy.polynomial_basis.MetaMSS is deprecated. Use sysidentpy.model_structure_selection.MetaMSS instead. <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__<ul> <li>Now the user doesn't need to pass the number of inputs as a parameter.</li> <li>Added the elag parameter for unbiased_estimator. Now the user can define the number of lags of the residues for parameter estimation using the Extended Least Squares algorithm.</li> </ul> </li> <li>API Change: sysidentpy.polynomial_basis.AOLS is deprecated. Use sysidentpy.model_structure_selection.AOLS instead. <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__</li> <li>API Change: sysidentpy.polynomial_basis.SimulatePolynomialNarmax is deprecated. Use sysidentpy.simulation.SimulateNARMAX instead.</li> <li>API Change: Introducing sysidentpy.basis_function. Because NARMAX models can be built on different basis function, a new module is added to make easier to implement new basis functions in future updates <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__.<ul> <li>Each basis function class must have a fit and predict method to be used in training and prediction respectively.</li> </ul> </li> <li>API Change: unbiased_estimator method moved to Estimators class.<ul> <li>added elag option</li> <li>change the build_information_matrix method to build_output_matrix</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base<ul> <li>This is the new base for building NARMAX models. The classes have been rewritten to make it easier to expand functionality.</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base.GenerateRegressors<ul> <li>create_narmax_code: Creates the base coding that allows representation for the NARMAX, NAR, and NFIR models.</li> <li>regressor_space: Creates the encoding representation for the NARMAX, NAR, and NFIR models.</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base.ModelInformation<ul> <li>_get_index_from_regressor_code: Get the index of the model code representation in regressor space.</li> <li>_list_output_regressor_code: Create a flattened array of output regressors.</li> <li>_list_input_regressor_code: Create a flattened array of input regressors.</li> <li>_get_lag_from_regressor_code: Get the maximum lag from array of regressors.</li> <li>_get_max_lag_from_model_code: the name says it all.</li> <li>_get_max_lag: Get the maximum lag from ylag and xlag.</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base.InformationMatrix<ul> <li>_create_lagged_X: Create a lagged matrix of inputs without combinations.</li> <li>_create_lagged_y: Create a lagged matrix of the output without combinations.</li> <li>build_output_matrix: Build the information matrix of output values.</li> <li>build_input_matrix: Build the information matrix of input values.</li> <li>build_input_output_matrix: Build the information matrix of input and output values.</li> </ul> </li> <li>API Change (new): sysidentpy.narmax_base.ModelPrediction<ul> <li>predict: base method for prediction. Support infinity_steps ahead, one-step ahead and n-steps ahead prediction and any basis function.</li> <li>_one_step_ahead_prediction: Perform the 1-step-ahead prediction for any basis function.</li> <li>_n_step_ahead_prediction: Perform the n-step-ahead prediction for polynomial basis.</li> <li>_model_prediction: Perform the infinity-step-ahead prediction for polynomial basis.</li> <li>_narmax_predict: wrapper for NARMAX and NAR models.</li> <li>_nfir_predict: wrapper for NFIR models.</li> <li>_basis_function_predict: Perform the infinity-step-ahead prediction for basis functions other than polynomial.</li> <li>basis_function_n_step_prediction: Perform the n-step-ahead prediction for basis functions other than polynomial.</li> </ul> </li> <li>API Change (new): sysidentpy.model_structure_selection.FROLS <code>Issue62 &lt;https://github.com/wilsonrljr/sysidentpy/issues/62&gt;</code>, <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code><ul> <li>Based on the old sysidentpy.polynomial_basis.PolynomialNARMAX. The class has been rebuilt with new functions and optimized code.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>Add support for new basis functions.</li> <li>The user can choose the residual lags.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>many under the hood changes.</li> </ul> </li> <li>API Change (new): sysidentpy.model_structure_selection.MetaMSS <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__<ul> <li>Based on the old sysidentpy.polynomial_basis.MetaMSS. The class has been rebuilt with new functions and optimized code.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>The user can choose the residual lags.</li> <li>Extended Least Squares support.</li> <li>Add support for new basis functions.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>many under the hood changes.</li> </ul> </li> <li>API Change (new): sysidentpy.model_structure_selection.AOLS <code>Issue64 &lt;https://github.com/wilsonrljr/sysidentpy/issues/64&gt;</code>__<ul> <li>Based on the old sysidentpy.polynomial_basis.AOLS. The class has been rebuilt with new functions and optimized code.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>Add support for new basis functions.</li> <li>No need to pass the number of inputs anymore.</li> <li>Improved docstring.</li> <li>Change \"l\" parameter to \"L\".</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>many under the hood changes.</li> </ul> </li> <li>API Change (new): sysidentpy.simulation.SimulateNARMAX<ul> <li>Based on the old sysidentpy.polynomial_basis.SimulatePolynomialNarmax. The class has been rebuilt with new functions and optimized code.</li> <li>Fix the Extended Least Squares support.</li> <li>Fix n-steps ahead prediction and 1-step ahead prediction.</li> <li>Enforcing keyword-only arguments. This is an effort to promote clear and non-ambiguous use of the library.</li> <li>The user can choose the residual lags.</li> <li>Improved docstring.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>New prediction method.</li> <li>Do not inherit from the structure selection algorithm anymore, only from narmax_base. Avoid circular import and other issues.</li> <li>many under the hood changes.</li> </ul> </li> <li>API Change (new): sysidentpy.residues<ul> <li>compute_residues_autocorrelation: the name says it all.</li> <li>calculate_residues: get the residues from y and yhat.</li> <li>get_unnormalized_e_acf: compute the unnormalized autocorrelation of the residues.</li> <li>compute_cross_correlation: compute cross correlation between two signals.</li> <li>_input_ccf</li> <li>_normalized_correlation: compute the normalized correlation between two signals.</li> </ul> </li> <li>API Change (new): sysidentpy.utils.plotting<ul> <li>plot_results: plot the forecast</li> <li>plot_residues_correlation: the name says it all.</li> </ul> </li> <li>API Change (new): sysidentpy.utils.display_results<ul> <li>results: return the model regressors, estimated parameter and ERR index of the fitted model in a table.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#documentation_11","title":"Documentation","text":"<ul> <li>DOC: Air passenger benchmark <code>Issue65 &lt;https://github.com/wilsonrljr/sysidentpy/issues/65&gt;</code>__<ul> <li>Added notebook with Air passenger forecasting benchmark.</li> <li>We compare SysIdentPy against prophet, neuralprophet, autoarima, tbats and many more.</li> </ul> </li> <li>DOC: Load forecasting benchmark <code>Issue65 &lt;https://github.com/wilsonrljr/sysidentpy/issues/65&gt;</code>__<ul> <li>Added notebook with load forecasting benchmark.</li> </ul> </li> <li>DOC: PV forecasting benchmark <code>Issue65 &lt;https://github.com/wilsonrljr/sysidentpy/issues/65&gt;</code>__<ul> <li>Added notebook with PV forecasting benchmark.</li> </ul> </li> <li>DOC: Presenting main functionality<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Multiple Inputs usage<ul> <li>Example rewritten following the new api</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Information Criteria - Examples<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Important notes and examples of how to use Extended Least Squares<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Setting specific lags<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Parameter Estimation<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Using the Meta-Model Structure Selection (MetaMSS) algorithm for building Polynomial NARX models<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Using the Accelerated Orthogonal Least-Squares algorithm for building Polynomial NARX models<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Example: F-16 Ground Vibration Test benchmark<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Building NARX Neural Network using Sysidentpy<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Building NARX models using general estimators<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Simulate a Predefined Model<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: System Identification Using Adaptive Filters<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Identification of an electromechanical system<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Example: N-steps-ahead prediction - F-16 Ground Vibration Test benchmark<ul> <li>Example rewritten following the new api.</li> <li>Fixed minor grammatical and spelling mistakes.</li> </ul> </li> <li>DOC: Introduction to NARMAX models<ul> <li>Fixed grammatical and spelling mistakes.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#v016","title":"v0.1.6","text":""},{"location":"changelog/changelog/#contributors_16","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_16","title":"CHANGES","text":""},{"location":"changelog/changelog/#api-changes_16","title":"API Changes","text":"<ul> <li>MAJOR: Meta-Model Structure Selection Algorithm (Meta-MSS).<ul> <li>A new method for build NARMAX models based on metaheuristics. The algorithm uses a Binary hybrid Particle Swarm Optimization and Gravitational Search Algorithm with a new cost function to build parsimonious models.</li> <li>New class for the BPSOGSA algorithm. New algorithms can be adapted in the Meta-MSS framework.</li> <li>Future updates will add NARX models for classification and multiobjective model structure selection.</li> </ul> </li> <li>MAJOR: Accelerated Orthogonal Least-Squares algorithm.<ul> <li>Added the new class AOLS to build NARX models using the Accelerated Orthogonal Least-Squares algorithm.</li> <li>At the best of my knowledge, this is the first time this algorithm is used in the NARMAX framework. The tests I've made are promising, but use it with caution until the results are formalized into a research paper.</li> </ul> </li> </ul>"},{"location":"changelog/changelog/#documentation_12","title":"Documentation","text":"<ul> <li>Added notebook with a simple example of how to use MetaMSS and a simple model comparison of the Electromechanical system.</li> <li>Added notebook with a simple example of how to use AOLS</li> <li>Added ModelInformation class. This class have methods to return model information such as max_lag of a model code.<ul> <li>added _list_output_regressor_code</li> <li>added _list_input_regressor_code</li> <li>added _get_lag_from_regressor_code</li> <li>added _get_max_lag_from_model_code</li> </ul> </li> <li>Minor performance improvement: added the argument \"predefined_regressors\" in build_information_matrix function on base.py to improve the performance of the Simulation method.</li> <li>Pytorch is now an optional dependency. Use pip install sysidentpy['full']</li> <li>Fix code format issues.</li> <li>Fixed minor grammatical and spelling mistakes.</li> <li>Fix issues related to html on Jupyter notebooks examples on documentation.</li> <li>Updated Readme with examples of how to use.</li> <li>Improved descriptions and comments in methods.</li> <li>metaheuristics.bpsogsa (detailed description on code docstring)<ul> <li>added evaluate_objective_function</li> <li>added optimize</li> <li>added generate_random_population</li> <li>added mass_calculation</li> <li>added calculate_gravitational_constant</li> <li>added calculate_acceleration</li> <li>added update_velocity_position</li> </ul> </li> <li>FIX issue #52</li> </ul>"},{"location":"changelog/changelog/#v015","title":"v0.1.5","text":""},{"location":"changelog/changelog/#contributors_17","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_17","title":"CHANGES","text":""},{"location":"changelog/changelog/#api-changes_17","title":"API Changes","text":"<ul> <li>MAJOR: n-steps-ahead prediction.<ul> <li>Now you can define the numbers of steps ahead in the predict function.</li> <li>Only for Polynomial models for now. Next update will bring this functionality to Neural NARX and General Estimators.</li> </ul> </li> <li>MAJOR: Simulating predefined models.<ul> <li>Added the new class SimulatePolynomialNarmax to handle the simulation of known model structures.</li> <li>Now you can simulate predefined models by just passing the model structure codification. Check the notebook examples.</li> </ul> </li> <li>Fix code format issues.</li> <li>Added new tests for SimulatePolynomialNarmax and generate_data.</li> <li>Started changes related to numpy 1.19.4 update. There are still some Deprecation warnings that will be fixed in next update.</li> </ul>"},{"location":"changelog/changelog/#documentation_13","title":"Documentation","text":"<ul> <li>Added 4 new notebooks in the example section.</li> <li>Added iterative notebooks. Now you can run the notebooks in Jupyter notebook section of the documentation in Colab.</li> <li>Fix issues related to html on Jupyter notebooks examples on documentation.</li> <li>Updated Readme with examples of how to use.</li> </ul>"},{"location":"changelog/changelog/#v014","title":"v0.1.4","text":""},{"location":"changelog/changelog/#contributors_18","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> </ul>"},{"location":"changelog/changelog/#changes_18","title":"CHANGES","text":""},{"location":"changelog/changelog/#api-changes_18","title":"API Changes","text":"<ul> <li>MAJOR: Introducing NARX Neural Network in SysIdentPy.<ul> <li>Now you can build NARX Neural Network on SysIdentPy.</li> <li>This feature is built on top of Pytorch. See the docs for more details and examples of how to use.</li> </ul> </li> <li>MAJOR: Introducing general estimators in SysIdentPy.<ul> <li>Now you are able to use any estimator that have Fit/Predict methods (estimators from Sklearn and Catboost, for example) and build NARX models based on those estimators.</li> <li>We use the core functions of SysIdentPy and keep the Fit/Predict approach from those estimators to keep the process easy to use.</li> <li>More estimators are coming soon like XGboost.</li> </ul> </li> <li>Changed the default parameters of the plot_results function.</li> </ul>"},{"location":"changelog/changelog/#documentation_14","title":"Documentation","text":"<ul> <li>Added notebooks to show how to build NARX neural Network.</li> <li>Added notebooks to show how to build NARX models using general estimators.</li> <li>New template for the documentation site.</li> <li>Fix issues related to html on Jupyter notebooks examples on documentation.</li> <li> <p>Updated Readme with examples of how to use.</p> </li> <li> <p>NOTE: We will keep improving the Polynomial NARX models (new model structure selection algorithms and multiobjective identification is on our roadmap). These recent modifications will allow us to introduce new NARX models like PWARX models very soon.</p> </li> </ul>"},{"location":"changelog/changelog/#v013","title":"v0.1.3","text":""},{"location":"changelog/changelog/#contributors_19","title":"CONTRIBUTORS","text":"<ul> <li>wilsonrljr</li> <li>renard162</li> </ul>"},{"location":"changelog/changelog/#changes_19","title":"CHANGES","text":""},{"location":"changelog/changelog/#api-changes_19","title":"API Changes","text":"<ul> <li>Fixed a bug concerning the xlag and ylag in multiple input scenarios.</li> <li>Refactored predict function. Improved performance up to 87% depending on the number of regressors.</li> <li>You can set lags with different size for each input.</li> <li>Added a new function to get the max value of xlag and ylag. Work with int, list, nested lists.</li> <li>Fixed tests for information criteria.</li> <li>Refactored code of all classes following PEP 8 guidelines to improve readability.</li> <li>Changes on information Criteria tests.</li> <li>Added workflow to run the tests when merge branch into master.</li> </ul>"},{"location":"changelog/changelog/#documentation_15","title":"Documentation","text":"<ul> <li>Added SysIdentPy logo.</li> <li>Added Citation information on Readme.</li> <li>Added new site domain.</li> <li>Updated docs.</li> </ul>"},{"location":"community-support/community-overview/","title":"Contribute","text":"\ud83d\udcac Get Help <p>Need assistance? Explore various options for getting help, including discussion forums, GitHub issues, and community support channels.</p> \ud83e\udd1d Meetups <p>Connect with other SysIdentPy users through meetups, online events, and community discussions. Stay updated on upcoming gatherings and share your experience.</p>"},{"location":"community-support/get-help/","title":"Get Help","text":"<p>Before asking others for help, it\u2019s generally a good idea for you to try to help yourself. SysIdentPy includes several examples in the documentation with tips and notes about the package that might help you. However, if you have any issues and you can't find the answer, reach out using any method described below.</p>"},{"location":"community-support/get-help/#connect-with-the-author","title":"Connect with the author","text":"<p>You can:</p> <ul> <li>Follow me on GitHub.</li> <li>Follow me on <li>Connect with me on Linkedin.<ul> <li>I'll start to use Twitter more often \ud83e\udd37\u200d\u2642 (probably).</li> </ul> </li> <li>Read what I write (or follow me) on Medium.</li>"},{"location":"community-support/get-help/#create-issues","title":"Create issues","text":"<p>You can create a new issue in the GitHub repository, for example to:</p> <ul> <li>Ask a question or ask about a problem.</li> <li>Suggest a new feature.</li> </ul>"},{"location":"community-support/get-help/#join-the-chat","title":"Join the chat","text":"<p>Join the \ud83d\udc65 Discord chat server \ud83d\udc65 and hang out with others in the SysIdentPy community.</p> <p>You can use the chat for anything</p> <p>Have in mind that you can use the chat to talk about anything related to SysIdentPy. Conversations about system identification, dynamical systems, new papers, issues, new features are allowed, but have in mind that if some of the questions could help other users, I'll kindly ask you to open an discussion or an issue on Github as well.</p> <p>I can make sure I always answer everything, even if it takes some time.</p>"},{"location":"community-support/meetups/ai-networks-meetup/","title":"AI Networks Meetup","text":""},{"location":"community-support/meetups/ai-networks-meetup/#about-ai-networks","title":"About AI Networks","text":"<p>Founded on August 5, 2019, AI Networks has quickly established itself as one of the most engaged and focused communities in the realm of artificial intelligence. Our community thrives on a shared passion for knowledge and a commitment to exploring the latest advancements in AI and machine learning.</p> <p>As we celebrate our 5<sup>th</sup> anniversary in August 2024, we reflect on five years of enriching knowledge exchange and vibrant discussions.</p> <p>Our community currently hosts four specialized groups:</p> <ul> <li>AI/ML Brasil - Principal 01 &amp; 02: Our main groups dedicated to a wide range of AI and machine learning topics.</li> <li>AI/ML Brasil - Prompt Engineering: Focused on the art and science of designing effective prompts for AI systems.</li> <li>AI/ML Brasil - Neuroci\u00eancia: Exploring the intersection of artificial intelligence and neuroscience.</li> <li>AI/ML Brasil - Divulga\u00e7\u00e3o Servi\u00e7os IA: Dedicated to the dissemination and promotion of AI services.</li> </ul> <p>Join us to connect with like-minded individuals, engage in thought-provoking discussions, and stay at the forefront of AI innovation.</p> <p>In Portuguese</p> <p>Talk: Modelando Sistemas Din\u00e2micos | AI Networks Meetup</p> <p>Just click in the link above to watch the video.</p>"},{"location":"community-support/meetups/estatidados/","title":"Estatidados Meetup","text":"<p>Estatidados is a big statistic and data science community in Brazil. They host an online meetup that brings together leading researchers and developers from the Statistics and Data science community to join a multiple set of talks covering current trends in the Data Science field.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, gave a meetup talk about SysIdentPy and the video is available bellow:</p> <p>In Portuguese</p> <p>Talk: SysIdentPy: Uma Biblioteca Para Cria\u00e7\u00e3o de Modelos N\u00e3o Lineares para S\u00e9ries Temporais</p> <p>Just click in the link above to watch the video.</p>"},{"location":"community-support/meetups/gcom-meetup/","title":"GCoM Meetup","text":"<p>GCoM (Control and Modelling Group) is a research group of the Department of Electrical Engineering Federal University of S\u00e3o Jo\u00e3o del-Rei. GCoM works on two main areas: Analysis and Modelling Systems and Control Systems. They are devoted to undertake knowledge from Electrical Engineering, Mathematics, Computer Science and Physics to build and analyze models that mimics and control real dynamical systems. Some of applications are on robust control of uncertainty systems, Assistive Technology, Hysteresis system identification, Chaotic Dynamics. Recently, a great effort has been undertaken to better understand the play that numerical computation plays at modelling and control of nonlinear dynamical systems.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, gave a meetup talk about SysIdentPy and the video is available bellow:</p> <p>In Portuguese</p> <p>Talk: Aplica\u00e7\u00f5es em Data Science e Identifica\u00e7\u00e3o de Sistemas com o SysIdentPy</p> <p>Just click in the link above to watch the video.</p>"},{"location":"community-support/meetups/nubank-meetup-open-source/","title":"Nubank Meetup","text":"<p>Nubank is a leading financial technology company in Latin America with more than 54 million clients in Brazil, Mexico and Colombia. They host an online meetup that brings together leading researchers and developers from the Machine Learning (ML) community to join a multiple set of talks covering current trends in ML development.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, joined Bruno Rocha (software engineer at Red Hat) and Tatyana Zabanova (Data Scientist at Nubank )in a talk about the experience of making a open source package. The video is available bellow:</p> <p>In Portuguese</p> <p>Talk: Painel sobre desenvolvimento de bibliotecas em Data Science | Nubank DS &amp; ML Meetup</p> <p>Just click in the link above to watch the video.</p>"},{"location":"community-support/meetups/nubank-meetup/","title":"Nubank Meetup","text":"<p>Nubank is a leading financial technology company in Latin America with more than 54 million clients in Brazil, Mexico and Colombia. They host an online meetup that brings together leading researchers and developers from the Machine Learning (ML) community to join a multiple set of talks covering current trends in ML development.</p> <p>Wilson Rocha, the maintainer of SysIdentPy, gave a meetup talk about SysIdentPy and the video is available bellow:</p> <p>In Portuguese</p> <p>Talk: SysIdentPy: Uma Biblioteca Para Cria\u00e7\u00e3o de Modelos N\u00e3o Lineares para S\u00e9ries Temporais</p> <p>Just click in the link above to watch the video.</p>"},{"location":"developer-guide/contribute/","title":"Contributing","text":"<p>SysIdentPy is intended to be a community project, hence all contributions are welcome! There exist many possible use cases in System Identification field and we can not test all scenarios without your help! If you find any bugs or have suggestions, please report them on issue tracker on GitHub.</p> <p>We welcome new contributors of all experience levels. The SysIdentPy community goals are to be helpful, welcoming, and effective.</p>"},{"location":"developer-guide/contribute/#help-others-with-issues-in-github","title":"Help others with issues in GitHub","text":"<p>You can see existing issues and try and help others, most of the times they are questions that you might already know the answer for.</p>"},{"location":"developer-guide/contribute/#watch-the-github-repository","title":"Watch the GitHub repository","text":"<p>You can watch SysIdentPy in GitHub (clicking the \"watch\" button at the top right):</p> <p>If you select \"Watching\" instead of \"Releases only\" you will receive notifications when someone creates a new issue.</p> <p>Then you can try and help them solve those issues.</p>"},{"location":"developer-guide/contribute/#documentation","title":"Documentation","text":"<p>Documentation is as important as the library itself. English is not the primary language of the main authors, so if you find any typo or anything wrong do not hesitate to point out to us.</p>"},{"location":"developer-guide/contribute/#create-a-pull-request","title":"Create a Pull Request","text":"<p>You can contribute to the source code with Pull Requests, for example:</p> <ul> <li>To fix a typo you found on the documentation.</li> <li>To share an article, video, or podcast you created or found about SysIdentPy.</li> <li>To propose new documentation sections.</li> <li>To fix an existing issue/bug.</li> <li>To add a new feature.</li> </ul>"},{"location":"developer-guide/contribute/#development-environment","title":"Development environment","text":"<p>These are some basic steps to help us with code:</p> <ul> <li> Install and Setup Git on your computer.</li> <li> Fork SysIdentPy.</li> <li> Clone the fork on your local machine.</li> <li> Create a new branch.</li> <li> Make changes following the coding style of the project (or suggesting improvements).</li> <li> Run the tests.</li> <li> Write and/or adapt existing test if needed.</li> <li> Add documentation if needed.</li> <li> Commit.</li> <li> Push to your fork.</li> <li> Open a pull_request.</li> </ul>"},{"location":"developer-guide/contribute/#environment","title":"Environment","text":"<p>Clone the repository using</p> <pre><code>git clone https://github.com/wilsonrljr/sysidentpy.git\n</code></pre> <p>If you already cloned the repository and you know that you need to deep dive in the code, here are some guidelines to set up your environment.</p>"},{"location":"developer-guide/contribute/#virtual-environment-with-venv","title":"Virtual environment with <code>venv</code>","text":"<p>You can create a virtual environment in a directory using Python's <code>venv</code> module or Conda:</p> venvconda <pre><code>$ python -m venv env\n</code></pre> <pre><code>conda create -n env\n</code></pre> <p>That will create a directory <code>./env/</code> with the Python binaries and then you will be able to install packages for that isolated environment.</p>"},{"location":"developer-guide/contribute/#activate-the-environment","title":"Activate the environment","text":"<p>If you created the environment using Python's <code>venv</code> module, activate it with:</p> Linux, macOSWindows PowerShellWindows Bash <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\Activate.ps1\n</code></pre> <p>Or if you use Bash for Windows (e.g. Git Bash):</p> <pre><code>source ./env/Scripts/activate\n</code></pre> <p>If you created the environment using Conda, activate it with:</p> Conda Bash <p>Or if you use Bash for Windows (e.g. Git Bash):</p> <pre><code>conda activate env\n</code></pre> <p>To check it worked, use:</p> Linux, macOS, Windows BashWindows PowerShellWindows Bash <pre><code>$ which pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <pre><code>$ Get-Command pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <pre><code>$ where pip\n\nsome/directory/sysidentpy/env/Scripts/pip\n</code></pre> <p>If it shows the <code>pip</code> binary at <code>env/bin/pip</code> then it worked.</p> <p>Tip</p> <p>Every time you install a new package with <code>pip</code> under that environment, activate the environment again.</p> <p>Note</p> <p>We use the <code>pytest</code> package for testing. The test functions are located in tests subdirectories at each folder inside SysIdentPy, which check the validity of the algorithms.</p>"},{"location":"developer-guide/contribute/#dependencies","title":"Dependencies","text":"<p>Install SysIdentPy with the <code>dev</code> and <code>docs</code> option to get all the necessary dependencies to run the tests</p> Dev and Docs dependencies <pre><code>pip install \"sysidentpy[dev, docs]\"\n</code></pre>"},{"location":"developer-guide/contribute/#docs","title":"Docs","text":"<p>First, make sure you set up your environment as described above, that will install all the requirements.</p> <p>The documentation uses MkDocs and Material for MKDocs.</p> <p>All the documentation is in Markdown format in the directory <code>./docs/</code>.</p>"},{"location":"developer-guide/contribute/#check-the-changes","title":"Check the changes","text":"<p>During local development, you can serve the website locally and checks for any changes. This helps making sure that:</p> <ul> <li>All of your modifications were applied.</li> <li>The unmodified files are displaying as expected.</li> </ul> <pre><code>$ mkdocs serve\n\nINFO     -  [13:25:00] Browser connected: http://127.0.0.1:8000\n</code></pre> <p>It will serve the documentation on <code>http://127.0.0.1:8008</code>.</p> <p>That way, you can keep editing the source files and see the changes live.</p> <p>Warning</p> <p>If any modification break the build, you have to serve the website again. Always check your <code>console</code> to make sure you are serving the website.</p>"},{"location":"developer-guide/contribute/#run-tests-locally","title":"Run tests locally","text":"<p>Its always good to check if your implementations/modifications does not break any other part of the package. You can run the SysIdentPy tests locally using <code>pytest</code> in the respective folder to perform all the tests of the corresponding sub-packages.</p>"},{"location":"developer-guide/contribute/#example-of-how-to-run-the-tests","title":"Example of how to run the tests:","text":"<p>Open a terminal emulator of your choice and go to the main directory, e.g,</p> <pre><code>\\sysidentpy\\\n</code></pre> <p>Just type <code>pytest</code> in the terminal emulator</p> <pre><code>pytest\n</code></pre> <p>and you get a result like:</p> <pre><code>========== test session starts ==========\n\nplatform linux -- Python 3.7.6, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\n\nrootdir: ~/sysidentpy\n\nplugins: cov-2.8.1\n\ncollected 12 items\n\ntests/test_regression.py ............ [100%]\n\n========== 12 passed in 2.45s ==================\n</code></pre>"},{"location":"developer-guide/documentation-overview/","title":"SysIdentPy Documentation Restructuring Proposal","text":"<p>This document outlines a reorganization of SysIdentPy\u2019s documentation to improve discoverability, reduce friction for beginners, and align with modern documentation standards. The structure will follow four key categories: Tutorials, How-Tos, Explanations, and Reference Guide, with additional sections for contributors and real-world examples.</p> <p>Acknowledgments: This documentation restructuring draws inspiration from NumPy\u2019s NEP 44, adapting its principles of clarity and logical organization to SysIdentPy\u2019s domain-specific needs in system identification and time series forecasting, while emphasizing tutorials and reproducibility.</p>"},{"location":"developer-guide/documentation-overview/#motivation-and-scope","title":"Motivation and Scope","text":"<p>SysIdentPy\u2019s current documentation (like many scientific Python packages) mixes conceptual explanations, code examples, and API references, which can overwhelm new users. By adopting a user-centric structure inspired by Di\u00e1taxis, we aim to:</p> <ul> <li>Separate learning paths for beginners (Tutorials) and practitioners (How-Tos).</li> <li>Improve material for conceptual understanding (Explanations).</li> <li>Maintain a clean, searchable Reference Guide.</li> <li>Highlight SysIdentPy\u2019s features.</li> </ul> <p>A well-organized documentation structure can significantly improve the experience of our community by providing specific resources for different user groups:</p> <ul> <li> <p>For Beginners: A clear, guided pathway with tutorials and step-by-step instructions helps new users overcome the learning curve.</p> </li> <li> <p>For Researchers: Features such as custom basis functions and model configurations can be easily discovered and understood. With clearly defined sections, researchers can quickly locate the information they need to experiment with new methods.</p> </li> <li> <p>For Industry/Corporate Users: Benchmarking guides and model comparison examples are readily accessible, making it easier for industry and corporate professionals to evaluate and choose the right tools for their specific needs.</p> </li> </ul> <p>The goal is to structure the documentation to meet the specific needs of these diverse user groups, making the learning process faster and more efficient for everyone in the community.</p>"},{"location":"developer-guide/documentation-overview/#proposed-structure","title":"Proposed Structure","text":"<p>Here\u2019s an overview of the main sections in the documentation, outlining the purpose and proposed content for each:</p> <ul> <li>Getting Started</li> <li>User Guide</li> <li>Developer Guide</li> <li>Community &amp; Support</li> <li>About</li> </ul>"},{"location":"developer-guide/documentation-overview/#user-guide","title":"User Guide","text":"<p>The User Guide section is designed to provide a comprehensive understanding of SysIdentPy, covering essential concepts, practical examples, and advanced features. The proposed structure includes:</p>"},{"location":"developer-guide/documentation-overview/#tutorials","title":"Tutorials","text":"<p>Audience: New users with minimal system identification experience.</p> <p>Suggested Content:</p> <ul> <li> Absolute Beginner\u2019s Guide   Start from scratch with easy-to-follow guides designed for those new to SysIdentPy and NARMAX models.</li> <li> Domain-Specific Tutorials   Examples and use cases for fields like engineering, healthcare, finance and so on.</li> </ul> <p>Format: Jupyter Notebooks with narrative explanations and code.</p>"},{"location":"developer-guide/documentation-overview/#how-tos","title":"How-Tos","text":"<p>Audience: Practitioners solving specific problems.</p> <p>Suggested Content:</p> <ul> <li> Model Optimization</li> <li> Advanced Customizations</li> <li> Error Analysis</li> <li> Reproducibility</li> </ul> <p>Format: Short, task-focused markdown files with code snippets.</p>"},{"location":"developer-guide/documentation-overview/#explanations","title":"Explanations","text":"<p>Audience: Users seeking rigorous mathematical foundations.</p> <ul> <li> Companion Book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy. Offer theoretical context for SysIdentPy\u2019s algorithms through a companion book.</li> </ul>"},{"location":"developer-guide/documentation-overview/#reference-guide","title":"Reference Guide","text":"<p>Audience: Advanced users needing API details.</p> <ul> <li> API Reference   Access the complete SysIdentPy source code with well-documented modules and methods.</li> </ul> <p>Format: Auto-generated API docs with cross-linked \"See Also\" sections.</p>"},{"location":"developer-guide/documentation-overview/#developer-guide","title":"Developer Guide","text":"<p>The Developer Guide section aims to provide clear information on the internal structure of SysIdentPy, focusing on implementation details, code examples, and options for customization. The proposed structure includes:</p>"},{"location":"developer-guide/documentation-overview/#how-to-contribute","title":"How To Contribute","text":"<p>Audience: Maintainers and open-source contributors.</p> <ul> <li> Contributor Guide</li> </ul>"},{"location":"developer-guide/documentation-overview/#documentation-guide","title":"Documentation Guide","text":"<p>Audience: Maintainers and open-source contributors.</p> <ul> <li> Writing a tutorial</li> <li> Creating a how-to guide</li> <li> Creating content for the book</li> </ul>"},{"location":"developer-guide/documentation-overview/#community-support","title":"Community &amp; Support","text":"<p>Audience: Individuals of all experience levels, from beginners to experts, with an interest in Python and SysIdentPy.</p> <ul> <li> Get Help</li> <li> Workshops</li> <li> Reading Suggestions</li> <li> Community Discussions</li> </ul>"},{"location":"developer-guide/how-to-add-a-translation/","title":"How to Add a Translation","text":"<p>This page explains how to add or improve a translation of the docs for any language.</p> <p>The site uses MkDocs + Material + <code>mkdocs-static-i18n</code>. English is the reference (fallback). Any other language mirrors its folder structure. If a page is missing in a target language, the English version is shown automatically.</p>"},{"location":"developer-guide/how-to-add-a-translation/#1-overview","title":"1. Overview","text":"<p>There are three typical scenarios:</p> <ol> <li>Improve an existing translated page.</li> <li>Translate an English page that currently has no localized version.</li> <li>Add an entirely new language to the project.</li> </ol> <p>Everything below covers all three.</p>"},{"location":"developer-guide/how-to-add-a-translation/#2-folder-layout","title":"2. Folder layout","text":"<pre><code>/docs\n  en/\n    developer-guide/\n    getting-started/\n    user-guide/\n  &lt;locale&gt;/\n    (mirrors the structure you translate)\n</code></pre> <p>Examples of <code>&lt;locale&gt;</code>: <code>pt</code>, <code>es</code>, <code>fr</code>, <code>de</code>, <code>it</code>, <code>ja</code>, <code>zh</code>, <code>ru</code>. Use short IETF / BCP\u201147 language tags (no country code unless needed, e.g. <code>pt</code> vs <code>pt-BR</code> only if a regional variant is required). Keep it consistent with <code>mkdocs.yml</code>.</p> <p>Relative paths must match. Example:</p> <pre><code>English: docs/en/developer-guide/how-to-add-a-translation.md\nSpanish: docs/es/developer-guide/how-to-add-a-translation.md\nFrench:  docs/fr/developer-guide/how-to-add-a-translation.md\n</code></pre>"},{"location":"developer-guide/how-to-add-a-translation/#3-quick-start-improving-or-adding-a-translation","title":"3. Quick start (improving or adding a translation)","text":"<ol> <li>Fork and clone your fork.</li> <li>Create / activate a virtual environment.</li> <li>Install docs extras:     <pre><code>pip install -e \".[docs]\"\n</code></pre></li> <li>Run the local server:     <pre><code>mkdocs serve\n</code></pre></li> <li>Open the local URL and switch language with the selector.</li> </ol> <p>Restart the server if new files do not show up (MkDocs sometimes needs a restart after new file additions).</p>"},{"location":"developer-guide/how-to-add-a-translation/#4-adding-a-new-language-one-time-setup","title":"4. Adding a new language (one-time setup)","text":"<p>If the language already exists (e.g. <code>pt/</code>), skip this section.</p> <ol> <li>Pick a locale code (e.g. <code>es</code>).</li> <li>Create the folder: <code>docs/es/</code>.</li> <li>Copy the English <code>index.md</code> to <code>docs/es/index.md</code> and translate its visible text.</li> <li>(Optional) Start by copying only a minimal subset (index + key getting-started pages) to keep the first PR reviewable.</li> <li>Update <code>mkdocs.yml</code> <code>i18n.languages</code> section:     <pre><code>- locale: es\n  name: Espa\u00f1ol\n  build: true\n  site_description: &lt;translated site tagline&gt;\n  theme:\n    docs_dir: docs/es/\n    custom_dir: docs/es/\n    site_dir: site/es/\n    logo: overrides/assets/img/logotype-sysidentpy.svg\n</code></pre></li> <li>Do not duplicate nav entries for the new language: the plugin maps them automatically based on folder structure.</li> <li>Run <code>mkdocs serve</code> and confirm the new language appears in the selector.</li> </ol> <p>If you need a regional variant (e.g. <code>pt-BR</code>), use that code consistently in both folder name and <code>mkdocs.yml</code>.</p>"},{"location":"developer-guide/how-to-add-a-translation/#5-adding-a-new-english-page-source-language","title":"5. Adding a new English page (source language)","text":"<ol> <li>Create the file under <code>docs/en/...</code> in the right section.</li> <li>Add front matter:     <pre><code>---\ntemplate: overrides/main.html\ntitle: My Page Title\n---\n</code></pre></li> <li>Add its path to the <code>nav</code> tree in <code>mkdocs.yml</code> (English only).</li> <li>Verify the site builds.</li> <li>(Optional) Add a translator note (HTML comment) for tricky concepts:     <pre><code>&lt;!-- Translator note: keep the term \"NARMAX\" in English. --&gt;\n</code></pre></li> </ol>"},{"location":"developer-guide/how-to-add-a-translation/#6-creating-the-translated-file","title":"6. Creating the translated file","text":"<ol> <li> <p>Mirror the folder path: <code>docs/&lt;locale&gt;/&lt;same relative path&gt;.md</code>.</p> </li> <li> <p>Copy the English file.</p> </li> <li> <p>Translate only human-readable prose. Keep intact:</p> <ul> <li>Code blocks (except inline comments if clarity improves)</li> <li>Identifiers: function/class names, parameters, imports</li> <li>File paths, config keys, URLs</li> <li>Markdown link targets (unless they refer to a language-specific external resource)</li> </ul> </li> <li> <p>Preserve heading hierarchy (<code>#</code>, <code>##</code>, etc.) but translate heading text itself.</p> </li> <li> <p>Keep admonition types (<code>!!! note</code>, <code>!!! warning</code>, etc.). You may translate the title string after the admonition type.</p> </li> <li> <p>For unfinished parts, you can temporarily keep English or add:    <pre><code>!!! note \"Pending translation\"\n    This paragraph still needs translation.\n</code></pre></p> </li> <li> <p>Remove all \"Pending\" notes before final review if completed.</p> </li> </ol>"},{"location":"developer-guide/how-to-add-a-translation/#7-internal-links","title":"7. Internal links","text":"<p>Use relative links without language prefixes: <pre><code>See the [Contribute guide](contribute.md).\n</code></pre> The i18n plugin rewrites them per language. Avoid hardcoding <code>/en/</code> or another locale path unless you intentionally want a fallback link.</p> <p>Anchor links: if you translate a heading, Material generates a localized slug. Keep anchor usage consistent or adjust any <code>(#anchor)</code> references accordingly.</p>"},{"location":"developer-guide/how-to-add-a-translation/#8-images-and-media","title":"8. Images and media","text":"<p>If screenshots contain embedded language text:</p> <ul> <li>Option A: replicate and localize the image inside <code>docs/&lt;locale&gt;/assets/</code> and keep the same filename (per-locale path isolates it).</li> <li>Option B: reuse the English image if text is minimal or language-agnostic.</li> </ul> <p>SVG diagrams: prefer keeping labels English if they are code or model symbols; translate UI captions where helpful.</p>"},{"location":"developer-guide/how-to-add-a-translation/#9-formatting-style-guidelines","title":"9. Formatting &amp; style guidelines","text":"Aspect Guideline Numbers Keep numeric precision; adapt decimal separators only if standard for the target audience (optional). Units Do not translate SI units (e.g. <code>ms</code>, <code>Hz</code>). API names Never translate identifiers. Quotes Follow local typographic conventions only if they do not break Markdown. Capitalization Mirror English capitalization only where it's a proper noun or API name. Tone Neutral, concise, instructional. <p>Avoid machine\u2011translated chunks without revision. Prefer shorter, unambiguous sentences.</p>"},{"location":"developer-guide/how-to-add-a-translation/#10-translation-glossary","title":"10. Translation glossary","text":"<p>Keep these terms consistent across languages. Add target language equivalents as needed:</p> English Term Portuguese (pt) Core concepts model structure estrutura do modelo parameter estimation estima\u00e7\u00e3o de par\u00e2metros residual analysis an\u00e1lise dos res\u00edduos time series s\u00e9rie temporal identification identifica\u00e7\u00e3o Technical terms basis function fun\u00e7\u00e3o de base regression regress\u00e3o algorithm algoritmo validation valida\u00e7\u00e3o simulation simula\u00e7\u00e3o Development terms feature funcionalidade pull request (PR) pull request (PR) branch branch commit commit documentation documenta\u00e7\u00e3o <p>For other languages, follow similar patterns. Prefer clarity over literal translation.</p>"},{"location":"developer-guide/how-to-add-a-translation/#11-review-checklist-per-translated-file","title":"11. Review checklist (per translated file)","text":"<ul> <li> Builds locally (<code>mkdocs serve</code>).</li> <li> File path mirrors English.</li> <li> Relative links work; no <code>/en/</code> hardcoding.</li> <li> Code blocks untouched (except explanatory comments if needed).</li> <li> Terminology consistent with existing pages.</li> <li> No leftover placeholder notes (unless intentionally partial PR).</li> <li> Front matter present and <code>title:</code> localized.</li> </ul>"},{"location":"developer-guide/how-to-add-a-translation/#12-commit-pull-request","title":"12. Commit &amp; Pull Request","text":"<p>Add both English and translated files if you introduce a page + its translation; otherwise just the translated file.</p> <p>Example: <pre><code>git add docs/en/developer-guide/new-topic.md docs/es/developer-guide/new-topic.md\ngit commit -m \"docs: add Spanish translation for new-topic\"\n</code></pre></p> <p>PR description template:</p> <ul> <li>Language: <code>&lt;locale&gt;</code> (e.g. <code>es</code>)</li> <li>Pages added/updated: list paths</li> <li>Sections intentionally left untranslated: (if any)</li> <li>Glossary additions: (if any)</li> <li>Notes for reviewer: context, tricky terms</li> </ul> <p>Partial translations are acceptable\u2014label them clearly so others can help.</p>"},{"location":"developer-guide/how-to-add-a-translation/#13-updating-existing-translations","title":"13. Updating existing translations","text":"<p>When an English page changes:</p> <ol> <li>Open the diff to see what changed.</li> <li>Apply equivalent edits to the translated file.</li> <li>If you cannot translate immediately, keep the English change and add a temporary \"Pending translation\" note.</li> <li>Remove the note once updated.</li> </ol> <p>Aim for incremental PRs rather than large rewrites.</p>"},{"location":"developer-guide/how-to-add-a-translation/#14-common-issues","title":"14. Common issues","text":"Symptom Cause Fix Page shows in English only Missing localized file or wrong path Mirror path under new locale folder Build error after adding language Misconfigured <code>i18n.languages</code> entry Re-check <code>locale</code> key + indentation 404 on internal link Link path differs from English Match relative path to English version Broken anchor Heading translated; old anchor used Update anchor or keep similar heading slug Language not in selector Forgot to add language in <code>mkdocs.yml</code> Add <code>locale</code> entry and restart server"},{"location":"developer-guide/how-to-add-a-translation/#15-automation-optional","title":"15. Automation (optional)","text":"<p>You may use external CAT / translation tools, but always manually review technical terms. Avoid committing raw machine output.</p> <p>If you script copying English files to a new locale, exclude already translated ones to prevent overwriting.</p>"},{"location":"developer-guide/how-to-add-a-translation/#16-questions-support","title":"16. Questions &amp; support","text":"<p>If you are unsure about terminology, open an Issue or Discussion before translating large sections. Early feedback prevents rework.</p> <p>Thanks for helping make the documentation accessible to more people.</p>"},{"location":"developer-guide/how-to-write-a-how-to-guide/","title":"How To Write a How-to Guide","text":"<p>Coming soon</p>"},{"location":"developer-guide/how-to-write-a-tutorial/","title":"How To Write a Tutorial","text":"<p>Coming soon</p>"},{"location":"getting-started/getting-started/","title":"Getting Started","text":"<p>Welcome to SysIdentPy's documentation! Learn how to get started with SysIdentPy in your project. Then, explore SysIdentPy's main concepts and discover additional resources to help you model dynamic systems and time series.</p>      \ud83d\udcda Looking for more details on NARMAX models? \u25bc <p>       For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book:     </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>       This book provides in-depth guidance to support your work with SysIdentPy.     </p> <p>       \ud83d\udee0\ufe0f You can also explore the tutorials in the documentation for practical, hands-on examples.     </p>"},{"location":"getting-started/getting-started/#what-is-sysidentpy","title":"What is SysIdentPy","text":"<p>SysIdentPy is an open-source Python module for System Identification using NARMAX models built on top of numpy and is distributed under the 3-Clause BSD license. SysIdentPy provides an easy-to-use and  flexible framework for building Dynamical Nonlinear Models for time series and dynamic systems.</p> <p>With SysIdentPy, you can:</p> <ul> <li>Build and customize nonlinear forecasting models.</li> <li>Utilize state-of-the-art techniques for model structure selection and parameter estimation.</li> <li>Experiment with neural NARX models and other advanced algorithms.</li> </ul>"},{"location":"getting-started/getting-started/#installation","title":"Installation","text":"<p>SysIdentPy is published as a Python package and can be installed with <code>pip</code>, ideally by using a virtual environment. If not, scroll down and expand the help box. Install with:</p> Latest <pre><code>pip install sysidentpy</code></pre> Neural NARX Support <pre><code>pip install sysidentpy[\"all\"]</code></pre> Specific Version <pre><code>pip install sysidentpy==\"0.5.3\"</code></pre> From Git <pre><code>pip install git+https://github.com/wilsonrljr/sysidentpy.git</code></pre>      \u2753 How to manage my projects dependencies? \u25bc <p>       If you don't have prior experience with Python, we recommend reading                Using Python's pip to Manage Your Projects' Dependencies       , which is a really good introduction on the mechanics of Python package management and helps you troubleshoot if you run into errors.     </p>"},{"location":"getting-started/getting-started/#what-are-the-main-features-of-sysidentpy","title":"What are the main features of SysIdentPy?","text":"\ud83e\udde9 NARMAX Philosophy <p>Build variations like NARX, NAR, ARMA, NFIR, and more.</p> \ud83d\udcdd Model Structure Selection <p>Use methods like FROLS, MetaMSS, and combinations with parameter estimation techniques.</p> \ud83d\udd17 Basis Function <p>Choose from 8+ basis functions, combining linear and nonlinear types for custom NARMAX models.</p> \ud83c\udfaf Parameter Estimation <p>Over 15 parameter estimation methods for exploring various structure selection scenarios.</p> \u2696\ufe0f Multiobjective Estimation <p>Minimize different objective functions using affine information for parameter estimation.</p> \ud83d\udd04 Model Simulation <p>Reproduce paper results easily with SimulateNARMAX. Test and compare published models effortlessly.</p> \ud83e\udd16 Neural NARX (PyTorch) <p>Integrate with PyTorch for custom neural NARX architectures using all PyTorch optimizers and loss functions.</p> \ud83d\udee0\ufe0f General Estimators <p>Compatible with scikit-learn, Catboost, and more for creating NARMAX models.</p>"},{"location":"getting-started/getting-started/#additional-resources","title":"Additional resources","text":"<ul> <li> \ud83e\udd1d Contribute to SysIdentPy </li> <li> \ud83d\udcdc License Information </li> <li> \ud83c\udd98 Get Help &amp; Support </li> <li> \ud83d\udcc5 Meetups </li> <li> \ud83d\udc96 Become a Sponsor </li> <li> \ud83e\udde9 Explore NARMAX Base Code </li> </ul>"},{"location":"getting-started/getting-started/#do-you-like-sysidentpy","title":"Do you like SysIdentPy?","text":"<p>Would you like to help SysIdentPy, other users, and the author? You can \"star\" SysIdentPy in GitHub by clicking in the star button at the top right of the page: https://github.com/wilsonrljr/sysidentpy. \u2b50\ufe0f</p> <p>Starring a repository makes it easy to find it later and help you to find similar projects on GitHub based on Github recommendation contents. Besides, by starring a repository also shows appreciation to the SysIdentPy maintainer for their work.</p> <p> \u00a0 Join our  \"Sponsor\" in github</p>"},{"location":"getting-started/license/","title":"License","text":"<p>BSD 3-Clause License</p> <p>Copyright \u00a9 2019, Wilson Rocha; Luan Pascoal; Samuel Oliveira; Samir Martins All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ul> <li> <p>Redistributions of source code must retain the above copyright notice, this   list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,   this list of conditions and the following disclaimer in the documentation   and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its   contributors may be used to endorse or promote products derived from   this software without specific prior written permission.</p> </li> </ul> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"getting-started/narmax-intro/","title":"Introduction","text":"<p>Author: Wilson Rocha Lacerda Junior</p> <p>This is the first in a series of publications explaining a little bit about NARMAX<sup>1</sup> models. I hope the content of these publications will help those who use or would like to use the SysIdentPy library.</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>You can also explore the tutorials in the documentation for practical, hands-on examples.</p>"},{"location":"getting-started/narmax-intro/#system-identification","title":"System Identification","text":"<p>As I will use the term Systems Identification here and there, let me make a brief definition regarding these terms.</p> <p> Systems identification is one of the major areas that deals with the modeling of data-based processes. In this context, the term \"system\" can be interpreted as any set of operations that process one or more inputs and return one or more outputs. Examples include electrical systems, mechanical systems, biological systems, financial systems, chemical systems \u2026 literally anything you can relate to input and output data. The electricity demand is part of a system whose inputs can be, for example, quantity of the population, quantity of water in the reservoirs, season, events. The price of a property is the output of a system whose entries can be the city, per capita income, neighborhood, number of rooms, how old the house is, and many others. You got the idea.</p> <p> Although there are many things related with Machine Learning, Statistical Learning and other fields,  each field has its particularities.</p>"},{"location":"getting-started/narmax-intro/#so-what-is-a-narmax-model","title":"So, what is a NARMAX model?","text":"<p>You may have noticed the similarity between the acronym NARMAX with the well-known models ARX, ARMAX, etc., which are widely used for forecasting time series. And this resemblance is not by chance. The Autoregressive models with Moving Average and Exogenous Input (ARMAX) and their variations AR, ARX, ARMA (to name just a few) are one of the most used mathematical representations for identifying linear systems.</p> <p> Let's go back to the model. I said that the ARX family of models is commonly used to model linear systems. Linear is the key word here. For nonlinear scenarios we have the NARMAX class. As reported by Billings (one of the creators of NARMAX model) in the book [Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains],  NARMAX started out as a model name, but soon became a philosophy when it comes to identifying nonlinear systems. Obtaining NARMAX models consists of performing the following steps:</p> <p>Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains</p> <ul> <li>Dynamical tests and collecting data;</li> <li>Choice of mathematical representation;</li> <li>Detection of the model structure;</li> <li>Estimation of parameters;</li> <li>Validation;</li> <li>Analysis of the model.</li> </ul> <p>We will cover each of these steps in further publications. The idea of this text is to present an overview of NARMAX models.</p> <p> NARMAX models are not, however, a simple extension of ARMAX models. NARMAX models are able to represent the most different and complex nonlinear systems. Introduced in 1981 by the Electrical Engineer Stephen A. Billings, NARMAX models can be described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\) , are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>If we do not include noise terms, \\(e_{k-n_e}\\), we have NARX models. If we set \\(\\ell = 1\\) then we deal with ARMAX models; if \\(\\ell = 1\\) and we do not include input and noise terms, it turns to AR model (ARX if we include inputs, ARMA if we include noise terms instead); if \\(\\ell&gt;1\\) and there is no input terms, we have the NARMA. If there is no input or noise terms, we have NAR. There are several variants, but that is sufficient for now.</p>"},{"location":"getting-started/narmax-intro/#narmax-representation","title":"NARMAX Representation","text":"<p>There are several nonlinear functions representations to approximate the unknown mapping \\(\\mathrm{f}[\\cdot]\\) in the NARMAX methods, e.g.,</p> <ul> <li>neural networks;</li> <li>fuzzy logic-based models;</li> <li>radial basis functions;</li> <li>wavelet basis;</li> <li>polynomial basis;</li> <li>generalized additive models;</li> </ul> <p>The remainder of this post contemplates methods related to the power-form polynomial models, which is the most common used representation. Polynomial NARMAX is a mathematical model based on difference equations and relates the current output as a function of past inputs and outputs.</p>"},{"location":"getting-started/narmax-intro/#polynomial-narmax","title":"Polynomial NARMAX","text":"<p>The polynomial NARMAX model with asymptotically stable equilibrium points can be described as:</p> \\[\\begin{align}     y_k =&amp; \\sum_{0} + \\sum_{i=1}^{p}\\Theta_{y}^{i}y_{k-i} + \\sum_{j=1}^{q}\\Theta_{e}^{j}e_{k-j} + \\sum_{m=1}^{r}\\Theta_{x}^{m}x_{k-m}\\\\     &amp;+ \\sum_{i=1}^{p}\\sum_{j=1}^{q}\\Theta_{ye}^{ij}y_{k-i} e_{k-j} + \\sum_{i=1}^{p}\\sum_{m=1}^{r}\\Theta_{yx}^{im}y_{k-i} x_{k-m} \\\\     &amp;+ \\sum_{j=1}^{q}\\sum_{m=1}^{r}\\Theta_{e x}^{jm}e_{k-j} x_{k-m} \\\\     &amp;+ \\sum_{i=1}^{p}\\sum_{j=1}^{q}\\sum_{m=1}^{r}\\Theta_{y e x}^{ijm}y_{k-i} e_{k-j} x_{k-m} \\\\     &amp;+ \\sum_{m_1=1}^{r} \\sum_{m_2=m_1}^{r}\\Theta_{x^2}^{m_1 m_2} x_{k-m_1} x_{k-m_2} \\dotsc \\\\     &amp;+ \\sum_{m_1=1}^{r} \\dotsc \\sum_{m_l=m_{l-1}}^{r} \\Theta_{x^l}^{m_1, \\dotsc, m_2} x_{k-m_1} x_{k-m_l} \\end{align}\\] <p>where \\(\\sum\\nolimits_{0}\\), \\(c_{y}^{i}\\), \\(c_{e}^{j}\\), \\(c_{x}^{m}\\), \\(c_{y\\e}^{ij}\\), \\(c_{yx}^{im}\\), \\(c_{e x}^{jm}\\), \\(c_{y e x}^{ijm}\\), \\(c_{x^2}^{m_1 m_2} \\dotsc c_{x^l}^{m_1, \\dotsc, ml}\\) are constant parameters.</p> <p> Let's take a look at an example of a NARMAX model for an easy understanding. The following is a NARMAX model of degree~\\(2\\), identified from experimental data of a DC motor/generator with no prior knowledge of the model form. If you want more information about the identification process, I wrote a paper comparing a polynomial NARMAX with a neural NARX model using that data (IN PORTUGUESE: Identifica\u00e7\u00e3o de um motor/gerador CC por meio de modelos polinomiais autorregressivos e redes neurais artificiais)</p> \\[\\begin{align}     y_k =&amp; 1.7813y_{k-1}-0,7962y_{k-2}+0,0339x_{k-1} -0,1597x_{k-1} y_{k-1} +0,0338x_{k-2} \\\\     &amp; + 0,1297x_{k-1}y_{k-2} - 0,1396x_{k-2}y_{k-1}+ 0,1086x_{k-2}y_{k-2}+0,0085y_{k-2}^2 + 0.1938e_{k-1}e_{k-2} \\end{align}\\] <p>But how those terms were selected? How the parameters were estimated? These questions will lead us to model structure selection and parameter estimation topics, but, for now,  let us discuss about those topics in a more simple manner.</p> <p> First, the \"structure\" of a model is the set of terms (also called regressors) included in the final model. The parameters are the values multiplying each of theses terms. And looking at the example above we can notice an really important thing regarding polynomial NARMAX models dealt in this text: they have a non-linear structure, but they are linear-in-the-parameters. You will see how this note is important in the post about parameter estimation.</p> <p> In this respect, consider the case where we have the input and output data of some system. For the sake of simplicity, suppose one input and one output. We have the data, but we do not know which lags to choose for the input or the output. Also, we know nothing about the system non-linearity. So, we have to define some values for maximum lags of the input, output and the noise terms, besides the choice of the \\(\\ell\\) value. It's worth to notice that many assumptions taken for linear cases are not valid in the nonlinear scenario and therefore select the maximum lags is not straightforward. So, how those values can make the modeling harder?</p> <p> So we have one input and one output (disregard the noise terms for now). What if we choose the \\(n_y = n_x = \\ell = 2\\)? With these values, we have the following possibilities for compose the final model:</p> \\[\\begin{align}     &amp; constant, y_{k-1}, y_{k-2}, y_{k-1}^2, y_{k-2}^2, x_{k-1}, x_{k-2}, x_{k-1}^2, x_{k-2}^2,y_{k-1}y_{k-2},\\\\     &amp; y_{k-1}x_{k-1}, y_{k-1}x_{k-2}, y_{k-2}x_{k-1}, y_{k-2}x_{k-2}, x_{k-1}x_{k-2} . \\end{align}\\] <p>So we have \\(15\\) candidate terms to compose the final model.</p> <p> Again, we do not know how of those terms are significant to compose the model. One should decide to use all the terms because there are only \\(15\\). This, even in a simple scenario like this, can lead to a very wrong representation of the system that you are trying to modeling. Ok, what if we run a brute force algorithm to test the candidate regressors so we can select only the significant ones? In this case, we have \\(2^{15} = 32768\\) possible model structures to be tested.</p> <p> You can think that it is ok, we have computer power for that. But this case is very simple and the system might have lags equal to \\(10\\) for input and output. If we define \\(n_y = n_x = 10\\) and \\(\\ell=2\\), the number of possible models to be tested increases to \\(2^{231}=3.4508732\\times10^{69}\\). If the non-linearity is set to \\(3\\) then we have \\(2^{1771} = 1.3308291989700907535925992... \\times 10^{533}\\) candidate models.</p> <p> Now, think about the case when we have not 1, but 5, 10 or more inputs... and have to include terms for the noise, and maximum lags are higher than 10... and nonlinearity is higher than 3...</p> <p> And the problem is not solved by only identifying the most significant terms. How do you choose the number of terms to include in the final model. It is not just about check the relevance of each regressor, we have to think about the impact of including \\(5\\), \\(10\\) or \\(50\\) regressors in the model. And do not forget: after selecting the terms, we have to estimate its parameters.</p> <p> As you can see, to select the most significant terms from a huge dictionary of possible terms is not an easy task. And it is hard not only because the complex combinatorial problem and the uncertainty concerning the model order. Identifying the most significant terms in a nonlinear scenario is very difficult because depends on the type of the non-linearity (sparse singularity or near-singular behavior, memory or dumping effects and many others), dynamical response (spatial-temporal systems, time-dependent), the steady-state response,  frequency of the data, the noise...</p> <p> Despite all this complexity, NARMAX models are widely used because it is able to represent complex system with simple and transparent models, which terms are selected using robust algorithms for model structure selection. Model structure selection is the core of NARMAX methods and the scientific community is very active on improving classical methods and developing new ones. As I said, I will introduce some of those methods in another post.</p> <p> I hope this publication served as a brief introduction to NARMAX models. Furthermore, I hope I have sparked your interest in this model class. The link to the other texts will be made available soon, but feel free to contact us if you are interested in collaborating with the SysIdentPy library or if you want to address any questions.</p> <ol> <li> <p>Non-linear Autoregressive Models with Moving Average and Exogenous Input.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting-started/quickstart-guide/","title":"Basic Usage","text":""},{"location":"getting-started/quickstart-guide/#1-prerequisites","title":"1. Prerequisites","text":"<p>You\u2019ll need to know a bit of Python.</p> <p>To work the examples, you\u2019ll need <code>pandas</code> installed in addition to NumPy.</p> <pre><code>pip install sysidentpy pandas\n# Optional: For neural networks and advanced features\npip install sysidentpy[\"all\"]\n</code></pre>"},{"location":"getting-started/quickstart-guide/#2-key-features","title":"2. Key Features","text":"<p>SysIdentPy provides a flexible framework for building, predicting, validating, and visualizing nonlinear time series models. The modeling process involves several key decisions: defining the mathematical representation of the model, choosing the parameter estimation algorithm, selecting the appropriate model structure, and determining the prediction approach.</p> <p>The following features are available in SysIdentPy:</p>"},{"location":"getting-started/quickstart-guide/#model-classes","title":"Model Classes","text":"<ul> <li>NARMAX, NARX, NARMA, NAR, NFIR, ARMAX, ARX, AR, and their variants.</li> </ul>"},{"location":"getting-started/quickstart-guide/#mathematical-representations","title":"Mathematical Representations","text":"<ul> <li>Polynomial</li> <li>Neural</li> <li>Fourier</li> <li>Laguerre</li> <li>Bernstein</li> <li>Bilinear</li> <li>Legendre</li> <li>Hermite</li> <li>HermiteNormalized</li> </ul> <p>You can also define advanced NARX models such as Bayesian and Gradient Boosting models using the GeneralNARX class, which provides seamless integration with various machine learning algorithms.</p>"},{"location":"getting-started/quickstart-guide/#model-structure-selection-algorithms","title":"Model Structure Selection Algorithms","text":"<ul> <li>Forward Regression Orthogonal Least Squares (FROLS)</li> <li>Meta-model Structure Selection (MeMoSS)</li> <li>Accelerated Orthogonal Least Squares (AOLS)</li> <li>Entropic Regression</li> <li>Ultra Orthogonal Least Squares (UOLS)</li> </ul>"},{"location":"getting-started/quickstart-guide/#parameter-estimation-methods","title":"Parameter Estimation Methods","text":"<ul> <li>Least Squares (LS)</li> <li>Total Least Squares (TLS)</li> <li>Recursive Least Squares (RLS)</li> <li>Ridge Regression</li> <li>Non-Negative Least Squares (NNLS)</li> <li>Least Squares Minimal Residues (LSMR)</li> <li>Bounded Variable Least Squares (BVLS)</li> <li>Least Mean Squares (LMS) and its variants:</li> <li>Affine LMS</li> <li>LMS with Sign Error</li> <li>Normalized LMS</li> <li>LMS with Normalized Sign Error</li> <li>LMS with Sign Regressor</li> <li>Normalized LMS with Sign Sign</li> <li>Leaky LMS</li> <li>Fourth-Order LMS</li> <li>Mixed Norm LMS</li> </ul>"},{"location":"getting-started/quickstart-guide/#order-selection-criteria","title":"Order Selection Criteria","text":"<ul> <li>Akaike Information Criterion (AIC)</li> <li>Corrected Akaike Information Criterion (AICc)</li> <li>Bayesian Information Criterion (BIC)</li> <li>Final Prediction Error (FPE)</li> <li>Khundrin's Law of Iterated Logarithm Criterion</li> </ul>"},{"location":"getting-started/quickstart-guide/#prediction-methods","title":"Prediction Methods","text":"<ul> <li>One-step ahead</li> <li>n-steps ahead</li> <li>Infinity-steps ahead</li> </ul>"},{"location":"getting-started/quickstart-guide/#visualization-tools","title":"Visualization Tools","text":"<ul> <li>Prediction plots</li> <li>Residual analysis</li> <li>Model structure visualization</li> <li>Parameter visualization</li> </ul> <p>As you can see, SysIdentPy supports numerous model combinations, each tailored to different use cases. But don\u2019t worry about picking the perfect combination right away. Let\u2019s start with the default settings to get you up and running quickly.</p>      \ud83d\udcda Looking for more details on NARMAX models? \u25bc <p>       For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book:     </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>       This book provides in-depth guidance to support your work with SysIdentPy.     </p> <p>       \ud83d\udee0\ufe0f You can also explore the tutorials in the documentation for practical, hands-on examples.     </p>"},{"location":"getting-started/quickstart-guide/#3-quickstart","title":"3. Quickstart","text":"<p>To keep things simple, let's load some simulated data for the examples.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\n\n# Generate a dataset from a simulated dynamic system.\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=300,\n    colored_noise=False,\n    sigma=0.0001,\n    train_percentage=80\n)\n</code></pre>"},{"location":"getting-started/quickstart-guide/#build-your-first-narx-model","title":"Build your first NARX model","text":"<p>With the loaded dataset, let's build a Polynomial NARX model. Using SysIdentPy's default options, you need to define at least the model structure selection method and the mathematical representation of the model (specified here by the basis function).</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n</code></pre> <p>The model structure selection (MSS) method enables the model's fit and predict operations.</p> <p>While different MSS algorithms come with various hyperparameters, they are not the focus here. In this guide, we will show how to modify these hyperparameters but will not discuss the best configurations.</p> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>To evaluate the model's performance, you can use any of the native metric functions available in SysIdentPy. For example, the Root Relative Squared Error (RRSE) metric can be used as follows:</p> <pre><code>from sysidentpy.metrics import root_relative_squared_error\n\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00014\n</code></pre> <p>To view the final mathematical equation of the Polynomial NARX model, use the results function. This function requires:</p> <ul> <li><code>final_model</code>: The selected regressors after fitting</li> <li><code>theta</code>: The estimated parameters</li> <li><code>err</code>: The error reduction ratio (ERR)</li> </ul> <p>Here\u2019s how to display the results:</p> <p><pre><code>from sysidentpy.utils.display_results import results\n\nr = pd.DataFrame(\n    results(\n        model.final_model, model.theta, model.err,\n        model.n_terms, err_precision=8, dtype='sci'\n        ),\n    columns=['Regressors', 'Parameters', 'ERR'])\nprint(r)\n</code></pre> This output shows the selected regressors, their corresponding estimated parameters, and the contribution of each regressor to the model\u2019s performance (ERR).</p> <pre><code>Regressors     Parameters        ERR\n0        x1(k-2)     0.9000  0.95556574\n1         y(k-1)     0.1999  0.04107943\n2  x1(k-1)y(k-1)     0.1000  0.00335113\n</code></pre> <p>To visualize the model's performance, you can use the <code>plot_results</code> function. This method plots the predicted values against the actual data, allowing you to see how well the model fits the dataset.</p> <p><pre><code>from sysidentpy.utils.plotting import plot_results\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> </p> <p>Residual analysis is essential to check if the model has captured all the relevant dynamics of the system. You can analyze the residuals by computing their autocorrelation and the cross-correlation between the residuals and one of the model inputs.</p> <pre><code>from sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n\n# Compute and plot autocorrelation of the residuals\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\n\n# Compute and plot cross-correlation between residuals and an input\nx1e = compute_cross_correlation(y_valid, yhat, x2_val)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p>Here\u2019s the full code example for your reference:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n\n# Generate a dataset from a simulated dynamic system.\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=300,\n    colored_noise=False,\n    sigma=0.0001,\n    train_percentage=80\n)\n\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model, model.theta, model.err,\n        model.n_terms, err_precision=8, dtype='sci'\n        ),\n    columns=['Regressors', 'Parameters', 'ERR'])\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000, figsize=(15, 4))\n# Compute and plot autocorrelation of the residuals\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\n# Compute and plot cross-correlation between residuals and an input\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre>"},{"location":"getting-started/quickstart-guide/#customizing-your-model-configuration","title":"Customizing your model configuration","text":"<p>In the previous section, we showed how easy it is to fit a Polynomial NARX model with SysIdentPy using the default configuration. But what if you want to experiment with different combinations of algorithms for model structure selection, parameter estimation, and other settings?</p>"},{"location":"getting-started/quickstart-guide/#model-structure-selection","title":"Model Structure Selection","text":"<p>SysIdentPy makes this process simple. For instance, if you want to use the Accelerated Orthogonal Least Squares (AOLS) algorithm instead of the default <code>FROLS</code>, you only need to import and use it when defining your model.</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = AOLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>The evaluation, residual analysis, and plots remain the same as before, so they are not shown here.</p> <p>With just a small change in the import statement, you can explore different algorithms. Similarly, you can customize the parameter estimation methods, prediction strategies, and mathematical representations to suit your specific needs.</p> <p>Similarly, you can use the Meta-model Structure Selection</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = MetaMSS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>and the Entropic Regression</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"getting-started/quickstart-guide/#parameter-estimation","title":"Parameter Estimation","text":"<p>Changing the parameter estimation algorithm in SysIdentPy is also straightforward. You can check the list of available algorithms with the following code:</p> <pre><code>from sysidentpy import parameter_estimation\n\nprint(\"Parameter Estimation Algorithms available:\", parameter_estimation.__all__)\n</code></pre> <pre><code>Parameter Estimation Algorithms available: ['LeastSquares', 'RidgeRegression', 'RecursiveLeastSquares', 'TotalLeastSquares', 'LeastMeanSquareMixedNorm', 'LeastMeanSquares', 'LeastMeanSquaresFourth', 'LeastMeanSquaresLeaky', 'LeastMeanSquaresNormalizedLeaky', 'LeastMeanSquaresNormalizedSignRegressor', 'LeastMeanSquaresNormalizedSignSign', 'LeastMeanSquaresSignError', 'LeastMeanSquaresSignSign', 'AffineLeastMeanSquares', 'NormalizedLeastMeanSquares', 'NormalizedLeastMeanSquaresSignError', 'LeastMeanSquaresSignRegressor', 'NonNegativeLeastSquares', 'LeastSquaresMinimalResidual', 'BoundedVariableLeastSquares']\n</code></pre> <p>Although the default estimator may change depending on the model structure selection method, it is usually <code>LeastSquares</code> or <code>RecursiveLeastSquares</code>. To define a specific estimator, simply import the desired method and set it using the estimator hyperparameter:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquaresMinimalResidual\n\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    estimator=LeastSquaresMinimalResidual(),\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>You can apply this approach to any model structure selection algorithm. It\u2019s that simple to change the parameter estimation method.</p>"},{"location":"getting-started/quickstart-guide/#customizing-the-mathematical-representation-basis-function","title":"Customizing the Mathematical Representation - Basis Function","text":"<p>Changing the mathematical representation (basis function) in SysIdentPy is as simple as customizing the parameter estimation method. For example, to build a Fourier NARX model, you just need to import the desired basis function and set it in the model structure selection algorithm.</p> <p>To check all available basis functions, use:</p> <pre><code>from sysidentpy import basis_function\n\nprint(\"Basis Function Available:\", basis_function.__all__)\n</code></pre> <pre><code>Basis Function Available: ['Bersntein', 'Bilinear', 'Fourier', 'Legendre', 'Laguerre', 'Hermite', 'HermiteNormalized', 'Polynomial']\n</code></pre> <p>After choosing the basis function, you can define it in your model as shown below:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Fourier\nfrom sysidentpy.parameter_estimation import LeastSquaresMinimalResidual\n\n\nbasis_function = Fourier(degree=2)\nmodel = AOLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    estimator=LeastSquaresMinimalResidual(),\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>With this approach, you can easily explore different mathematical representations by simply switching the basis function. No complex changes required.</p> <p>Note</p> <p>The <code>results</code> method, which returns the mathematical equation of the fitted model, currently supports only the Polynomial basis function. Support for all basis functions is planned for version 1.0.</p>"},{"location":"getting-started/quickstart-guide/#customizing-the-model-type","title":"Customizing the Model Type","text":"<p>The key difference between a NARX and an ARX model lies in the presence of nonlinear relationships between the regressors. For instance, if you set the degree of the basis function to 2 for Polynomial basis function, as shown in previous examples, you'll have a NARX model. If the degree is set to 1, it results in an ARX model.</p> <p>However, the distinction isn't purely based on the degree of the basis function. It ultimately depends on the final model equation. Even with a degree set to 2, the fitted model might be linear if the model structure selection algorithm removes the nonlinear terms. This means that while setting the degree to 2 gives the algorithm an opportunity to explore nonlinear relationships, the final model might still be linear.</p> <p>Always check the final model to confirm whether it is linear or nonlinear, regardless of the degree you set for the basis function.</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\n# basis_function = Polynomial(degree=2) for NARX (and maybe ARX) or\nbasis_function = Polynomial(degree=1)  # ARX model\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>The difference between NARX, NAR, and NFIR models lies in the type of regressors used. NARX models involve both input and output regressors, while NAR models use only output regressors, and NFIR models use only input regressors.</p> <p>To create a NAR model, you simply need to specify the <code>model_type</code> argument as <code>\"NAR\"</code>. In this case, you don't need to define the lags of the inputs since you're working with output-only regressors. You also don't need to pass input data in the <code>fit</code> and <code>predict</code> methods. Only the output data is required.</p> <p>Because NAR models do not include input variables to define the forecasting horizon, you must set the <code>forecast_horizon</code> parameter to specify how many periods ahead you want to predict.</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\n# basis_function = Polynomial(degree=2) for NARX (and maybe ARX) or\nbasis_function = Polynomial(degree=1)  # ARX model\nmodel = FROLS(\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\n\nmodel.fit(y=y_train)\nyhat = model.predict(y=y_valid, forecast_horizon=23)\n</code></pre> <p>For the NFIR model, however, you still need to pass the output array because autoregressive models require initial conditions to operate.</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\n# basis_function = Polynomial(degree=2) for NARX (and maybe ARX) or\nbasis_function = Polynomial(degree=1)  # ARX model\nmodel = FROLS(\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NFIR\",  # Specify NFIR model type\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>      \ud83d\udcda Looking for more details on what are initial conditions? \u25bc <p>       Check chapter 9 of our companion book for more information on why autoregressive models need initial conditions to operate:     </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy"},{"location":"getting-started/quickstart-guide/#prediction-and-forecasting-horizon","title":"Prediction and Forecasting Horizon","text":"<p>By default, when you call <code>model.predict(X=x_valid, y=y_valid)</code>, it performs an infinity-steps ahead prediction, also known as a free run simulation. However, if you need to make a specific number of steps ahead prediction, such as a one-step ahead or n-steps ahead forecast, you can simply pass the <code>steps_ahead</code> hyperparameter in the <code>predict</code> method:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\n# basis_function = Polynomial(degree=2) for NARX (and maybe ARX) or\nbasis_function = Polynomial(degree=2)  # ARX model\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\n# one-step ahead\nyhat = model.predict(X=x_valid, y=y_valid, steps_ahead=1)\n\n# 4-steps ahead\nyhat_4_steps = model.predict(X=x_valid, y=y_valid, steps_ahead=4)\n</code></pre>      \ud83d\udcda Looking for more details about how steps-ahead prediction works? \u25bc <p>       Check chapter 9 of our companion book for more information on how infinity-steps, n-steps and one-step ahead prediction works:     </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy"},{"location":"getting-started/quickstart-guide/#order-selection","title":"Order Selection","text":"<p>Order selection is a classical approach to automatically determine the optimal model order when using the FROLS algorithm. It helps in identifying the best combination of lags and regressors by evaluating different models based on an information criterion.</p> <p>Important</p> <p>Information criteria are only applicable when using the FROLS algorithm. Other algorithms employ alternative methods for model order selection, each developed to their specific approach.</p> <p>To enable order selection, simply: 1. Set <code>order_selection=True</code>. 2. Specify the desired <code>info_criteria</code> (e.g., <code>\"aic\"</code>, <code>\"aicc\"</code>, <code>\"bic\"</code>, <code>\"fpe\"</code>, or <code>\"lilc\"</code>).</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    order_selection=True,\n    info_criteria=\"bic\"\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>You can control how many regressors are tested during order selection using the <code>n_info_values</code> hyperparameter. The default is <code>15</code>, but you might want to increase it when working with high lag orders or multiple input variables.</p> <pre><code>model = FROLS(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    order_selection=True,\n    info_criteria=\"bic\",\n    n_info_values=50\n)\n</code></pre> <p>Important</p> <p>Increasing <code>n_info_values</code> can improve accuracy but will also increase computational time.</p>"},{"location":"getting-started/quickstart-guide/#narx-neural-network","title":"NARX Neural Network","text":"<p>You can create a Neural NARX Network with SysIdentPy, thanks to its seamless integration with PyTorch. This flexibility allows you to design various Neural NARX architectures by customizing not only the hidden layer configurations and other neural network parameters but also by selecting any available basis function, just like in other NARX representations.</p> <pre><code>from torch import nn\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\n\n\nbasis_function=Polynomial(degree=1)\n\nclass NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(4, 10)\n        self.lin2 = nn.Linear(10, 10)\n        self.lin3 = nn.Linear(10, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n\n\nnarx_net = NARXNN(\n    net=NARX(),\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func='mse_loss',\n    optimizer='Adam',\n    epochs=200,\n    verbose=False,\n    optim_params={'betas': (0.9, 0.999), 'eps': 1e-05} # optional parameters of the optimizer\n)\n\nnarx_net.fit(X=x_train, y=y_train)\nyhat = narx_net.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=1000, figsize=(15, 4))\n</code></pre> <p></p>"},{"location":"getting-started/quickstart-guide/#general-estimators","title":"General Estimators","text":"<p>SysIdentPy also offers the flexibility to integrate any regression method from popular packages like <code>scikit-learn</code>, <code>xgboost</code>, <code>catboost</code>, and many others. To make it work, the estimator simply needs to follow the standard <code>fit</code> and <code>predict</code> API.</p> <p>This significantly expands the range of possible NARX model representations, enabling diverse analyses to help you build the best model for your specific use case.</p> <p>The following example demonstrates how to use a <code>catboost</code> model. Ensure you have <code>catboost</code> installed before running the example.</p> <pre><code>from sysidentpy.general_estimators import NARX\nfrom catboost import CatBoostRegressor\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\n\ncatboost_narx = NARX(\n    base_estimator=CatBoostRegressor(\n        iterations=300,\n        learning_rate=0.1,\n        depth=6),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    fit_params={'verbose': False}\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=200)\n</code></pre> <p></p> <p>To highlight the importance of transforming <code>catboost</code> into a NARX model, the following example shows the performance of <code>catboost</code> without the NARX configuration.</p> <pre><code>catboost = CatBoostRegressor(\n    iterations=300,\n    learning_rate=0.1,\n    depth=6\n)\n\ncatboost.fit(x_train, y_train, verbose=False)\nplot_results(y=y_valid, yhat=catboost.predict(x_valid), figsize=(15, 4))\n</code></pre> <p></p> <p>Note that you can still explore various combinations to better fit your use case. For example, you can create a CatBoost NARX model using a Fourier basis function and perform an n-steps ahead prediction. This flexibility allows you to capture complex seasonality patterns while leveraging CatBoost's powerful gradient boosting capabilities. The same approach applies to any other regression model you choose, enabling you to experiment with different basis functions, prediction horizons, and estimators to find the best configuration for your specific problem.</p> <p>This is just a quickstart guide to SysIdentPy. For more comprehensive tutorials, step-by-step guides, detailed explanations, and advanced use cases, be sure to check out our full documentation and companion book. They provide in-depth content to help you get the most out of SysIdentPy for your system identification and forecasting tasks.</p>"},{"location":"landing-page/about-us/","title":"Project History","text":"<p>The project was started by Wilson R. L. Junior, Luan Pascoal and Samir A. M. Martins as a project for System Identification discipline. We have been working with System Identification for several years (Nonlinear Systems, Machine Learning, Chaotic Systems, Hysteretic models, etc) for several years.</p> <p> Every work we did was using a great tool, but a paid one: Matlab. We started looking for some free alternatives to build NARMAX and its variants (AR, ARX, ARMAX, NAR, NARX, NFIR, Neural NARX, etc.) models using the methods known in System Identification community, but we didn't find any package written in Python with the features we needed to keep doing our research.  Besides, it was always too difficult to find source code of the papers working with NARMAX models and reproduce results was a really hard thing to do.</p> <p> In that context, SysIdentPy was idealized with the following goal: be a free and open source package to help the community design NARMAX models. More than that, be a free and robust alternative to one of the most used tools to build NARMAX models, which is the Matlab's System Identification Toolbox.</p> <p> Samuel joined early in 2019 to help us achieve our goal.</p> <p></p>"},{"location":"landing-page/about-us/#active-maintainers","title":"Active Maintainers","text":"<p>The project is actively maintained by Wilson Rocha Lacerda Junior and looking for contributors.</p>"},{"location":"landing-page/about-us/#citation","title":"Citation","text":"<p>If you use SysIdentPy on your project, please drop me a line.</p> <p>Send email </p> <p>If you use SysIdentPy on your scientific publication, we would appreciate citations to the following paper:</p> <p>Lacerda et al., (2020). SysIdentPy: A Python package for System Identification using NARMAX models. Journal of Open Source Software, 5(54), 2384, https://doi.org/10.21105/joss.02384 <pre><code>    @article{Lacerda2020,\n      doi = {10.21105/joss.02384},\n      url = {https://doi.org/10.21105/joss.02384},\n      year = {2020},\n      publisher = {The Open Journal},\n      volume = {5},\n      number = {54},\n      pages = {2384},\n      author = {Wilson Rocha Lacerda Junior and Luan Pascoal Costa da Andrade and Samuel Carlos Pessoa Oliveira and Samir Angelo Milani Martins},\n      title = {SysIdentPy: A Python package for System Identification using NARMAX models},\n      journal = {Journal of Open Source Software}\n    }\n</code></pre></p>"},{"location":"landing-page/about-us/#inspiration","title":"Inspiration","text":"<p>The documentation and structure (even this section) is openly inspired by sklearn, einsteinpy, and many others as we used (and keep using) them to learn.</p>"},{"location":"landing-page/about-us/#future","title":"Future","text":"<p>SysIdentPy is already useful for many researchers and companies to build NARX models for dynamical systems. But still, there are many improvements and features to come. SysIdentPy has a great future ahead, and your help is greatly appreciated.</p>"},{"location":"landing-page/about-us/#contributors","title":"Contributors","text":""},{"location":"landing-page/sponsor/","title":"Sponsors","text":"<p>As a free and open source project, SysIdentPy relies on the support of the community for its development. If you work for an organization that uses and benefits from SysIdentPy, please consider supporting us.</p> <p>SysIdentPy does not follow the sponsorware release strategy, which means that all features are released to the public at the same time. SysIdentPy is a community driven project, however sponsorships will help to assure its sustainability.</p> <p>The main goal of sponsorships it to make this project sustainable. Your donation goes to support a variety of services and development as they buy the maintainers of this project time to work on the development of new features, bug fixing, stability improvement, issue triage and general support.</p> <p>Read on to learn how to become a sponsor!</p>"},{"location":"landing-page/sponsor/#sponsorships","title":"Sponsorships","text":"<p>Every donation counts and would be greatly appreciated!</p> <p>Sponsorships start as low as $1 a month.<sup>1</sup></p>"},{"location":"landing-page/sponsor/#how-to-become-a-sponsor","title":"How to become a sponsor","text":"<p>Thanks for your interest in sponsoring! In order to become an eligible sponsor with your GitHub account, visit wilsonrljr's sponsor profile, and complete a sponsorship of $1 a month or more. You can use your individual or organization GitHub account for sponsoring.</p> <p> \u00a0 Join our  awesome sponsors</p> <p>If you're in Brazil, you can support me by making a donation via Pix. Just scan the QR code below.</p> <p> </p> <p>Special thanks to our sponsors:</p> <p> Monthly Sponsors</p> <p> </p> <p> Individual Sponsors</p> <p> </p>"},{"location":"landing-page/sponsor/#powered-by","title":"Powered by","text":""},{"location":"landing-page/sponsor/#goals","title":"Goals","text":"<p>The following section lists all funding goals. Each goal contains a list of features prefixed with a checkmark symbol, denoting whether a feature is  already available or  planned, but not yet implemented. When the funding goal is hit, the features are released for general availability.</p>"},{"location":"landing-page/sponsor/#frequently-asked-questions","title":"Frequently asked questions","text":"I don't want to sponsor anymore. Can I cancel my sponsorship? <p>Yes, you can cancel your sponsorship anytime! If you no longer want to sponsor SysIdentPy in GitHub Sponsors, you can request a cancellation which will become effective at the end of the billing cycle. Just remember: sponsorships are non-refundable!</p> <p>If you have any problems or further questions, please reach out to wilsonrljr@outlook.com.</p> We don't want to pay for sponsorship every month. Are there any other options? <p>Yes. You can sponsor on a yearly basis by switching your GitHub account to a yearly billing cycle or just choose an one time donation.</p> <p>If you have any problems or further questions, please reach out to wilsonrljr@outlook.com.</p> <ol> <li> <p>Note that $1 a month is the minimum amount to become a sponsor in Github Sponsor Program.\u00a0\u21a9</p> </li> </ol>"},{"location":"user-guide/overview/","title":"Overview","text":"\ud83e\udde9 Tutorials <p>Follow practical tutorials covering how to build models such as NARX, Neural NARX, NFIR and variants. Learn essential steps, from data preparation to model evaluation, using SysIdentPy\u2019s main features.</p> \ud83d\udcdd Companion Book <p>Looking for more details on NARMAX models? Our book, Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy, covers the theory behind models and methods, while showing how to implement them using SysIdentPy through a wide range of examples and benchmarks.</p> \ud83d\udd17 How To <p>Find practical \"how-to\" guides on a variety of topics, including selecting basis functions, configuring model parameters, performing predictions, and evaluating model performance. Get straightforward solutions for common modeling tasks.</p> \ud83c\udfaf API <p>Explore the complete API reference with detailed documentation of SysIdentPy\u2019s source code. Understand class structures, methods, and parameters to extend and customize functionalities for your projects.</p>"},{"location":"user-guide/API/aols/","title":"Documentation for <code>AOLS</code>","text":"<p>NARMAX Models using the Accelerated Orthogonal Least-Squares algorithm.</p>"},{"location":"user-guide/API/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS","title":"<code>AOLS</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Accelerated Orthogonal Least Squares Algorithm.</p> <p>Build Polynomial NARMAX model using the Accelerated Orthogonal Least-Squares ([1]_). This algorithm is based on the Matlab code available on: https://github.com/realabolfazl/AOLS/</p> <p>The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},     \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>k</code> <code>int</code> <p>The sparsity level.</p> <code>1</code> <code>L</code> <code>int</code> <p>Number of selected indices per iteration.</p> <code>1</code> <code>threshold</code> <code>float</code> <p>The desired accuracy.</p> <code>10e10</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import AOLS\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n...                                                    colored_noise=True,\n...                                                    sigma=0.2,\n...                                                    train_percentage=90)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = AOLS(basis_function=basis_function,\n...              ylag=2, xlag=2\n...              )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale    Sparse Reconstruction    https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311</li> <li>Code:    https://github.com/realabolfazl/AOLS/</li> </ul> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>class AOLS(BaseMSS):\n    r\"\"\"Accelerated Orthogonal Least Squares Algorithm.\n\n    Build Polynomial NARMAX model using the Accelerated Orthogonal Least-Squares ([1]_).\n    This algorithm is based on the Matlab code available on:\n    https://github.com/realabolfazl/AOLS/\n\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},\n        \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    k : int, default=1\n        The sparsity level.\n    L : int, default=1\n        Number of selected indices per iteration.\n    threshold : float, default=10e10\n        The desired accuracy.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import AOLS\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = AOLS(basis_function=basis_function,\n    ...              ylag=2, xlag=2\n    ...              )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale\n       Sparse Reconstruction\n       https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311\n    - Code:\n       https://github.com/realabolfazl/AOLS/\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        k: int = 1,\n        L: int = 1,\n        threshold: float = 10e-10,\n        model_type: str = \"NARMAX\",\n        estimator: Estimators = LeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n    ):\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.xlag = xlag\n        self.ylag = ylag\n        self.max_lag = self._get_max_lag()\n        self.k = k\n        self.L = L\n        self.estimator = estimator\n        self.threshold = threshold\n        self.res = None\n        self.n_inputs = None\n        self.theta = None\n        self.regressor_code = None\n        self.pivv = None\n        self.final_model = None\n        self.n_terms = None\n        self.err = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if not isinstance(self.k, int) or self.k &lt; 1:\n            raise ValueError(f\"k must be integer and &gt; zero. Got {self.k}\")\n\n        if not isinstance(self.L, int) or self.L &lt; 1:\n            raise ValueError(f\"k must be integer and &gt; zero. Got {self.L}\")\n\n        if not isinstance(self.threshold, (int, float)) or self.threshold &lt; 0:\n            raise ValueError(\n                f\"threshold must be integer and &gt; zero. Got {self.threshold}\"\n            )\n\n    def aols(\n        self, psi: np.ndarray, y: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the Accelerated Orthogonal Least-Squares algorithm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = n_samples\n            The target data used in the identification process.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        residual_norm : float\n            The final residual norm.\n\n        References\n        ----------\n        - Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale\n           Sparse Reconstruction\n           https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311\n\n        \"\"\"\n        n, m = psi.shape\n        theta = np.zeros([m, 1])\n        r = y[self.max_lag :].reshape(-1, 1).copy()\n        it = 0\n        max_iter = int(min(self.k, np.floor(n / self.L)))\n        selected_indices = np.zeros(max_iter * self.L)\n        basis_matrix = np.zeros([n, max_iter * self.L])\n        transformed_psi = psi.copy()\n        while norm(r) &gt; self.threshold and it &lt; max_iter:\n            it = it + 1\n            offset = (it - 1) * self.L\n            if it &gt; 1:\n                transformed_psi = transformed_psi - basis_matrix[:, offset].reshape(\n                    -1, 1\n                ) @ (basis_matrix[:, offset].reshape(-1, 1).T @ psi)\n\n            q = ((r.T @ psi) / np.sum(psi * transformed_psi, axis=0)).ravel()\n            contribution = np.sum(transformed_psi**2, axis=0) * (q**2)\n            sub_ind = list(selected_indices[:offset].astype(int))\n            contribution[sub_ind] = 0\n            sorted_indices = np.argsort(contribution)[::-1].ravel()\n            selected_indices[offset : offset + self.L] = sorted_indices[: self.L]\n            for i in range(self.L):\n                temp = (\n                    transformed_psi[:, sorted_indices[i]].reshape(-1, 1)\n                    * q[sorted_indices[i]]\n                )\n                basis_matrix[:, offset + i] = (\n                    temp / np.linalg.norm(temp, axis=0)\n                ).ravel()\n                r = r - temp\n                if i == self.L:\n                    break\n\n                transformed_psi = transformed_psi - basis_matrix[:, offset + i].reshape(\n                    -1, 1\n                ) @ (basis_matrix[:, offset + i].reshape(-1, 1).T @ psi)\n                q = ((r.T @ psi) / np.sum(psi * transformed_psi, axis=0)).ravel()\n\n        selected_indices = selected_indices[selected_indices &gt; 0].ravel().astype(int)\n        residual_norm = norm(r)\n        theta[selected_indices] = self.estimator.optimize(\n            psi[:, selected_indices], y[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if self.L &gt; 1:\n            sorted_indices = np.argsort(np.abs(theta))[::-1]\n            selected_indices = sorted_indices[: self.k].ravel().astype(int)\n            theta[selected_indices] = self.estimator.optimize(\n                psi[:, selected_indices], y[self.max_lag :, 0].reshape(-1, 1)\n            )\n            residual_norm = norm(\n                y[self.max_lag :].reshape(-1, 1)\n                - psi[:, selected_indices] @ theta[selected_indices]\n            )\n\n        pivv = np.argwhere(theta.ravel() != 0).ravel()\n        theta = theta[theta != 0]\n        return theta.reshape(-1, 1), pivv, residual_norm\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: Optional[np.ndarray] = None):\n        \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n        The 'fit' function allows a friendly usage by the user.\n        Given two arguments, x and y, fit training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n        if X is not None:\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        (self.theta, self.pivv, self.res) = self.aols(reg_matrix, y)\n        repetition = len(reg_matrix)\n        if isinstance(self.basis_function, Polynomial):\n            self.final_model = self.regressor_code[self.pivv, :].copy()\n        else:\n            self.regressor_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.final_model = self.regressor_code[self.pivv, :].copy()\n\n        self.n_terms = len(\n            self.theta\n        )  # the number of terms we selected (necessary in the 'results' methods)\n        self.err = self.n_terms * [\n            0\n        ]  # just to use the `results` method. Will be changed in future updates.\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: Optional[np.ndarray] = None,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(\n        self, x: Optional[np.ndarray], y: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n        x_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(\n        self, x: Optional[np.ndarray], y_initial: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = super()._basis_function_predict(x, y_initial, forecast_horizon)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = super()._basis_function_n_step_prediction(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        yhat = super()._basis_function_n_steps_horizon(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.aols","title":"<code>aols(psi, y)</code>","text":"<p>Perform the Accelerated Orthogonal Least-Squares algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape = n_samples</code> <p>The target data used in the identification process.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>residual_norm</code> <code>float</code> <p>The final residual norm.</p> References <ul> <li>Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale    Sparse Reconstruction    https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311</li> </ul> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>def aols(\n    self, psi: np.ndarray, y: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the Accelerated Orthogonal Least-Squares algorithm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = n_samples\n        The target data used in the identification process.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    residual_norm : float\n        The final residual norm.\n\n    References\n    ----------\n    - Manuscript: Accelerated Orthogonal Least-Squares for Large-Scale\n       Sparse Reconstruction\n       https://www.sciencedirect.com/science/article/abs/pii/S1051200418305311\n\n    \"\"\"\n    n, m = psi.shape\n    theta = np.zeros([m, 1])\n    r = y[self.max_lag :].reshape(-1, 1).copy()\n    it = 0\n    max_iter = int(min(self.k, np.floor(n / self.L)))\n    selected_indices = np.zeros(max_iter * self.L)\n    basis_matrix = np.zeros([n, max_iter * self.L])\n    transformed_psi = psi.copy()\n    while norm(r) &gt; self.threshold and it &lt; max_iter:\n        it = it + 1\n        offset = (it - 1) * self.L\n        if it &gt; 1:\n            transformed_psi = transformed_psi - basis_matrix[:, offset].reshape(\n                -1, 1\n            ) @ (basis_matrix[:, offset].reshape(-1, 1).T @ psi)\n\n        q = ((r.T @ psi) / np.sum(psi * transformed_psi, axis=0)).ravel()\n        contribution = np.sum(transformed_psi**2, axis=0) * (q**2)\n        sub_ind = list(selected_indices[:offset].astype(int))\n        contribution[sub_ind] = 0\n        sorted_indices = np.argsort(contribution)[::-1].ravel()\n        selected_indices[offset : offset + self.L] = sorted_indices[: self.L]\n        for i in range(self.L):\n            temp = (\n                transformed_psi[:, sorted_indices[i]].reshape(-1, 1)\n                * q[sorted_indices[i]]\n            )\n            basis_matrix[:, offset + i] = (\n                temp / np.linalg.norm(temp, axis=0)\n            ).ravel()\n            r = r - temp\n            if i == self.L:\n                break\n\n            transformed_psi = transformed_psi - basis_matrix[:, offset + i].reshape(\n                -1, 1\n            ) @ (basis_matrix[:, offset + i].reshape(-1, 1).T @ psi)\n            q = ((r.T @ psi) / np.sum(psi * transformed_psi, axis=0)).ravel()\n\n    selected_indices = selected_indices[selected_indices &gt; 0].ravel().astype(int)\n    residual_norm = norm(r)\n    theta[selected_indices] = self.estimator.optimize(\n        psi[:, selected_indices], y[self.max_lag :, 0].reshape(-1, 1)\n    )\n    if self.L &gt; 1:\n        sorted_indices = np.argsort(np.abs(theta))[::-1]\n        selected_indices = sorted_indices[: self.k].ravel().astype(int)\n        theta[selected_indices] = self.estimator.optimize(\n            psi[:, selected_indices], y[self.max_lag :, 0].reshape(-1, 1)\n        )\n        residual_norm = norm(\n            y[self.max_lag :].reshape(-1, 1)\n            - psi[:, selected_indices] @ theta[selected_indices]\n        )\n\n    pivv = np.argwhere(theta.ravel() != 0).ravel()\n    theta = theta[theta != 0]\n    return theta.reshape(-1, 1), pivv, residual_norm\n</code></pre>"},{"location":"user-guide/API/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Fit polynomial NARMAX model using AOLS algorithm.</p> <p>The 'fit' function allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>info_values</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: Optional[np.ndarray] = None):\n    \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n    The 'fit' function allows a friendly usage by the user.\n    Given two arguments, x and y, fit training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n    reg_matrix = self.basis_function.fit(\n        lagged_data,\n        self.max_lag,\n        self.ylag,\n        self.xlag,\n        self.model_type,\n        predefined_regressors=None,\n    )\n\n    if X is not None:\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    (self.theta, self.pivv, self.res) = self.aols(reg_matrix, y)\n    repetition = len(reg_matrix)\n    if isinstance(self.basis_function, Polynomial):\n        self.final_model = self.regressor_code[self.pivv, :].copy()\n    else:\n        self.regressor_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.final_model = self.regressor_code[self.pivv, :].copy()\n\n    self.n_terms = len(\n        self.theta\n    )  # the number of terms we selected (necessary in the 'results' methods)\n    self.err = self.n_terms * [\n        0\n    ]  # just to use the `results` method. Will be changed in future updates.\n    return self\n</code></pre>"},{"location":"user-guide/API/aols/#sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=0)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/accelerated_orthogonal_least_squares.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: Optional[np.ndarray] = None,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/basis-function/","title":"Documentation for <code>Basis Functions</code>","text":"<p>Bersntein Basis Function for NARMAX models.</p> <p>Bilinear Basis Function for NARMAX models.</p> <p>Fourier Basis Function for NARMAX models.</p> <p>Legendre Basis Function for NARMAX models.</p> <p>Polynomial Basis Function for NARMAX models.</p> <p>Hermite Basis Function for NARMAX models.</p> <p>Hermite Basis Function for NARMAX models.</p> <p>Laguerre Basis Function for NARMAX models.</p>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bernstein.Bernstein","title":"<code>Bernstein</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Bersntein basis function.</p> <p>Generate Bernstein basis functions.</p> <p>This class constructs a new feature matrix consisting of Bernstein basis functions for a given degree. Bernstein polynomials are useful in numerical analysis, curve fitting, and approximation theory due to their smoothness and the ability to approximate any continuous function on a closed interval.</p> <p>The Bernstein polynomial of degree \\(n\\) for a variable \\(x\\) is defined as:</p> \\[     B_{i,n}(x) = \\binom{n}{i} x^i (1 - x)^{n - i} \\quad \\text{for} \\quad i = 0, 1,     \\ldots, n \\] <p>where \\(\\binom{n}{i}\\) is the binomial coefficient, given by:</p> \\[     \\binom{n}{i} = \\frac{n!}{i! (n - i)!} \\] <p>Bernstein polynomials form a basis for the space of polynomials of degree at most \\(n\\). They are particularly useful in approximation theory because they can approximate any continuous function on the interval \\([0, 1]\\) as \\(n\\) increases.</p> <p>Be aware that the number of features in the output array scales significantly with the number of inputs, the maximum lag of the input, and the polynomial degree.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int(max_degree)</code> <p>The maximum degree of the polynomial features.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix. deprecated in v.0.5.0    <code>bias</code> is deprecated in 0.5.0 and will be removed in 0.6.0.    Use <code>include_bias</code> instead.</p> <code>True</code> <code>n</code> <code>int</code> <p>The maximum degree of the bersntein polynomial features. deprecated in v.0.5.0    <code>n</code> is deprecated in 0.5.0 and will be removed in 0.6.0.    Use <code>degree</code> instead.</p> <code>1</code> Notes <p>Be aware that the number of features in the output array scales significantly as the number of inputs, the max lag of the input and output.</p> References <ul> <li>Blog: this method is based on the content provided by Alex Shtoff in his blog.     The content is easy to follow and every user is referred to is blog to check     not only the Bersntein method, but also different topics that Alex discuss     there!     https://alexshtf.github.io/2024/01/21/Bernstein.html</li> <li>Wikipedia: Bernstein polynomial     https://en.wikipedia.org/wiki/Bernstein_polynomial</li> </ul> Source code in <code>sysidentpy/basis_function/_bernstein.py</code> <pre><code>@deprecated(\n    version=\"v0.5.0\",\n    future_version=\"v1.0.0\",\n    message=(\n        \" `bias` and `n` are deprecated in 0.5.0 and will be removed in 1.0.0.\"\n        \" Use `include_bias` and `degree`, respectively, instead.\"\n    ),\n)\nclass Bernstein(BaseBasisFunction):\n    r\"\"\"Build Bersntein basis function.\n\n    Generate Bernstein basis functions.\n\n    This class constructs a new feature matrix consisting of Bernstein basis functions\n    for a given degree. Bernstein polynomials are useful in numerical analysis, curve\n    fitting, and approximation theory due to their smoothness and the ability to\n    approximate any continuous function on a closed interval.\n\n    The Bernstein polynomial of degree \\(n\\) for a variable \\(x\\) is defined as:\n\n    $$\n        B_{i,n}(x) = \\binom{n}{i} x^i (1 - x)^{n - i} \\quad \\text{for} \\quad i = 0, 1,\n        \\ldots, n\n    $$\n\n    where \\(\\binom{n}{i}\\) is the binomial coefficient, given by:\n\n    $$\n        \\binom{n}{i} = \\frac{n!}{i! (n - i)!}\n    $$\n\n    Bernstein polynomials form a basis for the space of polynomials of degree at most\n    \\(n\\). They are particularly useful in approximation theory because they can\n    approximate any continuous function on the interval \\([0, 1]\\) as \\(n\\) increases.\n\n    Be aware that the number of features in the output array scales significantly with\n    the number of inputs, the maximum lag of the input, and the polynomial degree.\n\n    Parameters\n    ----------\n    degree : int (max_degree), default=1\n        The maximum degree of the polynomial features.\n    bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n        deprecated in v.0.5.0\n           `bias` is deprecated in 0.5.0 and will be removed in 0.6.0.\n           Use `include_bias` instead.\n    n : int, default=1\n        The maximum degree of the bersntein polynomial features.\n        deprecated in v.0.5.0\n           `n` is deprecated in 0.5.0 and will be removed in 0.6.0.\n           Use `degree` instead.\n\n    Notes\n    -----\n    Be aware that the number of features in the output array scales\n    significantly as the number of inputs, the max lag of the input and output.\n\n    References\n    ----------\n    - Blog: this method is based on the content provided by Alex Shtoff in his blog.\n        The content is easy to follow and every user is referred to is blog to check\n        not only the Bersntein method, but also different topics that Alex discuss\n        there!\n        https://alexshtf.github.io/2024/01/21/Bernstein.html\n    - Wikipedia: Bernstein polynomial\n        https://en.wikipedia.org/wiki/Bernstein_polynomial\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        n: Optional[int] = None,\n        bias: Optional[bool] = None,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        if n is not None:\n            self.degree = n\n        else:\n            self.degree = degree\n\n        if bias is not None:\n            self.include_bias = bias\n        else:\n            self.include_bias = include_bias\n\n        self.ensemble = ensemble\n\n    def _bernstein_expansion(self, data: np.ndarray):\n        k = np.arange(1 + self.degree)\n        base = binom.pmf(k, self.degree, data[:, None])\n        return base\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because the data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._bernstein_expansion(data[:, col]) for col in range(n_features)]\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Bersntein Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bernstein.Bernstein.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Bersntein Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_bernstein.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Bersntein Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bilinear.Bilinear","title":"<code>Bilinear</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Bilinear basis function.</p> <p>A general bilinear input-output model takes the form</p> \\[     y(k) = a_0 + \\sum_{i=1}^{n_y} a_i y(k-i) + \\sum_{i=1}^{n_u} b_i u(k-i) +     \\sum_{i=1}^{n_y} \\sum_{j=1}^{n_u} c_{ij} y(k-i) u(k-j) \\] <p>This is a special case of the Polynomial NARMAX model.</p> <p>Bilinear system theory has been widely studied and it plays an important role in the context of continuous-time systems.  This is because, roughly speaking, the set of bilinear systems is dense in the space of continuous-time systems and any continuous causal functional can be arbitrarily well approximated by bilinear systems within any bounded time interval (see for example Fliess and Normand-Cyrot 1982). Moreover, many real continuous-time processes are naturally in bilinear form. A few examples are distillation columns (Espa\u00f1a and Landau 1978), nuclear and thermal control processes (Mohler 1973).</p> <p>Sampling the continuous-time bilinear system, however, produces a NARMAX model which is more complex than a discrete-time bilinear model.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int(max_degree)</code> <p>The maximum degree of the polynomial features.</p> <code>2</code> Notes <p>Be aware that the number of features in the output array scales significantly as the number of inputs, the max lag of the input and output, and degree increases. High degrees can cause overfitting.</p> Source code in <code>sysidentpy/basis_function/_bilinear.py</code> <pre><code>class Bilinear(BaseBasisFunction):\n    r\"\"\"Build Bilinear basis function.\n\n    A general bilinear input-output model takes the form\n\n    $$\n        y(k) = a_0 + \\sum_{i=1}^{n_y} a_i y(k-i) + \\sum_{i=1}^{n_u} b_i u(k-i) +\n        \\sum_{i=1}^{n_y} \\sum_{j=1}^{n_u} c_{ij} y(k-i) u(k-j)\n    $$\n\n    This is a special case of the Polynomial NARMAX model.\n\n    Bilinear system theory has been widely studied and it plays an important role in the\n    context of continuous-time systems.  This is because, roughly speaking, the set of\n    bilinear systems is dense in the space of continuous-time systems and any continuous\n    causal functional can be arbitrarily well approximated by bilinear systems within\n    any bounded time interval (see for example Fliess and Normand-Cyrot 1982). Moreover,\n    many real continuous-time processes are naturally in bilinear form. A few examples\n    are distillation columns (Espa\u00f1a and Landau 1978), nuclear and thermal control\n    processes (Mohler 1973).\n\n    Sampling the continuous-time bilinear system, however, produces a NARMAX model\n    which is more complex than a discrete-time bilinear model.\n\n    Parameters\n    ----------\n    degree : int (max_degree), default=2\n        The maximum degree of the polynomial features.\n\n    Notes\n    -----\n    Be aware that the number of features in the output array scales\n    significantly as the number of inputs, the max lag of the input and output, and\n    degree increases. High degrees can cause overfitting.\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 2,\n    ):\n        self.degree = degree\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build the Bilinear information matrix.\n\n        Each column of the information matrix represents a candidate\n        regressor. The set of candidate regressors are based on xlag,\n        ylag, and degree defined by the user.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Target data used on training phase.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors : ndarray of int\n            The index of the selected regressors by the Model Structure\n            Selection algorithm.\n\n        Returns\n        -------\n        psi = ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        \"\"\"\n        # Create combinations of all columns based on its index\n        iterable_list = range(data.shape[1])\n        combination_list = list(\n            combinations_with_replacement(iterable_list, self.degree)\n        )\n        if self.degree == 1:\n            warnings.warn(\n                \"You choose a bilinear basis function and nonlinear degree = 1.\"\n                \"In this case, you have a linear polynomial model.\",\n                stacklevel=2,\n            )\n\n        ny = get_max_ylag(ylag)\n        combination_ylag = list(\n            combinations_with_replacement(list(range(1, ny + 1)), self.degree)\n        )\n        if isinstance(xlag, int):\n            xlag = [xlag]\n\n        combination_xlag = []\n        ni = 0\n        for lag in xlag:\n            nx = get_max_xlag(lag)\n            combination_lag = list(\n                combinations_with_replacement(\n                    list(range(ny + 1 + ni, nx + ny + 1 + ni)), self.degree\n                )\n            )\n            combination_xlag.append(combination_lag)\n            ni += nx\n\n        combination_xlag = list(chain.from_iterable(combination_xlag))\n        combinations_xy = combination_xlag + combination_ylag\n        combination_list = list(set(combination_list) - set(combinations_xy))\n\n        if predefined_regressors is not None:\n            combination_list = [\n                combination_list[index] for index in predefined_regressors\n            ]\n\n        psi = np.column_stack(\n            [\n                np.prod(data[:, combination_list[i]], axis=1)\n                for i in range(len(combination_list))\n            ]\n        )\n        psi = psi[max_lag:, :]\n        return psi\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Polynomial Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bilinear.Bilinear.fit","title":"<code>fit(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build the Bilinear information matrix.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree defined by the user.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Target data used on training phase.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>ndarray of int</code> <p>The index of the selected regressors by the Model Structure Selection algorithm.</p> <code>None</code> <p>Returns:</p> Type Description <code>psi = ndarray of floats</code> <p>The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/basis_function/_bilinear.py</code> <pre><code>def fit(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build the Bilinear information matrix.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree defined by the user.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Target data used on training phase.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors : ndarray of int\n        The index of the selected regressors by the Model Structure\n        Selection algorithm.\n\n    Returns\n    -------\n    psi = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # Create combinations of all columns based on its index\n    iterable_list = range(data.shape[1])\n    combination_list = list(\n        combinations_with_replacement(iterable_list, self.degree)\n    )\n    if self.degree == 1:\n        warnings.warn(\n            \"You choose a bilinear basis function and nonlinear degree = 1.\"\n            \"In this case, you have a linear polynomial model.\",\n            stacklevel=2,\n        )\n\n    ny = get_max_ylag(ylag)\n    combination_ylag = list(\n        combinations_with_replacement(list(range(1, ny + 1)), self.degree)\n    )\n    if isinstance(xlag, int):\n        xlag = [xlag]\n\n    combination_xlag = []\n    ni = 0\n    for lag in xlag:\n        nx = get_max_xlag(lag)\n        combination_lag = list(\n            combinations_with_replacement(\n                list(range(ny + 1 + ni, nx + ny + 1 + ni)), self.degree\n            )\n        )\n        combination_xlag.append(combination_lag)\n        ni += nx\n\n    combination_xlag = list(chain.from_iterable(combination_xlag))\n    combinations_xy = combination_xlag + combination_ylag\n    combination_list = list(set(combination_list) - set(combinations_xy))\n\n    if predefined_regressors is not None:\n        combination_list = [\n            combination_list[index] for index in predefined_regressors\n        ]\n\n    psi = np.column_stack(\n        [\n            np.prod(data[:, combination_list[i]], axis=1)\n            for i in range(len(combination_list))\n        ]\n    )\n    psi = psi[max_lag:, :]\n    return psi\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._bilinear.Bilinear.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Polynomial Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_bilinear.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Polynomial Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._fourier.Fourier","title":"<code>Fourier</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Fourier basis function.</p> <p>Generate a new feature matrix consisting of all Fourier features with respect to the number of harmonics.</p> <p>The Fourier expansion is given by:</p> <p>If you set \\(\\mathcal{F}\\) as the Fourier extension</p> \\[ \\mathcal{F}(x) = [\\cos(\\pi x), \\sin(\\pi x), \\cos(2\\pi x), \\sin(2\\pi x), \\ldots, \\cos(N\\pi x), \\sin(N\\pi x)] \\] <p>In this case, the Fourier ARX representation will be:</p> \\[\\begin{aligned} y_k = &amp;\\Big[ \\cos(\\pi y_{k-1}), \\sin(\\pi y_{k-1}), \\cos(2\\pi y_{k-1}), \\sin(2\\pi y_{k-1}), \\ldots, \\cos(N\\pi y_{k-1}), \\sin(N\\pi y_{k-1}), \\\\ &amp;\\ \\ \\cos(\\pi y_{k-2}), \\sin(\\pi y_{k-2}), \\ldots, \\cos(N\\pi y_{k-n_y}), \\sin(N\\pi y_{k-n_y}), \\\\ &amp;\\ \\ \\cos(\\pi x_{k-1}), \\sin(\\pi x_{k-1}), \\cos(2\\pi x_{k-1}), \\sin(2\\pi x_{k-1}), \\ldots, \\cos(N\\pi x_{k-n_x}), \\sin(N\\pi x_{k-n_x}) \\Big] \\\\ &amp;\\ \\ + e_k \\end{aligned}\\] <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int(max_degree)</code> <p>The maximum degree of the polynomial features.</p> <code>2</code> Notes <p>Be aware that the number of features in the output array scales significantly as the number of inputs, the max lag of the input and output.</p> Source code in <code>sysidentpy/basis_function/_fourier.py</code> <pre><code>class Fourier(BaseBasisFunction):\n    r\"\"\"Build Fourier basis function.\n\n    Generate a new feature matrix consisting of all Fourier features\n    with respect to the number of harmonics.\n\n    The Fourier expansion is given by:\n\n    If you set $\\mathcal{F}$ as the Fourier extension\n\n    $$\n    \\mathcal{F}(x) = [\\cos(\\pi x), \\sin(\\pi x), \\cos(2\\pi x), \\sin(2\\pi x), \\ldots,\n    \\cos(N\\pi x), \\sin(N\\pi x)]\n    $$\n\n    In this case, the Fourier ARX representation will be:\n\n    \\begin{aligned}\n    y_k = &amp;\\Big[ \\cos(\\pi y_{k-1}), \\sin(\\pi y_{k-1}), \\cos(2\\pi y_{k-1}),\n    \\sin(2\\pi y_{k-1}), \\ldots, \\cos(N\\pi y_{k-1}), \\sin(N\\pi y_{k-1}), \\\\\n    &amp;\\ \\ \\cos(\\pi y_{k-2}), \\sin(\\pi y_{k-2}), \\ldots, \\cos(N\\pi y_{k-n_y}),\n    \\sin(N\\pi y_{k-n_y}), \\\\\n    &amp;\\ \\ \\cos(\\pi x_{k-1}), \\sin(\\pi x_{k-1}), \\cos(2\\pi x_{k-1}), \\sin(2\\pi x_{k-1}),\n    \\ldots, \\cos(N\\pi x_{k-n_x}), \\sin(N\\pi x_{k-n_x}) \\Big] \\\\\n    &amp;\\ \\ + e_k\n    \\end{aligned}\n\n    Parameters\n    ----------\n    degree : int (max_degree), default=2\n        The maximum degree of the polynomial features.\n\n    Notes\n    -----\n    Be aware that the number of features in the output array scales\n    significantly as the number of inputs, the max lag of the input and output.\n\n    \"\"\"\n\n    def __init__(\n        self, n: int = 1, p: float = 2 * np.pi, degree: int = 1, ensemble: bool = True\n    ):\n        self.n = n\n        self.p = p\n        self.degree = degree\n        self.ensemble = ensemble\n\n    def _fourier_expansion(self, data: np.ndarray, n: int):\n        base = np.column_stack(\n            [\n                np.cos(2 * np.pi * data * n / self.p),\n                np.sin(2 * np.pi * data * n / self.p),\n            ]\n        )\n        return base\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build the Polynomial information matrix.\n\n        Each column of the information matrix represents a candidate\n        regressor. The set of candidate regressors are based on xlag,\n        ylag, and degree defined by the user.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Target data used on training phase.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors : ndarray of int\n            The index of the selected regressors by the Model Structure\n            Selection algorithm.\n\n        Returns\n        -------\n        psi = ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        \"\"\"\n        # remove intercept (because the data always have the intercept)\n        if self.degree &gt; 1:\n            data = Polynomial().fit(\n                data, max_lag, ylag, xlag, model_type, predefined_regressors=None\n            )\n            data = data[:, 1:]\n        else:\n            data = data[max_lag:, 1:]\n\n        columns = list(range(data.shape[1]))\n        harmonics = list(range(1, self.n + 1))\n        psi = np.zeros([len(data), 1])\n\n        for col in columns:\n            base_col = np.column_stack(\n                [self._fourier_expansion(data[:, col], h) for h in harmonics]\n            )\n            psi = np.column_stack([psi, base_col])\n\n        if self.ensemble:\n            psi = psi[:, 1:]\n            psi = np.column_stack([data, psi])\n        else:\n            psi = psi[:, 1:]\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Fourier Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._fourier.Fourier.fit","title":"<code>fit(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build the Polynomial information matrix.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree defined by the user.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Target data used on training phase.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>ndarray of int</code> <p>The index of the selected regressors by the Model Structure Selection algorithm.</p> <code>None</code> <p>Returns:</p> Type Description <code>psi = ndarray of floats</code> <p>The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/basis_function/_fourier.py</code> <pre><code>def fit(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build the Polynomial information matrix.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree defined by the user.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Target data used on training phase.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors : ndarray of int\n        The index of the selected regressors by the Model Structure\n        Selection algorithm.\n\n    Returns\n    -------\n    psi = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # remove intercept (because the data always have the intercept)\n    if self.degree &gt; 1:\n        data = Polynomial().fit(\n            data, max_lag, ylag, xlag, model_type, predefined_regressors=None\n        )\n        data = data[:, 1:]\n    else:\n        data = data[max_lag:, 1:]\n\n    columns = list(range(data.shape[1]))\n    harmonics = list(range(1, self.n + 1))\n    psi = np.zeros([len(data), 1])\n\n    for col in columns:\n        base_col = np.column_stack(\n            [self._fourier_expansion(data[:, col], h) for h in harmonics]\n        )\n        psi = np.column_stack([psi, base_col])\n\n    if self.ensemble:\n        psi = psi[:, 1:]\n        psi = np.column_stack([data, psi])\n    else:\n        psi = psi[:, 1:]\n\n    if predefined_regressors is None:\n        return psi\n\n    return psi[:, predefined_regressors]\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._fourier.Fourier.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Fourier Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_fourier.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Fourier Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._legendre.Legendre","title":"<code>Legendre</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Legendre basis function expansion.</p> <p>This class constructs a feature matrix consisting of Legendre polynomial basis functions up to a specified degree. Legendre polynomials, denoted by \\(P_n(x)\\), are orthogonal polynomials over the interval \\([-1, 1]\\) with respect to the uniform weight function \\(w(x) = 1\\). They are widely used in numerical analysis, curve fitting, and approximation theory.</p> <p>The Legendre polynomial \\(P_n(x)\\) of degree \\(n\\) is defined by the following recurrence relation:</p> \\[ P_0(x) = 1 \\] \\[ P_1(x) = x \\] \\[ (n+1) P_{n+1}(x) = (2n + 1)x P_n(x) - n P_{n-1}(x) \\] <p>where \\(P_n(x)\\) represents the Legendre polynomial of degree \\(n\\).</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>The maximum degree of the Legendre polynomial basis functions to be generated.</p> <code>2</code> <code>include_bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix.</p> <code>True</code> <code>ensemble</code> <code>bool</code> <p>If True, the original data is concatenated with the polynomial features.</p> <code>False</code> Notes <p>The number of features in the output matrix increases as the degree of the polynomial increases, which can lead to a high-dimensional feature space. Consider using dimensionality reduction techniques if overfitting becomes an issue.</p> References <ul> <li>Wikipedia: Legendre polynomial     https://en.wikipedia.org/wiki/Legendre_polynomials</li> </ul> Source code in <code>sysidentpy/basis_function/_legendre.py</code> <pre><code>class Legendre(BaseBasisFunction):\n    r\"\"\"Build Legendre basis function expansion.\n\n    This class constructs a feature matrix consisting of Legendre polynomial basis\n    functions up to a specified degree. Legendre polynomials, denoted by $P_n(x)$,\n    are orthogonal polynomials over the interval $[-1, 1]$ with respect to the\n    uniform weight function $w(x) = 1$. They are widely used in numerical analysis,\n    curve fitting, and approximation theory.\n\n    The Legendre polynomial $P_n(x)$ of degree $n$ is defined by the following\n    recurrence relation:\n\n    $$\n    P_0(x) = 1\n    $$\n\n    $$\n    P_1(x) = x\n    $$\n\n    $$\n    (n+1) P_{n+1}(x) = (2n + 1)x P_n(x) - n P_{n-1}(x)\n    $$\n\n    where $P_n(x)$ represents the Legendre polynomial of degree $n$.\n\n    Parameters\n    ----------\n    degree : int, default=2\n        The maximum degree of the Legendre polynomial basis functions to be generated.\n\n    include_bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n\n    ensemble : bool, default=False\n        If True, the original data is concatenated with the polynomial features.\n\n    Notes\n    -----\n    The number of features in the output matrix increases as the degree of the\n    polynomial increases, which can lead to a high-dimensional feature space.\n    Consider using dimensionality reduction techniques if overfitting becomes an issue.\n\n    References\n    ----------\n    - Wikipedia: Legendre polynomial\n        https://en.wikipedia.org/wiki/Legendre_polynomials\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        self.degree = degree\n        self.include_bias = include_bias\n        self.ensemble = ensemble\n\n    def _legendre_expansion(self, data: np.ndarray):\n        num_samples = data.shape[0]\n        basis = np.zeros((num_samples, self.degree + 1))\n        for n in range(self.degree + 1):\n            basis[:, n] = eval_legendre(n, data)\n        return basis\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._legendre_expansion(data[:, col]) for col in range(n_features)]\n        # remove P0(x) = 1 from every column expansion\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Bersntein Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._legendre.Legendre.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Bersntein Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_legendre.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Bersntein Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._polynomial.Polynomial","title":"<code>Polynomial</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build polynomial basis function.</p> <p>Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree.</p> \\[     y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}u_{k-j}^{b_i, j}     \\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m} \\] <p>where \\(p\\) is the number of regressors, \\(\\Theta_i\\) are the model parameters, and \\(a_i, m, b_i, j\\) and \\(d_i, l \\in \\mathbb{N}\\) are the exponents of the output, input and noise terms, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int(max_degree)</code> <p>The maximum degree of the polynomial features.</p> <code>2</code> Notes <p>Be aware that the number of features in the output array scales significantly as the number of inputs, the max lag of the input and output, and degree increases. High degrees can cause overfitting.</p> Source code in <code>sysidentpy/basis_function/_polynomial.py</code> <pre><code>class Polynomial(BaseBasisFunction):\n    r\"\"\"Build polynomial basis function.\n\n    Generate a new feature matrix consisting of all polynomial combinations\n    of the features with degree less than or equal to the specified degree.\n\n    $$\n        y_k = \\sum_{i=1}^{p}\\Theta_i \\times \\prod_{j=0}^{n_x}u_{k-j}^{b_i, j}\n        \\prod_{l=1}^{n_e}e_{k-l}^{d_i, l}\\prod_{m=1}^{n_y}y_{k-m}^{a_i, m}\n    $$\n\n    where $p$ is the number of regressors, $\\Theta_i$ are the\n    model parameters, and $a_i, m, b_i, j$ and $d_i, l \\in \\mathbb{N}$\n    are the exponents of the output, input and noise terms, respectively.\n\n    Parameters\n    ----------\n    degree : int (max_degree), default=2\n        The maximum degree of the polynomial features.\n\n    Notes\n    -----\n    Be aware that the number of features in the output array scales\n    significantly as the number of inputs, the max lag of the input and output, and\n    degree increases. High degrees can cause overfitting.\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 2,\n    ):\n        self.degree = degree\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build the Polynomial information matrix.\n\n        Each columns of the information matrix represents a candidate\n        regressor. The set of candidate regressors are based on xlag,\n        ylag, and degree defined by the user.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Target data used on training phase.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors : ndarray of int\n            The index of the selected regressors by the Model Structure\n            Selection algorithm.\n\n        Returns\n        -------\n        psi = ndarray of floats\n            The lagged matrix built in respect with each lag and column.\n\n        \"\"\"\n        # Create combinations of all columns based on its index\n        iterable_list = range(data.shape[1])\n        combination_list = list(\n            combinations_with_replacement(iterable_list, self.degree)\n        )\n        if predefined_regressors is not None:\n            combination_list = [\n                combination_list[index] for index in predefined_regressors\n            ]\n\n        psi = np.column_stack(\n            [\n                np.prod(data[:, combination_list[i]], axis=1)\n                for i in range(len(combination_list))\n            ]\n        )\n        psi = psi[max_lag:, :]\n        return psi\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Polynomial Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._polynomial.Polynomial.fit","title":"<code>fit(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build the Polynomial information matrix.</p> <p>Each columns of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree defined by the user.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Target data used on training phase.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>ndarray of int</code> <p>The index of the selected regressors by the Model Structure Selection algorithm.</p> <code>None</code> <p>Returns:</p> Type Description <code>psi = ndarray of floats</code> <p>The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/basis_function/_polynomial.py</code> <pre><code>def fit(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build the Polynomial information matrix.\n\n    Each columns of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree defined by the user.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Target data used on training phase.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors : ndarray of int\n        The index of the selected regressors by the Model Structure\n        Selection algorithm.\n\n    Returns\n    -------\n    psi = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # Create combinations of all columns based on its index\n    iterable_list = range(data.shape[1])\n    combination_list = list(\n        combinations_with_replacement(iterable_list, self.degree)\n    )\n    if predefined_regressors is not None:\n        combination_list = [\n            combination_list[index] for index in predefined_regressors\n        ]\n\n    psi = np.column_stack(\n        [\n            np.prod(data[:, combination_list[i]], axis=1)\n            for i in range(len(combination_list))\n        ]\n    )\n    psi = psi[max_lag:, :]\n    return psi\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._polynomial.Polynomial.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Polynomial Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_polynomial.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Polynomial Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._hermite.Hermite","title":"<code>Hermite</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Hermite basis function expansion.</p> <p>This class constructs a feature matrix consisting of Hermite polynomial basis functions up to a specified degree. Hermite polynomials, denoted by \\(H_n(x)\\), are orthogonal polynomials over the interval \\((-\\infty, \\infty)\\) with respect to the weight function \\(w(x) = e^{-x^2}\\). These polynomials are widely used in probability theory, quantum mechanics, and numerical analysis, particularly in solving the quantum harmonic oscillator and in the field of statistics.</p> <p>Physicist's Hermite polynomials \\(H_n(x)\\), often used in physics: $$ H_n(x) = (-1)^n e<sup>{x</sup>2} \\frac{d<sup>n}{dx</sup>n} e<sup>{-x</sup>2} $$</p> <p>The Hermite polynomial \\(H_n(x)\\) of degree \\(n\\) can be also defined by the following recurrence relation:</p> \\[ H_0(x) = 1 \\] \\[ H_1(x) = 2x \\] \\[ H_{n+1}(x) = 2x H_n(x) - 2n H_{n-1}(x) \\] <p>where \\(H_n(x)\\) represents the Hermite polynomial of degree \\(n\\).</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>The maximum degree of the Hermite polynomial basis functions to be generated.</p> <code>2</code> <code>include_bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix.</p> <code>True</code> <code>ensemble</code> <code>bool</code> <p>If True, the original data is concatenated with the polynomial features.</p> <code>False</code> Notes <p>The number of features in the output matrix increases as the degree of the polynomial increases, which can lead to a high-dimensional feature space. Consider using dimensionality reduction techniques if overfitting becomes an issue.</p> References <ul> <li>Wikipedia: Hermite polynomial     https://en.wikipedia.org/wiki/Hermite_polynomials</li> <li>Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_hermite.html</li> </ul> Source code in <code>sysidentpy/basis_function/_hermite.py</code> <pre><code>class Hermite(BaseBasisFunction):\n    r\"\"\"Build Hermite basis function expansion.\n\n    This class constructs a feature matrix consisting of Hermite polynomial basis\n    functions up to a specified degree. Hermite polynomials, denoted by $H_n(x)$,\n    are orthogonal polynomials over the interval $(-\\infty, \\infty)$ with respect\n    to the weight function $w(x) = e^{-x^2}$. These polynomials are widely used in\n    probability theory, quantum mechanics, and numerical analysis, particularly in\n    solving the quantum harmonic oscillator and in the field of statistics.\n\n    **Physicist's Hermite polynomials** $H_n(x)$, often used in physics:\n    $$\n    H_n(x) = (-1)^n e^{x^2} \\frac{d^n}{dx^n} e^{-x^2}\n    $$\n\n    The Hermite polynomial $H_n(x)$ of degree $n$ can be also defined by the following\n    recurrence relation:\n\n    $$\n    H_0(x) = 1\n    $$\n\n    $$\n    H_1(x) = 2x\n    $$\n\n    $$\n    H_{n+1}(x) = 2x H_n(x) - 2n H_{n-1}(x)\n    $$\n\n    where $H_n(x)$ represents the Hermite polynomial of degree $n$.\n\n    Parameters\n    ----------\n    degree : int, default=2\n        The maximum degree of the Hermite polynomial basis functions to be generated.\n\n    include_bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n\n    ensemble : bool, default=False\n        If True, the original data is concatenated with the polynomial features.\n\n    Notes\n    -----\n    The number of features in the output matrix increases as the degree of the\n    polynomial increases, which can lead to a high-dimensional feature space.\n    Consider using dimensionality reduction techniques if overfitting becomes an issue.\n\n    References\n    ----------\n    - Wikipedia: Hermite polynomial\n        https://en.wikipedia.org/wiki/Hermite_polynomials\n    - Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_hermite.html\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        self.degree = degree\n        self.include_bias = include_bias\n        self.ensemble = ensemble\n\n    def _hermite_expansion(self, data: np.ndarray):\n        num_samples = data.shape[0]\n        basis = np.zeros((num_samples, self.degree + 1))\n        for n in range(self.degree + 1):\n            basis[:, n] = eval_hermite(n, data)\n        return basis\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._hermite_expansion(data[:, col]) for col in range(n_features)]\n        # remove P0(x) = 1 from every column expansion\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Bersntein Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._hermite.Hermite.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Bersntein Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_hermite.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Bersntein Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._hermite_normalized.HermiteNormalized","title":"<code>HermiteNormalized</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build probabilist's Normalized Hermite basis function expansion.</p> <p>This class constructs a feature matrix consisting of Hermite Normalized polynomial basis functions up to a specified degree. Hermite (normalized) polynomials, denoted by \\(H_n(x)\\), are orthogonal polynomials over the interval \\((-\\infty, \\infty)\\) with respect to the weight function \\(w(x) = e^{-x^2}\\). These polynomials are widely used in probability theory, quantum mechanics, and numerical analysis, particularly in solving the quantum harmonic oscillator and in the field of statistics.</p> <p>Probabilist's Hermite polynomials \\(He_n(x)\\): $$ He_n(x) = (-1)^n e<sup>{x</sup>2/2} \\frac{d<sup>n}{dx</sup>n} e<sup>{-x</sup>2/2} $$</p> <p>where \\(He_n(x)\\) represents the probabilist's Normalized Hermite polynomial of degree \\(n\\).</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>The maximum degree of the Hermite polynomial basis functions to be generated.</p> <code>2</code> <code>include_bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix.</p> <code>True</code> <code>ensemble</code> <code>bool</code> <p>If True, the original data is concatenated with the polynomial features.</p> <code>False</code> Notes <p>The number of features in the output matrix increases as the degree of the polynomial increases, which can lead to a high-dimensional feature space. Consider using dimensionality reduction techniques if overfitting becomes an issue.</p> References <ul> <li>Wikipedia: Hermite polynomial     https://en.wikipedia.org/wiki/Hermite_polynomials</li> <li>Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_hermitenorm.html</li> </ul> Source code in <code>sysidentpy/basis_function/_hermite_normalized.py</code> <pre><code>class HermiteNormalized(BaseBasisFunction):\n    r\"\"\"Build probabilist's Normalized Hermite basis function expansion.\n\n    This class constructs a feature matrix consisting of Hermite Normalized polynomial\n    basis functions up to a specified degree. Hermite (normalized) polynomials,\n    denoted by $H_n(x)$, are orthogonal polynomials over the interval\n    $(-\\infty, \\infty)$ with respect to the weight function $w(x) = e^{-x^2}$.\n    These polynomials are widely used in probability theory, quantum mechanics,\n    and numerical analysis, particularly in solving the quantum harmonic oscillator\n    and in the field of statistics.\n\n    Probabilist's Hermite polynomials $He_n(x)$:\n    $$\n    He_n(x) = (-1)^n e^{x^2/2} \\frac{d^n}{dx^n} e^{-x^2/2}\n    $$\n\n    where $He_n(x)$ represents the probabilist's Normalized Hermite polynomial\n    of degree $n$.\n\n    Parameters\n    ----------\n    degree : int, default=2\n        The maximum degree of the Hermite polynomial basis functions to be generated.\n\n    include_bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n\n    ensemble : bool, default=False\n        If True, the original data is concatenated with the polynomial features.\n\n    Notes\n    -----\n    The number of features in the output matrix increases as the degree of the\n    polynomial increases, which can lead to a high-dimensional feature space.\n    Consider using dimensionality reduction techniques if overfitting becomes an issue.\n\n    References\n    ----------\n    - Wikipedia: Hermite polynomial\n        https://en.wikipedia.org/wiki/Hermite_polynomials\n    - Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_hermitenorm.html\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        self.degree = degree\n        self.include_bias = include_bias\n        self.ensemble = ensemble\n\n    def _hermitenorm_expansion(self, data: np.ndarray):\n        num_samples = data.shape[0]\n        basis = np.zeros((num_samples, self.degree + 1))\n        for n in range(self.degree + 1):\n            basis[:, n] = eval_hermitenorm(n, data)\n        return basis\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._hermitenorm_expansion(data[:, col]) for col in range(n_features)]\n        # remove P0(x) = 1 from every column expansion\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Bersntein Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._hermite_normalized.HermiteNormalized.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Bersntein Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_hermite_normalized.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Bersntein Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._laguerre.Laguerre","title":"<code>Laguerre</code>","text":"<p>               Bases: <code>BaseBasisFunction</code></p> <p>Build Laguerre basis function expansion.</p> <p>This class constructs a feature matrix consisting of Laguerre polynomial basis functions up to a specified degree. Laguerre polynomials, denoted by \\(L_n(x)\\), are orthogonal polynomials over the interval \\([0, \\infty)\\) with respect to the weight function \\(w(x) = e^{-x}\\). These polynomials are commonly used in physics, particularly in quantum mechanics, and in numerical analysis.</p> <p>The Laguerre polynomial \\(L_n(x)\\) of degree \\(n\\) is defined by the following recurrence relation:</p> \\[ L_0(x) = 1 \\] \\[ L_1(x) = 1 - x \\] \\[ (n+1) L_{n+1}(x) = (2n + 1 - x) L_n(x) - n L_{n-1}(x) \\] <p>where \\(L_n(x)\\) represents the Laguerre polynomial of degree \\(n\\).</p> <p>Parameters:</p> Name Type Description Default <code>degree</code> <code>int</code> <p>The maximum degree of the Laguerre polynomial basis functions to be generated.</p> <code>2</code> <code>include_bias</code> <code>bool</code> <p>Whether to include the bias (constant) term in the output feature matrix.</p> <code>True</code> <code>ensemble</code> <code>bool</code> <p>If True, the original data is concatenated with the polynomial features.</p> <code>False</code> Notes <p>The number of features in the output matrix increases as the degree of the polynomial increases, which can lead to a high-dimensional feature space. Consider using dimensionality reduction techniques if overfitting becomes an issue.</p> References <ul> <li>Wikipedia: Laguerre polynomial     https://en.wikipedia.org/wiki/Laguerre_polynomials</li> <li>Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_laguerre.html</li> <li>Milton Abramowitz and Irene A. Stegun, eds. Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables. New York: Dover, 1972.</li> </ul> Source code in <code>sysidentpy/basis_function/_laguerre.py</code> <pre><code>class Laguerre(BaseBasisFunction):\n    r\"\"\"Build Laguerre basis function expansion.\n\n    This class constructs a feature matrix consisting of Laguerre polynomial basis\n    functions up to a specified degree. Laguerre polynomials, denoted by $L_n(x)$,\n    are orthogonal polynomials over the interval $[0, \\infty)$ with respect to the\n    weight function $w(x) = e^{-x}$. These polynomials are commonly used in\n    physics, particularly in quantum mechanics, and in numerical analysis.\n\n    The Laguerre polynomial $L_n(x)$ of degree $n$ is defined by the following\n    recurrence relation:\n\n    $$\n    L_0(x) = 1\n    $$\n\n    $$\n    L_1(x) = 1 - x\n    $$\n\n    $$\n    (n+1) L_{n+1}(x) = (2n + 1 - x) L_n(x) - n L_{n-1}(x)\n    $$\n\n    where $L_n(x)$ represents the Laguerre polynomial of degree $n$.\n\n    Parameters\n    ----------\n    degree : int, default=2\n        The maximum degree of the Laguerre polynomial basis functions to be generated.\n\n    include_bias : bool, default=True\n        Whether to include the bias (constant) term in the output feature matrix.\n\n    ensemble : bool, default=False\n        If True, the original data is concatenated with the polynomial features.\n\n    Notes\n    -----\n    The number of features in the output matrix increases as the degree of the\n    polynomial increases, which can lead to a high-dimensional feature space.\n    Consider using dimensionality reduction techniques if overfitting becomes an issue.\n\n    References\n    ----------\n    - Wikipedia: Laguerre polynomial\n        https://en.wikipedia.org/wiki/Laguerre_polynomials\n    - Scipy: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.eval_laguerre.html\n    - Milton Abramowitz and Irene A. Stegun, eds. Handbook of Mathematical Functions\n    with Formulas, Graphs, and Mathematical Tables. New York: Dover, 1972.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        degree: int = 1,\n        include_bias: bool = True,\n        ensemble: bool = False,\n    ):\n        self.degree = degree\n        self.include_bias = include_bias\n        self.ensemble = ensemble\n\n    def _laguerre_expansion(self, data: np.ndarray):\n        num_samples = data.shape[0]\n        basis = np.zeros((num_samples, self.degree + 1))\n        for n in range(self.degree + 1):\n            basis[:, n] = eval_laguerre(n, data)\n        return basis\n\n    def fit(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        # remove intercept (because data always have the intercept)\n        data = data[max_lag:, 1:]\n\n        n_features = data.shape[1]\n        psi = [self._laguerre_expansion(data[:, col]) for col in range(n_features)]\n        # remove P0(x) = 1 from every column expansion\n        psi = [basis[:, 1:] for basis in psi]\n        psi = np.hstack(psi)\n        psi = np.nan_to_num(psi, 0)\n        if self.include_bias:\n            bias_column = np.ones((psi.shape[0], 1))\n            psi = np.hstack((bias_column, psi))\n\n        if self.ensemble:\n            psi = np.column_stack([data, psi])\n\n        if predefined_regressors is None:\n            return psi\n\n        return psi[:, predefined_regressors]\n\n    def transform(\n        self,\n        data: np.ndarray,\n        max_lag: int = 1,\n        ylag: int = 1,\n        xlag: int = 1,\n        model_type: str = \"NARMAX\",\n        predefined_regressors: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Build Bersntein Basis Functions.\n\n        Parameters\n        ----------\n        data : ndarray of floats\n            The lagged matrix built with respect to each lag and column.\n        max_lag : int\n            Maximum lag of list of regressors.\n        ylag : ndarray of int\n            The range of lags according to user definition.\n        xlag : ndarray of int\n            The range of lags according to user definition.\n        model_type : str\n            The type of the model (NARMAX, NAR or NFIR).\n        predefined_regressors: ndarray\n            Regressors to be filtered in the transformation.\n\n        Returns\n        -------\n        x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n            Transformed array.\n\n        \"\"\"\n        return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/basis-function/#sysidentpy.basis_function._laguerre.Laguerre.transform","title":"<code>transform(data, max_lag=1, ylag=1, xlag=1, model_type='NARMAX', predefined_regressors=None)</code>","text":"<p>Build Bersntein Basis Functions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray of floats</code> <p>The lagged matrix built with respect to each lag and column.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> <code>1</code> <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>xlag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <code>model_type</code> <code>str</code> <p>The type of the model (NARMAX, NAR or NFIR).</p> <code>'NARMAX'</code> <code>predefined_regressors</code> <code>Optional[ndarray]</code> <p>Regressors to be filtered in the transformation.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>x_tr</code> <code>{ndarray, sparse matrix} of shape (n_samples, n_features)</code> <p>Transformed array.</p> Source code in <code>sysidentpy/basis_function/_laguerre.py</code> <pre><code>def transform(\n    self,\n    data: np.ndarray,\n    max_lag: int = 1,\n    ylag: int = 1,\n    xlag: int = 1,\n    model_type: str = \"NARMAX\",\n    predefined_regressors: Optional[np.ndarray] = None,\n):\n    \"\"\"Build Bersntein Basis Functions.\n\n    Parameters\n    ----------\n    data : ndarray of floats\n        The lagged matrix built with respect to each lag and column.\n    max_lag : int\n        Maximum lag of list of regressors.\n    ylag : ndarray of int\n        The range of lags according to user definition.\n    xlag : ndarray of int\n        The range of lags according to user definition.\n    model_type : str\n        The type of the model (NARMAX, NAR or NFIR).\n    predefined_regressors: ndarray\n        Regressors to be filtered in the transformation.\n\n    Returns\n    -------\n    x_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        Transformed array.\n\n    \"\"\"\n    return self.fit(data, max_lag, ylag, xlag, model_type, predefined_regressors)\n</code></pre>"},{"location":"user-guide/API/entropic-regression/","title":"Documentation for <code>Entropic Regression</code>","text":"<p>Build Polynomial NARMAX Models using the Entropic Regression algorithm.</p>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER","title":"<code>ER</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Entropic Regression Algorithm.</p> <p>Build Polynomial NARMAX model using the Entropic Regression Algorithm ([1]_). This algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p> <p>The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},     e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>k</code> <code>int</code> <p>The kth nearest neighbor to be used in estimation.</p> <code>2</code> <code>q</code> <code>float</code> <p>Quantile to compute, which must be between 0 and 1 inclusive.</p> <code>0.99</code> <code>p</code> <code>default=inf,</code> <p>Lp Measure of the distance in Knn estimator.</p> <code>inf</code> <code>n_perm</code> <code>int</code> <p>Number of permutation to be used in shuffle test</p> <code>200</code> <code>estimator</code> <code>str</code> <p>The parameter estimation method.</p> <code>\"least_squares\"</code> <code>skip_forward</code> <code>bool</code> <p>To be used for difficult and highly uncertain problems. Skipping the forward selection results in more accurate solution, but comes with higher computational cost.</p> <code>False</code> <code>model_type</code> <code>str</code> <p>The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models</p> <code>'NARMAX'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import ER\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n...                                                    colored_noise=True,\n...                                                    sigma=0.2,\n...                                                    train_percentage=90)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = ER(basis_function=basis_function,\n...              ylag=2, xlag=2\n...              )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>class ER(BaseMSS):\n    r\"\"\"Entropic Regression Algorithm.\n\n    Build Polynomial NARMAX model using the Entropic Regression Algorithm ([1]_).\n    This algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},\n        e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    k : int, default=2\n        The kth nearest neighbor to be used in estimation.\n    q : float, default=0.99\n        Quantile to compute, which must be between 0 and 1 inclusive.\n    p : default=inf,\n        Lp Measure of the distance in Knn estimator.\n    n_perm: int, default=200\n        Number of permutation to be used in shuffle test\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    skip_forward = bool, default=False\n        To be used for difficult and highly uncertain problems.\n        Skipping the forward selection results in more accurate solution,\n        but comes with higher computational cost.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import ER\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = ER(basis_function=basis_function,\n    ...              ylag=2, xlag=2\n    ...              )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 1,\n        xlag: Union[int, list] = 1,\n        q: float = 0.99,\n        estimator: Estimators_Union = LeastSquares(),\n        h: float = 0.01,\n        k: int = 2,\n        mutual_information_estimator: str = \"mutual_information_knn\",\n        n_perm: int = 200,\n        p: float = np.inf,\n        skip_forward: bool = False,\n        model_type: str = \"NARMAX\",\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        random_state: Optional[int] = None,\n    ):\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.xlag = xlag\n        self.ylag = ylag\n        self.non_degree = basis_function.degree\n        self.max_lag = self._get_max_lag()\n        self.k = k\n        self.estimator = estimator\n        self.q = q\n        self.h = h\n        self.mutual_information_estimator = mutual_information_estimator\n        self.n_perm = n_perm\n        self.p = p\n        self.skip_forward = skip_forward\n        self.random_state = random_state\n        self.rng = check_random_state(random_state)\n        self.tol = None\n        self.n_inputs = None\n        self.estimated_tolerance = None\n        self.regressor_code = None\n        self.final_model = None\n        self.theta = None\n        self.n_terms = None\n        self.err = None\n        self.pivv = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if not isinstance(self.k, int) or self.k &lt; 1:\n            raise ValueError(f\"k must be integer and &gt; zero. Got {self.k}\")\n\n        if not isinstance(self.n_perm, int) or self.n_perm &lt; 1:\n            raise ValueError(f\"n_perm must be integer and &gt; zero. Got {self.n_perm}\")\n\n        if not isinstance(self.q, float) or self.q &gt; 1 or self.q &lt;= 0:\n            raise ValueError(\n                f\"q must be float and must be between 0 and 1 inclusive. Got {self.q}\"\n            )\n\n        if not isinstance(self.skip_forward, bool):\n            raise TypeError(\n                f\"skip_forward must be False or True. Got {self.skip_forward}\"\n            )\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def mutual_information_knn(self, y, y_perm):\n        \"\"\"Find the mutual information.\n\n        Finds the mutual information between $x$ and $y$ given $z$.\n\n        This code is based on Matlab Entropic Regression package.\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The source signal.\n        y_perm : ndarray of floats\n            The destination signal.\n\n        Returns\n        -------\n        ksg_estimation : float\n            The conditioned mutual information.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        joint_space = np.concatenate([y, y_perm], axis=1)\n        smallest_distance = np.sort(\n            cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n        )\n        idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n        smallest_distance = smallest_distance[:, idx]\n        epsilon = smallest_distance[:, -1].reshape(-1, 1)\n        smallest_distance_y = cdist(y, y, \"minkowski\", p=self.p)\n        less_than_array_nx = np.array((smallest_distance_y &lt; epsilon)).astype(int)\n        nx = (np.sum(less_than_array_nx, axis=1) - 1).reshape(-1, 1)\n        smallest_distance_y_perm = cdist(y_perm, y_perm, \"minkowski\", p=self.p)\n        less_than_array_ny = np.array((smallest_distance_y_perm &lt; epsilon)).astype(int)\n        ny = (np.sum(less_than_array_ny, axis=1) - 1).reshape(-1, 1)\n        arr = psi(nx + 1) + psi(ny + 1)\n        ksg_estimation = (\n            psi(self.k) + psi(y.shape[0]) - np.nanmean(arr[np.isfinite(arr)])\n        )\n        return ksg_estimation\n\n    def entropic_regression_backward(self, reg_matrix, y, piv):\n        \"\"\"Entropic Regression Backward Greedy Feature Elimination.\n\n        This algorithm is based on the Matlab package available on:\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        reg_matrix : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        piv : ndarray of ints\n            The set of indices to investigate\n\n        Returns\n        -------\n        piv : ndarray of ints\n            The set of remaining indices after the\n            Backward Greedy Feature Elimination.\n\n        \"\"\"\n        min_value = -np.inf\n        piv = np.array(piv)\n        ix = []\n        while (min_value &lt;= self.tol) and (len(piv) &gt; 1):\n            initial_array = np.full((1, len(piv)), np.inf)\n            for i in range(initial_array.shape[1]):\n                if piv[i] not in []:  # if you want to keep any regressor\n                    rem = np.setdiff1d(piv, piv[i])\n                    f1 = reg_matrix[:, piv] @ pinv(reg_matrix[:, piv]) @ y\n                    f2 = reg_matrix[:, rem] @ pinv(reg_matrix[:, rem]) @ y\n                    initial_array[0, i] = self.conditional_mutual_information(y, f1, f2)\n\n            ix = np.argmin(initial_array)\n            min_value = initial_array[0, ix]\n            piv = np.delete(piv, ix)\n\n        return piv\n\n    def entropic_regression_forward(self, reg_matrix, y):\n        \"\"\"Entropic Regression Forward Greedy Feature Selection.\n\n        This algorithm is based on the Matlab package available on:\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        reg_matrix : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n\n        Returns\n        -------\n        selected_terms : ndarray of ints\n            The set of selected regressors after the\n            Forward Greedy Feature Selection.\n        success : boolean\n            Indicate if the forward selection succeed.\n            If high degree of uncertainty is detected, and many parameters are\n            selected, the success flag will be set to false. Then, the\n            backward elimination will be applied for all indices.\n\n        \"\"\"\n        success = True\n        ix = []\n        selected_terms = []\n        reg_matrix_columns = np.array(list(range(reg_matrix.shape[1])))\n        self.tol = self.tolerance_estimator(y)\n        ksg_max = getattr(self, self.mutual_information_estimator)(\n            y, reg_matrix @ pinv(reg_matrix) @ y\n        )\n        stop_criteria = False\n        while stop_criteria is False:\n            selected_terms = np.ravel(\n                [*selected_terms, *np.array([reg_matrix_columns[ix]])]\n            )\n            if len(selected_terms) != 0:\n                ksg_local = getattr(self, self.mutual_information_estimator)(\n                    y,\n                    reg_matrix[:, selected_terms]\n                    @ pinv(reg_matrix[:, selected_terms])\n                    @ y,\n                )\n            else:\n                ksg_local = getattr(self, self.mutual_information_estimator)(\n                    y, np.zeros_like(y)\n                )\n\n            initial_vector = np.full((1, reg_matrix.shape[1]), -np.inf)\n            for i in range(reg_matrix.shape[1]):\n                if reg_matrix_columns[i] not in selected_terms:\n                    f1 = (\n                        reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]]\n                        @ pinv(reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]])\n                        @ y\n                    )\n                    if len(selected_terms) != 0:\n                        f2 = (\n                            reg_matrix[:, selected_terms]\n                            @ pinv(reg_matrix[:, selected_terms])\n                            @ y\n                        )\n                    else:\n                        f2 = np.zeros_like(y)\n                    vp_estimation = self.conditional_mutual_information(y, f1, f2)\n                    initial_vector[0, i] = vp_estimation\n                else:\n                    continue\n\n            ix = np.nanargmax(initial_vector)\n            max_value = initial_vector[0, ix]\n\n            if (ksg_max - ksg_local &lt;= self.tol) or (max_value &lt;= self.tol):\n                stop_criteria = True\n            elif len(selected_terms) &gt; np.max([8, reg_matrix.shape[1] / 2]):\n                success = False\n                stop_criteria = True\n\n        return selected_terms, success\n\n    def conditional_mutual_information(self, y, f1, f2):\n        \"\"\"Find the conditional mutual information.\n\n        Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n        This code is based on Matlab Entropic Regression package.\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The source signal.\n        f1 : ndarray of floats\n            The destination signal.\n        f2 : ndarray of floats\n            The condition set.\n\n        Returns\n        -------\n        vp_estimation : float\n            The conditioned mutual information.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        joint_space = np.concatenate([y, f1, f2], axis=1)\n        smallest_distance = np.sort(\n            cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n        )\n        idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n        smallest_distance = smallest_distance[:, idx]\n        epsilon = smallest_distance[:, -1].reshape(-1, 1)\n        # Find number of points from (y,f2), (f1,f2), and (f2,f2) that lies withing the\n        # k^{th} nearest neighbor distance from each point of themselves.\n        smallest_distance_y_f2 = cdist(\n            np.concatenate([y, f2], axis=1),\n            np.concatenate([y, f2], axis=1),\n            \"minkowski\",\n            p=self.p,\n        )\n        less_than_array_y_f2 = np.array((smallest_distance_y_f2 &lt; epsilon)).astype(int)\n        y_f2 = (np.sum(less_than_array_y_f2, axis=1) - 1).reshape(-1, 1)\n\n        smallest_distance_f1_f2 = cdist(\n            np.concatenate([f1, f2], axis=1),\n            np.concatenate([f1, f2], axis=1),\n            \"minkowski\",\n            p=self.p,\n        )\n        less_than_array_f1_f2 = np.array((smallest_distance_f1_f2 &lt; epsilon)).astype(\n            int\n        )\n        f1_f2 = (np.sum(less_than_array_f1_f2, axis=1) - 1).reshape(-1, 1)\n\n        smallest_distance_f2 = cdist(f2, f2, \"minkowski\", p=self.p)\n        less_than_array_f2 = np.array((smallest_distance_f2 &lt; epsilon)).astype(int)\n        f2_f2 = (np.sum(less_than_array_f2, axis=1) - 1).reshape(-1, 1)\n        arr = psi(y_f2 + 1) + psi(f1_f2 + 1) - psi(f2_f2 + 1)\n        vp_estimation = psi(self.k) - np.nanmean(arr[np.isfinite(arr)])\n        return vp_estimation\n\n    def tolerance_estimator(self, y):\n        \"\"\"Tolerance Estimation for mutual independence test.\n\n        Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n        This code is based on Matlab Entropic Regression package.\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The source signal.\n\n        Returns\n        -------\n        tol : float\n            The tolerance value given q.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        ksg_estimation = []\n        for _ in range(self.n_perm):\n            mutual_information_output = getattr(\n                self, self.mutual_information_estimator\n            )(y, self.rng.permutation(y))\n\n            ksg_estimation.append(mutual_information_output)\n\n        ksg_estimation = np.array(ksg_estimation)\n        tol = np.quantile(ksg_estimation, self.q)\n        return tol\n\n    def fit(self, *, X=None, y=None):\n        \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n        The 'fit' function allows a friendly usage by the user.\n        Given two arguments, x and y, fit training data.\n\n        The Entropic Regression algorithm is based on the Matlab package available on:\n        https://github.com/almomaa/ERFit-Package\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n            Regression Beats the Outliers Problem in Nonlinear System\n            Identification. Chaos 30, 013107 (2020).\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n        - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n            Estimating mutual information. Physical Review E, 69:066-138,2004\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n        if X is not None:\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n\n        if self.regressor_code.shape[0] &gt; 90:\n            warnings.warn(\n                \"Given the higher number of possible regressors\"\n                f\" ({self.regressor_code.shape[0]}), the Entropic Regression\"\n                \" algorithm may take long time to run. Consider reducing the\"\n                \" number of regressors \",\n                stacklevel=2,\n            )\n\n        y_full = y.copy()\n        y = y[self.max_lag :].reshape(-1, 1)\n        self.tol = 0\n        ksg_estimation = []\n        for _ in range(self.n_perm):\n            mutual_information_output = getattr(\n                self, self.mutual_information_estimator\n            )(y, self.rng.permutation(y))\n            ksg_estimation.append(mutual_information_output)\n\n        ksg_estimation = np.array(ksg_estimation).reshape(-1, 1)\n        self.tol = np.quantile(ksg_estimation, self.q)\n        self.estimated_tolerance = self.tol\n        success = False\n        if not self.skip_forward:\n            selected_terms, success = self.entropic_regression_forward(reg_matrix, y)\n\n        if not success or self.skip_forward:\n            selected_terms = np.array(list(range(reg_matrix.shape[1])))\n\n        selected_terms_backward = self.entropic_regression_backward(\n            reg_matrix[:, selected_terms], y, list(range(len(selected_terms)))\n        )\n\n        final_model = selected_terms[selected_terms_backward]\n        # re-check for the constant term (add it to the estimated indices)\n        if 0 not in final_model:\n            final_model = np.array([0, *final_model])\n\n        repetition = len(reg_matrix)\n        if isinstance(self.basis_function, Polynomial):\n            self.final_model = self.regressor_code[final_model, :].copy()\n        else:\n            self.regressor_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.final_model = self.regressor_code[final_model, :].copy()\n\n        self.theta = self.estimator.optimize(\n            reg_matrix[:, final_model], y_full[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if (np.abs(self.theta[0]) &lt; self.h) and (\n            np.sum((self.theta != 0).astype(int)) &gt; 1\n        ):\n            self.theta = self.theta[1:].reshape(-1, 1)\n            self.final_model = self.final_model[1:, :]\n            final_model = final_model[1:]\n\n        self.n_terms = len(\n            self.theta\n        )  # the number of terms we selected (necessary in the 'results' methods)\n        self.err = self.n_terms * [\n            0\n        ]  # just to use the `results` method. Will be changed in next update.\n        self.pivv = final_model\n        return self\n\n    def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(self, x, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n\n        x_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, x, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        elif self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n        else:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def _narmax_predict(self, x, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(self, x, y_initial):\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = super()._basis_function_predict(x, y_initial, forecast_horizon)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = super()._basis_function_n_step_prediction(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(self, x, y, steps_ahead, forecast_horizon):\n        yhat = super()._basis_function_n_steps_horizon(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.conditional_mutual_information","title":"<code>conditional_mutual_information(y, f1, f2)</code>","text":"<p>Find the conditional mutual information.</p> <p>Finds the conditioned mutual information between \\(y\\) and \\(f1\\) given \\(f2\\).</p> <p>This code is based on Matlab Entropic Regression package. https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The source signal.</p> required <code>f1</code> <code>ndarray of floats</code> <p>The destination signal.</p> required <code>f2</code> <code>ndarray of floats</code> <p>The condition set.</p> required <p>Returns:</p> Name Type Description <code>vp_estimation</code> <code>float</code> <p>The conditioned mutual information.</p> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def conditional_mutual_information(self, y, f1, f2):\n    \"\"\"Find the conditional mutual information.\n\n    Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n    This code is based on Matlab Entropic Regression package.\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The source signal.\n    f1 : ndarray of floats\n        The destination signal.\n    f2 : ndarray of floats\n        The condition set.\n\n    Returns\n    -------\n    vp_estimation : float\n        The conditioned mutual information.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    joint_space = np.concatenate([y, f1, f2], axis=1)\n    smallest_distance = np.sort(\n        cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n    )\n    idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n    smallest_distance = smallest_distance[:, idx]\n    epsilon = smallest_distance[:, -1].reshape(-1, 1)\n    # Find number of points from (y,f2), (f1,f2), and (f2,f2) that lies withing the\n    # k^{th} nearest neighbor distance from each point of themselves.\n    smallest_distance_y_f2 = cdist(\n        np.concatenate([y, f2], axis=1),\n        np.concatenate([y, f2], axis=1),\n        \"minkowski\",\n        p=self.p,\n    )\n    less_than_array_y_f2 = np.array((smallest_distance_y_f2 &lt; epsilon)).astype(int)\n    y_f2 = (np.sum(less_than_array_y_f2, axis=1) - 1).reshape(-1, 1)\n\n    smallest_distance_f1_f2 = cdist(\n        np.concatenate([f1, f2], axis=1),\n        np.concatenate([f1, f2], axis=1),\n        \"minkowski\",\n        p=self.p,\n    )\n    less_than_array_f1_f2 = np.array((smallest_distance_f1_f2 &lt; epsilon)).astype(\n        int\n    )\n    f1_f2 = (np.sum(less_than_array_f1_f2, axis=1) - 1).reshape(-1, 1)\n\n    smallest_distance_f2 = cdist(f2, f2, \"minkowski\", p=self.p)\n    less_than_array_f2 = np.array((smallest_distance_f2 &lt; epsilon)).astype(int)\n    f2_f2 = (np.sum(less_than_array_f2, axis=1) - 1).reshape(-1, 1)\n    arr = psi(y_f2 + 1) + psi(f1_f2 + 1) - psi(f2_f2 + 1)\n    vp_estimation = psi(self.k) - np.nanmean(arr[np.isfinite(arr)])\n    return vp_estimation\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_backward","title":"<code>entropic_regression_backward(reg_matrix, y, piv)</code>","text":"<p>Entropic Regression Backward Greedy Feature Elimination.</p> <p>This algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>reg_matrix</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> required <code>piv</code> <code>ndarray of ints</code> <p>The set of indices to investigate</p> required <p>Returns:</p> Name Type Description <code>piv</code> <code>ndarray of ints</code> <p>The set of remaining indices after the Backward Greedy Feature Elimination.</p> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def entropic_regression_backward(self, reg_matrix, y, piv):\n    \"\"\"Entropic Regression Backward Greedy Feature Elimination.\n\n    This algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    reg_matrix : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    piv : ndarray of ints\n        The set of indices to investigate\n\n    Returns\n    -------\n    piv : ndarray of ints\n        The set of remaining indices after the\n        Backward Greedy Feature Elimination.\n\n    \"\"\"\n    min_value = -np.inf\n    piv = np.array(piv)\n    ix = []\n    while (min_value &lt;= self.tol) and (len(piv) &gt; 1):\n        initial_array = np.full((1, len(piv)), np.inf)\n        for i in range(initial_array.shape[1]):\n            if piv[i] not in []:  # if you want to keep any regressor\n                rem = np.setdiff1d(piv, piv[i])\n                f1 = reg_matrix[:, piv] @ pinv(reg_matrix[:, piv]) @ y\n                f2 = reg_matrix[:, rem] @ pinv(reg_matrix[:, rem]) @ y\n                initial_array[0, i] = self.conditional_mutual_information(y, f1, f2)\n\n        ix = np.argmin(initial_array)\n        min_value = initial_array[0, ix]\n        piv = np.delete(piv, ix)\n\n    return piv\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.entropic_regression_forward","title":"<code>entropic_regression_forward(reg_matrix, y)</code>","text":"<p>Entropic Regression Forward Greedy Feature Selection.</p> <p>This algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>reg_matrix</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> required <p>Returns:</p> Name Type Description <code>selected_terms</code> <code>ndarray of ints</code> <p>The set of selected regressors after the Forward Greedy Feature Selection.</p> <code>success</code> <code>boolean</code> <p>Indicate if the forward selection succeed. If high degree of uncertainty is detected, and many parameters are selected, the success flag will be set to false. Then, the backward elimination will be applied for all indices.</p> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def entropic_regression_forward(self, reg_matrix, y):\n    \"\"\"Entropic Regression Forward Greedy Feature Selection.\n\n    This algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    reg_matrix : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n\n    Returns\n    -------\n    selected_terms : ndarray of ints\n        The set of selected regressors after the\n        Forward Greedy Feature Selection.\n    success : boolean\n        Indicate if the forward selection succeed.\n        If high degree of uncertainty is detected, and many parameters are\n        selected, the success flag will be set to false. Then, the\n        backward elimination will be applied for all indices.\n\n    \"\"\"\n    success = True\n    ix = []\n    selected_terms = []\n    reg_matrix_columns = np.array(list(range(reg_matrix.shape[1])))\n    self.tol = self.tolerance_estimator(y)\n    ksg_max = getattr(self, self.mutual_information_estimator)(\n        y, reg_matrix @ pinv(reg_matrix) @ y\n    )\n    stop_criteria = False\n    while stop_criteria is False:\n        selected_terms = np.ravel(\n            [*selected_terms, *np.array([reg_matrix_columns[ix]])]\n        )\n        if len(selected_terms) != 0:\n            ksg_local = getattr(self, self.mutual_information_estimator)(\n                y,\n                reg_matrix[:, selected_terms]\n                @ pinv(reg_matrix[:, selected_terms])\n                @ y,\n            )\n        else:\n            ksg_local = getattr(self, self.mutual_information_estimator)(\n                y, np.zeros_like(y)\n            )\n\n        initial_vector = np.full((1, reg_matrix.shape[1]), -np.inf)\n        for i in range(reg_matrix.shape[1]):\n            if reg_matrix_columns[i] not in selected_terms:\n                f1 = (\n                    reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]]\n                    @ pinv(reg_matrix[:, [*selected_terms, reg_matrix_columns[i]]])\n                    @ y\n                )\n                if len(selected_terms) != 0:\n                    f2 = (\n                        reg_matrix[:, selected_terms]\n                        @ pinv(reg_matrix[:, selected_terms])\n                        @ y\n                    )\n                else:\n                    f2 = np.zeros_like(y)\n                vp_estimation = self.conditional_mutual_information(y, f1, f2)\n                initial_vector[0, i] = vp_estimation\n            else:\n                continue\n\n        ix = np.nanargmax(initial_vector)\n        max_value = initial_vector[0, ix]\n\n        if (ksg_max - ksg_local &lt;= self.tol) or (max_value &lt;= self.tol):\n            stop_criteria = True\n        elif len(selected_terms) &gt; np.max([8, reg_matrix.shape[1] / 2]):\n            success = False\n            stop_criteria = True\n\n    return selected_terms, success\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Fit polynomial NARMAX model using AOLS algorithm.</p> <p>The 'fit' function allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>The Entropic Regression algorithm is based on the Matlab package available on: https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def fit(self, *, X=None, y=None):\n    \"\"\"Fit polynomial NARMAX model using AOLS algorithm.\n\n    The 'fit' function allows a friendly usage by the user.\n    Given two arguments, x and y, fit training data.\n\n    The Entropic Regression algorithm is based on the Matlab package available on:\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n\n    reg_matrix = self.basis_function.fit(\n        lagged_data,\n        self.max_lag,\n        self.ylag,\n        self.xlag,\n        self.model_type,\n        predefined_regressors=None,\n    )\n\n    if X is not None:\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n\n    if self.regressor_code.shape[0] &gt; 90:\n        warnings.warn(\n            \"Given the higher number of possible regressors\"\n            f\" ({self.regressor_code.shape[0]}), the Entropic Regression\"\n            \" algorithm may take long time to run. Consider reducing the\"\n            \" number of regressors \",\n            stacklevel=2,\n        )\n\n    y_full = y.copy()\n    y = y[self.max_lag :].reshape(-1, 1)\n    self.tol = 0\n    ksg_estimation = []\n    for _ in range(self.n_perm):\n        mutual_information_output = getattr(\n            self, self.mutual_information_estimator\n        )(y, self.rng.permutation(y))\n        ksg_estimation.append(mutual_information_output)\n\n    ksg_estimation = np.array(ksg_estimation).reshape(-1, 1)\n    self.tol = np.quantile(ksg_estimation, self.q)\n    self.estimated_tolerance = self.tol\n    success = False\n    if not self.skip_forward:\n        selected_terms, success = self.entropic_regression_forward(reg_matrix, y)\n\n    if not success or self.skip_forward:\n        selected_terms = np.array(list(range(reg_matrix.shape[1])))\n\n    selected_terms_backward = self.entropic_regression_backward(\n        reg_matrix[:, selected_terms], y, list(range(len(selected_terms)))\n    )\n\n    final_model = selected_terms[selected_terms_backward]\n    # re-check for the constant term (add it to the estimated indices)\n    if 0 not in final_model:\n        final_model = np.array([0, *final_model])\n\n    repetition = len(reg_matrix)\n    if isinstance(self.basis_function, Polynomial):\n        self.final_model = self.regressor_code[final_model, :].copy()\n    else:\n        self.regressor_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.final_model = self.regressor_code[final_model, :].copy()\n\n    self.theta = self.estimator.optimize(\n        reg_matrix[:, final_model], y_full[self.max_lag :, 0].reshape(-1, 1)\n    )\n    if (np.abs(self.theta[0]) &lt; self.h) and (\n        np.sum((self.theta != 0).astype(int)) &gt; 1\n    ):\n        self.theta = self.theta[1:].reshape(-1, 1)\n        self.final_model = self.final_model[1:, :]\n        final_model = final_model[1:]\n\n    self.n_terms = len(\n        self.theta\n    )  # the number of terms we selected (necessary in the 'results' methods)\n    self.err = self.n_terms * [\n        0\n    ]  # just to use the `results` method. Will be changed in next update.\n    self.pivv = final_model\n    return self\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.mutual_information_knn","title":"<code>mutual_information_knn(y, y_perm)</code>","text":"<p>Find the mutual information.</p> <p>Finds the mutual information between \\(x\\) and \\(y\\) given \\(z\\).</p> <p>This code is based on Matlab Entropic Regression package.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The source signal.</p> required <code>y_perm</code> <code>ndarray of floats</code> <p>The destination signal.</p> required <p>Returns:</p> Name Type Description <code>ksg_estimation</code> <code>float</code> <p>The conditioned mutual information.</p> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def mutual_information_knn(self, y, y_perm):\n    \"\"\"Find the mutual information.\n\n    Finds the mutual information between $x$ and $y$ given $z$.\n\n    This code is based on Matlab Entropic Regression package.\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The source signal.\n    y_perm : ndarray of floats\n        The destination signal.\n\n    Returns\n    -------\n    ksg_estimation : float\n        The conditioned mutual information.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    joint_space = np.concatenate([y, y_perm], axis=1)\n    smallest_distance = np.sort(\n        cdist(joint_space, joint_space, \"minkowski\", p=self.p).T\n    )\n    idx = np.argpartition(smallest_distance[-1, :], self.k + 1)[: self.k + 1]\n    smallest_distance = smallest_distance[:, idx]\n    epsilon = smallest_distance[:, -1].reshape(-1, 1)\n    smallest_distance_y = cdist(y, y, \"minkowski\", p=self.p)\n    less_than_array_nx = np.array((smallest_distance_y &lt; epsilon)).astype(int)\n    nx = (np.sum(less_than_array_nx, axis=1) - 1).reshape(-1, 1)\n    smallest_distance_y_perm = cdist(y_perm, y_perm, \"minkowski\", p=self.p)\n    less_than_array_ny = np.array((smallest_distance_y_perm &lt; epsilon)).astype(int)\n    ny = (np.sum(less_than_array_ny, axis=1) - 1).reshape(-1, 1)\n    arr = psi(nx + 1) + psi(ny + 1)\n    ksg_estimation = (\n        psi(self.k) + psi(y.shape[0]) - np.nanmean(arr[np.isfinite(arr)])\n    )\n    return ksg_estimation\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/entropic-regression/#sysidentpy.model_structure_selection.entropic_regression.ER.tolerance_estimator","title":"<code>tolerance_estimator(y)</code>","text":"<p>Tolerance Estimation for mutual independence test.</p> <p>Finds the conditioned mutual information between \\(y\\) and \\(f1\\) given \\(f2\\).</p> <p>This code is based on Matlab Entropic Regression package. https://github.com/almomaa/ERFit-Package</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The source signal.</p> required <p>Returns:</p> Name Type Description <code>tol</code> <code>float</code> <p>The tolerance value given q.</p> References <ul> <li>Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic     Regression Beats the Outliers Problem in Nonlinear System     Identification. Chaos 30, 013107 (2020).</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> <li>Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.     Estimating mutual information. Physical Review E, 69:066-138,2004</li> </ul> Source code in <code>sysidentpy/model_structure_selection/entropic_regression.py</code> <pre><code>def tolerance_estimator(self, y):\n    \"\"\"Tolerance Estimation for mutual independence test.\n\n    Finds the conditioned mutual information between $y$ and $f1$ given $f2$.\n\n    This code is based on Matlab Entropic Regression package.\n    https://github.com/almomaa/ERFit-Package\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The source signal.\n\n    Returns\n    -------\n    tol : float\n        The tolerance value given q.\n\n    References\n    ----------\n    - Abd AlRahman R. AlMomani, Jie Sun, and Erik Bollt. How Entropic\n        Regression Beats the Outliers Problem in Nonlinear System\n        Identification. Chaos 30, 013107 (2020).\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n    - Alexander Kraskov, Harald St\u00a8ogbauer, and Peter Grassberger.\n        Estimating mutual information. Physical Review E, 69:066-138,2004\n\n    \"\"\"\n    ksg_estimation = []\n    for _ in range(self.n_perm):\n        mutual_information_output = getattr(\n            self, self.mutual_information_estimator\n        )(y, self.rng.permutation(y))\n\n        ksg_estimation.append(mutual_information_output)\n\n    ksg_estimation = np.array(ksg_estimation)\n    tol = np.quantile(ksg_estimation, self.q)\n    return tol\n</code></pre>"},{"location":"user-guide/API/frols/","title":"Documentation for <code>FROLS</code>","text":"<p>Build Polynomial NARMAX Models using FROLS algorithm.</p>"},{"location":"user-guide/API/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS","title":"<code>FROLS</code>","text":"<p>               Bases: <code>OFRBase</code></p> <p>Forward Regression Orthogonal Least Squares algorithm.</p> <p>This class uses the FROLS algorithm ([1], [2]) to build NARMAX models. The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},     \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>elag</code> <code>int</code> <p>The maximum lag of the residues regressors.</p> <code>2</code> <code>order_selection</code> <code>bool</code> <p>Whether to use information criteria for order selection.</p> <code>True</code> <code>info_criteria</code> <code>str</code> <p>The information criteria method to be used.</p> <code>\"aic\"</code> <code>n_terms</code> <code>int</code> <p>The number of the model terms to be selected. Note that n_terms overwrite the information criteria values.</p> <code>None</code> <code>n_info_values</code> <code>int</code> <p>The number of iterations of the information criteria method.</p> <code>10</code> <code>estimator</code> <code>str</code> <p>The parameter estimation method.</p> <code>\"least_squares\"</code> <code>model_type</code> <code>str</code> <p>The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models</p> <code>'NARMAX'</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <code>alpha</code> <code>float</code> <p>Regularization parameter used in ridge regression. Ridge regression parameter that regularizes the algorithm to prevent over fitting. If the input is a noisy signal, the ridge parameter is likely to be set close to the noise level, at least as a starting point. Entered through the self data structure.</p> <code>np.finfo(np.float64).eps</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import FROLS\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n...                                                    colored_noise=True,\n...                                                    sigma=0.2,\n...                                                    train_percentage=90)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = FROLS(basis_function=basis_function,\n...               order_selection=True,\n...               n_info_values=10,\n...               extended_least_squares=False,\n...               ylag=2,\n...               xlag=2,\n...               info_criteria='aic',\n...               )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>class FROLS(OFRBase):\n    r\"\"\"Forward Regression Orthogonal Least Squares algorithm.\n\n    This class uses the FROLS algorithm ([1]_, [2]_) to build NARMAX models.\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},\n        \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    elag : int, default=2\n        The maximum lag of the residues regressors.\n    order_selection: bool, default=False\n        Whether to use information criteria for order selection.\n    info_criteria : str, default=\"aic\"\n        The information criteria method to be used.\n    n_terms : int, default=None\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_info_values : int, default=10\n        The number of iterations of the information\n        criteria method.\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    alpha : float, default=np.finfo(np.float64).eps\n        Regularization parameter used in ridge regression.\n        Ridge regression parameter that regularizes the algorithm to prevent over\n        fitting. If the input is a noisy signal, the ridge parameter is likely to be\n        set close to the noise level, at least as a starting point.\n        Entered through the self data structure.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import FROLS\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = FROLS(basis_function=basis_function,\n    ...               order_selection=True,\n    ...               n_info_values=10,\n    ...               extended_least_squares=False,\n    ...               ylag=2,\n    ...               xlag=2,\n    ...               info_criteria='aic',\n    ...               )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        elag: Union[int, list] = 2,\n        order_selection: bool = True,\n        info_criteria: str = \"aic\",\n        n_terms: Union[int, None] = None,\n        n_info_values: int = 15,\n        estimator: Estimators = RecursiveLeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n        eps: np.float64 = np.finfo(np.float64).eps,\n        alpha: float = 0,\n        err_tol: Optional[float] = None,\n    ):\n        self.order_selection = order_selection\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.info_criteria = info_criteria\n        self.info_criteria_function = get_info_criteria(info_criteria)\n        self.n_info_values = n_info_values\n        self.n_terms = n_terms\n        self.estimator = estimator\n        self.elag = elag\n        self.model_type = model_type\n        self.basis_function = basis_function\n        self.eps = eps\n        if isinstance(self.estimator, RidgeRegression):\n            self.alpha = self.estimator.alpha\n        else:\n            self.alpha = alpha\n\n        self.err_tol = err_tol\n        self._validate_params()\n        self.n_inputs = None\n        self.regressor_code = None\n        self.info_values = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n\n    def error_reduction_ratio(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the Error Reduction Ration algorithm.\n\n        Parameters\n        ----------\n        y : array-like of shape = n_samples\n            The target data used in the identification process.\n        psi : ndarray of floats\n            The information matrix of the model.\n        process_term_number : int\n            Number of Process Terms defined by the user.\n\n        Returns\n        -------\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        psi_orthogonal : ndarray of floats\n            The updated and orthogonal information matrix.\n\n        References\n        ----------\n        - Manuscript: Orthogonal least squares methods and their application\n           to non-linear system identification\n           https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n        - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n           Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n           e Novos Resultados\n\n        \"\"\"\n        squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n        tmp_psi = psi.copy()\n        y = y[self.max_lag :, 0].reshape(-1, 1)\n        tmp_y = y.copy()\n        dimension = tmp_psi.shape[1]\n        piv = np.arange(dimension)\n        tmp_err = np.zeros(dimension)\n        err = np.zeros(dimension)\n\n        for i in np.arange(0, dimension):\n            for j in np.arange(i, dimension):\n                # Add `eps` in the denominator to omit division by zero if\n                # denominator is zero\n                # To implement regularized regression (ridge regression), add\n                # alpha to psi.T @ psi.   See S. Chen, Local regularization assisted\n                # orthogonal least squares regression, Neurocomputing 69 (2006) 559-585.\n                # The version implemented below uses the same regularization for every\n                # feature, # What Chen refers to Uniform regularized orthogonal least\n                # squares (UROLS) Set to tiny (self.eps) when you are not regularizing.\n                # alpha = eps is the default.\n                tmp_err[j] = (\n                    (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                    / (\n                        (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) + self.alpha)\n                        * squared_y\n                    )\n                    + self.eps\n                )[0, 0]\n\n            piv_index = np.argmax(tmp_err[i:]) + i\n            err[i] = tmp_err[piv_index]\n            if i == process_term_number:\n                break\n\n            if (self.err_tol is not None) and (err.cumsum()[i] &gt;= self.err_tol):\n                self.n_terms = i + 1\n                process_term_number = i + 1\n                break\n\n            tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n            piv[[piv_index, i]] = piv[[i, piv_index]]\n            v = house(tmp_psi[i:, i])\n            row_result = rowhouse(tmp_psi[i:, i:], v)\n            tmp_y[i:] = rowhouse(tmp_y[i:], v)\n            tmp_psi[i:, i:] = np.copy(row_result)\n\n        tmp_piv = piv[0:process_term_number]\n        psi_orthogonal = psi[:, tmp_piv]\n        return err, tmp_piv, psi_orthogonal\n\n    def run_mss_algorithm(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        return self.error_reduction_ratio(psi, y, process_term_number)\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n        \"\"\"Fit polynomial NARMAX model.\n\n        This is an 'alpha' version of the 'fit' function which allows\n        a friendly usage by the user. Given two arguments, x and y, fit\n        training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        super().fit(X=X, y=y)\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        yhat = super().predict(\n            X=X, y=y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        return yhat\n</code></pre>"},{"location":"user-guide/API/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.error_reduction_ratio","title":"<code>error_reduction_ratio(psi, y, process_term_number)</code>","text":"<p>Perform the Error Reduction Ration algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = n_samples</code> <p>The target data used in the identification process.</p> required <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>process_term_number</code> <code>int</code> <p>Number of Process Terms defined by the user.</p> required <p>Returns:</p> Name Type Description <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>psi_orthogonal</code> <code>ndarray of floats</code> <p>The updated and orthogonal information matrix.</p> References <ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def error_reduction_ratio(\n    self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the Error Reduction Ration algorithm.\n\n    Parameters\n    ----------\n    y : array-like of shape = n_samples\n        The target data used in the identification process.\n    psi : ndarray of floats\n        The information matrix of the model.\n    process_term_number : int\n        Number of Process Terms defined by the user.\n\n    Returns\n    -------\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    psi_orthogonal : ndarray of floats\n        The updated and orthogonal information matrix.\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n    squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n    tmp_psi = psi.copy()\n    y = y[self.max_lag :, 0].reshape(-1, 1)\n    tmp_y = y.copy()\n    dimension = tmp_psi.shape[1]\n    piv = np.arange(dimension)\n    tmp_err = np.zeros(dimension)\n    err = np.zeros(dimension)\n\n    for i in np.arange(0, dimension):\n        for j in np.arange(i, dimension):\n            # Add `eps` in the denominator to omit division by zero if\n            # denominator is zero\n            # To implement regularized regression (ridge regression), add\n            # alpha to psi.T @ psi.   See S. Chen, Local regularization assisted\n            # orthogonal least squares regression, Neurocomputing 69 (2006) 559-585.\n            # The version implemented below uses the same regularization for every\n            # feature, # What Chen refers to Uniform regularized orthogonal least\n            # squares (UROLS) Set to tiny (self.eps) when you are not regularizing.\n            # alpha = eps is the default.\n            tmp_err[j] = (\n                (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                / (\n                    (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) + self.alpha)\n                    * squared_y\n                )\n                + self.eps\n            )[0, 0]\n\n        piv_index = np.argmax(tmp_err[i:]) + i\n        err[i] = tmp_err[piv_index]\n        if i == process_term_number:\n            break\n\n        if (self.err_tol is not None) and (err.cumsum()[i] &gt;= self.err_tol):\n            self.n_terms = i + 1\n            process_term_number = i + 1\n            break\n\n        tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n        piv[[piv_index, i]] = piv[[i, piv_index]]\n        v = house(tmp_psi[i:, i])\n        row_result = rowhouse(tmp_psi[i:, i:], v)\n        tmp_y[i:] = rowhouse(tmp_y[i:], v)\n        tmp_psi[i:, i:] = np.copy(row_result)\n\n    tmp_piv = piv[0:process_term_number]\n    psi_orthogonal = psi[:, tmp_piv]\n    return err, tmp_piv, psi_orthogonal\n</code></pre>"},{"location":"user-guide/API/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.fit","title":"<code>fit(*, X=None, y)</code>","text":"<p>Fit polynomial NARMAX model.</p> <p>This is an 'alpha' version of the 'fit' function which allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>info_values</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n    \"\"\"Fit polynomial NARMAX model.\n\n    This is an 'alpha' version of the 'fit' function which allows\n    a friendly usage by the user. Given two arguments, x and y, fit\n    training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    super().fit(X=X, y=y)\n    return self\n</code></pre>"},{"location":"user-guide/API/frols/#sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS.predict","title":"<code>predict(*, X=None, y, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> required <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/forward_regression_orthogonal_least_squares.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: np.ndarray,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    yhat = super().predict(\n        X=X, y=y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    return yhat\n</code></pre>"},{"location":"user-guide/API/general-estimators/","title":"Documentation for <code>General Estimators</code>","text":"<p>Build NARX Models Using general estimators.</p>"},{"location":"user-guide/API/general-estimators/#sysidentpy.general_estimators.narx.NARX","title":"<code>NARX</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>NARX model build on top of general estimators.</p> <p>The Nonlinear AutoRegressive with eXogenous inputs (NARX) model is mathematically described by:</p> \\[     y(t) = F\\left(\\mathbf{\\phi}(t)\\right) + \\epsilon(t) \\] <p>where \\(\\mathbf{\\phi}(t)\\) is the regression vector composed of lagged inputs and outputs:</p> \\[     \\mathbf{\\phi}(t) = [y(t-1), \\ldots, y(t-n_y), x(t-1), \\ldots, x(t-n_x)] \\] <p>Here, \\(n_y\\) (<code>ylag</code>) and \\(n_x\\) (<code>xlag</code>) are the maximum lags for the output and input, respectively. The function \\(F\\) is approximated by the base estimator. For NARMAX models, the regression vector includes lagged residuals:</p> \\[     \\mathbf{\\phi}(t) = [y(t-1), \\ldots, y(t-n_y), x(t-1), \\ldots, x(t-n_x),     \\epsilon(t-1), \\ldots, \\epsilon(t-n_e)] \\] <p>where \\(n_e\\) is determined by the basis function and <code>model_type</code> parameter.</p> <p>This implementation uses <code>GenerateRegressors</code> and <code>InformationMatrix</code> to construct lagged features and allows infinite-step-ahead prediction via iterative methods.</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag order of the output \\(n_y\\) (number of past output terms used).</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag order of the input \\(n_x\\) (number of past input terms used).</p> <code>2</code> <code>fit_params</code> <code>dict</code> <p>Additional parameters to pass to the <code>fit</code> method of the base estimator.</p> <code>None</code> <code>base_estimator</code> <code>estimator object</code> <p>An sklearn-compatible estimator with <code>fit</code> and <code>predict</code> methods.</p> <code>None</code> <code>basis_function</code> <code>basis function object</code> <p>Nonlinear transformation applied to regressors (e.g., Polynomial, Fourier).</p> <code>Polynomial</code> <code>model_type</code> <code>(NARMAX, NAR, NFIR)</code> <p>Model structure. Use \"NARMAX\" to include lagged residuals in the regression vector.</p> <code>\"NARMAX\"</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sysidentpy.general_estimators import NARX\n&gt;&gt;&gt; from sklearn.linear_model import BayesianRidge\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n&gt;&gt;&gt; # Generate data and fit model\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = NARX(\n...     base_estimator=BayesianRidge(),\n...     xlag=2,\n...     ylag=2,\n...     basis_function=basis_function,\n...     model_type=\"NARMAX\"\n... )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; # Evaluation and plotting code here\n0.000131\n</code></pre> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>class NARX(BaseMSS):\n    r\"\"\"NARX model build on top of general estimators.\n\n    The Nonlinear AutoRegressive with eXogenous inputs (NARX) model is mathematically\n    described by:\n\n    $$\n        y(t) = F\\left(\\mathbf{\\phi}(t)\\right) + \\epsilon(t)\n    $$\n\n    where $\\mathbf{\\phi}(t)$ is the regression vector composed of lagged inputs and\n    outputs:\n\n    $$\n        \\mathbf{\\phi}(t) = [y(t-1), \\ldots, y(t-n_y), x(t-1), \\ldots, x(t-n_x)]\n    $$\n\n    Here, $n_y$ (``ylag``) and $n_x$ (``xlag``) are the maximum lags for the output and\n    input, respectively. The function $F$ is approximated by the base estimator. For\n    NARMAX models, the regression vector includes lagged residuals:\n\n    $$\n        \\mathbf{\\phi}(t) = [y(t-1), \\ldots, y(t-n_y), x(t-1), \\ldots, x(t-n_x),\n        \\epsilon(t-1), \\ldots, \\epsilon(t-n_e)]\n    $$\n\n    where $n_e$ is determined by the basis function and ``model_type`` parameter.\n\n    This implementation uses ``GenerateRegressors`` and ``InformationMatrix`` to\n    construct lagged features and allows infinite-step-ahead prediction via iterative\n    methods.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag order of the output $n_y$ (number of past output terms used).\n    xlag : int, default=2\n        The maximum lag order of the input $n_x$ (number of past input terms used).\n    fit_params : dict, default=None\n        Additional parameters to pass to the ``fit`` method of the base estimator.\n    base_estimator : estimator object, default=None\n        An sklearn-compatible estimator with ``fit`` and ``predict`` methods.\n    basis_function : basis function object, default=Polynomial\n        Nonlinear transformation applied to regressors (e.g., Polynomial, Fourier).\n    model_type : {\"NARMAX\", \"NAR\", \"NFIR\"}, default=\"NARMAX\"\n        Model structure. Use \"NARMAX\" to include lagged residuals in the regression\n        vector.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sysidentpy.general_estimators import NARX\n    &gt;&gt;&gt; from sklearn.linear_model import BayesianRidge\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; # Generate data and fit model\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = NARX(\n    ...     base_estimator=BayesianRidge(),\n    ...     xlag=2,\n    ...     ylag=2,\n    ...     basis_function=basis_function,\n    ...     model_type=\"NARMAX\"\n    ... )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; # Evaluation and plotting code here\n    0.000131\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[List[Any], Any] = 1,\n        xlag: Union[List[Any], Any] = 1,\n        model_type: str = \"NARMAX\",\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        base_estimator=None,\n        fit_params=None,\n    ):\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.non_degree = basis_function.degree\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.base_estimator = base_estimator\n        if fit_params is None:\n            fit_params = {}\n\n        self.fit_params = fit_params\n        self.ensemble = None\n        self.n_inputs = None\n        self.regressor_code = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def fit(self, *, X=None, y=None):\n        \"\"\"Train a NARX Neural Network model.\n\n        This is a training pipeline that allows a friendly usage\n        by the user. All the lagged features are built using the\n        SysIdentPy classes and we use the fit method of the base\n        estimator of the sklearn to fit the model.\n\n        Parameters\n        ----------\n        X : ndarrays of floats\n            The input data to be used in the training process.\n        y : ndarrays of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        base_estimator : sklearn estimator\n            The model fitted.\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n        if X is not None:\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        self.final_model = self.regressor_code\n        y = y[self.max_lag :].ravel()\n\n        self.base_estimator.fit(reg_matrix, y, **self.fit_params)\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[NDArray] = None,\n        y: Optional[NDArray] = None,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = 1,\n    ) -&gt; NDArray:\n        \"\"\"Return the predicted given an input and initial values.\n\n        The predict function allows a friendly usage by the user.\n        Given a trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future).\n\n        Currently, we only support infinity-steps-ahead prediction,\n        but run 1-step-ahead prediction manually is straightforward.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(self, x, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n        x_base = self.basis_function.transform(\n            lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n        )\n\n        yhat = self.base_estimator.predict(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _nar_step_ahead(self, y, steps_ahead):\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        yhat = np.zeros(len(y) + steps_ahead, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    x=None, y_initial=y[step:i], forecast_horizon=steps_ahead\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=None, y_initial=y[steps[-1] : i]\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=None, y_initial=y[0:i], forecast_horizon=steps_ahead\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    def narmax_n_step_ahead(self, x, y, steps_ahead):\n        \"\"\"N steps ahead prediction method for NARMAX model.\"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        x = x.reshape(-1, self.n_inputs)\n        yhat = np.zeros(x.shape[0], dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    x=x[step : i + steps_ahead],\n                    y_initial=y[step:i],\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[steps[-1] : i + steps_ahead],\n                y_initial=y[steps[-1] : i],\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[0 : i + steps_ahead],\n                y_initial=y[0:i],\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, x, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if self.model_type == \"NARMAX\":\n            return self.narmax_n_step_ahead(x, y, steps_ahead)\n\n        if self.model_type == \"NAR\":\n            return self._nar_step_ahead(y, steps_ahead)\n\n    def _model_prediction(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(self, x, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = np.zeros(forecast_horizon, dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, forecast_horizon):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            y_output[i] = self.base_estimator.predict(regressor_value.reshape(1, -1))[0]\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    def _nfir_predict(self, x, y_initial):\n        y_output = np.zeros(x.shape[0], dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n        x = x.reshape(-1, self.n_inputs)\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, x.shape[0]):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            y_output[i] = self.base_estimator.predict(regressor_value.reshape(1, -1))[0]\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        analyzed_elements_number = self.max_lag + 1\n\n        for i in range(forecast_horizon - self.max_lag):\n            lagged_data = build_lagged_matrix(\n                x[i : i + analyzed_elements_number],\n                yhat[i : i + analyzed_elements_number].reshape(-1, 1),\n                self.xlag,\n                self.ylag,\n                self.model_type,\n            )\n            x_tmp = self.basis_function.transform(\n                lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n            )\n\n            a = self.base_estimator.predict(x_tmp)\n            yhat[i + self.max_lag] = a[0]\n\n        return yhat[self.max_lag :].reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead],\n                    y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        return yhat[self.max_lag : :].reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(self, x, y, steps_ahead, forecast_horizon):\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead],\n                    y[k : i + steps_ahead],\n                    forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat[self.max_lag : :].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/general-estimators/#sysidentpy.general_estimators.narx.NARX.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Train a NARX Neural Network model.</p> <p>This is a training pipeline that allows a friendly usage by the user. All the lagged features are built using the SysIdentPy classes and we use the fit method of the base estimator of the sklearn to fit the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarrays of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarrays of floats</code> <p>The output data to be used in the training process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>base_estimator</code> <code>sklearn estimator</code> <p>The model fitted.</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>def fit(self, *, X=None, y=None):\n    \"\"\"Train a NARX Neural Network model.\n\n    This is a training pipeline that allows a friendly usage\n    by the user. All the lagged features are built using the\n    SysIdentPy classes and we use the fit method of the base\n    estimator of the sklearn to fit the model.\n\n    Parameters\n    ----------\n    X : ndarrays of floats\n        The input data to be used in the training process.\n    y : ndarrays of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    base_estimator : sklearn estimator\n        The model fitted.\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n    reg_matrix = self.basis_function.fit(\n        lagged_data,\n        self.max_lag,\n        self.ylag,\n        self.xlag,\n        self.model_type,\n        predefined_regressors=None,\n    )\n\n    if X is not None:\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    self.final_model = self.regressor_code\n    y = y[self.max_lag :].ravel()\n\n    self.base_estimator.fit(reg_matrix, y, **self.fit_params)\n    return self\n</code></pre>"},{"location":"user-guide/API/general-estimators/#sysidentpy.general_estimators.narx.NARX.narmax_n_step_ahead","title":"<code>narmax_n_step_ahead(x, y, steps_ahead)</code>","text":"<p>N steps ahead prediction method for NARMAX model.</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>def narmax_n_step_ahead(self, x, y, steps_ahead):\n    \"\"\"N steps ahead prediction method for NARMAX model.\"\"\"\n    if len(y) &lt; self.max_lag:\n        raise ValueError(\n            \"Insufficient initial condition elements! Expected at least\"\n            f\" {self.max_lag} elements.\"\n        )\n\n    to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n    x = x.reshape(-1, self.n_inputs)\n    yhat = np.zeros(x.shape[0], dtype=float)\n    yhat.fill(np.nan)\n    yhat[: self.max_lag] = y[: self.max_lag, 0]\n    i = self.max_lag\n    steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n    if len(steps) &gt; 1:\n        for step in steps[:-1]:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[step : i + steps_ahead],\n                y_initial=y[step:i],\n            )[-steps_ahead:].ravel()\n            i += steps_ahead\n\n        steps_ahead = np.sum(np.isnan(yhat))\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            x=x[steps[-1] : i + steps_ahead],\n            y_initial=y[steps[-1] : i],\n        )[-steps_ahead:].ravel()\n    else:\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            x=x[0 : i + steps_ahead],\n            y_initial=y[0:i],\n        )[-steps_ahead:].ravel()\n\n    yhat = yhat.ravel()[self.max_lag : :]\n    return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/general-estimators/#sysidentpy.general_estimators.narx.NARX.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=1)</code>","text":"<p>Return the predicted given an input and initial values.</p> <p>The predict function allows a friendly usage by the user. Given a trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future).</p> <p>Currently, we only support infinity-steps-ahead prediction, but run 1-step-ahead prediction manually is straightforward.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/general_estimators/narx.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[NDArray] = None,\n    y: Optional[NDArray] = None,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: Optional[int] = 1,\n) -&gt; NDArray:\n    \"\"\"Return the predicted given an input and initial values.\n\n    The predict function allows a friendly usage by the user.\n    Given a trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future).\n\n    Currently, we only support infinity-steps-ahead prediction,\n    but run 1-step-ahead prediction manually is straightforward.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/metaheuristics/","title":"Documentation for <code>Metaheuristics</code>","text":"<p>Binary Hybrid Particle Swarm Optimization and Gravitational Search Algorithm.</p>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA","title":"<code>BPSOGSA</code>","text":"<p>Binary Hybrid Particle Swarm Optimization and Gravitational Search Algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>maxiter</code> <code>int</code> <p>The maximum number of iterations.</p> <code>30</code> <code>alpha</code> <code>int</code> <p>The descending coefficient of the gravitational constant.</p> <code>23</code> <code>g_zero</code> <code>int</code> <p>The initial value of the gravitational constant.</p> <code>100</code> <code>k_agents_percent</code> <p>Percent of agents applying force to the others in the last iteration.</p> <code>2</code> <code>norm</code> <code>int</code> <p>The information criteria method to be used.</p> <code>-2</code> <code>power</code> <code>int</code> <p>The number of the model terms to be selected. Note that n_terms overwrite the information criteria values.</p> <code>2</code> <code>n_agents</code> <code>int</code> <p>The number of agents to search the optimal solution.</p> <code>10</code> <code>dimension</code> <code>int</code> <p>The dimension of the search space. criteria method.</p> <code>15</code> <code>p_zeros</code> <code>float</code> <p>The probability of getting ones in the construction of the population.</p> <code>0.5</code> <code>p_zeros</code> <code>float</code> <p>The probability of getting zeros in the construction of the population.</p> <code>0.5</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.metaheuristics import BPSOGSA\n&gt;&gt;&gt; opt = BPSOGSA(maxiter=100,\n...               k_agents_percent=2,\n...               n_agents=10,\n...               dimension=20\n...               )\n&gt;&gt;&gt; opt.optimize()\n&gt;&gt;&gt; plt.plot(opt.best_by_iter)\n&gt;&gt;&gt; plt.show()\n&gt;&gt;&gt; print(opt.optimal_fitness_value)\n</code></pre> References <ul> <li>A New Hybrid PSOGSA Algorithm for Function Optimization,    https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa</li> <li>Manuscript: Particle swarm optimization: developments, applications and resources.</li> <li>Manuscript: S-shaped versus V-shaped transfer functions for binary    particle swarm optimization</li> <li>Manuscript: BGSA: Binary Gravitational Search Algorithm.</li> <li>Manuscript: A taxonomy of hybrid metaheuristics</li> </ul> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>class BPSOGSA:\n    \"\"\"Binary Hybrid Particle Swarm Optimization and Gravitational Search Algorithm.\n\n    Parameters\n    ----------\n    maxiter : int, default=30\n        The maximum number of iterations.\n    alpha : int, default=23\n        The descending coefficient of the gravitational constant.\n    g_zero : int, default=100\n        The initial value of the gravitational constant.\n    k_agents_percent: int, default=2\n        Percent of agents applying force to the others in the last iteration.\n    norm : int, default=-2\n        The information criteria method to be used.\n    power : int, default=2\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_agents : int, default=10\n        The number of agents to search the optimal solution.\n    dimension : int, default=15\n        The dimension of the search space.\n        criteria method.\n    p_zeros : float, default=0.5\n        The probability of getting ones in the construction of the population.\n    p_zeros : float, default=0.5\n        The probability of getting zeros in the construction of the population.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.metaheuristics import BPSOGSA\n    &gt;&gt;&gt; opt = BPSOGSA(maxiter=100,\n    ...               k_agents_percent=2,\n    ...               n_agents=10,\n    ...               dimension=20\n    ...               )\n    &gt;&gt;&gt; opt.optimize()\n    &gt;&gt;&gt; plt.plot(opt.best_by_iter)\n    &gt;&gt;&gt; plt.show()\n    &gt;&gt;&gt; print(opt.optimal_fitness_value)\n\n    References\n    ----------\n    - A New Hybrid PSOGSA Algorithm for Function Optimization,\n       https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa\n    - Manuscript: Particle swarm optimization: developments, applications and resources.\n    - Manuscript: S-shaped versus V-shaped transfer functions for binary\n       particle swarm optimization\n    - Manuscript: BGSA: Binary Gravitational Search Algorithm.\n    - Manuscript: A taxonomy of hybrid metaheuristics\n\n    \"\"\"\n\n    def __init__(\n        self,\n        maxiter=30,\n        alpha=23,\n        g_zero=100,\n        k_agents_percent=2,\n        norm=-2,\n        power=2,\n        n_agents=10,\n        dimension=15,\n        p_zeros=0.5,\n        p_ones=0.5,\n    ):\n        self.dimension = dimension\n        self.n_agents = n_agents\n        self.maxiter = maxiter\n        self.g_zero = g_zero\n        self.alpha = alpha\n        self.k_agents_percent = k_agents_percent\n        self.norm = norm\n        self.power = power\n        self.p_zeros = p_zeros\n        self.p_ones = p_ones\n        self.best_by_iter = None\n        self.mean_by_iter = None\n        self.optimal_fitness_value = None\n        self.optimal_model = None\n        super(BPSOGSA, self).__init__()\n\n    def evaluate_objective_function(self, candidate_solution):\n        \"\"\"Define a function to be optimized.\"\"\"\n        total = 0\n        for candidate in candidate_solution:\n            total += candidate**2\n        return total\n\n    def optimize(self):\n        \"\"\"Run the BPSOGSA algorithm.\n\n        This algorithm is based on the Matlab implementation provided by the\n        author of the BPSOGSA algorithm.\n\n        References\n        ----------\n        - A New Hybrid PSOGSA Algorithm for Function Optimization.\n           https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa\n        - Manuscript: Particle swarm optimization: developments, applications and\n            resources.\n        - Manuscript: S-shaped versus V-shaped transfer functions for binary.\n           particle swarm optimization\n        - Manuscript: BGSA: Binary Gravitational Search Algorithm.\n        - Manuscript: A taxonomy of hybrid metaheuristics.\n\n        \"\"\"\n        velocity = np.zeros([self.dimension, self.n_agents])\n        population = self.generate_random_population()\n        self.best_by_iter = []\n        self.mean_by_iter = []\n        self.optimal_fitness_value = np.inf\n        self.optimal_model = None\n\n        for i in range(self.maxiter):\n            fitness = self.evaluate_objective_function(population)\n\n            column_of_best_solution = np.argmin(fitness)\n            current_best_fitness = fitness[column_of_best_solution]\n\n            if current_best_fitness &lt; self.optimal_fitness_value:\n                self.optimal_fitness_value = current_best_fitness\n                self.optimal_model = population[:, column_of_best_solution].copy()\n\n            self.best_by_iter.append(self.optimal_fitness_value)\n            self.mean_by_iter.append(np.mean(fitness))\n            agent_mass = self.mass_calculation(fitness)\n            gravitational_constant = self.calculate_gravitational_constant(i)\n            acceleration = self.calculate_acceleration(\n                population, agent_mass, gravitational_constant, i\n            )\n            velocity, population = self.update_velocity_position(\n                population,\n                acceleration,\n                velocity,\n                i,\n            )\n\n        return self\n\n    def generate_random_population(self, random_state=None):\n        \"\"\"Generate the initial population of agents randomly.\n\n        Returns\n        -------\n        population : ndarray of zeros and ones\n            The initial population of agents.\n\n        \"\"\"\n        rng = check_random_state(random_state)\n        population = rng.choice(\n            [0, 1], size=(self.dimension, self.n_agents), p=[self.p_zeros, self.p_ones]\n        )\n        return population\n\n    def mass_calculation(self, fitness_value):\n        \"\"\"Calculate the inertial masses of the agents.\n\n        Parameters\n        ----------\n        fitness_value : ndarray\n            The fitness value of each agent.\n\n        Returns\n        -------\n        agent_mass : ndarray of floats\n            The mass of each agent.\n\n        \"\"\"\n        highest_fitness_value = np.nanmax(fitness_value)\n        lowest_fitness_value = np.nanmin(fitness_value)\n\n        column_fitness = len(fitness_value)\n        if highest_fitness_value == lowest_fitness_value:\n            agent_mass = np.ones([column_fitness, 1])\n        else:\n            best_fitness_value = lowest_fitness_value\n            worst_fitness_value = highest_fitness_value\n            agent_mass = (fitness_value - 0.99 * worst_fitness_value) / (\n                best_fitness_value - worst_fitness_value\n            )\n\n        agent_mass = (5 * agent_mass) / np.sum(agent_mass)\n        return agent_mass\n\n    def calculate_gravitational_constant(self, iteration):\n        \"\"\"Update the gravitational constant.\n\n        Parameters\n        ----------\n        iteration : int\n            The specific time.\n\n        Returns\n        -------\n        gravitational_constant : float\n            The gravitational_constant at time defined by the iteration.\n\n        \"\"\"\n        gravitational_constant = self.g_zero * np.exp(\n            (-self.alpha * (iteration + 1)) / self.maxiter\n        )\n\n        return gravitational_constant\n\n    def calculate_acceleration(\n        self, population, agent_mass, gravitational_constant, iteration\n    ):\n        \"\"\"Calculate the acceleration of each agent.\n\n        Parameters\n        ----------\n        population : ndarray of zeros and ones\n            The population defined by the agents.\n        agent_mass : ndarray of floats\n            The mass of each agent.\n        gravitational_constant : float\n            The gravitational_constant at time defined by the iteration.\n        iteration : int\n            The current iteration.\n\n        Returns\n        -------\n        acceleration : ndarray of floats\n            The acceleration of each agent.\n\n        \"\"\"\n        k_best_agents = self.k_agents_percent + (1 - iteration / self.maxiter) * (\n            100 - self.k_agents_percent\n        )\n        k_best_agents = round(self.n_agents * k_best_agents / 100)\n\n        maximum_value_index = np.argsort(agent_mass)[::-1].ravel()\n        gravitational_force = np.zeros([self.dimension, self.n_agents])\n        for i in range(self.n_agents):\n            for j in range(k_best_agents):\n                if maximum_value_index[j] != i:\n                    euclidean_distance = np.linalg.norm(\n                        population[:, i] - population[:, maximum_value_index[j]],\n                        self.norm,\n                    )\n                    gravitational_force[:, i] = gravitational_force[\n                        :, i\n                    ] + np.random.rand(self.dimension) * agent_mass[\n                        maximum_value_index[j]\n                    ] * (\n                        population[:, maximum_value_index[j]] - population[:, i]\n                    ) / (\n                        euclidean_distance**self.power + np.finfo(np.float64).eps\n                    )\n\n        acceleration = gravitational_force * gravitational_constant\n        return acceleration\n\n    def update_velocity_position(\n        self,\n        population,\n        acceleration,\n        velocity,\n        iteration,\n    ):\n        \"\"\"Update the velocity and position of each agent.\n\n        Parameters\n        ----------\n        population : ndarray of zeros and ones\n            The population defined by the agents.\n        acceleration : ndarray of floats\n            The acceleration of each agent.\n        velocity : ndarray of floats\n            The velocity of each agent.\n        iteration : int\n            The current iteration.\n\n        Returns\n        -------\n        velocity : ndarray of floats\n            The updated velocity of each agent.\n        population : ndarray of zeros and ones\n            The updated population defined by the agents.\n\n        \"\"\"\n        c_factor_local_best = -2 * ((iteration**3) / (self.maxiter**3)) + 2\n        c_factor_global_best = 2 * ((iteration**3) / (self.maxiter**3)) + 2\n        global_best = np.repeat(self.optimal_model, self.n_agents, axis=0).reshape(\n            self.dimension, self.n_agents\n        )\n\n        velocity = (\n            np.random.rand(self.dimension, self.n_agents) * velocity\n            + c_factor_local_best * acceleration\n            + c_factor_global_best * (global_best - population)\n        )\n        r = np.random.rand(self.dimension, self.n_agents)\n        transform_to_binary = np.absolute(np.tanh(velocity))\n        ind = np.where(r &lt; transform_to_binary)\n        population[ind] = 1 - population[ind]\n        return velocity, population\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_acceleration","title":"<code>calculate_acceleration(population, agent_mass, gravitational_constant, iteration)</code>","text":"<p>Calculate the acceleration of each agent.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>ndarray of zeros and ones</code> <p>The population defined by the agents.</p> required <code>agent_mass</code> <code>ndarray of floats</code> <p>The mass of each agent.</p> required <code>gravitational_constant</code> <code>float</code> <p>The gravitational_constant at time defined by the iteration.</p> required <code>iteration</code> <code>int</code> <p>The current iteration.</p> required <p>Returns:</p> Name Type Description <code>acceleration</code> <code>ndarray of floats</code> <p>The acceleration of each agent.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def calculate_acceleration(\n    self, population, agent_mass, gravitational_constant, iteration\n):\n    \"\"\"Calculate the acceleration of each agent.\n\n    Parameters\n    ----------\n    population : ndarray of zeros and ones\n        The population defined by the agents.\n    agent_mass : ndarray of floats\n        The mass of each agent.\n    gravitational_constant : float\n        The gravitational_constant at time defined by the iteration.\n    iteration : int\n        The current iteration.\n\n    Returns\n    -------\n    acceleration : ndarray of floats\n        The acceleration of each agent.\n\n    \"\"\"\n    k_best_agents = self.k_agents_percent + (1 - iteration / self.maxiter) * (\n        100 - self.k_agents_percent\n    )\n    k_best_agents = round(self.n_agents * k_best_agents / 100)\n\n    maximum_value_index = np.argsort(agent_mass)[::-1].ravel()\n    gravitational_force = np.zeros([self.dimension, self.n_agents])\n    for i in range(self.n_agents):\n        for j in range(k_best_agents):\n            if maximum_value_index[j] != i:\n                euclidean_distance = np.linalg.norm(\n                    population[:, i] - population[:, maximum_value_index[j]],\n                    self.norm,\n                )\n                gravitational_force[:, i] = gravitational_force[\n                    :, i\n                ] + np.random.rand(self.dimension) * agent_mass[\n                    maximum_value_index[j]\n                ] * (\n                    population[:, maximum_value_index[j]] - population[:, i]\n                ) / (\n                    euclidean_distance**self.power + np.finfo(np.float64).eps\n                )\n\n    acceleration = gravitational_force * gravitational_constant\n    return acceleration\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.calculate_gravitational_constant","title":"<code>calculate_gravitational_constant(iteration)</code>","text":"<p>Update the gravitational constant.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>The specific time.</p> required <p>Returns:</p> Name Type Description <code>gravitational_constant</code> <code>float</code> <p>The gravitational_constant at time defined by the iteration.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def calculate_gravitational_constant(self, iteration):\n    \"\"\"Update the gravitational constant.\n\n    Parameters\n    ----------\n    iteration : int\n        The specific time.\n\n    Returns\n    -------\n    gravitational_constant : float\n        The gravitational_constant at time defined by the iteration.\n\n    \"\"\"\n    gravitational_constant = self.g_zero * np.exp(\n        (-self.alpha * (iteration + 1)) / self.maxiter\n    )\n\n    return gravitational_constant\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.evaluate_objective_function","title":"<code>evaluate_objective_function(candidate_solution)</code>","text":"<p>Define a function to be optimized.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def evaluate_objective_function(self, candidate_solution):\n    \"\"\"Define a function to be optimized.\"\"\"\n    total = 0\n    for candidate in candidate_solution:\n        total += candidate**2\n    return total\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.generate_random_population","title":"<code>generate_random_population(random_state=None)</code>","text":"<p>Generate the initial population of agents randomly.</p> <p>Returns:</p> Name Type Description <code>population</code> <code>ndarray of zeros and ones</code> <p>The initial population of agents.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def generate_random_population(self, random_state=None):\n    \"\"\"Generate the initial population of agents randomly.\n\n    Returns\n    -------\n    population : ndarray of zeros and ones\n        The initial population of agents.\n\n    \"\"\"\n    rng = check_random_state(random_state)\n    population = rng.choice(\n        [0, 1], size=(self.dimension, self.n_agents), p=[self.p_zeros, self.p_ones]\n    )\n    return population\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.mass_calculation","title":"<code>mass_calculation(fitness_value)</code>","text":"<p>Calculate the inertial masses of the agents.</p> <p>Parameters:</p> Name Type Description Default <code>fitness_value</code> <code>ndarray</code> <p>The fitness value of each agent.</p> required <p>Returns:</p> Name Type Description <code>agent_mass</code> <code>ndarray of floats</code> <p>The mass of each agent.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def mass_calculation(self, fitness_value):\n    \"\"\"Calculate the inertial masses of the agents.\n\n    Parameters\n    ----------\n    fitness_value : ndarray\n        The fitness value of each agent.\n\n    Returns\n    -------\n    agent_mass : ndarray of floats\n        The mass of each agent.\n\n    \"\"\"\n    highest_fitness_value = np.nanmax(fitness_value)\n    lowest_fitness_value = np.nanmin(fitness_value)\n\n    column_fitness = len(fitness_value)\n    if highest_fitness_value == lowest_fitness_value:\n        agent_mass = np.ones([column_fitness, 1])\n    else:\n        best_fitness_value = lowest_fitness_value\n        worst_fitness_value = highest_fitness_value\n        agent_mass = (fitness_value - 0.99 * worst_fitness_value) / (\n            best_fitness_value - worst_fitness_value\n        )\n\n    agent_mass = (5 * agent_mass) / np.sum(agent_mass)\n    return agent_mass\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.optimize","title":"<code>optimize()</code>","text":"<p>Run the BPSOGSA algorithm.</p> <p>This algorithm is based on the Matlab implementation provided by the author of the BPSOGSA algorithm.</p> References <ul> <li>A New Hybrid PSOGSA Algorithm for Function Optimization.    https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa</li> <li>Manuscript: Particle swarm optimization: developments, applications and     resources.</li> <li>Manuscript: S-shaped versus V-shaped transfer functions for binary.    particle swarm optimization</li> <li>Manuscript: BGSA: Binary Gravitational Search Algorithm.</li> <li>Manuscript: A taxonomy of hybrid metaheuristics.</li> </ul> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def optimize(self):\n    \"\"\"Run the BPSOGSA algorithm.\n\n    This algorithm is based on the Matlab implementation provided by the\n    author of the BPSOGSA algorithm.\n\n    References\n    ----------\n    - A New Hybrid PSOGSA Algorithm for Function Optimization.\n       https://www.mathworks.com/matlabcentral/fileexchange/35939-hybrid-particle-swarm-optimization-and-gravitational-search-algorithm-psogsa\n    - Manuscript: Particle swarm optimization: developments, applications and\n        resources.\n    - Manuscript: S-shaped versus V-shaped transfer functions for binary.\n       particle swarm optimization\n    - Manuscript: BGSA: Binary Gravitational Search Algorithm.\n    - Manuscript: A taxonomy of hybrid metaheuristics.\n\n    \"\"\"\n    velocity = np.zeros([self.dimension, self.n_agents])\n    population = self.generate_random_population()\n    self.best_by_iter = []\n    self.mean_by_iter = []\n    self.optimal_fitness_value = np.inf\n    self.optimal_model = None\n\n    for i in range(self.maxiter):\n        fitness = self.evaluate_objective_function(population)\n\n        column_of_best_solution = np.argmin(fitness)\n        current_best_fitness = fitness[column_of_best_solution]\n\n        if current_best_fitness &lt; self.optimal_fitness_value:\n            self.optimal_fitness_value = current_best_fitness\n            self.optimal_model = population[:, column_of_best_solution].copy()\n\n        self.best_by_iter.append(self.optimal_fitness_value)\n        self.mean_by_iter.append(np.mean(fitness))\n        agent_mass = self.mass_calculation(fitness)\n        gravitational_constant = self.calculate_gravitational_constant(i)\n        acceleration = self.calculate_acceleration(\n            population, agent_mass, gravitational_constant, i\n        )\n        velocity, population = self.update_velocity_position(\n            population,\n            acceleration,\n            velocity,\n            i,\n        )\n\n    return self\n</code></pre>"},{"location":"user-guide/API/metaheuristics/#sysidentpy.metaheuristics.bpsogsa.BPSOGSA.update_velocity_position","title":"<code>update_velocity_position(population, acceleration, velocity, iteration)</code>","text":"<p>Update the velocity and position of each agent.</p> <p>Parameters:</p> Name Type Description Default <code>population</code> <code>ndarray of zeros and ones</code> <p>The population defined by the agents.</p> required <code>acceleration</code> <code>ndarray of floats</code> <p>The acceleration of each agent.</p> required <code>velocity</code> <code>ndarray of floats</code> <p>The velocity of each agent.</p> required <code>iteration</code> <code>int</code> <p>The current iteration.</p> required <p>Returns:</p> Name Type Description <code>velocity</code> <code>ndarray of floats</code> <p>The updated velocity of each agent.</p> <code>population</code> <code>ndarray of zeros and ones</code> <p>The updated population defined by the agents.</p> Source code in <code>sysidentpy/metaheuristics/bpsogsa.py</code> <pre><code>def update_velocity_position(\n    self,\n    population,\n    acceleration,\n    velocity,\n    iteration,\n):\n    \"\"\"Update the velocity and position of each agent.\n\n    Parameters\n    ----------\n    population : ndarray of zeros and ones\n        The population defined by the agents.\n    acceleration : ndarray of floats\n        The acceleration of each agent.\n    velocity : ndarray of floats\n        The velocity of each agent.\n    iteration : int\n        The current iteration.\n\n    Returns\n    -------\n    velocity : ndarray of floats\n        The updated velocity of each agent.\n    population : ndarray of zeros and ones\n        The updated population defined by the agents.\n\n    \"\"\"\n    c_factor_local_best = -2 * ((iteration**3) / (self.maxiter**3)) + 2\n    c_factor_global_best = 2 * ((iteration**3) / (self.maxiter**3)) + 2\n    global_best = np.repeat(self.optimal_model, self.n_agents, axis=0).reshape(\n        self.dimension, self.n_agents\n    )\n\n    velocity = (\n        np.random.rand(self.dimension, self.n_agents) * velocity\n        + c_factor_local_best * acceleration\n        + c_factor_global_best * (global_best - population)\n    )\n    r = np.random.rand(self.dimension, self.n_agents)\n    transform_to_binary = np.absolute(np.tanh(velocity))\n    ind = np.where(r &lt; transform_to_binary)\n    population[ind] = 1 - population[ind]\n    return velocity, population\n</code></pre>"},{"location":"user-guide/API/metamss/","title":"Documentation for <code>MetaMSS</code>","text":"<p>Meta Model Structure Selection.</p>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS","title":"<code>MetaMSS</code>","text":"<p>               Bases: <code>SimulateNARMAX</code>, <code>BPSOGSA</code></p> <p>Meta-Model Structure Selection: Building Polynomial NARMAX model.</p> <p>This class uses the MetaMSS ([1], [2], [3]_) algorithm to build NARMAX models. The NARMAX model is described as:</p> \\[     y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},     e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input and output regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>loss_func</code> <code>str</code> <p>The loss function to be minimized.</p> <code>\"metamss_loss\"</code> <code>estimator</code> <code>str</code> <p>The parameter estimation method.</p> <code>\"least_squares\"</code> <code>estimate_parameter</code> <code>bool</code> <p>Whether to estimate the model parameters.</p> <code>True</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>eps</code> <code>maxiter</code> <code>int</code> <p>The maximum number of iterations.</p> <code>30</code> <code>alpha</code> <code>int</code> <p>The descending coefficient of the gravitational constant.</p> <code>23</code> <code>g_zero</code> <code>int</code> <p>The initial value of the gravitational constant.</p> <code>100</code> <code>k_agents_percent</code> <code>int</code> <p>Percent of agents applying force to the others in the last iteration.</p> <code>2</code> <code>norm</code> <code>int</code> <p>The information criteria method to be used.</p> <code>-2</code> <code>power</code> <code>int</code> <p>The number of the model terms to be selected. Note that n_terms overwrite the information criteria values.</p> <code>2</code> <code>n_agents</code> <code>int</code> <p>The number of agents to search the optimal solution.</p> <code>10</code> <code>p_zeros</code> <code>float</code> <p>The probability of getting ones in the construction of the population.</p> <code>0.5</code> <code>p_zeros</code> <code>float</code> <p>The probability of getting zeros in the construction of the population.</p> <code>0.5</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import MetaMSS\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=400,\n...                                                    colored_noise=False,\n...                                                    sigma=0.001,\n...                                                    train_percentage=80)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = MetaMSS(\n...     basis_function=basis_function,\n...     norm=-2,\n...     xlag=7,\n...     ylag=7,\n...     k_agents_percent=2,\n...     estimate_parameter=True,\n...     maxiter=30,\n...     n_agents=10,\n...     p_value=0.05,\n...     loss_func='metamss_loss'\n... )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Manuscript: Meta-Model Structure Selection: Building Polynomial NARX Model    for Regression and Classification    https://arxiv.org/pdf/2109.09917.pdf</li> <li>Manuscript (Portuguese): Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares    Utilizando o Algoritmo H\u00edbrido e Bin\u00e1rio de Otimiza\u00e7\u00e3o por    Enxame de Part\u00edculas e Busca Gravitacional    DOI: 10.17648/sbai-2019-111317</li> <li>Master thesis: Meta model structure selection: an algorithm for    building polynomial NARX models for regression and classification</li> </ul> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>class MetaMSS(SimulateNARMAX, BPSOGSA):\n    r\"\"\"Meta-Model Structure Selection: Building Polynomial NARMAX model.\n\n    This class uses the MetaMSS ([1]_, [2]_, [3]_) algorithm to build NARMAX models.\n    The NARMAX model is described as:\n\n    $$\n        y_k= F^\\ell[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x},\n        e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}$ stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}^\\ell$ is some nonlinear function\n    of the input and output regressors with nonlinearity degree $\\ell \\in \\mathbb{N}$\n    and $d$ is a time delay typically set to $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    loss_func : str, default=\"metamss_loss\"\n        The loss function to be minimized.\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    estimate_parameter : bool, default=True\n        Whether to estimate the model parameters.\n    eps : float\n        Normalization factor of the normalized filters.\n    maxiter : int, default=30\n        The maximum number of iterations.\n    alpha : int, default=23\n        The descending coefficient of the gravitational constant.\n    g_zero : int, default=100\n        The initial value of the gravitational constant.\n    k_agents_percent: int, default=2\n        Percent of agents applying force to the others in the last iteration.\n    norm : int, default=-2\n        The information criteria method to be used.\n    power : int, default=2\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_agents : int, default=10\n        The number of agents to search the optimal solution.\n    p_zeros : float, default=0.5\n        The probability of getting ones in the construction of the population.\n    p_zeros : float, default=0.5\n        The probability of getting zeros in the construction of the population.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import MetaMSS\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=400,\n    ...                                                    colored_noise=False,\n    ...                                                    sigma=0.001,\n    ...                                                    train_percentage=80)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = MetaMSS(\n    ...     basis_function=basis_function,\n    ...     norm=-2,\n    ...     xlag=7,\n    ...     ylag=7,\n    ...     k_agents_percent=2,\n    ...     estimate_parameter=True,\n    ...     maxiter=30,\n    ...     n_agents=10,\n    ...     p_value=0.05,\n    ...     loss_func='metamss_loss'\n    ... )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Meta-Model Structure Selection: Building Polynomial NARX Model\n       for Regression and Classification\n       https://arxiv.org/pdf/2109.09917.pdf\n    - Manuscript (Portuguese): Identifica\u00e7\u00e3o de Sistemas N\u00e3o Lineares\n       Utilizando o Algoritmo H\u00edbrido e Bin\u00e1rio de Otimiza\u00e7\u00e3o por\n       Enxame de Part\u00edculas e Busca Gravitacional\n       DOI: 10.17648/sbai-2019-111317\n    - Master thesis: Meta model structure selection: an algorithm for\n       building polynomial NARX models for regression and classification\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        maxiter: int = 30,\n        alpha: int = 23,\n        g_zero: int = 100,\n        k_agents_percent: int = 2,\n        norm: float = -2,\n        power: int = 2,\n        n_agents: int = 10,\n        p_zeros: float = 0.5,\n        p_ones: float = 0.5,\n        p_value: float = 0.05,\n        xlag: Union[int, list] = 1,\n        ylag: Union[int, list] = 1,\n        elag: Union[int, list] = 1,\n        estimator: Estimators = LeastSquares(),\n        eps: np.float64 = np.finfo(np.float64).eps,\n        estimate_parameter: bool = True,\n        loss_func: str = \"metamss_loss\",\n        model_type: str = \"NARMAX\",\n        basis_function: Polynomial = Polynomial(),\n        steps_ahead: Optional[int] = None,\n        random_state: Optional[int] = None,\n        test_size: float = 0.25,\n    ):\n        super().__init__(\n            estimator=estimator,\n            eps=eps,\n            estimate_parameter=estimate_parameter,\n            model_type=model_type,\n            basis_function=basis_function,\n        )\n\n        BPSOGSA.__init__(\n            self,\n            n_agents=n_agents,\n            maxiter=maxiter,\n            g_zero=g_zero,\n            alpha=alpha,\n            k_agents_percent=k_agents_percent,\n            norm=norm,\n            power=power,\n            p_zeros=p_zeros,\n            p_ones=p_ones,\n        )\n\n        self.xlag = xlag\n        self.ylag = ylag\n        self.elag = elag\n        self.p_value = p_value\n        self.estimator = estimator\n        self.estimate_parameter = estimate_parameter\n        self.loss_func = loss_func\n        self.steps_ahead = steps_ahead\n        self.random_state = random_state\n        self.test_size = test_size\n        self.n_inputs = None\n        self.regressor_code = None\n        self.best_model_history = None\n        self.tested_models = None\n        self.final_model = None\n        self._validate_metamss_params()\n\n    def _validate_metamss_params(self):\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n    def fit(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Fit the polynomial NARMAX model.\n\n        Parameters\n        ----------\n        X : ndarray, optional\n            The input data to be used in the training process.\n        y : ndarray\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        self : returns an instance of self.\n\n        \"\"\"\n        if not isinstance(self.basis_function, Polynomial):\n            raise NotImplementedError(\n                \"Currently MetaMSS only supports polynomial models.\"\n            )\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        if X is not None:\n            check_x_y(X, y)\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.max_lag = self._get_max_lag()\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        self.dimension = self.regressor_code.shape[0]\n        velocity = np.zeros([self.dimension, self.n_agents])\n        self.random_state = check_random_state(self.random_state)\n        population = self.generate_random_population(self.random_state)\n        self.best_by_iter = []\n        self.mean_by_iter = []\n        self.optimal_fitness_value = np.inf\n        self.optimal_model = None\n        self.best_model_history = []\n        self.tested_models = []\n\n        x, x_test, y, y_test = train_test_split(X, y, test_size=self.test_size)\n\n        for i in range(self.maxiter):\n            fitness = self.evaluate_objective_function(x, y, x_test, y_test, population)\n            column_of_best_solution = np.nanargmin(fitness)\n            current_best_fitness = fitness[column_of_best_solution]\n\n            if current_best_fitness &lt; self.optimal_fitness_value:\n                self.optimal_fitness_value = current_best_fitness\n                self.optimal_model = population[:, column_of_best_solution].copy()\n                self.best_model_history.append(self.optimal_model)\n\n            self.best_by_iter.append(self.optimal_fitness_value)\n            self.mean_by_iter.append(np.mean(fitness))\n            agent_mass = self.mass_calculation(fitness)\n            gravitational_constant = self.calculate_gravitational_constant(i)\n            acceleration = self.calculate_acceleration(\n                population, agent_mass, gravitational_constant, i\n            )\n            velocity, population = self.update_velocity_position(\n                population,\n                acceleration,\n                velocity,\n                i,\n            )\n\n        self.final_model = self.regressor_code[self.optimal_model == 1].copy()\n        _ = self.simulate(\n            X_train=x,\n            y_train=y,\n            X_test=x_test,\n            y_test=y_test,\n            model_code=self.final_model,\n            steps_ahead=self.steps_ahead,\n        )\n        self.max_lag = self._get_max_lag()\n        return self\n\n    def evaluate_objective_function(\n        self,\n        x_train: Optional[np.ndarray],\n        y_train: Optional[np.ndarray],\n        x_test: Optional[np.ndarray],\n        y_test: Optional[np.ndarray],\n        population: np.ndarray,\n    ):\n        \"\"\"Fit the polynomial NARMAX model.\n\n        Parameters\n        ----------\n        x_train : ndarray of floats\n            The input data to be used in the training process.\n        y_train : ndarray of floats\n            The output data to be used in the training process.\n        x_test : ndarray of floats\n            The input data to be used in the prediction process.\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        population : ndarray of zeros and ones\n            The initial population of agents.\n\n        Returns\n        -------\n        fitness_value : ndarray\n            The fitness value of each agent.\n        \"\"\"\n        fitness = []\n        for agent in population.T:\n            if np.all(agent == 0):\n                fitness.append(30)  # penalty for cases where there is no terms\n                continue\n\n            m = self.regressor_code[agent == 1].copy()\n            yhat = self.simulate(\n                X_train=x_train,\n                y_train=y_train,\n                X_test=x_test,\n                y_test=y_test,\n                model_code=m,\n                steps_ahead=self.steps_ahead,\n            )\n\n            residues = y_test - yhat\n            self.max_lag = self._get_max_lag()\n            lagged_data = build_lagged_matrix(\n                x_train, y_train, self.xlag, self.ylag, self.model_type\n            )\n\n            psi = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.xlag,\n                self.ylag,\n                self.model_type,\n                predefined_regressors=self.pivv,\n            )\n\n            pos_insignificant_terms, _, _ = self.perform_t_test(\n                psi, self.theta, residues\n            )\n\n            pos_aux = np.where(agent == 1)[0]\n            pos_aux = pos_aux[pos_insignificant_terms]\n            agent[pos_aux] = 0\n\n            m = self.regressor_code[agent == 1].copy()\n\n            if np.all(agent == 0):\n                fitness.append(1000)  # just a big number as penalty\n                continue\n\n            yhat = self.simulate(\n                X_train=x_train,\n                y_train=y_train,\n                X_test=x_test,\n                y_test=y_test,\n                model_code=m,\n                steps_ahead=self.steps_ahead,\n            )\n\n            self.final_model = m.copy()\n            self.tested_models.append(m)\n            if len(self.theta) == 0:\n                print(m)\n            d = getattr(self, self.loss_func)(y_test, yhat, len(self.theta))\n            fitness.append(d)\n\n        return fitness\n\n    def perform_t_test(\n        self, psi: np.ndarray, theta: np.ndarray, residues: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the t-test given the p-value defined by the user.\n\n        Parameters\n        ----------\n        psi : array\n            the data matrix of regressors\n        theta : array\n            the parameters estimated via least squares algorithm\n        residues : array\n            the identification residues of the solution\n\n        Returns\n        -------\n        pos_insignificant_terms : array\n            these regressors in the actual candidate solution are removed\n            from the population since they are insignificant\n        t_test : array\n            the values of the p_value of each regressor of the model\n        tail2p: array\n            The calculated two-tailed p-value.\n\n        \"\"\"\n        sum_of_squared_residues = np.sum(residues**2)\n        variance_of_residues = sum_of_squared_residues / (len(residues) - psi.shape[1])\n        if np.isnan(variance_of_residues):\n            variance_of_residues = 4.3645e05\n\n        skk = np.linalg.pinv(psi.T.dot(psi))\n        skk_diag = np.diag(skk)\n        var_e = variance_of_residues * skk_diag\n        se_theta = np.sqrt(var_e)\n        se_theta = se_theta.reshape(-1, 1)\n        t_test = theta / se_theta\n        degree_of_freedom = psi.shape[0] - psi.shape[1]\n\n        tail2p = 2 * t.cdf(-np.abs(t_test), degree_of_freedom)\n\n        pos_insignificant_terms = np.where(tail2p &gt; self.p_value)[0]\n        pos_insignificant_terms = pos_insignificant_terms.reshape(-1, 1).T\n        if pos_insignificant_terms.shape == 0:\n            return np.array([]), t_test, tail2p\n\n        return pos_insignificant_terms, t_test, tail2p\n\n    def aic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n        \"\"\"Calculate the Akaike Information Criterion.\n\n        Parameters\n        ----------\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        yhat : ndarray of floats\n            The n-steps-ahead predicted values of the model.\n        n_theta : ndarray of floats\n            The number of model parameters.\n\n        Returns\n        -------\n        aic : float\n            The Akaike Information Criterion\n\n        \"\"\"\n        mse = mean_squared_error(y_test, yhat)\n        n = y_test.shape[0]\n        return n * np.log(mse) + 2 * n_theta\n\n    def bic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n        \"\"\"Calculate the Bayesian Information Criterion.\n\n        Parameters\n        ----------\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        yhat : ndarray of floats\n            The n-steps-ahead predicted values of the model.\n        n_theta : ndarray of floats\n            The number of model parameters.\n\n        Returns\n        -------\n        bic : float\n            The Bayesian Information Criterion\n\n        \"\"\"\n        mse = mean_squared_error(y_test, yhat)\n        n = y_test.shape[0]\n        return n * np.log(mse) + n_theta + np.log(n)\n\n    def metamss_loss(self, y_test: np.ndarray, yhat: np.ndarray, n_terms: int) -&gt; float:\n        \"\"\"Calculate the MetaMSS loss function.\n\n        Parameters\n        ----------\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n        yhat : ndarray of floats\n            The n-steps-ahead predicted values of the model.\n        n_terms : ndarray of floats\n            The number of model parameters.\n\n        Returns\n        -------\n        metamss_loss : float\n            The MetaMSS loss function\n\n        \"\"\"\n        penalty_count = np.arange(0, self.dimension)\n        penalty_distribution = (np.log(n_terms + 1) ** (-1)) / self.dimension\n        penalty = self.sigmoid_linear_unit_derivative(\n            penalty_count, self.dimension / 2, penalty_distribution\n        )\n\n        penalty = penalty - np.min(penalty)\n        rmse = root_relative_squared_error(y_test, yhat)\n        fitness = rmse * penalty[n_terms]\n        if np.isnan(fitness):\n            fitness = 30\n\n        return fitness\n\n    def sigmoid_linear_unit_derivative(self, x, c, a):\n        \"\"\"Calculate the derivative of the Sigmoid Linear Unit function.\n\n        The derivative of Sigmoid Linear Unit (dSiLU) function can be\n        viewed as a overshooting version of the sigmoid function.\n\n        Parameters\n        ----------\n        x : ndarray\n            The range of the regressors space.\n        a : float\n            The rate of change.\n        c : int\n            Corresponds to the x value where y = 0.5.\n\n        Returns\n        -------\n        penalty : ndarray of floats\n            The values of the penalty function\n\n        \"\"\"\n        return (\n            1\n            / (1 + np.exp(-a * (x - c)))\n            * (1 + (a * (x - c)) * (1 - 1 / (1 + np.exp(-a * (x - c)))))\n        )\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: Optional[np.ndarray] = None,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        raise NotImplementedError(\n            \"MetaMSS doesn't support basis functions other than polynomial yet.\",\n        )\n\n    def _one_step_ahead_prediction(\n        self, x: Optional[np.ndarray], y: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._one_step_ahead_prediction(x, y)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(\n        self, x: Optional[np.ndarray], y_initial: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in MetaMSS for now.\"\n        )\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in MetaMSS for now.\"\n        )\n\n    def _basis_function_n_steps_horizon(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in MetaMSS for now.\"\n        )\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.aic","title":"<code>aic(y_test, yhat, n_theta)</code>","text":"<p>Calculate the Akaike Information Criterion.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> required <code>yhat</code> <code>ndarray of floats</code> <p>The n-steps-ahead predicted values of the model.</p> required <code>n_theta</code> <code>ndarray of floats</code> <p>The number of model parameters.</p> required <p>Returns:</p> Name Type Description <code>aic</code> <code>float</code> <p>The Akaike Information Criterion</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def aic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n    \"\"\"Calculate the Akaike Information Criterion.\n\n    Parameters\n    ----------\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    yhat : ndarray of floats\n        The n-steps-ahead predicted values of the model.\n    n_theta : ndarray of floats\n        The number of model parameters.\n\n    Returns\n    -------\n    aic : float\n        The Akaike Information Criterion\n\n    \"\"\"\n    mse = mean_squared_error(y_test, yhat)\n    n = y_test.shape[0]\n    return n * np.log(mse) + 2 * n_theta\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.bic","title":"<code>bic(y_test, yhat, n_theta)</code>","text":"<p>Calculate the Bayesian Information Criterion.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> required <code>yhat</code> <code>ndarray of floats</code> <p>The n-steps-ahead predicted values of the model.</p> required <code>n_theta</code> <code>ndarray of floats</code> <p>The number of model parameters.</p> required <p>Returns:</p> Name Type Description <code>bic</code> <code>float</code> <p>The Bayesian Information Criterion</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def bic(self, y_test: np.ndarray, yhat: np.ndarray, n_theta: int) -&gt; float:\n    \"\"\"Calculate the Bayesian Information Criterion.\n\n    Parameters\n    ----------\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    yhat : ndarray of floats\n        The n-steps-ahead predicted values of the model.\n    n_theta : ndarray of floats\n        The number of model parameters.\n\n    Returns\n    -------\n    bic : float\n        The Bayesian Information Criterion\n\n    \"\"\"\n    mse = mean_squared_error(y_test, yhat)\n    n = y_test.shape[0]\n    return n * np.log(mse) + n_theta + np.log(n)\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.evaluate_objective_function","title":"<code>evaluate_objective_function(x_train, y_train, x_test, y_test, population)</code>","text":"<p>Fit the polynomial NARMAX model.</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> required <code>y_train</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <code>x_test</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> required <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> required <code>population</code> <code>ndarray of zeros and ones</code> <p>The initial population of agents.</p> required <p>Returns:</p> Name Type Description <code>fitness_value</code> <code>ndarray</code> <p>The fitness value of each agent.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def evaluate_objective_function(\n    self,\n    x_train: Optional[np.ndarray],\n    y_train: Optional[np.ndarray],\n    x_test: Optional[np.ndarray],\n    y_test: Optional[np.ndarray],\n    population: np.ndarray,\n):\n    \"\"\"Fit the polynomial NARMAX model.\n\n    Parameters\n    ----------\n    x_train : ndarray of floats\n        The input data to be used in the training process.\n    y_train : ndarray of floats\n        The output data to be used in the training process.\n    x_test : ndarray of floats\n        The input data to be used in the prediction process.\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    population : ndarray of zeros and ones\n        The initial population of agents.\n\n    Returns\n    -------\n    fitness_value : ndarray\n        The fitness value of each agent.\n    \"\"\"\n    fitness = []\n    for agent in population.T:\n        if np.all(agent == 0):\n            fitness.append(30)  # penalty for cases where there is no terms\n            continue\n\n        m = self.regressor_code[agent == 1].copy()\n        yhat = self.simulate(\n            X_train=x_train,\n            y_train=y_train,\n            X_test=x_test,\n            y_test=y_test,\n            model_code=m,\n            steps_ahead=self.steps_ahead,\n        )\n\n        residues = y_test - yhat\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(\n            x_train, y_train, self.xlag, self.ylag, self.model_type\n        )\n\n        psi = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.xlag,\n            self.ylag,\n            self.model_type,\n            predefined_regressors=self.pivv,\n        )\n\n        pos_insignificant_terms, _, _ = self.perform_t_test(\n            psi, self.theta, residues\n        )\n\n        pos_aux = np.where(agent == 1)[0]\n        pos_aux = pos_aux[pos_insignificant_terms]\n        agent[pos_aux] = 0\n\n        m = self.regressor_code[agent == 1].copy()\n\n        if np.all(agent == 0):\n            fitness.append(1000)  # just a big number as penalty\n            continue\n\n        yhat = self.simulate(\n            X_train=x_train,\n            y_train=y_train,\n            X_test=x_test,\n            y_test=y_test,\n            model_code=m,\n            steps_ahead=self.steps_ahead,\n        )\n\n        self.final_model = m.copy()\n        self.tested_models.append(m)\n        if len(self.theta) == 0:\n            print(m)\n        d = getattr(self, self.loss_func)(y_test, yhat, len(self.theta))\n        fitness.append(d)\n\n    return fitness\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Fit the polynomial NARMAX model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray</code> <p>The output data to be used in the training process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>returns an instance of self.</code> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def fit(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: Optional[np.ndarray] = None,\n):\n    \"\"\"Fit the polynomial NARMAX model.\n\n    Parameters\n    ----------\n    X : ndarray, optional\n        The input data to be used in the training process.\n    y : ndarray\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    self : returns an instance of self.\n\n    \"\"\"\n    if not isinstance(self.basis_function, Polynomial):\n        raise NotImplementedError(\n            \"Currently MetaMSS only supports polynomial models.\"\n        )\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    if X is not None:\n        check_x_y(X, y)\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.max_lag = self._get_max_lag()\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    self.dimension = self.regressor_code.shape[0]\n    velocity = np.zeros([self.dimension, self.n_agents])\n    self.random_state = check_random_state(self.random_state)\n    population = self.generate_random_population(self.random_state)\n    self.best_by_iter = []\n    self.mean_by_iter = []\n    self.optimal_fitness_value = np.inf\n    self.optimal_model = None\n    self.best_model_history = []\n    self.tested_models = []\n\n    x, x_test, y, y_test = train_test_split(X, y, test_size=self.test_size)\n\n    for i in range(self.maxiter):\n        fitness = self.evaluate_objective_function(x, y, x_test, y_test, population)\n        column_of_best_solution = np.nanargmin(fitness)\n        current_best_fitness = fitness[column_of_best_solution]\n\n        if current_best_fitness &lt; self.optimal_fitness_value:\n            self.optimal_fitness_value = current_best_fitness\n            self.optimal_model = population[:, column_of_best_solution].copy()\n            self.best_model_history.append(self.optimal_model)\n\n        self.best_by_iter.append(self.optimal_fitness_value)\n        self.mean_by_iter.append(np.mean(fitness))\n        agent_mass = self.mass_calculation(fitness)\n        gravitational_constant = self.calculate_gravitational_constant(i)\n        acceleration = self.calculate_acceleration(\n            population, agent_mass, gravitational_constant, i\n        )\n        velocity, population = self.update_velocity_position(\n            population,\n            acceleration,\n            velocity,\n            i,\n        )\n\n    self.final_model = self.regressor_code[self.optimal_model == 1].copy()\n    _ = self.simulate(\n        X_train=x,\n        y_train=y,\n        X_test=x_test,\n        y_test=y_test,\n        model_code=self.final_model,\n        steps_ahead=self.steps_ahead,\n    )\n    self.max_lag = self._get_max_lag()\n    return self\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.metamss_loss","title":"<code>metamss_loss(y_test, yhat, n_terms)</code>","text":"<p>Calculate the MetaMSS loss function.</p> <p>Parameters:</p> Name Type Description Default <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> required <code>yhat</code> <code>ndarray of floats</code> <p>The n-steps-ahead predicted values of the model.</p> required <code>n_terms</code> <code>ndarray of floats</code> <p>The number of model parameters.</p> required <p>Returns:</p> Name Type Description <code>metamss_loss</code> <code>float</code> <p>The MetaMSS loss function</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def metamss_loss(self, y_test: np.ndarray, yhat: np.ndarray, n_terms: int) -&gt; float:\n    \"\"\"Calculate the MetaMSS loss function.\n\n    Parameters\n    ----------\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n    yhat : ndarray of floats\n        The n-steps-ahead predicted values of the model.\n    n_terms : ndarray of floats\n        The number of model parameters.\n\n    Returns\n    -------\n    metamss_loss : float\n        The MetaMSS loss function\n\n    \"\"\"\n    penalty_count = np.arange(0, self.dimension)\n    penalty_distribution = (np.log(n_terms + 1) ** (-1)) / self.dimension\n    penalty = self.sigmoid_linear_unit_derivative(\n        penalty_count, self.dimension / 2, penalty_distribution\n    )\n\n    penalty = penalty - np.min(penalty)\n    rmse = root_relative_squared_error(y_test, yhat)\n    fitness = rmse * penalty[n_terms]\n    if np.isnan(fitness):\n        fitness = 30\n\n    return fitness\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.perform_t_test","title":"<code>perform_t_test(psi, theta, residues)</code>","text":"<p>Perform the t-test given the p-value defined by the user.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>array</code> <p>the data matrix of regressors</p> required <code>theta</code> <code>array</code> <p>the parameters estimated via least squares algorithm</p> required <code>residues</code> <code>array</code> <p>the identification residues of the solution</p> required <p>Returns:</p> Name Type Description <code>pos_insignificant_terms</code> <code>array</code> <p>these regressors in the actual candidate solution are removed from the population since they are insignificant</p> <code>t_test</code> <code>array</code> <p>the values of the p_value of each regressor of the model</p> <code>tail2p</code> <code>array</code> <p>The calculated two-tailed p-value.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def perform_t_test(\n    self, psi: np.ndarray, theta: np.ndarray, residues: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the t-test given the p-value defined by the user.\n\n    Parameters\n    ----------\n    psi : array\n        the data matrix of regressors\n    theta : array\n        the parameters estimated via least squares algorithm\n    residues : array\n        the identification residues of the solution\n\n    Returns\n    -------\n    pos_insignificant_terms : array\n        these regressors in the actual candidate solution are removed\n        from the population since they are insignificant\n    t_test : array\n        the values of the p_value of each regressor of the model\n    tail2p: array\n        The calculated two-tailed p-value.\n\n    \"\"\"\n    sum_of_squared_residues = np.sum(residues**2)\n    variance_of_residues = sum_of_squared_residues / (len(residues) - psi.shape[1])\n    if np.isnan(variance_of_residues):\n        variance_of_residues = 4.3645e05\n\n    skk = np.linalg.pinv(psi.T.dot(psi))\n    skk_diag = np.diag(skk)\n    var_e = variance_of_residues * skk_diag\n    se_theta = np.sqrt(var_e)\n    se_theta = se_theta.reshape(-1, 1)\n    t_test = theta / se_theta\n    degree_of_freedom = psi.shape[0] - psi.shape[1]\n\n    tail2p = 2 * t.cdf(-np.abs(t_test), degree_of_freedom)\n\n    pos_insignificant_terms = np.where(tail2p &gt; self.p_value)[0]\n    pos_insignificant_terms = pos_insignificant_terms.reshape(-1, 1).T\n    if pos_insignificant_terms.shape == 0:\n        return np.array([]), t_test, tail2p\n\n    return pos_insignificant_terms, t_test, tail2p\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=1)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: Optional[np.ndarray] = None,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: int = 1,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    raise NotImplementedError(\n        \"MetaMSS doesn't support basis functions other than polynomial yet.\",\n    )\n</code></pre>"},{"location":"user-guide/API/metamss/#sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS.sigmoid_linear_unit_derivative","title":"<code>sigmoid_linear_unit_derivative(x, c, a)</code>","text":"<p>Calculate the derivative of the Sigmoid Linear Unit function.</p> <p>The derivative of Sigmoid Linear Unit (dSiLU) function can be viewed as a overshooting version of the sigmoid function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The range of the regressors space.</p> required <code>a</code> <code>float</code> <p>The rate of change.</p> required <code>c</code> <code>int</code> <p>Corresponds to the x value where y = 0.5.</p> required <p>Returns:</p> Name Type Description <code>penalty</code> <code>ndarray of floats</code> <p>The values of the penalty function</p> Source code in <code>sysidentpy/model_structure_selection/meta_model_structure_selection.py</code> <pre><code>def sigmoid_linear_unit_derivative(self, x, c, a):\n    \"\"\"Calculate the derivative of the Sigmoid Linear Unit function.\n\n    The derivative of Sigmoid Linear Unit (dSiLU) function can be\n    viewed as a overshooting version of the sigmoid function.\n\n    Parameters\n    ----------\n    x : ndarray\n        The range of the regressors space.\n    a : float\n        The rate of change.\n    c : int\n        Corresponds to the x value where y = 0.5.\n\n    Returns\n    -------\n    penalty : ndarray of floats\n        The values of the penalty function\n\n    \"\"\"\n    return (\n        1\n        / (1 + np.exp(-a * (x - c)))\n        * (1 + (a * (x - c)) * (1 - 1 / (1 + np.exp(-a * (x - c)))))\n    )\n</code></pre>"},{"location":"user-guide/API/metrics/","title":"Documentation for <code>Metrics</code>","text":"<p>Common metrics to assess performance on NARX models.</p>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.explained_variance_score","title":"<code>explained_variance_score(y, yhat)</code>","text":"<p>Calculate the Explained Variance Score.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>EVS output is non-negative values. Becoming 1.0 means your model outputs are exactly matched by true target values. Lower values means worse results.</p> References <ul> <li>Wikipedia entry on the Explained Variance    https://en.wikipedia.org/wiki/Explained_variation</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; explained_variance_score(y, yhat)\n0.957\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def explained_variance_score(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Explained Variance Score.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        EVS output is non-negative values. Becoming 1.0 means your\n        model outputs are exactly matched by true target values.\n        Lower values means worse results.\n\n    References\n    ----------\n    - Wikipedia entry on the Explained Variance\n       https://en.wikipedia.org/wiki/Explained_variation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; explained_variance_score(y, yhat)\n    0.957\n\n    \"\"\"\n    y_diff_avg = np.average(y - yhat)\n    numerator = np.average((y - yhat - y_diff_avg) ** 2)\n    y_avg = np.average(y)\n    denominator = np.average((y - y_avg) ** 2)\n    nonzero_numerator = numerator != 0\n    nonzero_denominator = denominator != 0\n    valid_score = nonzero_numerator &amp; nonzero_denominator\n    output_scores = np.ones(y.shape[0])\n    output_scores[valid_score] = 1 - (numerator[valid_score] / denominator[valid_score])\n    output_scores[nonzero_numerator &amp; ~nonzero_denominator] = 0.0\n    return np.average(output_scores)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.forecast_error","title":"<code>forecast_error(y, yhat)</code>","text":"<p>Calculate the forecast error in a regression model.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>ndarray of floats</code> <p>The difference between the true target values and the predicted or forecast value in regression or any other phenomenon.</p> References <ul> <li>Wikipedia entry on the Forecast error    https://en.wikipedia.org/wiki/Forecast_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; forecast_error(y, yhat)\n[0.5, -0.5, 0, -1]\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def forecast_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the forecast error in a regression model.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : ndarray of floats\n        The difference between the true target values and the predicted\n        or forecast value in regression or any other phenomenon.\n\n    References\n    ----------\n    - Wikipedia entry on the Forecast error\n       https://en.wikipedia.org/wiki/Forecast_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; forecast_error(y, yhat)\n    [0.5, -0.5, 0, -1]\n\n    \"\"\"\n    return np.array(y - yhat)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.mean_absolute_error","title":"<code>mean_absolute_error(y, yhat)</code>","text":"<p>Calculate the Mean absolute error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float or ndarray of floats</code> <p>MAE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the Mean absolute error    https://en.wikipedia.org/wiki/Mean_absolute_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; mean_absolute_error(y, yhat)\n0.5\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_absolute_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Mean absolute error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float or ndarray of floats\n        MAE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Mean absolute error\n       https://en.wikipedia.org/wiki/Mean_absolute_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; mean_absolute_error(y, yhat)\n    0.5\n\n    \"\"\"\n    output_errors = np.average(np.abs(y - yhat))\n    return np.average(output_errors)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.mean_forecast_error","title":"<code>mean_forecast_error(y, yhat)</code>","text":"<p>Calculate the mean of forecast error of a regression model.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>The mean  value of the difference between the true target values and the predicted or forecast value in regression or any other phenomenon.</p> References <ul> <li>Wikipedia entry on the Forecast error    https://en.wikipedia.org/wiki/Forecast_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; mean_forecast_error(y, yhat)\n-0.25\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_forecast_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the mean of forecast error of a regression model.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        The mean  value of the difference between the true target\n        values and the predicted or forecast value in regression\n        or any other phenomenon.\n\n    References\n    ----------\n    - Wikipedia entry on the Forecast error\n       https://en.wikipedia.org/wiki/Forecast_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; mean_forecast_error(y, yhat)\n    -0.25\n\n    \"\"\"\n    return np.average(y - yhat)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.mean_squared_error","title":"<code>mean_squared_error(y, yhat)</code>","text":"<p>Calculate the Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>MSE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the Mean Squared Error    https://en.wikipedia.org/wiki/Mean_squared_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; mean_squared_error(y, yhat)\n0.375\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        MSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Mean Squared Error\n       https://en.wikipedia.org/wiki/Mean_squared_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; mean_squared_error(y, yhat)\n    0.375\n\n    \"\"\"\n    output_error = np.average((y - yhat) ** 2)\n    return np.average(output_error)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.mean_squared_log_error","title":"<code>mean_squared_log_error(y, yhat)</code>","text":"<p>Calculate the Mean Squared Logarithmic Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>MSLE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, 5, 2.5, 7]\n&gt;&gt;&gt; yhat = [2.5, 5, 4, 8]\n&gt;&gt;&gt; mean_squared_log_error(y, yhat)\n0.039\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def mean_squared_log_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Mean Squared Logarithmic Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        MSLE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, 5, 2.5, 7]\n    &gt;&gt;&gt; yhat = [2.5, 5, 4, 8]\n    &gt;&gt;&gt; mean_squared_log_error(y, yhat)\n    0.039\n\n    \"\"\"\n    return mean_squared_error(np.log1p(y), np.log1p(yhat))\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.median_absolute_error","title":"<code>median_absolute_error(y, yhat)</code>","text":"<p>Calculate the Median Absolute Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>MdAE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the Median absolute deviation    https://en.wikipedia.org/wiki/Median_absolute_deviation</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; median_absolute_error(y, yhat)\n0.5\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def median_absolute_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Median Absolute Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        MdAE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Median absolute deviation\n       https://en.wikipedia.org/wiki/Median_absolute_deviation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; median_absolute_error(y, yhat)\n    0.5\n\n    \"\"\"\n    return np.median(np.abs(y - yhat))\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.normalized_root_mean_squared_error","title":"<code>normalized_root_mean_squared_error(y, yhat)</code>","text":"<p>Calculate the normalized Root Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>nRMSE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the normalized Root Mean Squared Error    https://en.wikipedia.org/wiki/Root-mean-square_deviation</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; normalized_root_mean_squared_error(y, yhat)\n0.081\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def normalized_root_mean_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the normalized Root Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        nRMSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the normalized Root Mean Squared Error\n       https://en.wikipedia.org/wiki/Root-mean-square_deviation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; normalized_root_mean_squared_error(y, yhat)\n    0.081\n\n    \"\"\"\n    return root_mean_squared_error(y, yhat) / (y.max() - y.min())\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.r2_score","title":"<code>r2_score(y, yhat)</code>","text":"<p>Calculate the R2 score. Based on sklearn solution.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>R2 output can be non-negative values or negative value. Becoming 1.0 means your model outputs are exactly matched by true target values. Lower values means worse results.</p> Notes <p>This is not a symmetric function.</p> References <ul> <li>Wikipedia entry on the Coefficient of determination    https://en.wikipedia.org/wiki/Coefficient_of_determination</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; explained_variance_score(y, yhat)\n0.948\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def r2_score(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the R2 score. Based on sklearn solution.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        R2 output can be non-negative values or negative value.\n        Becoming 1.0 means your model outputs are exactly\n        matched by true target values. Lower values means worse results.\n\n    Notes\n    -----\n    This is not a symmetric function.\n\n    References\n    ----------\n    - Wikipedia entry on the Coefficient of determination\n       https://en.wikipedia.org/wiki/Coefficient_of_determination\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; explained_variance_score(y, yhat)\n    0.948\n\n    \"\"\"\n    numerator = ((y - yhat) ** 2).sum(axis=0, dtype=np.float64)\n    denominator = ((y - np.average(y, axis=0)) ** 2).sum(axis=0, dtype=np.float64)\n    nonzero_denominator = denominator != 0\n    nonzero_numerator = numerator != 0\n    valid_score = nonzero_denominator &amp; nonzero_numerator\n    output_scores = np.ones([y.shape[1]])\n    output_scores[valid_score] = 1 - (numerator[valid_score] / denominator[valid_score])\n    # arbitrary set to zero to avoid -inf scores, having a constant\n    # y_true is not interesting for scoring a regression anyway\n    output_scores[nonzero_numerator &amp; ~nonzero_denominator] = 0.0\n    return np.average(output_scores)\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.root_mean_squared_error","title":"<code>root_mean_squared_error(y, yhat)</code>","text":"<p>Calculate the Root Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>RMSE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> References <ul> <li>Wikipedia entry on the Root Mean Squared Error    https://en.wikipedia.org/wiki/Root-mean-square_deviation</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; root_mean_squared_error(y, yhat)\n0.612\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def root_mean_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Root Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        RMSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    References\n    ----------\n    - Wikipedia entry on the Root Mean Squared Error\n       https://en.wikipedia.org/wiki/Root-mean-square_deviation\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; root_mean_squared_error(y, yhat)\n    0.612\n\n    \"\"\"\n    return np.sqrt(mean_squared_error(y, yhat))\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.root_relative_squared_error","title":"<code>root_relative_squared_error(y, yhat)</code>","text":"<p>Calculate the Root Relative Mean Squared Error.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>RRSE output is non-negative values. Becoming 0.0 means your model outputs are exactly matched by true target values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; root_relative_mean_squared_error(y, yhat)\n0.206\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def root_relative_squared_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the Root Relative Mean Squared Error.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        RRSE output is non-negative values. Becoming 0.0 means your\n        model outputs are exactly matched by true target values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; root_relative_mean_squared_error(y, yhat)\n    0.206\n\n    \"\"\"\n    numerator = np.sum(np.square((yhat - y)))\n    denominator = np.sum(np.square((y - np.mean(y, axis=0))))\n    return np.sqrt(np.divide(numerator, denominator))\n</code></pre>"},{"location":"user-guide/API/metrics/#sysidentpy.metrics._regression.symmetric_mean_absolute_percentage_error","title":"<code>symmetric_mean_absolute_percentage_error(y, yhat)</code>","text":"<p>Calculate the SMAPE score.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = number_of_outputs</code> <p>Represent the target values.</p> required <code>yhat</code> <code>array-like of shape = number_of_outputs</code> <p>Target values predicted by the model.</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>SMAPE output is a non-negative value. The results are percentages values.</p> Notes <p>One supposed problem with SMAPE is that it is not symmetric since over-forecasts and under-forecasts are not treated equally.</p> References <ul> <li>Wikipedia entry on the Symmetric mean absolute percentage error    https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = [3, -0.5, 2, 7]\n&gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n&gt;&gt;&gt; symmetric_mean_absolute_percentage_error(y, yhat)\n57.87\n</code></pre> Source code in <code>sysidentpy/metrics/_regression.py</code> <pre><code>def symmetric_mean_absolute_percentage_error(y: NDArray, yhat: NDArray) -&gt; NDArray:\n    \"\"\"Calculate the SMAPE score.\n\n    Parameters\n    ----------\n    y : array-like of shape = number_of_outputs\n        Represent the target values.\n    yhat : array-like of shape = number_of_outputs\n        Target values predicted by the model.\n\n    Returns\n    -------\n    loss : float\n        SMAPE output is a non-negative value.\n        The results are percentages values.\n\n    Notes\n    -----\n    One supposed problem with SMAPE is that it is not symmetric since\n    over-forecasts and under-forecasts are not treated equally.\n\n    References\n    ----------\n    - Wikipedia entry on the Symmetric mean absolute percentage error\n       https://en.wikipedia.org/wiki/Symmetric_mean_absolute_percentage_error\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = [3, -0.5, 2, 7]\n    &gt;&gt;&gt; yhat = [2.5, 0.0, 2, 8]\n    &gt;&gt;&gt; symmetric_mean_absolute_percentage_error(y, yhat)\n    57.87\n\n    \"\"\"\n    return 100 / len(y) * np.sum(2 * np.abs(yhat - y) / (np.abs(y) + np.abs(yhat)))\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/","title":"Documentation for <code>Multiobjective Parameter Estimation</code>","text":"<p>Affine Information Least Squares for NARMAX models.</p>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS","title":"<code>AILS</code>","text":"<p>Affine Information Least Squares (AILS) for NARMAX Parameter Estimation.</p> <p>AILS is a non-iterative multiobjective Least Squares technique used for finding Pareto-set solutions in NARMAX (Nonlinear AutoRegressive Moving Average with eXogenous inputs) model parameter estimation. This method is suitable for linear-in-the-parameter model structures.</p> <p>Two types of auxiliary information can be incorporated: static function and steady-state gain.</p> <p>Parameters:</p> Name Type Description Default <code>static_gain</code> <code>bool</code> <p>Flag indicating the presence of data related to steady-state gain.</p> <code>True</code> <code>static_function</code> <code>bool</code> <p>Flag indicating the presence of data concerning static function.</p> <code>True</code> <code>final_model</code> <code>ndarray</code> <p>Model code representation.</p> <code>[[0], [0]]</code> References <ol> <li>Nepomuceno, E. G., Takahashi, R. H. C., &amp; Aguirre, L. A. (2007). \"Multiobjective parameter estimation for nonlinear systems: Affine information and least-squares formulation.\" International Journal of Control, 80, 863-871.</li> </ol> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>class AILS:\n    \"\"\"Affine Information Least Squares (AILS) for NARMAX Parameter Estimation.\n\n    AILS is a non-iterative multiobjective Least Squares technique used for finding\n    Pareto-set solutions in NARMAX (Nonlinear AutoRegressive Moving Average with\n    eXogenous inputs) model parameter estimation. This method is suitable for\n    linear-in-the-parameter model structures.\n\n    Two types of auxiliary information can be incorporated: static function and\n    steady-state gain.\n\n    Parameters\n    ----------\n    static_gain : bool, default=True\n        Flag indicating the presence of data related to steady-state gain.\n    static_function : bool, default=True\n        Flag indicating the presence of data concerning static function.\n    final_model : ndarray, default=[[0], [0]]\n        Model code representation.\n\n    References\n    ----------\n    1. Nepomuceno, E. G., Takahashi, R. H. C., &amp; Aguirre, L. A. (2007).\n    \"Multiobjective parameter estimation for nonlinear systems: Affine information and\n    least-squares formulation.\"\n    International Journal of Control, 80, 863-871.\n    \"\"\"\n\n    def __init__(\n        self,\n        static_gain: bool = True,\n        static_function: bool = True,\n        final_model: np.ndarray = np.zeros((1, 1)),\n        normalize: bool = True,\n    ):\n        self.n_inputs = np.max(final_model // 1000) - 1\n        self.degree = np.shape(final_model)[1]\n        self.max_lag = 1\n        self.final_model = final_model\n        self.static_gain = static_gain\n        self.static_function = static_function\n        self.normalize = normalize\n\n    def build_linear_mapping(self):\n        \"\"\"Assemble the linear mapping matrix R using the regressor-space method.\n\n        This function constructs the linear mapping matrix R, which plays a key role in\n        mapping the parameter vector to the cluster coefficients. It also generates a\n        row matrix qit that assists in locating terms within the linear mapping matrix.\n        This qit matrix is later used in creating the static regressor matrix (Q).\n\n        Returns\n        -------\n        R : ndarray of int\n            A constant matrix of ones and zeros that maps the parameter vector to\n            cluster coefficients.\n        qit : ndarray of int\n            A row matrix that helps locate terms within the linear mapping matrix R and\n            is used in the creation of the static regressor matrix (Q).\n\n        Notes\n        -----\n        The linear mapping matrix R is constructed using the regressor-space method.\n        It plays a crucial role in the parameter estimation process, facilitating the\n        mapping of parameter values to cluster coefficients. The qit matrix aids in\n        term localization within the linear mapping matrix R and is subsequently used\n        to build the static regressor matrix (Q).\n\n        \"\"\"\n        xlag = [1] * self.n_inputs\n\n        object_qit = RegressorDictionary(xlag=xlag, ylag=[1])\n        # Given xlag and ylag equal to 1, there is no repetition of terms, which is\n        # ideal for building qit.\n        qit = object_qit.regressor_space(n_inputs=self.n_inputs) // 1000\n        model = self.final_model // 1000\n        R = np.all(qit[:, None, :] == model, axis=2).astype(int)\n        # Find rows with all zeros in R (sum of row elements is 0)\n        null_rows = list(np.where(np.sum(R, axis=1) == 0)[0])\n\n        R = np.delete(R, null_rows, axis=0)\n        qit = np.delete(qit, null_rows, axis=0)\n        return R, get_term_clustering(qit)\n\n    def build_static_function_information(\n        self, x_static: np.ndarray, y_static: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Construct a matrix of static regressors for a NARMAX model.\n\n        Parameters\n        ----------\n        y_static : array-like, shape (n_samples_static_function,)\n            Output of the static function.\n        x_static : array-like, shape (n_samples_static_function,)\n            Static function input.\n\n        Returns\n        -------\n        Q_dot_R : ndarray of floats, shape (n_samples_static_function, n_parameters)\n            The result of multiplying the matrix of static regressors (Q) with the\n            linear mapping matrix (R), where n_parameters is the number of model\n            parameters.\n        static_covariance: ndarray of floats, shape (n_parameters, n_parameters)\n            The covariance QR'QR\n        static_response: ndarray of floats, shape (n_parameters,)\n            The response QR'y\n\n        Notes\n        -----\n        This function constructs a matrix of static regressors (Q) based on the provided\n        static function outputs (y_static) and inputs (X_static). The linear mapping\n        matrix (R) should be precomputed before calling this function. The result\n        Q_dot_R represents the static regressors for the NARMAX model.\n\n        \"\"\"\n        R, qit = self.build_linear_mapping()\n        Q = y_static ** qit[:, 0]\n        for k in range(self.n_inputs):\n            Q *= x_static ** qit[:, 1 + k]\n\n        Q = Q.reshape(len(y_static), len(qit))\n\n        QR = Q.dot(R)\n        static_covariance = QR.T.dot(QR)\n        static_response = QR.T.dot(y_static)\n        return QR, static_covariance, static_response\n\n    def build_static_gain_information(\n        self, x_static: np.ndarray, y_static: np.ndarray, gain: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Construct a matrix of static regressors referring to the derivative (gain).\n\n        Parameters\n        ----------\n        y_static : array-like, shape (n_samples_static_function,)\n            Output of the static function.\n        x_static : array-like, shape (n_samples_static_function,)\n            Static function input.\n        gain : array-like, shape (n_samples_static_gain,)\n            Static gain input.\n\n        Returns\n        -------\n        HR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n            The matrix of static regressors for the derivative (gain) multiplied by the\n            linear mapping matrix R.\n        gain_covariance : ndarray of floats, shape (n_parameters, n_parameters)\n            The covariance matrix (HR'HR) for the gain-related regressors.\n        gain_response : ndarray of floats, shape (n_parameters,)\n            The response vector (HR'y) for the gain-related regressors.\n\n        Notes\n        -----\n        This function constructs a matrix of static regressors (G+H) for the derivative\n        (gain) based on the provided static function outputs (y_static), inputs\n        (X_static), and gain values. The linear mapping matrix (R) should be\n        precomputed before calling this function.\n\n        \"\"\"\n        R, qit = self.build_linear_mapping()\n        H = np.zeros((len(y_static), len(qit)))\n        G = np.zeros((len(y_static), len(qit)))\n        for i in range(len(y_static)):\n            for j in range(1, len(qit)):\n                if y_static[i, 0] == 0:\n                    if (qit[j, 0]) == 1:\n                        H[i, j] = gain[i][0]\n                    else:\n                        H[i, j] = 0\n                else:\n                    H[i, j] = (gain[i] * qit[j, 0] * y_static[i, 0] ** (qit[j, 0] - 1))[\n                        0\n                    ]\n                for k in range(self.n_inputs):\n                    if x_static[i, k] == 0:\n                        if (qit[j, 1 + k]) == 1:\n                            G[i, j] = 1\n                        else:\n                            G[i, j] = 0\n                    else:\n                        G[i, j] = qit[j, 1 + k] * x_static[i, k] ** (qit[j, 1 + k] - 1)\n\n        HR = (G + H).dot(R)\n        gain_covariance = HR.T.dot(HR)\n        gain_response = HR.T.dot(gain)\n        return HR, gain_covariance, gain_response\n\n    def build_system_data(\n        self,\n        y: np.ndarray,\n        static_gain: np.ndarray,\n        static_function: np.ndarray,\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Construct a list of output data components for the NARMAX system.\n\n        Parameters\n        ----------\n        y : ndarray of floats\n            The target data used in the identification process.\n        static_gain : ndarray of floats\n            Static gain output data.\n        static_function : ndarray of floats\n            Static function output data.\n\n        Returns\n        -------\n        system_data : list of ndarrays\n            A list containing data components, including the target data (y),\n            static gain data (if present), and static function data (if present).\n\n        Notes\n        -----\n        This method constructs a list of data components that are used in the NARMAX\n        system identification process. The components may include the target data (y),\n        static gain data (if enabled), and static function data (if enabled).\n\n        \"\"\"\n        if not self.static_gain:\n            return [y] + [static_function]\n\n        if not self.static_function:\n            return [y] + [static_gain]\n\n        return [y] + [static_gain] + [static_function]\n\n    def build_affine_data(\n        self, psi: np.ndarray, HR: np.ndarray, QR: np.ndarray\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Construct a list of affine data components for NARMAX modeling.\n\n        Parameters\n        ----------\n        psi : ndarray of floats, shape (n_samples, n_parameters)\n            The matrix of dynamic regressors.\n        HR : ndarray of floats, shape (n_samples_static_gain, n_parameters)\n            The matrix of static gain regressors.\n        QR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n            The matrix of static function regressors.\n\n        Returns\n        -------\n        affine_data : list of ndarrays\n            A list containing affine data components, including the matrix of static\n            regressors (psi), static gain regressors (if present), and static function\n            regressors (if present).\n\n        Notes\n        -----\n        This method constructs a list of affine data components used in the NARMAX\n        modeling process. The components may include the matrix of static regressors\n        (psi), static gain regressors (if enabled), and static function regressors\n        (if enabled).\n\n        \"\"\"\n        if not self.static_gain:\n            return [psi] + [QR]\n\n        if not self.static_function:\n            return [psi] + [HR]\n\n        return [psi] + [HR] + [QR]\n\n    def build_psi(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Build the matrix of dynamic regressor for NARMAX modeling.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        psi : ndarray of floats, shape (n_samples, n_parameters)\n            The matrix of dynamic regressors.\n\n        \"\"\"\n        psi_builder = RegressorDictionary()\n        xlag_code = list_input_regressor_code(self.final_model)\n        ylag_code = list_output_regressor_code(self.final_model)\n        xlag = get_lag_from_regressor_code(xlag_code)\n        ylag = get_lag_from_regressor_code(ylag_code)\n        self.max_lag = get_max_lag_from_model_code(self.final_model)\n        if self.n_inputs != 1:\n            xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n        psi_builder.xlag = xlag\n        psi_builder.ylag = ylag\n        regressor_code = psi_builder.regressor_space(self.n_inputs)\n        pivv = get_index_from_regressor_code(regressor_code, self.final_model)\n        self.final_model = regressor_code[pivv]\n\n        lagged_data = build_input_output_matrix(x=X, y=y, xlag=xlag, ylag=ylag)\n\n        psi = Polynomial(degree=self.degree).fit(\n            lagged_data,\n            max_lag=self.max_lag,\n            ylag=ylag,\n            xlag=xlag,\n            predefined_regressors=pivv,\n        )\n        return psi\n\n    def estimate(\n        self,\n        y_static: np.ndarray = np.zeros(1),\n        X_static: np.ndarray = np.zeros(1),\n        gain: np.ndarray = np.zeros(1),\n        y: np.ndarray = np.zeros(1),\n        X: np.ndarray = np.zeros((1, 1)),\n        weighing_matrix: np.ndarray = np.zeros((1, 1)),\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.int64]:\n        \"\"\"Estimate the parameters via multi-objective techniques.\n\n        Parameters\n        ----------\n        y_static : array-like of shape = n_samples_static_function, default = ([0])\n            Output of static function.\n        X_static : array-like of shape = n_samples_static_function, default = ([0])\n            Static function input.\n        gain : array-like of shape = n_samples_static_gain, default = ([0])\n            Static gain input.\n        y : array-like of shape = n_samples, default = ([0])\n            The target data used in the identification process.\n        X : ndarray of floats, default = ([[0],[0]])\n            Matrix of static regressors.\n        weighing_matrix: ndarray\n            Weighing matrix for defining the weight of each objective.\n\n        Returns\n        -------\n        J : ndarray\n            Matrix referring to the objectives.\n        euclidean_norm : ndarray\n            Matrix of the Euclidean norm.\n        theta : ndarray\n            Matrix with parameters for each weight.\n        HR : ndarray\n            H matrix multiplied by R.\n        QR : ndarray\n            Q matrix multiplied by R.\n        position : ndarray, default = ([[0],[0]])\n            Position of the best theta set.\n\n        \"\"\"\n        psi = self.build_psi(X, y)\n        y = y[self.max_lag :]\n        HR, QR = np.zeros((1, 1)), np.zeros((1, 1))\n        n_parameters = weighing_matrix.shape[1]\n        num_objectives = self.static_function + self.static_gain + 1\n        euclidean_norm = np.zeros(n_parameters)\n        theta = np.zeros((n_parameters, self.final_model.shape[0]))\n        dynamic_covariance = psi.T.dot(psi)\n        dynamic_response = psi.T.dot(y)\n\n        if self.static_function:\n            QR, static_covariance, static_response = (\n                self.build_static_function_information(X_static, y_static)\n            )\n        if self.static_gain:\n            HR, gain_covariance, gain_response = self.build_static_gain_information(\n                X_static, y_static, gain\n            )\n        J = np.zeros((num_objectives, n_parameters))\n        system_data = self.build_system_data(y, gain, y_static)\n        affine_information_data = self.build_affine_data(psi, HR, QR)\n        for i in range(n_parameters):\n            theta1 = weighing_matrix[0, i] * dynamic_covariance\n            theta2 = weighing_matrix[0, i] * dynamic_response\n\n            w = 1\n            if self.static_function:\n                theta1 += weighing_matrix[w, i] * static_covariance\n                theta2 += weighing_matrix[w, i] * static_response.reshape(-1, 1)\n                w += 1\n\n            if self.static_gain:\n                theta1 += weighing_matrix[w, i] * gain_covariance\n                theta2 += weighing_matrix[w, i] * gain_response.reshape(-1, 1)\n                w += 1\n\n            tmp_theta = np.linalg.lstsq(theta1, theta2, rcond=None)[0]\n            theta[i, :] = tmp_theta.T\n\n            for j in range(num_objectives):\n                residuals = get_cost_function(\n                    system_data[j], affine_information_data[j], tmp_theta\n                )\n                J[j, i] = residuals[0]\n\n            euclidean_norm[i] = np.linalg.norm(J[:, i])\n\n        if self.normalize is True:\n            J /= np.max(J, axis=1)[:, np.newaxis]\n            euclidean_norm /= np.max(euclidean_norm)\n\n            euclidean_norm = euclidean_norm / np.max(euclidean_norm)\n\n        position = np.argmin(euclidean_norm)\n        return (\n            J,\n            euclidean_norm,\n            theta,\n            HR,\n            QR,\n            position,\n        )\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_affine_data","title":"<code>build_affine_data(psi, HR, QR)</code>","text":"<p>Construct a list of affine data components for NARMAX modeling.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats, shape (n_samples, n_parameters)</code> <p>The matrix of dynamic regressors.</p> required <code>HR</code> <code>ndarray of floats, shape (n_samples_static_gain, n_parameters)</code> <p>The matrix of static gain regressors.</p> required <code>QR</code> <code>ndarray of floats, shape (n_samples_static_function, n_parameters)</code> <p>The matrix of static function regressors.</p> required <p>Returns:</p> Name Type Description <code>affine_data</code> <code>list of ndarrays</code> <p>A list containing affine data components, including the matrix of static regressors (psi), static gain regressors (if present), and static function regressors (if present).</p> Notes <p>This method constructs a list of affine data components used in the NARMAX modeling process. The components may include the matrix of static regressors (psi), static gain regressors (if enabled), and static function regressors (if enabled).</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_affine_data(\n    self, psi: np.ndarray, HR: np.ndarray, QR: np.ndarray\n) -&gt; List[np.ndarray]:\n    \"\"\"Construct a list of affine data components for NARMAX modeling.\n\n    Parameters\n    ----------\n    psi : ndarray of floats, shape (n_samples, n_parameters)\n        The matrix of dynamic regressors.\n    HR : ndarray of floats, shape (n_samples_static_gain, n_parameters)\n        The matrix of static gain regressors.\n    QR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n        The matrix of static function regressors.\n\n    Returns\n    -------\n    affine_data : list of ndarrays\n        A list containing affine data components, including the matrix of static\n        regressors (psi), static gain regressors (if present), and static function\n        regressors (if present).\n\n    Notes\n    -----\n    This method constructs a list of affine data components used in the NARMAX\n    modeling process. The components may include the matrix of static regressors\n    (psi), static gain regressors (if enabled), and static function regressors\n    (if enabled).\n\n    \"\"\"\n    if not self.static_gain:\n        return [psi] + [QR]\n\n    if not self.static_function:\n        return [psi] + [HR]\n\n    return [psi] + [HR] + [QR]\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_linear_mapping","title":"<code>build_linear_mapping()</code>","text":"<p>Assemble the linear mapping matrix R using the regressor-space method.</p> <p>This function constructs the linear mapping matrix R, which plays a key role in mapping the parameter vector to the cluster coefficients. It also generates a row matrix qit that assists in locating terms within the linear mapping matrix. This qit matrix is later used in creating the static regressor matrix (Q).</p> <p>Returns:</p> Name Type Description <code>R</code> <code>ndarray of int</code> <p>A constant matrix of ones and zeros that maps the parameter vector to cluster coefficients.</p> <code>qit</code> <code>ndarray of int</code> <p>A row matrix that helps locate terms within the linear mapping matrix R and is used in the creation of the static regressor matrix (Q).</p> Notes <p>The linear mapping matrix R is constructed using the regressor-space method. It plays a crucial role in the parameter estimation process, facilitating the mapping of parameter values to cluster coefficients. The qit matrix aids in term localization within the linear mapping matrix R and is subsequently used to build the static regressor matrix (Q).</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_linear_mapping(self):\n    \"\"\"Assemble the linear mapping matrix R using the regressor-space method.\n\n    This function constructs the linear mapping matrix R, which plays a key role in\n    mapping the parameter vector to the cluster coefficients. It also generates a\n    row matrix qit that assists in locating terms within the linear mapping matrix.\n    This qit matrix is later used in creating the static regressor matrix (Q).\n\n    Returns\n    -------\n    R : ndarray of int\n        A constant matrix of ones and zeros that maps the parameter vector to\n        cluster coefficients.\n    qit : ndarray of int\n        A row matrix that helps locate terms within the linear mapping matrix R and\n        is used in the creation of the static regressor matrix (Q).\n\n    Notes\n    -----\n    The linear mapping matrix R is constructed using the regressor-space method.\n    It plays a crucial role in the parameter estimation process, facilitating the\n    mapping of parameter values to cluster coefficients. The qit matrix aids in\n    term localization within the linear mapping matrix R and is subsequently used\n    to build the static regressor matrix (Q).\n\n    \"\"\"\n    xlag = [1] * self.n_inputs\n\n    object_qit = RegressorDictionary(xlag=xlag, ylag=[1])\n    # Given xlag and ylag equal to 1, there is no repetition of terms, which is\n    # ideal for building qit.\n    qit = object_qit.regressor_space(n_inputs=self.n_inputs) // 1000\n    model = self.final_model // 1000\n    R = np.all(qit[:, None, :] == model, axis=2).astype(int)\n    # Find rows with all zeros in R (sum of row elements is 0)\n    null_rows = list(np.where(np.sum(R, axis=1) == 0)[0])\n\n    R = np.delete(R, null_rows, axis=0)\n    qit = np.delete(qit, null_rows, axis=0)\n    return R, get_term_clustering(qit)\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_psi","title":"<code>build_psi(X, y)</code>","text":"<p>Build the matrix of dynamic regressor for NARMAX modeling.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <p>Returns:</p> Name Type Description <code>psi</code> <code>ndarray of floats, shape (n_samples, n_parameters)</code> <p>The matrix of dynamic regressors.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_psi(self, X: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the matrix of dynamic regressor for NARMAX modeling.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    psi : ndarray of floats, shape (n_samples, n_parameters)\n        The matrix of dynamic regressors.\n\n    \"\"\"\n    psi_builder = RegressorDictionary()\n    xlag_code = list_input_regressor_code(self.final_model)\n    ylag_code = list_output_regressor_code(self.final_model)\n    xlag = get_lag_from_regressor_code(xlag_code)\n    ylag = get_lag_from_regressor_code(ylag_code)\n    self.max_lag = get_max_lag_from_model_code(self.final_model)\n    if self.n_inputs != 1:\n        xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n    psi_builder.xlag = xlag\n    psi_builder.ylag = ylag\n    regressor_code = psi_builder.regressor_space(self.n_inputs)\n    pivv = get_index_from_regressor_code(regressor_code, self.final_model)\n    self.final_model = regressor_code[pivv]\n\n    lagged_data = build_input_output_matrix(x=X, y=y, xlag=xlag, ylag=ylag)\n\n    psi = Polynomial(degree=self.degree).fit(\n        lagged_data,\n        max_lag=self.max_lag,\n        ylag=ylag,\n        xlag=xlag,\n        predefined_regressors=pivv,\n    )\n    return psi\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_function_information","title":"<code>build_static_function_information(x_static, y_static)</code>","text":"<p>Construct a matrix of static regressors for a NARMAX model.</p> <p>Parameters:</p> Name Type Description Default <code>y_static</code> <code>(array - like, shape(n_samples_static_function))</code> <p>Output of the static function.</p> required <code>x_static</code> <code>(array - like, shape(n_samples_static_function))</code> <p>Static function input.</p> required <p>Returns:</p> Name Type Description <code>Q_dot_R</code> <code>ndarray of floats, shape (n_samples_static_function, n_parameters)</code> <p>The result of multiplying the matrix of static regressors (Q) with the linear mapping matrix (R), where n_parameters is the number of model parameters.</p> <code>static_covariance</code> <code>ndarray of floats, shape (n_parameters, n_parameters)</code> <p>The covariance QR'QR</p> <code>static_response</code> <code>ndarray of floats, shape (n_parameters,)</code> <p>The response QR'y</p> Notes <p>This function constructs a matrix of static regressors (Q) based on the provided static function outputs (y_static) and inputs (X_static). The linear mapping matrix (R) should be precomputed before calling this function. The result Q_dot_R represents the static regressors for the NARMAX model.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_static_function_information(\n    self, x_static: np.ndarray, y_static: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Construct a matrix of static regressors for a NARMAX model.\n\n    Parameters\n    ----------\n    y_static : array-like, shape (n_samples_static_function,)\n        Output of the static function.\n    x_static : array-like, shape (n_samples_static_function,)\n        Static function input.\n\n    Returns\n    -------\n    Q_dot_R : ndarray of floats, shape (n_samples_static_function, n_parameters)\n        The result of multiplying the matrix of static regressors (Q) with the\n        linear mapping matrix (R), where n_parameters is the number of model\n        parameters.\n    static_covariance: ndarray of floats, shape (n_parameters, n_parameters)\n        The covariance QR'QR\n    static_response: ndarray of floats, shape (n_parameters,)\n        The response QR'y\n\n    Notes\n    -----\n    This function constructs a matrix of static regressors (Q) based on the provided\n    static function outputs (y_static) and inputs (X_static). The linear mapping\n    matrix (R) should be precomputed before calling this function. The result\n    Q_dot_R represents the static regressors for the NARMAX model.\n\n    \"\"\"\n    R, qit = self.build_linear_mapping()\n    Q = y_static ** qit[:, 0]\n    for k in range(self.n_inputs):\n        Q *= x_static ** qit[:, 1 + k]\n\n    Q = Q.reshape(len(y_static), len(qit))\n\n    QR = Q.dot(R)\n    static_covariance = QR.T.dot(QR)\n    static_response = QR.T.dot(y_static)\n    return QR, static_covariance, static_response\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_static_gain_information","title":"<code>build_static_gain_information(x_static, y_static, gain)</code>","text":"<p>Construct a matrix of static regressors referring to the derivative (gain).</p> <p>Parameters:</p> Name Type Description Default <code>y_static</code> <code>(array - like, shape(n_samples_static_function))</code> <p>Output of the static function.</p> required <code>x_static</code> <code>(array - like, shape(n_samples_static_function))</code> <p>Static function input.</p> required <code>gain</code> <code>(array - like, shape(n_samples_static_gain))</code> <p>Static gain input.</p> required <p>Returns:</p> Name Type Description <code>HR</code> <code>ndarray of floats, shape (n_samples_static_function, n_parameters)</code> <p>The matrix of static regressors for the derivative (gain) multiplied by the linear mapping matrix R.</p> <code>gain_covariance</code> <code>ndarray of floats, shape (n_parameters, n_parameters)</code> <p>The covariance matrix (HR'HR) for the gain-related regressors.</p> <code>gain_response</code> <code>ndarray of floats, shape (n_parameters,)</code> <p>The response vector (HR'y) for the gain-related regressors.</p> Notes <p>This function constructs a matrix of static regressors (G+H) for the derivative (gain) based on the provided static function outputs (y_static), inputs (X_static), and gain values. The linear mapping matrix (R) should be precomputed before calling this function.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_static_gain_information(\n    self, x_static: np.ndarray, y_static: np.ndarray, gain: np.ndarray\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Construct a matrix of static regressors referring to the derivative (gain).\n\n    Parameters\n    ----------\n    y_static : array-like, shape (n_samples_static_function,)\n        Output of the static function.\n    x_static : array-like, shape (n_samples_static_function,)\n        Static function input.\n    gain : array-like, shape (n_samples_static_gain,)\n        Static gain input.\n\n    Returns\n    -------\n    HR : ndarray of floats, shape (n_samples_static_function, n_parameters)\n        The matrix of static regressors for the derivative (gain) multiplied by the\n        linear mapping matrix R.\n    gain_covariance : ndarray of floats, shape (n_parameters, n_parameters)\n        The covariance matrix (HR'HR) for the gain-related regressors.\n    gain_response : ndarray of floats, shape (n_parameters,)\n        The response vector (HR'y) for the gain-related regressors.\n\n    Notes\n    -----\n    This function constructs a matrix of static regressors (G+H) for the derivative\n    (gain) based on the provided static function outputs (y_static), inputs\n    (X_static), and gain values. The linear mapping matrix (R) should be\n    precomputed before calling this function.\n\n    \"\"\"\n    R, qit = self.build_linear_mapping()\n    H = np.zeros((len(y_static), len(qit)))\n    G = np.zeros((len(y_static), len(qit)))\n    for i in range(len(y_static)):\n        for j in range(1, len(qit)):\n            if y_static[i, 0] == 0:\n                if (qit[j, 0]) == 1:\n                    H[i, j] = gain[i][0]\n                else:\n                    H[i, j] = 0\n            else:\n                H[i, j] = (gain[i] * qit[j, 0] * y_static[i, 0] ** (qit[j, 0] - 1))[\n                    0\n                ]\n            for k in range(self.n_inputs):\n                if x_static[i, k] == 0:\n                    if (qit[j, 1 + k]) == 1:\n                        G[i, j] = 1\n                    else:\n                        G[i, j] = 0\n                else:\n                    G[i, j] = qit[j, 1 + k] * x_static[i, k] ** (qit[j, 1 + k] - 1)\n\n    HR = (G + H).dot(R)\n    gain_covariance = HR.T.dot(HR)\n    gain_response = HR.T.dot(gain)\n    return HR, gain_covariance, gain_response\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.build_system_data","title":"<code>build_system_data(y, static_gain, static_function)</code>","text":"<p>Construct a list of output data components for the NARMAX system.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The target data used in the identification process.</p> required <code>static_gain</code> <code>ndarray of floats</code> <p>Static gain output data.</p> required <code>static_function</code> <code>ndarray of floats</code> <p>Static function output data.</p> required <p>Returns:</p> Name Type Description <code>system_data</code> <code>list of ndarrays</code> <p>A list containing data components, including the target data (y), static gain data (if present), and static function data (if present).</p> Notes <p>This method constructs a list of data components that are used in the NARMAX system identification process. The components may include the target data (y), static gain data (if enabled), and static function data (if enabled).</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def build_system_data(\n    self,\n    y: np.ndarray,\n    static_gain: np.ndarray,\n    static_function: np.ndarray,\n) -&gt; List[np.ndarray]:\n    \"\"\"Construct a list of output data components for the NARMAX system.\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The target data used in the identification process.\n    static_gain : ndarray of floats\n        Static gain output data.\n    static_function : ndarray of floats\n        Static function output data.\n\n    Returns\n    -------\n    system_data : list of ndarrays\n        A list containing data components, including the target data (y),\n        static gain data (if present), and static function data (if present).\n\n    Notes\n    -----\n    This method constructs a list of data components that are used in the NARMAX\n    system identification process. The components may include the target data (y),\n    static gain data (if enabled), and static function data (if enabled).\n\n    \"\"\"\n    if not self.static_gain:\n        return [y] + [static_function]\n\n    if not self.static_function:\n        return [y] + [static_gain]\n\n    return [y] + [static_gain] + [static_function]\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.AILS.estimate","title":"<code>estimate(y_static=np.zeros(1), X_static=np.zeros(1), gain=np.zeros(1), y=np.zeros(1), X=np.zeros((1, 1)), weighing_matrix=np.zeros((1, 1)))</code>","text":"<p>Estimate the parameters via multi-objective techniques.</p> <p>Parameters:</p> Name Type Description Default <code>y_static</code> <code>array-like of shape = n_samples_static_function</code> <p>Output of static function.</p> <code>= ([0])</code> <code>X_static</code> <code>array-like of shape = n_samples_static_function</code> <p>Static function input.</p> <code>= ([0])</code> <code>gain</code> <code>array-like of shape = n_samples_static_gain</code> <p>Static gain input.</p> <code>= ([0])</code> <code>y</code> <code>array-like of shape = n_samples</code> <p>The target data used in the identification process.</p> <code>= ([0])</code> <code>X</code> <code>ndarray of floats</code> <p>Matrix of static regressors.</p> <code>= ([[0],[0]])</code> <code>weighing_matrix</code> <code>ndarray</code> <p>Weighing matrix for defining the weight of each objective.</p> <code>zeros((1, 1))</code> <p>Returns:</p> Name Type Description <code>J</code> <code>ndarray</code> <p>Matrix referring to the objectives.</p> <code>euclidean_norm</code> <code>ndarray</code> <p>Matrix of the Euclidean norm.</p> <code>theta</code> <code>ndarray</code> <p>Matrix with parameters for each weight.</p> <code>HR</code> <code>ndarray</code> <p>H matrix multiplied by R.</p> <code>QR</code> <code>ndarray</code> <p>Q matrix multiplied by R.</p> <code>position</code> <code>ndarray, default = ([[0],[0]])</code> <p>Position of the best theta set.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def estimate(\n    self,\n    y_static: np.ndarray = np.zeros(1),\n    X_static: np.ndarray = np.zeros(1),\n    gain: np.ndarray = np.zeros(1),\n    y: np.ndarray = np.zeros(1),\n    X: np.ndarray = np.zeros((1, 1)),\n    weighing_matrix: np.ndarray = np.zeros((1, 1)),\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.int64]:\n    \"\"\"Estimate the parameters via multi-objective techniques.\n\n    Parameters\n    ----------\n    y_static : array-like of shape = n_samples_static_function, default = ([0])\n        Output of static function.\n    X_static : array-like of shape = n_samples_static_function, default = ([0])\n        Static function input.\n    gain : array-like of shape = n_samples_static_gain, default = ([0])\n        Static gain input.\n    y : array-like of shape = n_samples, default = ([0])\n        The target data used in the identification process.\n    X : ndarray of floats, default = ([[0],[0]])\n        Matrix of static regressors.\n    weighing_matrix: ndarray\n        Weighing matrix for defining the weight of each objective.\n\n    Returns\n    -------\n    J : ndarray\n        Matrix referring to the objectives.\n    euclidean_norm : ndarray\n        Matrix of the Euclidean norm.\n    theta : ndarray\n        Matrix with parameters for each weight.\n    HR : ndarray\n        H matrix multiplied by R.\n    QR : ndarray\n        Q matrix multiplied by R.\n    position : ndarray, default = ([[0],[0]])\n        Position of the best theta set.\n\n    \"\"\"\n    psi = self.build_psi(X, y)\n    y = y[self.max_lag :]\n    HR, QR = np.zeros((1, 1)), np.zeros((1, 1))\n    n_parameters = weighing_matrix.shape[1]\n    num_objectives = self.static_function + self.static_gain + 1\n    euclidean_norm = np.zeros(n_parameters)\n    theta = np.zeros((n_parameters, self.final_model.shape[0]))\n    dynamic_covariance = psi.T.dot(psi)\n    dynamic_response = psi.T.dot(y)\n\n    if self.static_function:\n        QR, static_covariance, static_response = (\n            self.build_static_function_information(X_static, y_static)\n        )\n    if self.static_gain:\n        HR, gain_covariance, gain_response = self.build_static_gain_information(\n            X_static, y_static, gain\n        )\n    J = np.zeros((num_objectives, n_parameters))\n    system_data = self.build_system_data(y, gain, y_static)\n    affine_information_data = self.build_affine_data(psi, HR, QR)\n    for i in range(n_parameters):\n        theta1 = weighing_matrix[0, i] * dynamic_covariance\n        theta2 = weighing_matrix[0, i] * dynamic_response\n\n        w = 1\n        if self.static_function:\n            theta1 += weighing_matrix[w, i] * static_covariance\n            theta2 += weighing_matrix[w, i] * static_response.reshape(-1, 1)\n            w += 1\n\n        if self.static_gain:\n            theta1 += weighing_matrix[w, i] * gain_covariance\n            theta2 += weighing_matrix[w, i] * gain_response.reshape(-1, 1)\n            w += 1\n\n        tmp_theta = np.linalg.lstsq(theta1, theta2, rcond=None)[0]\n        theta[i, :] = tmp_theta.T\n\n        for j in range(num_objectives):\n            residuals = get_cost_function(\n                system_data[j], affine_information_data[j], tmp_theta\n            )\n            J[j, i] = residuals[0]\n\n        euclidean_norm[i] = np.linalg.norm(J[:, i])\n\n    if self.normalize is True:\n        J /= np.max(J, axis=1)[:, np.newaxis]\n        euclidean_norm /= np.max(euclidean_norm)\n\n        euclidean_norm = euclidean_norm / np.max(euclidean_norm)\n\n    position = np.argmin(euclidean_norm)\n    return (\n        J,\n        euclidean_norm,\n        theta,\n        HR,\n        QR,\n        position,\n    )\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.get_cost_function","title":"<code>get_cost_function(y, psi, theta)</code>","text":"<p>Calculate the cost function based on residuals.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray of floats</code> <p>The target data used in the identification process.</p> required <code>psi</code> <code>ndarray of floats, shape (n_samples, n_parameters)</code> <p>The matrix of regressors.</p> required <code>theta</code> <code>ndarray of floats</code> <p>The parameter vector.</p> required <p>Returns:</p> Name Type Description <code>cost_function</code> <code>float</code> <p>The calculated cost function value.</p> Notes <p>This method computes the cost function value based on the residuals between the target data (y) and the predicted values using the regressors (dynamic and static) and parameter vector (theta). It quantifies the error in the model's predictions.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def get_cost_function(y: np.ndarray, psi: np.ndarray, theta: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the cost function based on residuals.\n\n    Parameters\n    ----------\n    y : ndarray of floats\n        The target data used in the identification process.\n    psi : ndarray of floats, shape (n_samples, n_parameters)\n        The matrix of regressors.\n    theta : ndarray of floats\n        The parameter vector.\n\n    Returns\n    -------\n    cost_function : float\n        The calculated cost function value.\n\n    Notes\n    -----\n    This method computes the cost function value based on the residuals between\n    the target data (y) and the predicted values using the regressors (dynamic\n    and static) and parameter vector (theta). It quantifies the error in the\n    model's predictions.\n\n    \"\"\"\n    residuals = y - psi.dot(theta)\n    return residuals.T.dot(residuals)\n</code></pre>"},{"location":"user-guide/API/multiobjective-parameter-estimation/#sysidentpy.multiobjective_parameter_estimation.estimators.get_term_clustering","title":"<code>get_term_clustering(qit)</code>","text":"<p>Get the term clustering of the model.</p> <p>This function takes a matrix <code>qit</code> and compute the term clustering based on their values. It calculates the number of occurrences of each value for each row in the matrix.</p> <p>Parameters:</p> Name Type Description Default <code>qit</code> <code>ndarray</code> <p>Input matrix containing terms clustering to be sorted.</p> required <p>Returns:</p> Name Type Description <code>N_aux</code> <code>ndarray</code> <p>A new matrix with rows representing the number of occurrences of each value for each row in the input matrix <code>qit</code>. The columns correspond to different values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; qit = np.array([[1, 2, 2],\n...                 [1, 3, 1],\n...                 [2, 2, 3]])\n&gt;&gt;&gt; result = get_term_clustering(qit)\n&gt;&gt;&gt; print(result)\n[[1. 2. 0. 0.]\n[2. 0. 1. 0.]\n[0. 2. 1. 0.]]\n</code></pre> Notes <p>The function calculates the number of occurrences of each value (from 1 to the maximum value in the input matrix <code>qit</code>) for each row and returns a matrix where rows represent rows of the input matrix <code>qit</code>, and columns represent different values.</p> Source code in <code>sysidentpy/multiobjective_parameter_estimation/estimators.py</code> <pre><code>def get_term_clustering(qit: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Get the term clustering of the model.\n\n    This function takes a matrix `qit` and compute the term clustering based\n    on their values. It calculates the number of occurrences of each value\n    for each row in the matrix.\n\n    Parameters\n    ----------\n    qit : ndarray\n        Input matrix containing terms clustering to be sorted.\n\n    Returns\n    -------\n    N_aux : ndarray\n        A new matrix with rows representing the number of occurrences of each value\n        for each row in the input matrix `qit`. The columns correspond to different\n        values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; qit = np.array([[1, 2, 2],\n    ...                 [1, 3, 1],\n    ...                 [2, 2, 3]])\n    &gt;&gt;&gt; result = get_term_clustering(qit)\n    &gt;&gt;&gt; print(result)\n    [[1. 2. 0. 0.]\n    [2. 0. 1. 0.]\n    [0. 2. 1. 0.]]\n\n    Notes\n    -----\n    The function calculates the number of occurrences of each value (from 1 to\n    the maximum value in the input matrix `qit`) for each row and returns a matrix\n    where rows represent rows of the input matrix `qit`, and columns represent\n    different values.\n\n    \"\"\"\n    max_value = int(np.max(qit))\n    counts_matrix = np.zeros((qit.shape[0], max_value))\n\n    for k in range(1, max_value + 1):\n        counts_matrix[:, k - 1] = np.sum(qit == k, axis=1)\n\n    return counts_matrix.astype(int)\n</code></pre>"},{"location":"user-guide/API/narmax-base/","title":"Documentation for <code>narmax-base</code>","text":"<p>Base classes for NARMAX estimator.</p>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.BaseMSS","title":"<code>BaseMSS</code>","text":"<p>               Bases: <code>RegressorDictionary</code></p> <p>Base class for Model Structure Selection.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>class BaseMSS(RegressorDictionary, metaclass=ABCMeta):\n    \"\"\"Base class for Model Structure Selection.\"\"\"\n\n    @abstractmethod\n    def __init__(self):\n        super().__init__(self)\n        self.max_lag = None\n        self.n_inputs = None\n        self.theta = None\n        self.final_model = None\n        self.pivv = None\n\n    @abstractmethod\n    def fit(self, *, X, y):\n        \"\"\"Abstract method.\"\"\"\n\n    @abstractmethod\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Abstract method.\"\"\"\n\n    def _code2exponents(self, *, code: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Convert regressor code to exponents array.\n\n        Parameters\n        ----------\n        code : 1D-array of int\n            Codification of one regressor.\n\n        Returns\n        -------\n        exponents = ndarray of ints\n        \"\"\"\n        regressors = np.array(list(set(code)))\n        regressors_count = Counter(code)\n\n        if np.all(regressors == 0):\n            return np.zeros(self.max_lag * (1 + self.n_inputs))\n\n        exponents = np.array([], dtype=float)\n        elements = np.round(np.divide(regressors, 1000), 0)[(regressors &gt; 0)].astype(\n            int\n        )\n\n        for j in range(1, self.n_inputs + 2):\n            base_exponents = np.zeros(self.max_lag, dtype=float)\n            if j in elements:\n                for i in range(1, self.max_lag + 1):\n                    regressor_code = int(j * 1000 + i)\n                    base_exponents[-i] = regressors_count[regressor_code]\n                exponents = np.append(exponents, base_exponents)\n\n            else:\n                exponents = np.append(exponents, base_exponents)\n\n        return exponents\n\n    def _one_step_ahead_prediction(self, x_base: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        x_base : ndarray of floats of shape = n_samples\n            Regressor matrix with input-output arrays.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = np.dot(x_base, self.theta.flatten())\n        return yhat.reshape(-1, 1)\n\n    @abstractmethod\n    def _model_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Model prediction wrapper.\"\"\"\n\n    def _narmax_predict(\n        self,\n        x: np.ndarray,\n        y_initial: np.ndarray,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"narmax_predict method.\"\"\"\n        y_output = np.zeros(forecast_horizon, dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, forecast_horizon):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            y_output[i] = np.dot(regressor_value, self.theta.flatten())\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    @abstractmethod\n    def _nfir_predict(self, x: np.ndarray, y_initial: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Nfir predict method.\"\"\"\n        y_output = np.zeros(x.shape[0], dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n        x = x.reshape(-1, self.n_inputs)\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, x.shape[0]):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            y_output[i] = np.dot(regressor_value, self.theta.flatten())\n        return y_output[self.max_lag : :].reshape(-1, 1)\n\n    def _nar_step_ahead(self, y: np.ndarray, steps_ahead: int) -&gt; np.ndarray:\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        yhat_length = len(y) + steps_ahead\n        yhat = np.zeros(yhat_length, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    x=None, y_initial=y[step:i], forecast_horizon=steps_ahead\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=None, y_initial=y[steps[-1] : i]\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=None, y_initial=y[0:i], forecast_horizon=steps_ahead\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    def narmax_n_step_ahead(\n        self,\n        x: np.ndarray,\n        y: np.ndarray,\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"n_steps ahead prediction method for NARMAX model.\"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n        x = x.reshape(-1, self.n_inputs)\n        yhat = np.zeros(x.shape[0], dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n        steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n        if len(steps) &gt; 1:\n            for step in steps[:-1]:\n                yhat[i : i + steps_ahead] = self._model_prediction(\n                    x=x[step : i + steps_ahead],\n                    y_initial=y[step:i],\n                )[-steps_ahead:].ravel()\n                i += steps_ahead\n\n            steps_ahead = np.sum(np.isnan(yhat))\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[steps[-1] : i + steps_ahead],\n                y_initial=y[steps[-1] : i],\n            )[-steps_ahead:].ravel()\n        else:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[0 : i + steps_ahead],\n                y_initial=y[0:i],\n            )[-steps_ahead:].ravel()\n\n        yhat = yhat.ravel()[self.max_lag : :]\n        return yhat.reshape(-1, 1)\n\n    @abstractmethod\n    def _n_step_ahead_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            Predicted values for NARMAX and NAR models.\n        \"\"\"\n        if self.model_type == \"NARMAX\":\n            return self.narmax_n_step_ahead(x, y, steps_ahead)\n\n        if self.model_type == \"NAR\":\n            return self._nar_step_ahead(y, steps_ahead)\n\n        raise ValueError(\n            \"n_steps_ahead prediction will be implemented for NFIR models in v0.4.*\"\n        )\n\n    @abstractmethod\n    def _basis_function_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 1,\n    ) -&gt; np.ndarray:\n        \"\"\"Basis function prediction.\"\"\"\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        # Discard unnecessary initial values\n        analyzed_elements_number = self.max_lag + 1\n\n        for i in range(forecast_horizon - self.max_lag):\n            if self.model_type == \"NARMAX\":\n                lagged_data = build_input_output_matrix(\n                    x[i : i + analyzed_elements_number],\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1),\n                    self.xlag,\n                    self.ylag,\n                )\n            elif self.model_type == \"NAR\":\n                lagged_data = build_output_matrix(\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1), self.ylag\n                )\n            elif self.model_type == \"NFIR\":\n                lagged_data = build_input_matrix(\n                    x[i : i + analyzed_elements_number], self.xlag\n                )\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            x_tmp = self.basis_function.transform(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=self.pivv[: len(self.final_model)],\n            )\n\n            a = x_tmp @ self.theta\n            yhat[i + self.max_lag] = a.item()\n\n        return yhat[self.max_lag :].reshape(-1, 1)\n\n    @abstractmethod\n    def _basis_function_n_step_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: np.ndarray,\n        steps_ahead: int,\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Basis function n step ahead.\"\"\"\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        # Discard unnecessary initial values\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead],\n                    y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-steps_ahead:].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        return yhat[self.max_lag :].reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(\n        self,\n        x: Optional[np.ndarray],\n        y: np.ndarray,\n        steps_ahead: int,\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Basis n steps horizon.\"\"\"\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        # Discard unnecessary initial values\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead], y[k : i + steps_ahead], forecast_horizon\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat[self.max_lag :].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.BaseMSS.fit","title":"<code>fit(*, X, y)</code>  <code>abstractmethod</code>","text":"<p>Abstract method.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>@abstractmethod\ndef fit(self, *, X, y):\n    \"\"\"Abstract method.\"\"\"\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.BaseMSS.narmax_n_step_ahead","title":"<code>narmax_n_step_ahead(x, y, steps_ahead)</code>","text":"<p>n_steps ahead prediction method for NARMAX model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def narmax_n_step_ahead(\n    self,\n    x: np.ndarray,\n    y: np.ndarray,\n    steps_ahead: Optional[int],\n) -&gt; np.ndarray:\n    \"\"\"n_steps ahead prediction method for NARMAX model.\"\"\"\n    if len(y) &lt; self.max_lag:\n        raise ValueError(\n            \"Insufficient initial condition elements! Expected at least\"\n            f\" {self.max_lag} elements.\"\n        )\n\n    to_remove = int(np.ceil((len(y) - self.max_lag) / steps_ahead))\n    x = x.reshape(-1, self.n_inputs)\n    yhat = np.zeros(x.shape[0], dtype=float)\n    yhat.fill(np.nan)\n    yhat[: self.max_lag] = y[: self.max_lag, 0]\n    i = self.max_lag\n    steps = [step for step in range(0, to_remove * steps_ahead, steps_ahead)]\n    if len(steps) &gt; 1:\n        for step in steps[:-1]:\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x=x[step : i + steps_ahead],\n                y_initial=y[step:i],\n            )[-steps_ahead:].ravel()\n            i += steps_ahead\n\n        steps_ahead = np.sum(np.isnan(yhat))\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            x=x[steps[-1] : i + steps_ahead],\n            y_initial=y[steps[-1] : i],\n        )[-steps_ahead:].ravel()\n    else:\n        yhat[i : i + steps_ahead] = self._model_prediction(\n            x=x[0 : i + steps_ahead],\n            y_initial=y[0:i],\n        )[-steps_ahead:].ravel()\n\n    yhat = yhat.ravel()[self.max_lag : :]\n    return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.BaseMSS.predict","title":"<code>predict(*, X=None, y, steps_ahead=None, forecast_horizon=1)</code>  <code>abstractmethod</code>","text":"<p>Abstract method.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: np.ndarray,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: int = 1,\n) -&gt; np.ndarray:\n    \"\"\"Abstract method.\"\"\"\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary","title":"<code>RegressorDictionary</code>","text":"<p>Base class for Model Structure Selection.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>class RegressorDictionary:\n    \"\"\"Base class for Model Structure Selection.\"\"\"\n\n    def __init__(\n        self,\n        xlag: Union[List[Any], Any] = 1,\n        ylag: Union[List[Any], Any] = 1,\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n    ):\n        self.xlag = xlag\n        self.ylag = ylag\n        self.basis_function = basis_function\n        self.model_type = model_type\n\n    def create_narmax_code(self, n_inputs: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Create the code representation of the regressors.\n\n        This function generates a codification from all possibles\n        regressors given the maximum lag of the input and output.\n        This is used to write the final terms of the model in a\n        readable form. [1001] -&gt; y(k-1).\n        This code format was based on a dissertation from UFMG. See\n        reference below.\n\n        Parameters\n        ----------\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        x_vec : ndarray of int\n            List of the input lags.\n        y_vec : ndarray of int\n            List of the output lags.\n\n        Examples\n        --------\n        The codification is defined as:\n\n        100n = y(k-n)\n        200n = u(k-n)\n        [100n, 100n] = y(k-n)y(k-n)\n        [200n, 200n] = u(k-n)u(k-n)\n\n        References\n        ----------\n        - Master Thesis: Barbosa, Al\u00edpio Monteiro.\n            T\u00e9cnicas de otimiza\u00e7\u00e3o bi-objetivo para a determina\u00e7\u00e3o\n            da estrutura de modelos NARX (2010).\n\n        \"\"\"\n        if self.basis_function.degree &lt; 1:\n            raise ValueError(\n                f\"degree must be integer and &gt; zero. Got {self.basis_function.degree}\"\n            )\n\n        if np.min(np.minimum(self.ylag, 1)) &lt; 1:\n            raise ValueError(\n                f\"ylag must be integer or list and &gt; zero. Got {self.ylag}\"\n            )\n\n        if (\n            np.min(\n                np.min(\n                    np.array(list(chain.from_iterable([[self.xlag]])), dtype=\"object\")\n                )\n            )\n            &lt; 1\n        ):\n            raise ValueError(\n                f\"xlag must be integer or list and &gt; zero. Got {self.xlag}\"\n            )\n\n        y_vec = self.get_y_lag_list()\n\n        if n_inputs == 1:\n            x_vec = self.get_siso_x_lag_list()\n        else:\n            x_vec = self.get_miso_x_lag_list(n_inputs)\n\n        return x_vec, y_vec\n\n    def get_y_lag_list(self) -&gt; np.ndarray:\n        \"\"\"Return y regressor code list.\n\n        Returns\n        -------\n        y_vec = ndarray of ints\n            The y regressor code list given the ylag.\n\n        \"\"\"\n        if isinstance(self.ylag, list):\n            # create only the lags passed from list\n            y_vec = []\n            y_vec.extend([lag + 1000 for lag in self.ylag])\n            return np.array(y_vec)\n\n        # create a range of lags if passed a int value\n        return np.arange(1001, 1001 + self.ylag)\n\n    def get_siso_x_lag_list(self) -&gt; np.ndarray:\n        \"\"\"Return x regressor code list for SISO models.\n\n        Returns\n        -------\n        x_vec_tmp = ndarray of ints\n            The x regressor code list given the xlag for a SISO model.\n\n        \"\"\"\n        if isinstance(self.xlag, list):\n            # create only the lags passed from list\n            x_vec_tmp = []\n            x_vec_tmp.extend([lag + 2000 for lag in self.xlag])\n            return np.array(x_vec_tmp)\n\n        # create a range of lags if passed a int value\n        return np.arange(2001, 2001 + self.xlag)\n\n    def get_miso_x_lag_list(self, n_inputs: int) -&gt; np.ndarray:\n        \"\"\"Return x regressor code list for MISO models.\n\n        Parameters\n        ----------\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        x_vec = ndarray of ints\n            The x regressor code list given the xlag for a MISO model.\n\n        \"\"\"\n        # only list are allowed if n_inputs &gt; 1\n        # the user must entered list of the desired lags explicitly\n        x_vec_tmp = []\n        for i in range(n_inputs):\n            if isinstance(self.xlag[i], list):\n                # create 200n, 300n,..., 400n to describe each input\n                x_vec_tmp.extend([lag + 2000 + i * 1000 for lag in self.xlag[i]])\n            elif isinstance(self.xlag[i], int) and n_inputs &gt; 1:\n                x_vec_tmp.extend(\n                    [np.arange(2001 + i * 1000, 2001 + i * 1000 + self.xlag[i])]\n                )\n\n        # if x_vec is a nested list, ensure all elements are arrays\n        all_arrays = [np.array([i]) if isinstance(i, int) else i for i in x_vec_tmp]\n        return np.concatenate([i for i in all_arrays])\n\n    def regressor_space(self, n_inputs: int) -&gt; np.ndarray:\n        \"\"\"Create regressor code based on model type.\n\n        Parameters\n        ----------\n        n_inputs : int\n            Number of input variables.\n\n        Returns\n        -------\n        regressor_code = ndarray of ints\n            The regressor code list given the xlag and ylag for a MISO model.\n\n        \"\"\"\n        x_vec, y_vec = self.create_narmax_code(n_inputs)\n        reg_aux = np.array([0])\n        if self.model_type == \"NARMAX\":\n            reg_aux = np.concatenate([reg_aux, y_vec, x_vec])\n        elif self.model_type == \"NAR\":\n            reg_aux = np.concatenate([reg_aux, y_vec])\n        elif self.model_type == \"NFIR\":\n            reg_aux = np.concatenate([reg_aux, x_vec])\n        else:\n            raise ValueError(\n                \"Unrecognized model type. Model type should be NARMAX, NAR or NFIR\"\n            )\n\n        regressor_code = list(\n            combinations_with_replacement(reg_aux, self.basis_function.degree)\n        )\n\n        regressor_code = np.array(regressor_code)\n        regressor_code = regressor_code[:, regressor_code.shape[1] :: -1]\n        return regressor_code\n\n    def _get_max_lag(self):\n        \"\"\"Get the max lag defined by the user.\n\n        Returns\n        -------\n        max_lag = int\n            The max lag value defined by the user.\n        \"\"\"\n        ny = np.max(list(chain.from_iterable([[self.ylag]])))\n        nx = np.max(list(chain.from_iterable([[np.array(self.xlag, dtype=object)]])))\n        return np.max([ny, np.max(nx)])\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.create_narmax_code","title":"<code>create_narmax_code(n_inputs)</code>","text":"<p>Create the code representation of the regressors.</p> <p>This function generates a codification from all possibles regressors given the maximum lag of the input and output. This is used to write the final terms of the model in a readable form. [1001] -&gt; y(k-1). This code format was based on a dissertation from UFMG. See reference below.</p> <p>Parameters:</p> Name Type Description Default <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> required <p>Returns:</p> Name Type Description <code>x_vec</code> <code>ndarray of int</code> <p>List of the input lags.</p> <code>y_vec</code> <code>ndarray of int</code> <p>List of the output lags.</p> <p>Examples:</p> <p>The codification is defined as:</p> <p>100n = y(k-n) 200n = u(k-n) [100n, 100n] = y(k-n)y(k-n) [200n, 200n] = u(k-n)u(k-n)</p> References <ul> <li>Master Thesis: Barbosa, Al\u00edpio Monteiro.     T\u00e9cnicas de otimiza\u00e7\u00e3o bi-objetivo para a determina\u00e7\u00e3o     da estrutura de modelos NARX (2010).</li> </ul> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def create_narmax_code(self, n_inputs: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Create the code representation of the regressors.\n\n    This function generates a codification from all possibles\n    regressors given the maximum lag of the input and output.\n    This is used to write the final terms of the model in a\n    readable form. [1001] -&gt; y(k-1).\n    This code format was based on a dissertation from UFMG. See\n    reference below.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input variables.\n\n    Returns\n    -------\n    x_vec : ndarray of int\n        List of the input lags.\n    y_vec : ndarray of int\n        List of the output lags.\n\n    Examples\n    --------\n    The codification is defined as:\n\n    100n = y(k-n)\n    200n = u(k-n)\n    [100n, 100n] = y(k-n)y(k-n)\n    [200n, 200n] = u(k-n)u(k-n)\n\n    References\n    ----------\n    - Master Thesis: Barbosa, Al\u00edpio Monteiro.\n        T\u00e9cnicas de otimiza\u00e7\u00e3o bi-objetivo para a determina\u00e7\u00e3o\n        da estrutura de modelos NARX (2010).\n\n    \"\"\"\n    if self.basis_function.degree &lt; 1:\n        raise ValueError(\n            f\"degree must be integer and &gt; zero. Got {self.basis_function.degree}\"\n        )\n\n    if np.min(np.minimum(self.ylag, 1)) &lt; 1:\n        raise ValueError(\n            f\"ylag must be integer or list and &gt; zero. Got {self.ylag}\"\n        )\n\n    if (\n        np.min(\n            np.min(\n                np.array(list(chain.from_iterable([[self.xlag]])), dtype=\"object\")\n            )\n        )\n        &lt; 1\n    ):\n        raise ValueError(\n            f\"xlag must be integer or list and &gt; zero. Got {self.xlag}\"\n        )\n\n    y_vec = self.get_y_lag_list()\n\n    if n_inputs == 1:\n        x_vec = self.get_siso_x_lag_list()\n    else:\n        x_vec = self.get_miso_x_lag_list(n_inputs)\n\n    return x_vec, y_vec\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_miso_x_lag_list","title":"<code>get_miso_x_lag_list(n_inputs)</code>","text":"<p>Return x regressor code list for MISO models.</p> <p>Parameters:</p> Name Type Description Default <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> required <p>Returns:</p> Type Description <code>x_vec = ndarray of ints</code> <p>The x regressor code list given the xlag for a MISO model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_miso_x_lag_list(self, n_inputs: int) -&gt; np.ndarray:\n    \"\"\"Return x regressor code list for MISO models.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input variables.\n\n    Returns\n    -------\n    x_vec = ndarray of ints\n        The x regressor code list given the xlag for a MISO model.\n\n    \"\"\"\n    # only list are allowed if n_inputs &gt; 1\n    # the user must entered list of the desired lags explicitly\n    x_vec_tmp = []\n    for i in range(n_inputs):\n        if isinstance(self.xlag[i], list):\n            # create 200n, 300n,..., 400n to describe each input\n            x_vec_tmp.extend([lag + 2000 + i * 1000 for lag in self.xlag[i]])\n        elif isinstance(self.xlag[i], int) and n_inputs &gt; 1:\n            x_vec_tmp.extend(\n                [np.arange(2001 + i * 1000, 2001 + i * 1000 + self.xlag[i])]\n            )\n\n    # if x_vec is a nested list, ensure all elements are arrays\n    all_arrays = [np.array([i]) if isinstance(i, int) else i for i in x_vec_tmp]\n    return np.concatenate([i for i in all_arrays])\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_siso_x_lag_list","title":"<code>get_siso_x_lag_list()</code>","text":"<p>Return x regressor code list for SISO models.</p> <p>Returns:</p> Type Description <code>x_vec_tmp = ndarray of ints</code> <p>The x regressor code list given the xlag for a SISO model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_siso_x_lag_list(self) -&gt; np.ndarray:\n    \"\"\"Return x regressor code list for SISO models.\n\n    Returns\n    -------\n    x_vec_tmp = ndarray of ints\n        The x regressor code list given the xlag for a SISO model.\n\n    \"\"\"\n    if isinstance(self.xlag, list):\n        # create only the lags passed from list\n        x_vec_tmp = []\n        x_vec_tmp.extend([lag + 2000 for lag in self.xlag])\n        return np.array(x_vec_tmp)\n\n    # create a range of lags if passed a int value\n    return np.arange(2001, 2001 + self.xlag)\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.get_y_lag_list","title":"<code>get_y_lag_list()</code>","text":"<p>Return y regressor code list.</p> <p>Returns:</p> Type Description <code>y_vec = ndarray of ints</code> <p>The y regressor code list given the ylag.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def get_y_lag_list(self) -&gt; np.ndarray:\n    \"\"\"Return y regressor code list.\n\n    Returns\n    -------\n    y_vec = ndarray of ints\n        The y regressor code list given the ylag.\n\n    \"\"\"\n    if isinstance(self.ylag, list):\n        # create only the lags passed from list\n        y_vec = []\n        y_vec.extend([lag + 1000 for lag in self.ylag])\n        return np.array(y_vec)\n\n    # create a range of lags if passed a int value\n    return np.arange(1001, 1001 + self.ylag)\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.RegressorDictionary.regressor_space","title":"<code>regressor_space(n_inputs)</code>","text":"<p>Create regressor code based on model type.</p> <p>Parameters:</p> Name Type Description Default <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> required <p>Returns:</p> Type Description <code>regressor_code = ndarray of ints</code> <p>The regressor code list given the xlag and ylag for a MISO model.</p> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def regressor_space(self, n_inputs: int) -&gt; np.ndarray:\n    \"\"\"Create regressor code based on model type.\n\n    Parameters\n    ----------\n    n_inputs : int\n        Number of input variables.\n\n    Returns\n    -------\n    regressor_code = ndarray of ints\n        The regressor code list given the xlag and ylag for a MISO model.\n\n    \"\"\"\n    x_vec, y_vec = self.create_narmax_code(n_inputs)\n    reg_aux = np.array([0])\n    if self.model_type == \"NARMAX\":\n        reg_aux = np.concatenate([reg_aux, y_vec, x_vec])\n    elif self.model_type == \"NAR\":\n        reg_aux = np.concatenate([reg_aux, y_vec])\n    elif self.model_type == \"NFIR\":\n        reg_aux = np.concatenate([reg_aux, x_vec])\n    else:\n        raise ValueError(\n            \"Unrecognized model type. Model type should be NARMAX, NAR or NFIR\"\n        )\n\n    regressor_code = list(\n        combinations_with_replacement(reg_aux, self.basis_function.degree)\n    )\n\n    regressor_code = np.array(regressor_code)\n    regressor_code = regressor_code[:, regressor_code.shape[1] :: -1]\n    return regressor_code\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.house","title":"<code>house(x)</code>","text":"<p>Perform a Householder reflection of vector.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array-like of shape = number_of_training_samples</code> <p>The respective column of the matrix of regressors in each iteration of ERR function.</p> required <p>Returns:</p> Name Type Description <code>v</code> <code>array-like of shape = number_of_training_samples</code> <p>The reflection of the array x.</p> References <ul> <li>Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).     Orthogonal least squares methods and their application to non-linear     system identification.</li> </ul> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def house(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Perform a Householder reflection of vector.\n\n    Parameters\n    ----------\n    x : array-like of shape = number_of_training_samples\n        The respective column of the matrix of regressors in each\n        iteration of ERR function.\n\n    Returns\n    -------\n    v : array-like of shape = number_of_training_samples\n        The reflection of the array x.\n\n    References\n    ----------\n    - Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).\n        Orthogonal least squares methods and their application to non-linear\n        system identification.\n\n    \"\"\"\n    u = np.linalg.norm(x, 2)\n    if u != 0:\n        aux_b = x[0] + np.sign(x[0]) * u\n        x = x[1:] / (aux_b + np.finfo(np.float64).eps)\n        x = np.concatenate((np.array([1]), x))\n    return x\n</code></pre>"},{"location":"user-guide/API/narmax-base/#sysidentpy.narmax_base.rowhouse","title":"<code>rowhouse(RA, v)</code>","text":"<p>Perform a row Householder transformation.</p> <p>Parameters:</p> Name Type Description Default <code>RA</code> <code>array-like of shape = number_of_training_samples</code> <p>The respective column of the matrix of regressors in each iteration of ERR function.</p> required <code>v</code> <code>array-like of shape = number_of_training_samples</code> <p>The reflected vector obtained by using the householder reflection.</p> required <p>Returns:</p> Name Type Description <code>B</code> <code>array-like of shape = number_of_training_samples</code> References <ul> <li>Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).     Orthogonal least squares methods and their application to     non-linear system identification. International Journal of     control, 50(5), 1873-1896.</li> </ul> Source code in <code>sysidentpy/narmax_base.py</code> <pre><code>def rowhouse(RA: np.ndarray, v: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Perform a row Householder transformation.\n\n    Parameters\n    ----------\n    RA : array-like of shape = number_of_training_samples\n        The respective column of the matrix of regressors in each\n        iteration of ERR function.\n    v : array-like of shape = number_of_training_samples\n        The reflected vector obtained by using the householder reflection.\n\n    Returns\n    -------\n    B : array-like of shape = number_of_training_samples\n\n    References\n    ----------\n    - Manuscript: Chen, S., Billings, S. A., &amp; Luo, W. (1989).\n        Orthogonal least squares methods and their application to\n        non-linear system identification. International Journal of\n        control, 50(5), 1873-1896.\n\n    \"\"\"\n    b = -2 / np.dot(v.T, v)\n    w = b * np.dot(RA.T, v)\n    w = w.reshape(1, -1)\n    v = v.reshape(-1, 1)\n    RA = RA + v * w\n    B = RA\n    return B\n</code></pre>"},{"location":"user-guide/API/neural-narx/","title":"Documentation for <code>Neural NARX</code>","text":"<p>Build Polynomial NARMAX Models.</p>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN","title":"<code>NARXNN</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>NARX Neural Network model built on top of Pytorch.</p> <p>Neural networks are models composed of interconnected layers of nodes (neurons) designed for tasks like classification and regression. Each neuron is a basic unit within these networks. Mathematically, a neuron is represented by a function \\(f\\) that takes an input vector \\(\\mathbf{x} = [x_1, x_2, \\ldots, x_n]\\) and generates an output \\(y\\). This function usually involves a weighted sum of the inputs, an optional bias term \\(b\\), and an activation function \\(\\phi\\):</p> \\[ y = \\phi \\left( \\sum_{i=1}^{n} w_i x_i + b \\right) \\tag{2.31} \\] <p>where \\(\\mathbf{w} = [w_1, w_2, \\ldots, w_n]\\) are the weights associated with the inputs. The activation function \\(\\phi\\) introduces nonlinearity into the model, allowing the network to learn complex patterns.</p> <p>Currently we support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier, and we convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>basis_function</code> <p>Defines which basis function will be used in the model.</p> <code>Polynomial()</code> <code>model_type</code> <p>The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models</p> <code>'NARMAX'</code> <code>batch_size</code> <code>int</code> <p>Size of mini-batches of data for stochastic optimizers</p> <code>100</code> <code>learning_rate</code> <code>float</code> <p>Learning rate schedule for weight updates</p> <code>0.01</code> <code>epochs</code> <code>int</code> <p>Number of training epochs</p> <code>100</code> <code>loss_func</code> <code>str</code> <p>Select the loss function available in torch.nn.functional</p> <code>'mse_loss'</code> <code>optimizer</code> <code>str</code> <p>The solver for weight optimization</p> <code>'SGD'</code> <code>optim_params</code> <code>dict</code> <p>Optional parameters for the optimizer</p> <code>None</code> <code>net</code> <code>default=None</code> <p>The defined network using nn.Module</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Show the training and validation loss at each iteration</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from torch import nn\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.metrics import mean_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n&gt;&gt;&gt; from sysidentpy.neural_network import NARXNN\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(\n...     n=1000,\n...     colored_noise=False,\n...     sigma=0.01,\n...     train_percentage=80\n... )\n&gt;&gt;&gt; narx_nn = NARXNN(\n...     ylag=2,\n...     xlag=2,\n...     basis_function=basis_function,\n...     model_type=\"NARMAX\",\n...     loss_func='mse_loss',\n...     optimizer='Adam',\n...     epochs=200,\n...     verbose=False,\n...     optim_params={'betas': (0.9, 0.999), 'eps': 1e-05} # for the optimizer\n... )\n&gt;&gt;&gt; class Net(nn.Module):\n...     def __init__(self):\n...         super().__init__()\n...         self.lin = nn.Linear(4, 10)\n...         self.lin2 = nn.Linear(10, 10)\n...         self.lin3 = nn.Linear(10, 1)\n...         self.tanh = nn.Tanh()\n&gt;&gt;&gt;\n...     def forward(self, xb):\n...         z = self.lin(xb)\n...         z = self.tanh(z)\n...         z = self.lin2(z)\n...         z = self.tanh(z)\n...         z = self.lin3(z)\n...         return z\n&gt;&gt;&gt;\n&gt;&gt;&gt; narx_nn.net = Net()\n&gt;&gt;&gt; neural_narx.fit(x=x_train, y=y_train)\n&gt;&gt;&gt; yhat = neural_narx.predict(x=x_valid, y=y_valid)\n&gt;&gt;&gt; print(mean_squared_error(y_valid, yhat))\n0.000131\n</code></pre> References <ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf`_</li> </ul> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>class NARXNN(BaseMSS):\n    r\"\"\"NARX Neural Network model built on top of Pytorch.\n\n    Neural networks are models composed of interconnected layers of nodes\n    (neurons) designed for tasks like classification and regression. Each neuron\n    is a basic unit within these networks. Mathematically, a neuron is\n    represented by a function $f$ that takes an input vector\n    $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$ and generates an output $y$.\n    This function usually involves a weighted sum of the inputs, an optional\n    bias term $b$, and an activation function $\\phi$:\n\n    $$\n    y = \\phi \\left( \\sum_{i=1}^{n} w_i x_i + b \\right)\n    \\tag{2.31}\n    $$\n\n    where $\\mathbf{w} = [w_1, w_2, \\ldots, w_n]$ are the weights associated with the\n    inputs. The activation function $\\phi$ introduces nonlinearity into the model,\n    allowing the network to learn complex patterns.\n\n    Currently we support a Series-Parallel (open-loop) Feedforward Network training\n    process, which make the training process easier, and we convert the\n    NARX network from Series-Parallel to the Parallel (closed-loop) configuration for\n    prediction.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    basis_function: Polynomial or Fourier basis functions\n        Defines which basis function will be used in the model.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n    batch_size : int, default=100\n        Size of mini-batches of data for stochastic optimizers\n    learning_rate : float, default=0.01\n        Learning rate schedule for weight updates\n    epochs : int, default=100\n        Number of training epochs\n    loss_func : str, default='mse_loss'\n        Select the loss function available in torch.nn.functional\n    optimizer : str, default='SGD'\n        The solver for weight optimization\n    optim_params : dict, default=None\n        Optional parameters for the optimizer\n    net : default=None\n        The defined network using nn.Module\n    verbose : bool, default=False\n        Show the training and validation loss at each iteration\n\n    Examples\n    --------\n    &gt;&gt;&gt; from torch import nn\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.metrics import mean_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; from sysidentpy.neural_network import NARXNN\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_siso_data\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(\n    ...     n=1000,\n    ...     colored_noise=False,\n    ...     sigma=0.01,\n    ...     train_percentage=80\n    ... )\n    &gt;&gt;&gt; narx_nn = NARXNN(\n    ...     ylag=2,\n    ...     xlag=2,\n    ...     basis_function=basis_function,\n    ...     model_type=\"NARMAX\",\n    ...     loss_func='mse_loss',\n    ...     optimizer='Adam',\n    ...     epochs=200,\n    ...     verbose=False,\n    ...     optim_params={'betas': (0.9, 0.999), 'eps': 1e-05} # for the optimizer\n    ... )\n    &gt;&gt;&gt; class Net(nn.Module):\n    ...     def __init__(self):\n    ...         super().__init__()\n    ...         self.lin = nn.Linear(4, 10)\n    ...         self.lin2 = nn.Linear(10, 10)\n    ...         self.lin3 = nn.Linear(10, 1)\n    ...         self.tanh = nn.Tanh()\n    &gt;&gt;&gt;\n    ...     def forward(self, xb):\n    ...         z = self.lin(xb)\n    ...         z = self.tanh(z)\n    ...         z = self.lin2(z)\n    ...         z = self.tanh(z)\n    ...         z = self.lin3(z)\n    ...         return z\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; narx_nn.net = Net()\n    &gt;&gt;&gt; neural_narx.fit(x=x_train, y=y_train)\n    &gt;&gt;&gt; yhat = neural_narx.predict(x=x_valid, y=y_valid)\n    &gt;&gt;&gt; print(mean_squared_error(y_valid, yhat))\n    0.000131\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       &lt;https://eprints.soton.ac.uk/251147/1/778742007_content.pdf&gt;`_\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag=1,\n        xlag=1,\n        model_type=\"NARMAX\",\n        basis_function=Polynomial(),\n        batch_size=100,\n        learning_rate=0.01,\n        epochs=200,\n        loss_func=\"mse_loss\",\n        optimizer=\"Adam\",\n        net=None,\n        train_percentage=80,\n        verbose=False,\n        optim_params=None,\n        device=\"cpu\",\n    ):\n        self.ylag = ylag\n        self.xlag = xlag\n        self.basis_function = basis_function\n        self.model_type = model_type\n        self.non_degree = basis_function.degree\n        self.max_lag = self._get_max_lag()\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.loss_func = getattr(F, loss_func)\n        self.optimizer = optimizer\n        self.net = net\n        self.train_percentage = train_percentage\n        self.verbose = verbose\n        self.optim_params = optim_params\n        self.device = _check_cuda(device)\n        self.regressor_code = None\n        self.train_loss = None\n        self.val_loss = None\n        self.ensemble = None\n        self.n_inputs = None\n        self.final_model = None\n        self._validate_params()\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if not isinstance(self.batch_size, int) or self.batch_size &lt; 1:\n            raise ValueError(\n                f\"bacth_size must be integer and &gt; zero. Got {self.batch_size}\"\n            )\n\n        if not isinstance(self.epochs, int) or self.epochs &lt; 1:\n            raise ValueError(f\"epochs must be integer and &gt; zero. Got {self.epochs}\")\n\n        if not isinstance(self.train_percentage, int) or self.train_percentage &lt; 0:\n            raise ValueError(\n                f\"bacth_size must be integer and &gt; zero. Got {self.train_percentage}\"\n            )\n\n        if not isinstance(self.verbose, bool):\n            raise TypeError(f\"verbose must be False or True. Got {self.verbose}\")\n\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n    def define_opt(self):\n        \"\"\"Define the optimizer using the user parameters.\"\"\"\n        opt = getattr(optim, self.optimizer)\n        return opt(self.net.parameters(), lr=self.learning_rate, **self.optim_params)\n\n    def loss_batch(self, x, y, opt=None):\n        \"\"\"Compute the loss for one batch.\n\n        Parameters\n        ----------\n        x : ndarray of floats\n            The regressor matrix.\n        y : ndarray of floats\n            The output data.\n        opt: Torch optimizer\n            Chosen by the user.\n\n        Returns\n        -------\n        loss : float\n            The loss of one batch.\n\n        \"\"\"\n        loss = self.loss_func(self.net(x), y)\n\n        if opt is not None:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n\n        return loss.item(), len(x)\n\n    def split_data(self, x, y):\n        \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n        Parameters\n        ----------\n        x : ndarray of floats\n            The input data.\n        y : ndarray of floats\n            The output data.\n\n        Returns\n        -------\n        y : ndarray of floats\n            The y values considering the lags.\n        reg_matrix : ndarray of floats\n            The information matrix of the model.\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n\n        if isinstance(self.basis_function, Polynomial):\n            reg_matrix = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=None,\n            )\n            reg_matrix = reg_matrix[:, 1:]\n        else:\n            reg_matrix = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=None,\n            )\n\n        if x is not None:\n            self.n_inputs = num_features(x)\n        else:\n            self.n_inputs = 1  # only used to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n        repetition = len(reg_matrix)\n        if not isinstance(self.basis_function, Polynomial):\n            tmp_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.regressor_code = tmp_code[list(range(len(reg_matrix))), :].copy()\n        else:\n            self.regressor_code = self.regressor_code[\n                1:\n            ]  # removes the column of the constant\n\n        self.final_model = self.regressor_code.copy()\n        reg_matrix = np.atleast_1d(reg_matrix).astype(np.float32)\n\n        y = np.atleast_1d(y[self.max_lag :]).astype(np.float32)\n        return reg_matrix, y\n\n    def get_data(self, train_ds):\n        \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n        Based on Pytorch official docs:\n        https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n        Parameters\n        ----------\n        train_ds: tensor\n            Tensors that have the same size of the first dimension.\n\n        Returns\n        -------\n        Dataloader: dataloader\n            tensors that have the same size of the first dimension.\n\n        \"\"\"\n        pin_memory = False if self.device.type == \"cpu\" else True\n        return DataLoader(\n            train_ds, batch_size=self.batch_size, pin_memory=pin_memory, shuffle=False\n        )\n\n    def data_transform(self, x, y):\n        \"\"\"Return the data transformed in tensors using Dataloader.\n\n        Parameters\n        ----------\n        x : ndarray of floats\n            The input data.\n        y : ndarray of floats\n            The output data.\n\n        Returns\n        -------\n        Tensors : Dataloader\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        x_train, y_train = self.split_data(x, y)\n        train_ds = convert_to_tensor(x_train, y_train)\n        train_dl = self.get_data(train_ds)\n        return train_dl\n\n    def fit(self, *, X=None, y=None, X_test=None, y_test=None):\n        \"\"\"Train a NARX Neural Network model.\n\n        This is an training pipeline that allows a friendly usage\n        by the user. The training pipeline was based on\n        https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n        X_test : ndarray of floats\n            The input data to be used in the prediction process.\n        y_test : ndarray of floats\n            The output data (initial conditions) to be used in the prediction process.\n\n        Returns\n        -------\n        net : nn.Module\n            The model fitted.\n        train_loss: ndarrays of floats\n            The training loss of each batch\n        val_loss: ndarrays of floats\n            The validation loss of each batch\n\n        \"\"\"\n        train_dl = self.data_transform(X, y)\n        if self.verbose:\n            if X_test is None or y_test is None:\n                raise ValueError(\n                    \"X_test and y_test cannot be None if you set verbose=True\"\n                )\n            valid_dl = self.data_transform(X_test, y_test)\n\n        opt = self.define_opt()\n        self.val_loss = []\n        self.train_loss = []\n        for epoch in range(self.epochs):\n            self.net.train()\n            for input_data, output_data in train_dl:\n                X, y = input_data.to(self.device), output_data.to(self.device)\n                self.loss_batch(X, y, opt=opt)\n\n            if self.verbose:\n                train_losses, train_nums = zip(\n                    *[\n                        self.loss_batch(X.to(self.device), y.to(self.device))\n                        for X, y in train_dl\n                    ]\n                )\n                self.train_loss.append(\n                    np.sum(np.multiply(train_losses, train_nums)) / np.sum(train_nums)\n                )\n\n                self.net.eval()\n                with torch.no_grad():\n                    losses, nums = zip(\n                        *[\n                            self.loss_batch(X.to(self.device), y.to(self.device))\n                            for X, y in valid_dl\n                        ]\n                    )\n                self.val_loss.append(np.sum(np.multiply(losses, nums)) / np.sum(nums))\n                logging.info(\n                    \"Train metrics: %s | Validation metrics: %s\",\n                    self.train_loss[epoch],\n                    self.val_loss[epoch],\n                )\n        return self\n\n    def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n        \"\"\"Return the predicted given an input and initial values.\n\n        The predict function allows a friendly usage by the user.\n        Given a trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future).\n\n        Currently, we only support infinity-steps-ahead prediction,\n        but run 1-step-ahead prediction manually is straightforward.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                return self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            if steps_ahead == 1:\n                return self._one_step_ahead_prediction(X, y)\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            return self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n\n        if steps_ahead is None:\n            return self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        if steps_ahead == 1:\n            return self._one_step_ahead_prediction(X, y)\n\n        return self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n\n    def _one_step_ahead_prediction(self, x, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n\n        if isinstance(self.basis_function, Polynomial):\n            x_base = self.basis_function.transform(\n                lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n            )\n            x_base = x_base[:, 1:]\n        else:\n            x_base = self.basis_function.transform(\n                lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n            )\n\n        yhat = np.zeros(x.shape[0], dtype=float)\n        x_base = np.atleast_1d(x_base).astype(np.float32)\n        yhat = yhat.astype(np.float32)\n        x_valid, _ = map(torch.tensor, (x_base, yhat))\n        yhat = self.net(x_valid.to(self.device)).detach().cpu().numpy()\n        yhat = np.concatenate([y.ravel()[: self.max_lag].flatten(), yhat.ravel()])\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, x, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        yhat = np.zeros(x.shape[0], dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n        i = self.max_lag\n        x = x.reshape(-1, self.n_inputs)\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            yhat[i : i + steps_ahead] = self._model_prediction(\n                x[k : i + steps_ahead], y[k : i + steps_ahead]\n            )[-steps_ahead:].ravel()\n\n            i += steps_ahead\n\n        yhat = yhat.ravel()\n        return yhat.reshape(-1, 1)\n\n    def _model_prediction(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(self, x, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = np.zeros(forecast_horizon, dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, forecast_horizon):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            raw_regressor[:final] = y_output[k:i]\n            for j in range(self.n_inputs):\n                init += self.max_lag\n                final += self.max_lag\n                raw_regressor[init:final] = x[k:i, j]\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            regressor_value = np.atleast_1d(regressor_value).astype(np.float32)\n            y_output = y_output.astype(np.float32)\n            x_valid, _ = map(torch.tensor, (regressor_value, y_output))\n            y_output[i] = self.net(x_valid.to(self.device))[0].detach().cpu().numpy()\n        return y_output.reshape(-1, 1)\n\n    def _nfir_predict(self, x, y_initial):\n        y_output = np.zeros(x.shape[0], dtype=float)\n        y_output.fill(np.nan)\n        y_output[: self.max_lag] = y_initial[: self.max_lag, 0]\n        x = x.reshape(-1, self.n_inputs)\n        model_exponents = [\n            self._code2exponents(code=model) for model in self.final_model\n        ]\n        raw_regressor = np.zeros(len(model_exponents[0]), dtype=float)\n        for i in range(self.max_lag, x.shape[0]):\n            init = 0\n            final = self.max_lag\n            k = int(i - self.max_lag)\n            for j in range(self.n_inputs):\n                raw_regressor[init:final] = x[k:i, j]\n                init += self.max_lag\n                final += self.max_lag\n\n            regressor_value = np.zeros(len(model_exponents))\n            for j, model_exponent in enumerate(model_exponents):\n                regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\n\n            regressor_value = np.atleast_1d(regressor_value).astype(np.float32)\n            y_output = y_output.astype(np.float32)\n            x_valid, _ = map(torch.tensor, (regressor_value, y_output))\n            y_output[i] = self.net(x_valid.to(self.device))[0].detach().cpu().numpy()\n        return y_output.reshape(-1, 1)\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y_initial[: self.max_lag, 0]\n\n        analyzed_elements_number = self.max_lag + 1\n\n        for i in range(forecast_horizon - self.max_lag):\n            if self.model_type == \"NARMAX\":\n                lagged_data = build_input_output_matrix(\n                    x[i : i + analyzed_elements_number],\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1),\n                    self.xlag,\n                    self.ylag,\n                )\n            elif self.model_type == \"NAR\":\n                lagged_data = build_output_matrix(\n                    yhat[i : i + analyzed_elements_number].reshape(-1, 1), self.ylag\n                )\n            elif self.model_type == \"NFIR\":\n                lagged_data = build_input_matrix(\n                    x[i : i + analyzed_elements_number], self.xlag\n                )\n            else:\n                raise ValueError(\n                    \"Unrecognized model type. The model_type should be NARMAX, NAR or\"\n                    \" NFIR.\"\n                )\n\n            x_tmp = self.basis_function.transform(\n                lagged_data, self.max_lag, self.ylag, self.xlag, self.model_type\n            )\n            x_tmp = np.atleast_1d(x_tmp).astype(np.float32)\n            yhat = yhat.astype(np.float32)\n            x_valid, _ = map(torch.tensor, (x_tmp, yhat))\n            yhat[i + self.max_lag] = (\n                self.net(x_valid.to(self.device))[0].detach().cpu().numpy()\n            )[0]\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = np.zeros(forecast_horizon, dtype=float)\n        yhat.fill(np.nan)\n        yhat[: self.max_lag] = y[: self.max_lag, 0]\n\n        i = self.max_lag\n\n        while i &lt; len(y):\n            k = int(i - self.max_lag)\n            if i + steps_ahead &gt; len(y):\n                steps_ahead = len(y) - i  # predicts the remaining values\n\n            if self.model_type == \"NARMAX\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x[k : i + steps_ahead], y[k : i + steps_ahead]\n                )[-steps_ahead:].ravel()\n            elif self.model_type == \"NAR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=None,\n                    y_initial=y[k : i + steps_ahead],\n                    forecast_horizon=forecast_horizon,\n                )[-forecast_horizon : -forecast_horizon + steps_ahead].ravel()\n            elif self.model_type == \"NFIR\":\n                yhat[i : i + steps_ahead] = self._basis_function_predict(\n                    x=x[k : i + steps_ahead],\n                    y_initial=y[k : i + steps_ahead],\n                )[-steps_ahead:].ravel()\n            else:\n                raise ValueError(\n                    f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n                )\n\n            i += steps_ahead\n\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.data_transform","title":"<code>data_transform(x, y)</code>","text":"<p>Return the data transformed in tensors using Dataloader.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required <p>Returns:</p> Name Type Description <code>Tensors</code> <code>Dataloader</code> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def data_transform(self, x, y):\n    \"\"\"Return the data transformed in tensors using Dataloader.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    Returns\n    -------\n    Tensors : Dataloader\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    x_train, y_train = self.split_data(x, y)\n    train_ds = convert_to_tensor(x_train, y_train)\n    train_dl = self.get_data(train_ds)\n    return train_dl\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.define_opt","title":"<code>define_opt()</code>","text":"<p>Define the optimizer using the user parameters.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def define_opt(self):\n    \"\"\"Define the optimizer using the user parameters.\"\"\"\n    opt = getattr(optim, self.optimizer)\n    return opt(self.net.parameters(), lr=self.learning_rate, **self.optim_params)\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.fit","title":"<code>fit(*, X=None, y=None, X_test=None, y_test=None)</code>","text":"<p>Train a NARX Neural Network model.</p> <p>This is an training pipeline that allows a friendly usage by the user. The training pipeline was based on https://pytorch.org/tutorials/beginner/nn_tutorial.html</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> <code>None</code> <code>X_test</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y_test</code> <code>ndarray of floats</code> <p>The output data (initial conditions) to be used in the prediction process.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>net</code> <code>Module</code> <p>The model fitted.</p> <code>train_loss</code> <code>ndarrays of floats</code> <p>The training loss of each batch</p> <code>val_loss</code> <code>ndarrays of floats</code> <p>The validation loss of each batch</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def fit(self, *, X=None, y=None, X_test=None, y_test=None):\n    \"\"\"Train a NARX Neural Network model.\n\n    This is an training pipeline that allows a friendly usage\n    by the user. The training pipeline was based on\n    https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n    X_test : ndarray of floats\n        The input data to be used in the prediction process.\n    y_test : ndarray of floats\n        The output data (initial conditions) to be used in the prediction process.\n\n    Returns\n    -------\n    net : nn.Module\n        The model fitted.\n    train_loss: ndarrays of floats\n        The training loss of each batch\n    val_loss: ndarrays of floats\n        The validation loss of each batch\n\n    \"\"\"\n    train_dl = self.data_transform(X, y)\n    if self.verbose:\n        if X_test is None or y_test is None:\n            raise ValueError(\n                \"X_test and y_test cannot be None if you set verbose=True\"\n            )\n        valid_dl = self.data_transform(X_test, y_test)\n\n    opt = self.define_opt()\n    self.val_loss = []\n    self.train_loss = []\n    for epoch in range(self.epochs):\n        self.net.train()\n        for input_data, output_data in train_dl:\n            X, y = input_data.to(self.device), output_data.to(self.device)\n            self.loss_batch(X, y, opt=opt)\n\n        if self.verbose:\n            train_losses, train_nums = zip(\n                *[\n                    self.loss_batch(X.to(self.device), y.to(self.device))\n                    for X, y in train_dl\n                ]\n            )\n            self.train_loss.append(\n                np.sum(np.multiply(train_losses, train_nums)) / np.sum(train_nums)\n            )\n\n            self.net.eval()\n            with torch.no_grad():\n                losses, nums = zip(\n                    *[\n                        self.loss_batch(X.to(self.device), y.to(self.device))\n                        for X, y in valid_dl\n                    ]\n                )\n            self.val_loss.append(np.sum(np.multiply(losses, nums)) / np.sum(nums))\n            logging.info(\n                \"Train metrics: %s | Validation metrics: %s\",\n                self.train_loss[epoch],\n                self.val_loss[epoch],\n            )\n    return self\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.get_data","title":"<code>get_data(train_ds)</code>","text":"<p>Return the lagged matrix and the y values given the maximum lags.</p> <p>Based on Pytorch official docs: https://pytorch.org/tutorials/beginner/nn_tutorial.html</p> <p>Parameters:</p> Name Type Description Default <code>train_ds</code> <p>Tensors that have the same size of the first dimension.</p> required <p>Returns:</p> Name Type Description <code>Dataloader</code> <code>dataloader</code> <p>tensors that have the same size of the first dimension.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def get_data(self, train_ds):\n    \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n    Based on Pytorch official docs:\n    https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n    Parameters\n    ----------\n    train_ds: tensor\n        Tensors that have the same size of the first dimension.\n\n    Returns\n    -------\n    Dataloader: dataloader\n        tensors that have the same size of the first dimension.\n\n    \"\"\"\n    pin_memory = False if self.device.type == \"cpu\" else True\n    return DataLoader(\n        train_ds, batch_size=self.batch_size, pin_memory=pin_memory, shuffle=False\n    )\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.loss_batch","title":"<code>loss_batch(x, y, opt=None)</code>","text":"<p>Compute the loss for one batch.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The regressor matrix.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required <code>opt</code> <p>Chosen by the user.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>The loss of one batch.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def loss_batch(self, x, y, opt=None):\n    \"\"\"Compute the loss for one batch.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The regressor matrix.\n    y : ndarray of floats\n        The output data.\n    opt: Torch optimizer\n        Chosen by the user.\n\n    Returns\n    -------\n    loss : float\n        The loss of one batch.\n\n    \"\"\"\n    loss = self.loss_func(self.net(x), y)\n\n    if opt is not None:\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n    return loss.item(), len(x)\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted given an input and initial values.</p> <p>The predict function allows a friendly usage by the user. Given a trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future).</p> <p>Currently, we only support infinity-steps-ahead prediction, but run 1-step-ahead prediction manually is straightforward.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n    \"\"\"Return the predicted given an input and initial values.\n\n    The predict function allows a friendly usage by the user.\n    Given a trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future).\n\n    Currently, we only support infinity-steps-ahead prediction,\n    but run 1-step-ahead prediction manually is straightforward.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            return self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n        if steps_ahead == 1:\n            return self._one_step_ahead_prediction(X, y)\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        return self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n\n    if steps_ahead is None:\n        return self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n    if steps_ahead == 1:\n        return self._one_step_ahead_prediction(X, y)\n\n    return self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.NARXNN.split_data","title":"<code>split_data(x, y)</code>","text":"<p>Return the lagged matrix and the y values given the maximum lags.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required <p>Returns:</p> Name Type Description <code>y</code> <code>ndarray of floats</code> <p>The y values considering the lags.</p> <code>reg_matrix</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def split_data(self, x, y):\n    \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    Returns\n    -------\n    y : ndarray of floats\n        The y values considering the lags.\n    reg_matrix : ndarray of floats\n        The information matrix of the model.\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n\n    if isinstance(self.basis_function, Polynomial):\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n        reg_matrix = reg_matrix[:, 1:]\n    else:\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n    if x is not None:\n        self.n_inputs = num_features(x)\n    else:\n        self.n_inputs = 1  # only used to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n    repetition = len(reg_matrix)\n    if not isinstance(self.basis_function, Polynomial):\n        tmp_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.regressor_code = tmp_code[list(range(len(reg_matrix))), :].copy()\n    else:\n        self.regressor_code = self.regressor_code[\n            1:\n        ]  # removes the column of the constant\n\n    self.final_model = self.regressor_code.copy()\n    reg_matrix = np.atleast_1d(reg_matrix).astype(np.float32)\n\n    y = np.atleast_1d(y[self.max_lag :]).astype(np.float32)\n    return reg_matrix, y\n</code></pre>"},{"location":"user-guide/API/neural-narx/#sysidentpy.neural_network.narx_nn.convert_to_tensor","title":"<code>convert_to_tensor(reg_matrix, y)</code>","text":"<p>Return the lagged matrix and the y values given the maximum lags.</p> <p>Based on Pytorch official docs: https://pytorch.org/tutorials/beginner/nn_tutorial.html</p> <p>Parameters:</p> Name Type Description Default <code>reg_matrix</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <code>tensor</code> <p>tensors that have the same size of the first dimension.</p> Source code in <code>sysidentpy/neural_network/narx_nn.py</code> <pre><code>def convert_to_tensor(reg_matrix, y):\n    \"\"\"Return the lagged matrix and the y values given the maximum lags.\n\n    Based on Pytorch official docs:\n    https://pytorch.org/tutorials/beginner/nn_tutorial.html\n\n    Parameters\n    ----------\n    reg_matrix : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats\n        The output data\n\n    Returns\n    -------\n    Tensor: tensor\n        tensors that have the same size of the first dimension.\n\n    \"\"\"\n    reg_matrix, y = map(torch.tensor, (reg_matrix, y))\n    return TensorDataset(reg_matrix, y)\n</code></pre>"},{"location":"user-guide/API/ofr-base/","title":"Documentation for <code>OFRBase</code>","text":"<p>Base methods for Orthogonal Forward Regression algorithm.</p>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase","title":"<code>OFRBase</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Base class for Model Structure Selection.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>class OFRBase(BaseMSS, metaclass=ABCMeta):\n    \"\"\"Base class for Model Structure Selection.\"\"\"\n\n    @abstractmethod\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        elag: Union[int, list] = 2,\n        order_selection: bool = True,\n        info_criteria: str = \"aic\",\n        n_terms: Union[int, None] = None,\n        n_info_values: int = 15,\n        estimator: Estimators = RecursiveLeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n        eps: np.float64 = np.finfo(np.float64).eps,\n        alpha: float = 0,\n        err_tol: Optional[float] = None,\n    ):\n        self.order_selection = order_selection\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.info_criteria = info_criteria\n        self.info_criteria_function = get_info_criteria(info_criteria)\n        self.n_info_values = n_info_values\n        self.n_terms = n_terms\n        self.estimator = estimator\n        self.elag = elag\n        self.model_type = model_type\n        self.basis_function = basis_function\n        self.eps = eps\n        if isinstance(self.estimator, RidgeRegression):\n            self.alpha = self.estimator.alpha\n        else:\n            self.alpha = alpha\n\n        self.err_tol = err_tol\n        self._validate_params()\n        self.n_inputs = None\n        self.regressor_code = None\n        self.info_values = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n\n    def _validate_params(self):\n        \"\"\"Validate input params.\"\"\"\n        if not isinstance(self.n_info_values, int) or self.n_info_values &lt; 1:\n            raise ValueError(\n                f\"n_info_values must be integer and &gt; zero. Got {self.n_info_values}\"\n            )\n\n        if isinstance(self.ylag, int) and self.ylag &lt; 1:\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if isinstance(self.xlag, int) and self.xlag &lt; 1:\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.xlag, (int, list)):\n            raise ValueError(f\"xlag must be integer and &gt; zero. Got {self.xlag}\")\n\n        if not isinstance(self.ylag, (int, list)):\n            raise ValueError(f\"ylag must be integer and &gt; zero. Got {self.ylag}\")\n\n        if not isinstance(self.order_selection, bool):\n            raise TypeError(\n                f\"order_selection must be False or True. Got {self.order_selection}\"\n            )\n\n        if self.info_criteria not in [\"aic\", \"aicc\", \"bic\", \"fpe\", \"lilc\"]:\n            raise ValueError(\n                f\"info_criteria must be aic, bic, fpe or lilc. Got {self.info_criteria}\"\n            )\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n            )\n\n        if (\n            not isinstance(self.n_terms, int) or self.n_terms &lt; 1\n        ) and self.n_terms is not None:\n            raise ValueError(f\"n_terms must be integer and &gt; zero. Got {self.n_terms}\")\n\n        if not isinstance(self.eps, float) or self.eps &lt; 0:\n            raise ValueError(f\"eps must be float and &gt; zero. Got {self.eps}\")\n\n    @abstractmethod\n    def run_mss_algorithm(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        return self.error_reduction_ratio(psi, y, process_term_number)\n\n    def error_reduction_ratio(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Perform the Error Reduction Ration algorithm.\n\n        Parameters\n        ----------\n        y : array-like of shape = n_samples\n            The target data used in the identification process.\n        psi : ndarray of floats\n            The information matrix of the model.\n        process_term_number : int\n            Number of Process Terms defined by the user.\n\n        Returns\n        -------\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        psi_orthogonal : ndarray of floats\n            The updated and orthogonal information matrix.\n\n        References\n        ----------\n        - Manuscript: Orthogonal least squares methods and their application\n           to non-linear system identification\n           https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n        - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n           Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n           e Novos Resultados\n\n        \"\"\"\n        squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n        tmp_psi = psi.copy()\n        y = y[self.max_lag :, 0].reshape(-1, 1)\n        tmp_y = y.copy()\n        dimension = tmp_psi.shape[1]\n        piv = np.arange(dimension)\n        tmp_err = np.zeros(dimension)\n        err = np.zeros(dimension)\n\n        for i in np.arange(0, dimension):\n            for j in np.arange(i, dimension):\n                # Add `eps` in the denominator to omit division by zero if\n                # denominator is zero\n                # To implement regularized regression (ridge regression), add\n                # alpha to psi.T @ psi.   See S. Chen, Local regularization assisted\n                # orthogonal least squares regression, Neurocomputing 69 (2006) 559-585.\n                # The version implemented below uses the same regularization for every\n                # feature, # What Chen refers to Uniform regularized orthogonal least\n                # squares (UROLS) Set to tiny (self.eps) when you are not regularizing.\n                # alpha = eps is the default.\n                tmp_err[j] = (\n                    (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                    / (\n                        (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) + self.alpha)\n                        * squared_y\n                    )\n                    + self.eps\n                )[0, 0]\n\n            piv_index = np.argmax(tmp_err[i:]) + i\n            err[i] = tmp_err[piv_index]\n            if i == process_term_number:\n                break\n\n            if (self.err_tol is not None) and (err.cumsum()[i] &gt;= self.err_tol):\n                self.n_terms = i + 1\n                process_term_number = i + 1\n                break\n\n            tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n            piv[[piv_index, i]] = piv[[i, piv_index]]\n            v = house(tmp_psi[i:, i])\n            row_result = rowhouse(tmp_psi[i:, i:], v)\n            tmp_y[i:] = rowhouse(tmp_y[i:], v)\n            tmp_psi[i:, i:] = np.copy(row_result)\n\n        tmp_piv = piv[0:process_term_number]\n        psi_orthogonal = psi[:, tmp_piv]\n        return err, tmp_piv, psi_orthogonal\n\n    def information_criterion(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Determine the model order.\n\n        This function uses a information criterion to determine the model size.\n        'Akaike'-  Akaike's Information Criterion with\n                   critical value 2 (AIC) (default).\n        'Bayes' -  Bayes Information Criterion (BIC).\n        'FPE'   -  Final Prediction Error (FPE).\n        'LILC'  -  Khundrin's law ofiterated logarithm criterion (LILC).\n\n        Parameters\n        ----------\n        y : array-like of shape = n_samples\n            Target values of the system.\n        x : array-like of shape = n_samples\n            Input system values measured by the user.\n\n        Returns\n        -------\n        output_vector : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        if self.n_info_values is not None and self.n_info_values &gt; x.shape[1]:\n            self.n_info_values = x.shape[1]\n            warnings.warn(\n                \"n_info_values is greater than the maximum number of all\"\n                \" regressors space considering the chosen y_lag, u_lag, and\"\n                f\" non_degree. We set as {x.shape[1]}\",\n                stacklevel=2,\n            )\n\n        output_vector = np.zeros(self.n_info_values)\n        output_vector[:] = np.nan\n\n        n_samples = len(y) - self.max_lag\n\n        for i in range(self.n_info_values):\n            n_theta = i + 1\n            regressor_matrix = self.run_mss_algorithm(x, y, n_theta)[2]\n\n            tmp_theta = self.estimator.optimize(\n                regressor_matrix, y[self.max_lag :, 0].reshape(-1, 1)\n            )\n\n            tmp_yhat = np.dot(regressor_matrix, tmp_theta)\n            tmp_residual = y[self.max_lag :] - tmp_yhat\n            e_var = np.var(tmp_residual, ddof=1)\n            output_vector[i] = self.info_criteria_function(n_theta, n_samples, e_var)\n\n        return output_vector\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n        \"\"\"Fit polynomial NARMAX model.\n\n        This is an 'alpha' version of the 'fit' function which allows\n        a friendly usage by the user. Given two arguments, x and y, fit\n        training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        if y is None:\n            raise ValueError(\"y cannot be None\")\n\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n\n        reg_matrix = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=None,\n        )\n\n        if X is not None:\n            self.n_inputs = num_features(X)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        self.regressor_code = self.regressor_space(self.n_inputs)\n\n        if self.order_selection is True:\n            self.info_values = self.information_criterion(reg_matrix, y)\n\n        if self.n_terms is None and self.order_selection is True:\n            model_length = get_min_info_value(self.info_values)\n            self.n_terms = model_length\n        elif self.n_terms is None and self.order_selection is not True:\n            raise ValueError(\n                \"If order_selection is False, you must define n_terms value.\"\n            )\n        else:\n            model_length = self.n_terms\n\n        (self.err, self.pivv, psi) = self.run_mss_algorithm(reg_matrix, y, model_length)\n\n        tmp_piv = self.pivv[0:model_length]\n        repetition = len(reg_matrix)\n        if isinstance(self.basis_function, Polynomial):\n            self.final_model = self.regressor_code[tmp_piv, :].copy()\n        else:\n            self.regressor_code = np.sort(\n                np.tile(self.regressor_code[1:, :], (repetition, 1)),\n                axis=0,\n            )\n            self.final_model = self.regressor_code[tmp_piv, :].copy()\n\n        self.theta = self.estimator.optimize(psi, y[self.max_lag :, 0].reshape(-1, 1))\n        if self.estimator.unbiased is True:\n            self.theta = self.estimator.unbiased_estimator(\n                psi,\n                y[self.max_lag :, 0].reshape(-1, 1),\n                self.theta,\n                self.elag,\n                self.max_lag,\n                self.estimator,\n                self.basis_function,\n                self.estimator.uiter,\n            )\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the prediction process.\n        y : ndarray of floats\n            The output data to be used in the prediction process.\n        steps_ahead : int (default = None)\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction.\n        forecast_horizon : int, default=None\n            The number of predictions over the time.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead, forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(\n        self, x: Optional[np.ndarray], y: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n\n        x_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(\n        self, x: Optional[np.ndarray], y: Optional[np.ndarray], steps_ahead: int\n    ) -&gt; float:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: np.ndarray,\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(\n        self, x: Optional[np.ndarray], y_initial: Optional[np.ndarray]\n    ) -&gt; np.ndarray:\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(\n        self,\n        x: Optional[np.ndarray],\n        y_initial: Optional[np.ndarray],\n        forecast_horizon: int = 0,\n    ) -&gt; np.ndarray:\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        yhat = super()._basis_function_predict(x, y_initial, forecast_horizon)\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_step_prediction(\n        self,\n        x: Optional[np.ndarray],\n        y: np.ndarray,\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : ndarray of floats of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : ndarray of floats\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        if len(y) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        yhat = super()._basis_function_n_step_prediction(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n\n    def _basis_function_n_steps_horizon(\n        self,\n        x: Optional[np.ndarray],\n        y: Optional[np.ndarray],\n        steps_ahead: Optional[int],\n        forecast_horizon: int,\n    ) -&gt; np.ndarray:\n        yhat = super()._basis_function_n_steps_horizon(\n            x, y, steps_ahead, forecast_horizon\n        )\n        return yhat.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase.error_reduction_ratio","title":"<code>error_reduction_ratio(psi, y, process_term_number)</code>","text":"<p>Perform the Error Reduction Ration algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = n_samples</code> <p>The target data used in the identification process.</p> required <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>process_term_number</code> <code>int</code> <p>Number of Process Terms defined by the user.</p> required <p>Returns:</p> Name Type Description <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>psi_orthogonal</code> <code>ndarray of floats</code> <p>The updated and orthogonal information matrix.</p> References <ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def error_reduction_ratio(\n    self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Perform the Error Reduction Ration algorithm.\n\n    Parameters\n    ----------\n    y : array-like of shape = n_samples\n        The target data used in the identification process.\n    psi : ndarray of floats\n        The information matrix of the model.\n    process_term_number : int\n        Number of Process Terms defined by the user.\n\n    Returns\n    -------\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    psi_orthogonal : ndarray of floats\n        The updated and orthogonal information matrix.\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n    squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n    tmp_psi = psi.copy()\n    y = y[self.max_lag :, 0].reshape(-1, 1)\n    tmp_y = y.copy()\n    dimension = tmp_psi.shape[1]\n    piv = np.arange(dimension)\n    tmp_err = np.zeros(dimension)\n    err = np.zeros(dimension)\n\n    for i in np.arange(0, dimension):\n        for j in np.arange(i, dimension):\n            # Add `eps` in the denominator to omit division by zero if\n            # denominator is zero\n            # To implement regularized regression (ridge regression), add\n            # alpha to psi.T @ psi.   See S. Chen, Local regularization assisted\n            # orthogonal least squares regression, Neurocomputing 69 (2006) 559-585.\n            # The version implemented below uses the same regularization for every\n            # feature, # What Chen refers to Uniform regularized orthogonal least\n            # squares (UROLS) Set to tiny (self.eps) when you are not regularizing.\n            # alpha = eps is the default.\n            tmp_err[j] = (\n                (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                / (\n                    (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) + self.alpha)\n                    * squared_y\n                )\n                + self.eps\n            )[0, 0]\n\n        piv_index = np.argmax(tmp_err[i:]) + i\n        err[i] = tmp_err[piv_index]\n        if i == process_term_number:\n            break\n\n        if (self.err_tol is not None) and (err.cumsum()[i] &gt;= self.err_tol):\n            self.n_terms = i + 1\n            process_term_number = i + 1\n            break\n\n        tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n        piv[[piv_index, i]] = piv[[i, piv_index]]\n        v = house(tmp_psi[i:, i])\n        row_result = rowhouse(tmp_psi[i:, i:], v)\n        tmp_y[i:] = rowhouse(tmp_y[i:], v)\n        tmp_psi[i:, i:] = np.copy(row_result)\n\n    tmp_piv = piv[0:process_term_number]\n    psi_orthogonal = psi[:, tmp_piv]\n    return err, tmp_piv, psi_orthogonal\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase.fit","title":"<code>fit(*, X=None, y)</code>","text":"<p>Fit polynomial NARMAX model.</p> <p>This is an 'alpha' version of the 'fit' function which allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>info_values</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n    \"\"\"Fit polynomial NARMAX model.\n\n    This is an 'alpha' version of the 'fit' function which allows\n    a friendly usage by the user. Given two arguments, x and y, fit\n    training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    if y is None:\n        raise ValueError(\"y cannot be None\")\n\n    self.max_lag = self._get_max_lag()\n    lagged_data = build_lagged_matrix(X, y, self.xlag, self.ylag, self.model_type)\n\n    reg_matrix = self.basis_function.fit(\n        lagged_data,\n        self.max_lag,\n        self.ylag,\n        self.xlag,\n        self.model_type,\n        predefined_regressors=None,\n    )\n\n    if X is not None:\n        self.n_inputs = num_features(X)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    self.regressor_code = self.regressor_space(self.n_inputs)\n\n    if self.order_selection is True:\n        self.info_values = self.information_criterion(reg_matrix, y)\n\n    if self.n_terms is None and self.order_selection is True:\n        model_length = get_min_info_value(self.info_values)\n        self.n_terms = model_length\n    elif self.n_terms is None and self.order_selection is not True:\n        raise ValueError(\n            \"If order_selection is False, you must define n_terms value.\"\n        )\n    else:\n        model_length = self.n_terms\n\n    (self.err, self.pivv, psi) = self.run_mss_algorithm(reg_matrix, y, model_length)\n\n    tmp_piv = self.pivv[0:model_length]\n    repetition = len(reg_matrix)\n    if isinstance(self.basis_function, Polynomial):\n        self.final_model = self.regressor_code[tmp_piv, :].copy()\n    else:\n        self.regressor_code = np.sort(\n            np.tile(self.regressor_code[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        self.final_model = self.regressor_code[tmp_piv, :].copy()\n\n    self.theta = self.estimator.optimize(psi, y[self.max_lag :, 0].reshape(-1, 1))\n    if self.estimator.unbiased is True:\n        self.theta = self.estimator.unbiased_estimator(\n            psi,\n            y[self.max_lag :, 0].reshape(-1, 1),\n            self.theta,\n            self.elag,\n            self.max_lag,\n            self.estimator,\n            self.basis_function,\n            self.estimator.uiter,\n        )\n    return self\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase.information_criterion","title":"<code>information_criterion(x, y)</code>","text":"<p>Determine the model order.</p> <p>This function uses a information criterion to determine the model size. 'Akaike'-  Akaike's Information Criterion with            critical value 2 (AIC) (default). 'Bayes' -  Bayes Information Criterion (BIC). 'FPE'   -  Final Prediction Error (FPE). 'LILC'  -  Khundrin's law ofiterated logarithm criterion (LILC).</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape = n_samples</code> <p>Target values of the system.</p> required <code>x</code> <code>array-like of shape = n_samples</code> <p>Input system values measured by the user.</p> required <p>Returns:</p> Name Type Description <code>output_vector</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def information_criterion(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Determine the model order.\n\n    This function uses a information criterion to determine the model size.\n    'Akaike'-  Akaike's Information Criterion with\n               critical value 2 (AIC) (default).\n    'Bayes' -  Bayes Information Criterion (BIC).\n    'FPE'   -  Final Prediction Error (FPE).\n    'LILC'  -  Khundrin's law ofiterated logarithm criterion (LILC).\n\n    Parameters\n    ----------\n    y : array-like of shape = n_samples\n        Target values of the system.\n    x : array-like of shape = n_samples\n        Input system values measured by the user.\n\n    Returns\n    -------\n    output_vector : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    if self.n_info_values is not None and self.n_info_values &gt; x.shape[1]:\n        self.n_info_values = x.shape[1]\n        warnings.warn(\n            \"n_info_values is greater than the maximum number of all\"\n            \" regressors space considering the chosen y_lag, u_lag, and\"\n            f\" non_degree. We set as {x.shape[1]}\",\n            stacklevel=2,\n        )\n\n    output_vector = np.zeros(self.n_info_values)\n    output_vector[:] = np.nan\n\n    n_samples = len(y) - self.max_lag\n\n    for i in range(self.n_info_values):\n        n_theta = i + 1\n        regressor_matrix = self.run_mss_algorithm(x, y, n_theta)[2]\n\n        tmp_theta = self.estimator.optimize(\n            regressor_matrix, y[self.max_lag :, 0].reshape(-1, 1)\n        )\n\n        tmp_yhat = np.dot(regressor_matrix, tmp_theta)\n        tmp_residual = y[self.max_lag :] - tmp_yhat\n        e_var = np.var(tmp_residual, ddof=1)\n        output_vector[i] = self.info_criteria_function(n_theta, n_samples, e_var)\n\n    return output_vector\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.OFRBase.predict","title":"<code>predict(*, X=None, y, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the prediction process.</p> required <code>steps_ahead</code> <code>int(default=None)</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>ndarray of floats</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def predict(\n    self,\n    *,\n    X: Optional[np.ndarray] = None,\n    y: np.ndarray,\n    steps_ahead: Optional[int] = None,\n    forecast_horizon: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the prediction process.\n    y : ndarray of floats\n        The output data to be used in the prediction process.\n    steps_ahead : int (default = None)\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction.\n    forecast_horizon : int, default=None\n        The number of predictions over the time.\n\n    Returns\n    -------\n    yhat : ndarray of floats\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead, forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.aic","title":"<code>aic(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Akaike information criteria value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>info_criteria_value</code> <code>float</code> <p>The computed value given the information criteria selected by the user.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def aic(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Akaike information criteria value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = 2 * n_theta\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.aicc","title":"<code>aicc(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Akaike information Criteria corrected value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>aicc</code> <code>float</code> <p>The computed aicc value.</p> References <ul> <li>https://www.mathworks.com/help/ident/ref/idmodel.aic.html</li> </ul> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def aicc(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Akaike information Criteria corrected value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    aicc : float\n        The computed aicc value.\n\n    References\n    ----------\n    - https://www.mathworks.com/help/ident/ref/idmodel.aic.html\n\n    \"\"\"\n    aic_values = aic(n_theta, n_samples, e_var)\n    aicc_values = aic_values + (2 * n_theta * (n_theta + 1) / (n_samples - n_theta - 1))\n\n    return aicc_values\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.bic","title":"<code>bic(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Bayesian information criteria value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>info_criteria_value</code> <code>float</code> <p>The computed value given the information criteria selected by the user.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def bic(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Bayesian information criteria value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = n_theta * np.log(n_samples)\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.fpe","title":"<code>fpe(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Final Error Prediction value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>info_criteria_value</code> <code>float</code> <p>The computed value given the information criteria selected by the user.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def fpe(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Final Error Prediction value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = n_samples * np.log((n_samples + n_theta) / (n_samples - n_theta))\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.get_info_criteria","title":"<code>get_info_criteria(info_criteria)</code>","text":"<p>Get info criteria.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def get_info_criteria(info_criteria: str):\n    \"\"\"Get info criteria.\"\"\"\n    info_criteria_options = {\n        \"aic\": aic,\n        \"aicc\": aicc,\n        \"bic\": bic,\n        \"fpe\": fpe,\n        \"lilc\": lilc,\n    }\n    return info_criteria_options.get(info_criteria)\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.get_min_info_value","title":"<code>get_min_info_value(info_values)</code>","text":"<p>Find the index of the first increasing value in an array.</p> <p>Parameters:</p> Name Type Description Default <code>info_values</code> <code>array - like</code> <p>A sequence of numeric values to be analyzed.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The index of the first element where the values start to increase monotonically. If no such element exists, the length of <code>info_values</code> is returned.</p> Notes <ul> <li>The function assumes that <code>info_values</code> is a 1-dimensional array-like structure.</li> <li>The function uses <code>np.diff</code> to compute the difference between consecutive elements in the sequence.</li> <li>The function checks if any differences are positive, indicating an increase in value.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyClass:\n...     def __init__(self, values):\n...         self.info_values = values\n...     def get_min_info_value(self):\n...         is_monotonique = np.diff(self.info_values) &gt; 0\n...         if any(is_monotonique):\n...             return np.where(is_monotonique)[0][0] + 1\n...         return len(self.info_values)\n&gt;&gt;&gt; instance = MyClass([3, 2, 1, 4, 5])\n&gt;&gt;&gt; instance.get_min_info_value()\n3\n</code></pre> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def get_min_info_value(info_values):\n    \"\"\"Find the index of the first increasing value in an array.\n\n    Parameters\n    ----------\n    info_values : array-like\n        A sequence of numeric values to be analyzed.\n\n    Returns\n    -------\n    int\n        The index of the first element where the values start to increase\n        monotonically. If no such element exists, the length of\n        `info_values` is returned.\n\n    Notes\n    -----\n    - The function assumes that `info_values` is a 1-dimensional array-like\n    structure.\n    - The function uses `np.diff` to compute the difference between consecutive\n    elements in the sequence.\n    - The function checks if any differences are positive, indicating an increase\n    in value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; class MyClass:\n    ...     def __init__(self, values):\n    ...         self.info_values = values\n    ...     def get_min_info_value(self):\n    ...         is_monotonique = np.diff(self.info_values) &gt; 0\n    ...         if any(is_monotonique):\n    ...             return np.where(is_monotonique)[0][0] + 1\n    ...         return len(self.info_values)\n    &gt;&gt;&gt; instance = MyClass([3, 2, 1, 4, 5])\n    &gt;&gt;&gt; instance.get_min_info_value()\n    3\n    \"\"\"\n    is_monotonique = np.diff(info_values) &gt; 0\n    if any(is_monotonique):\n        return np.where(is_monotonique)[0][0] + 1\n    return len(info_values)\n</code></pre>"},{"location":"user-guide/API/ofr-base/#sysidentpy.model_structure_selection.ofr_base.lilc","title":"<code>lilc(n_theta, n_samples, e_var)</code>","text":"<p>Compute the Lilc information criteria value.</p> <p>Parameters:</p> Name Type Description Default <code>n_theta</code> <code>int</code> <p>Number of parameters of the model.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples given the maximum lag.</p> required <code>e_var</code> <code>float</code> <p>Variance of the residues</p> required <p>Returns:</p> Name Type Description <code>info_criteria_value</code> <code>float</code> <p>The computed value given the information criteria selected by the user.</p> Source code in <code>sysidentpy/model_structure_selection/ofr_base.py</code> <pre><code>def lilc(n_theta: int, n_samples: int, e_var: float) -&gt; float:\n    \"\"\"Compute the Lilc information criteria value.\n\n    Parameters\n    ----------\n    n_theta : int\n        Number of parameters of the model.\n    n_samples : int\n        Number of samples given the maximum lag.\n    e_var : float\n        Variance of the residues\n\n    Returns\n    -------\n    info_criteria_value : float\n        The computed value given the information criteria selected by the\n        user.\n\n    \"\"\"\n    model_factor = 2 * n_theta * np.log(np.log(n_samples))\n    e_factor = n_samples * np.log(e_var)\n    info_criteria_value = e_factor + model_factor\n\n    return info_criteria_value\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/","title":"Documentation for <code>Parameters Estimation</code>","text":"<p>Methods for parameter estimation.</p>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares","title":"<code>AffineLeastMeanSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Affine Least Mean Squares (ALMS) filter for parameter estimation.</p> <p>The ALMS filter is an adaptive filter used to estimate the parameters of a model. It incorporates an offset covariance factor to improve the stability and convergence of the parameter estimation process.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>offset_covariance</code> <code>float</code> <p>The offset covariance factor of the affine least mean squares filter.</p> <code>0.2</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>offset_covariance</code> <code>float</code> <p>The offset covariance factor of the affine least mean squares filter.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the ALMS filter.</p> References <ul> <li>Poularikas, A. D. (2017). Adaptive filtering: Fundamentals of least mean squares with MATLAB\u00ae. CRC Press.</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class AffineLeastMeanSquares(BaseEstimator):\n    \"\"\"Affine Least Mean Squares (ALMS) filter for parameter estimation.\n\n    The ALMS filter is an adaptive filter used to estimate the parameters of a model.\n    It incorporates an offset covariance factor to improve the stability and convergence\n    of the parameter estimation process.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    offset_covariance : float, default=0.2\n        The offset covariance factor of the affine least mean squares filter.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    offset_covariance : float\n        The offset covariance factor of the affine least mean squares filter.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the ALMS filter.\n\n    References\n    ----------\n    - Poularikas, A. D. (2017). Adaptive filtering: Fundamentals of least mean squares\n    with MATLAB\u00ae. CRC Press.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        offset_covariance: float = 0.2,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.offset_covariance = offset_covariance\n        self.uiter = uiter\n        self.unbiased = unbiased\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Affine Least Mean Squares.\n\n        The ALMS method updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi = y - \\psi \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\psi (\\psi^T \\psi + \\text{offset_covariance}\n           \\cdot I)^{-1} \\xi\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameters estimation\n        will be available soon. For now, please refer to the mentioned references.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            self.xi = y - psi.dot(theta[:, i - 1].reshape(-1, 1))\n            aux = (\n                self.mu\n                * psi\n                @ np.linalg.pinv(psi.T @ psi + self.offset_covariance * np.eye(n_theta))\n            )\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + aux.T.dot(self.xi)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.AffineLeastMeanSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Affine Least Mean Squares.</p> <p>The ALMS method updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi = y - \\psi \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\psi (\\psi^T \\psi + \\text{offset_covariance}    \\cdot I)^{-1} \\xi    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Notes <p>A more in-depth documentation of all methods for parameters estimation will be available soon. For now, please refer to the mentioned references.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Affine Least Mean Squares.\n\n    The ALMS method updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi = y - \\psi \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\psi (\\psi^T \\psi + \\text{offset_covariance}\n       \\cdot I)^{-1} \\xi\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameters estimation\n    will be available soon. For now, please refer to the mentioned references.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        self.xi = y - psi.dot(theta[:, i - 1].reshape(-1, 1))\n        aux = (\n            self.mu\n            * psi\n            @ np.linalg.pinv(psi.T @ psi + self.offset_covariance * np.eye(n_theta))\n        )\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + aux.T.dot(self.xi)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares","title":"<code>BoundedVariableLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Solve a linear least-squares problem with bounds on the variables.</p> <p>This is a wrapper class for the <code>scipy.optimize.lsq_linear</code> method.</p> <p>Given a m-by-n design matrix A and a target vector b with m elements, <code>lsq_linear</code> solves the following optimization problem::</p> <pre><code>minimize 0.5 * ||A x - b||**2\nsubject to lb &lt;= x &lt;= ub\n</code></pre> <p>This optimization problem is convex, hence a found minimum (if iterations have converged) is guaranteed to be global.</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>30</code> <code>method</code> <code>trf or bvls</code> <p>Method to perform minimization.</p> <pre><code>* 'trf' : Trust Region Reflective algorithm adapted for a linear\n  least-squares problem. This is an interior-point-like method\n  and the required number of iterations is weakly correlated with\n  the number of variables.\n* 'bvls' : Bounded-variable least-squares algorithm. This is\n  an active set method, which requires the number of iterations\n  comparable to the number of variables. Can't be used when `A` is\n  sparse or LinearOperator.\n</code></pre> <p>Default is 'trf'.</p> <code>'trf'</code> <code>tol</code> <code>float</code> <p>Tolerance parameter. The algorithm terminates if a relative change of the cost function is less than <code>tol</code> on the last iteration. Additionally, the first-order optimality measure is considered:</p> <pre><code>* ``method='trf'`` terminates if the uniform norm of the gradient,\n  scaled to account for the presence of the bounds, is less than\n  `tol`.\n* ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n  are satisfied within `tol` tolerance.\n</code></pre> <code>1e-10</code> <code>lsq_solver</code> <code>(None, exact, lsmr)</code> <p>Method of solving unbounded least-squares problems throughout iterations:</p> <pre><code>* 'exact' : Use dense QR or SVD decomposition approach. Can't be\n  used when `A` is sparse or LinearOperator.\n* 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n  which requires only matrix-vector product evaluations. Can't\n  be used with ``method='bvls'``.\n</code></pre> <p>If None (default), the solver is chosen based on type of <code>A</code>.</p> <code>None</code> <code>lsmr_tol</code> <code>(None, float or auto)</code> <p>Tolerance parameters 'atol' and 'btol' for <code>scipy.sparse.linalg.lsmr</code> If None (default), it is set to <code>1e-2 * tol</code>. If 'auto', the tolerance will be adjusted based on the optimality of the current iterate, which can speed up the optimization process, but is not always reliable.</p> <code>None</code> <code>max_iter</code> <code>None or int</code> <p>Maximum number of iterations before termination. If None (default), it is set to 100 for <code>method='trf'</code> or to the number of variables for <code>method='bvls'</code> (not counting iterations for 'bvls' initialization).</p> <code>None</code> <code>verbose</code> <code>(0, 1, 2)</code> <p>Level of algorithm's verbosity:</p> <pre><code>* 0 : work silently (default).\n* 1 : display a termination report.\n* 2 : display progress during iterations.\n</code></pre> <code>0</code> <code>lsmr_maxiter</code> <code>None or int</code> <p>Maximum number of iterations for the lsmr least squares solver, if it is used (by setting <code>lsq_solver='lsmr'</code>). If None (default), it uses lsmr's default of <code>min(m, n)</code> where <code>m</code> and <code>n</code> are the number of rows and columns of <code>A</code>, respectively. Has no effect if <code>lsq_solver='exact'</code>.</p> <code>None</code> References <p>M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,     and Conjugate Gradient Method for Large-Scale Bound-Constrained     Minimization Problems,\" SIAM Journal on Scientific Computing,     Vol. 21, Number 1, pp 1-23, 1999. P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:     an Algorithm and Applications\", Computational Statistics, 10,     129-141, 1995.</p> Notes <p>This docstring is adapted from the <code>scipy.optimize.lsq_linear</code> method.</p> <p>Examples:</p> <p>In this example, a problem with a large sparse matrix and bounds on the variables is solved.</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.sparse import rand\n&gt;&gt;&gt; from sysidentpy.parameter_estimation import BoundedVariableLeastSquares\n&gt;&gt;&gt; rng = np.random.default_rng()\n...\n&gt;&gt;&gt; m = 20000\n&gt;&gt;&gt; n = 10000\n...\n&gt;&gt;&gt; A = rand(m, n, density=1e-4, random_state=rng)\n&gt;&gt;&gt; b = rng.standard_normal(m)\n...\n&gt;&gt;&gt; lb = rng.standard_normal(n)\n&gt;&gt;&gt; ub = lb + 1\n...\n&gt;&gt;&gt; res = BoundedVariableLeastSquares(A, b, bounds=(lb, ub), lsmr_tol='auto',\nverbose=1)\nThe relative change of the cost function is less than `tol`.\nNumber of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\nfirst-order optimality 4.66e-08.\n</code></pre> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class BoundedVariableLeastSquares(BaseEstimator):\n    \"\"\"Solve a linear least-squares problem with bounds on the variables.\n\n    This is a wrapper class for the `scipy.optimize.lsq_linear` method.\n\n    Given a m-by-n design matrix A and a target vector b with m elements,\n    `lsq_linear` solves the following optimization problem::\n\n        minimize 0.5 * ||A x - b||**2\n        subject to lb &lt;= x &lt;= ub\n\n    This optimization problem is convex, hence a found minimum (if iterations\n    have converged) is guaranteed to be global.\n\n    Parameters\n    ----------\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    method : 'trf' or 'bvls', optional\n        Method to perform minimization.\n\n            * 'trf' : Trust Region Reflective algorithm adapted for a linear\n              least-squares problem. This is an interior-point-like method\n              and the required number of iterations is weakly correlated with\n              the number of variables.\n            * 'bvls' : Bounded-variable least-squares algorithm. This is\n              an active set method, which requires the number of iterations\n              comparable to the number of variables. Can't be used when `A` is\n              sparse or LinearOperator.\n\n        Default is 'trf'.\n    tol : float, optional\n        Tolerance parameter. The algorithm terminates if a relative change\n        of the cost function is less than `tol` on the last iteration.\n        Additionally, the first-order optimality measure is considered:\n\n            * ``method='trf'`` terminates if the uniform norm of the gradient,\n              scaled to account for the presence of the bounds, is less than\n              `tol`.\n            * ``method='bvls'`` terminates if Karush-Kuhn-Tucker conditions\n              are satisfied within `tol` tolerance.\n\n    lsq_solver : {None, 'exact', 'lsmr'}, optional\n        Method of solving unbounded least-squares problems throughout\n        iterations:\n\n            * 'exact' : Use dense QR or SVD decomposition approach. Can't be\n              used when `A` is sparse or LinearOperator.\n            * 'lsmr' : Use `scipy.sparse.linalg.lsmr` iterative procedure\n              which requires only matrix-vector product evaluations. Can't\n              be used with ``method='bvls'``.\n\n        If None (default), the solver is chosen based on type of `A`.\n    lsmr_tol : None, float or 'auto', optional\n        Tolerance parameters 'atol' and 'btol' for `scipy.sparse.linalg.lsmr`\n        If None (default), it is set to ``1e-2 * tol``. If 'auto', the\n        tolerance will be adjusted based on the optimality of the current\n        iterate, which can speed up the optimization process, but is not always\n        reliable.\n    max_iter : None or int, optional\n        Maximum number of iterations before termination. If None (default), it\n        is set to 100 for ``method='trf'`` or to the number of variables for\n        ``method='bvls'`` (not counting iterations for 'bvls' initialization).\n    verbose : {0, 1, 2}, optional\n        Level of algorithm's verbosity:\n\n            * 0 : work silently (default).\n            * 1 : display a termination report.\n            * 2 : display progress during iterations.\n    lsmr_maxiter : None or int, optional\n        Maximum number of iterations for the lsmr least squares solver,\n        if it is used (by setting ``lsq_solver='lsmr'``). If None (default), it\n        uses lsmr's default of ``min(m, n)`` where ``m`` and ``n`` are the\n        number of rows and columns of `A`, respectively. Has no effect if\n        ``lsq_solver='exact'``.\n\n    References\n    ----------\n    M. A. Branch, T. F. Coleman, and Y. Li, \"A Subspace, Interior,\n        and Conjugate Gradient Method for Large-Scale Bound-Constrained\n        Minimization Problems,\" SIAM Journal on Scientific Computing,\n        Vol. 21, Number 1, pp 1-23, 1999.\n    P. B. Start and R. L. Parker, \"Bounded-Variable Least-Squares:\n        an Algorithm and Applications\", Computational Statistics, 10,\n        129-141, 1995.\n\n    Notes\n    -----\n    This docstring is adapted from the `scipy.optimize.lsq_linear` method.\n\n    Examples\n    --------\n    In this example, a problem with a large sparse matrix and bounds on the\n    variables is solved.\n\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from scipy.sparse import rand\n    &gt;&gt;&gt; from sysidentpy.parameter_estimation import BoundedVariableLeastSquares\n    &gt;&gt;&gt; rng = np.random.default_rng()\n    ...\n    &gt;&gt;&gt; m = 20000\n    &gt;&gt;&gt; n = 10000\n    ...\n    &gt;&gt;&gt; A = rand(m, n, density=1e-4, random_state=rng)\n    &gt;&gt;&gt; b = rng.standard_normal(m)\n    ...\n    &gt;&gt;&gt; lb = rng.standard_normal(n)\n    &gt;&gt;&gt; ub = lb + 1\n    ...\n    &gt;&gt;&gt; res = BoundedVariableLeastSquares(A, b, bounds=(lb, ub), lsmr_tol='auto',\n    verbose=1)\n    The relative change of the cost function is less than `tol`.\n    Number of iterations 16, initial cost 1.5039e+04, final cost 1.1112e+04,\n    first-order optimality 4.66e-08.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        unbiased: bool = False,\n        uiter: int = 30,\n        bounds=(-np.inf, np.inf),\n        method=\"trf\",\n        tol=1e-10,\n        lsq_solver=None,\n        lsmr_tol=None,\n        max_iter=None,\n        verbose=0,\n        lsmr_maxiter=None,\n    ):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self.max_iter = max_iter\n        self.bounds = bounds\n        self.method = method\n        self.tol = tol\n        self.lsq_solver = lsq_solver\n        self.lsmr_tol = lsmr_tol\n        self.verbose = verbose\n        self.lsmr_maxiter = lsmr_maxiter\n\n    def optimize(self, psi, y):\n        \"\"\"Parameter estimation using the BoundedVariableLeastSquares algorithm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        This is a wrapper class for the `scipy.optimize.lsq_linear` method.\n\n        References\n        ----------\n        .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html\n        \"\"\"\n        theta = lsq_linear(\n            psi,\n            y.ravel(),\n            bounds=self.bounds,\n            method=self.method,\n            tol=self.tol,\n            lsq_solver=self.lsq_solver,\n            lsmr_tol=self.lsmr_tol,\n            max_iter=self.max_iter,\n            verbose=self.verbose,\n            lsmr_maxiter=self.lsmr_maxiter,\n        )\n        return theta.x.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.BoundedVariableLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the BoundedVariableLeastSquares algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> Notes <p>This is a wrapper class for the <code>scipy.optimize.lsq_linear</code> method.</p> References <p>.. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi, y):\n    \"\"\"Parameter estimation using the BoundedVariableLeastSquares algorithm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    This is a wrapper class for the `scipy.optimize.lsq_linear` method.\n\n    References\n    ----------\n    .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.lsq_linear.html\n    \"\"\"\n    theta = lsq_linear(\n        psi,\n        y.ravel(),\n        bounds=self.bounds,\n        method=self.method,\n        tol=self.tol,\n        lsq_solver=self.lsq_solver,\n        lsmr_tol=self.lsmr_tol,\n        max_iter=self.max_iter,\n        verbose=self.verbose,\n        lsmr_maxiter=self.lsmr_maxiter,\n    )\n    return theta.x.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.EstimatorError","title":"<code>EstimatorError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Generic Python-exception-derived object raised by estimator functions.</p> <p>General purpose exception class, derived from Python's ValueError class, programmatically raised in estimators functions when a Estimator-related condition would prevent further correct execution of the function.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> required Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class EstimatorError(Exception):\n    \"\"\"Generic Python-exception-derived object raised by estimator functions.\n\n    General purpose exception class, derived from Python's ValueError\n    class, programmatically raised in estimators functions when a Estimator-related\n    condition would prevent further correct execution of the function.\n\n    Parameters\n    ----------\n    None\n\n    \"\"\"\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm","title":"<code>LeastMeanSquareMixedNorm</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Square Mixed Norm (LMS-MN) Adaptive Filter.</p> <p>This class implements the Mixed-norm Least Mean Square (LMS) adaptive filter algorithm, which incorporates an additional weight factor to control the proportions of the error norms, thus providing an extra degree of freedom in the adaptation process.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The adaptation step size. Default is 0.01.</p> <code>0.01</code> <code>weight</code> <code>float</code> <p>The weight factor for mixed-norm control. This factor controls the proportions of the error norms and offers an extra degree of freedom within the adaptation of the LMS mixed norm method.</p> <code>0.02</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The adaptation step size.</p> <code>weight</code> <code>float</code> <p>The weight factor for mixed-norm control.</p> <code>xi</code> <code>ndarray or None</code> <p>The error signal, initialized to None.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMSF filter.</p> References <ul> <li>Chambers, J. A., Tanrikulu, O., &amp; Constantinides, A. G. (1994).   Least mean mixed-norm adaptive filtering.   Electronics letters, 30(19), 1574-1575.   https://ieeexplore.ieee.org/document/326382</li> <li>Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,   an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo   vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares   https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquareMixedNorm(BaseEstimator):\n    \"\"\"Least Mean Square Mixed Norm (LMS-MN) Adaptive Filter.\n\n    This class implements the Mixed-norm Least Mean Square (LMS) adaptive filter\n    algorithm, which incorporates an additional weight factor to control the\n    proportions of the error norms, thus providing an extra degree of freedom\n    in the adaptation process.\n\n    Parameters\n    ----------\n    mu : float, optional\n        The adaptation step size. Default is 0.01.\n    weight : float, optional\n        The weight factor for mixed-norm control. This factor controls the\n        proportions of the error norms and offers an extra degree of freedom\n        within the adaptation of the LMS mixed norm method.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The adaptation step size.\n    weight : float\n        The weight factor for mixed-norm control.\n    xi : ndarray or None\n        The error signal, initialized to None.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMSF filter.\n\n    References\n    ----------\n    - Chambers, J. A., Tanrikulu, O., &amp; Constantinides, A. G. (1994).\n      Least mean mixed-norm adaptive filtering.\n      Electronics letters, 30(19), 1574-1575.\n      https://ieeexplore.ieee.org/document/326382\n    - Dissertation (Portuguese): Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o,\n      an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo\n      vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n      https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        weight: float = 0.02,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.weight = weight\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n        The LMS-MN algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i (\\text{weight}\n           + (1 - \\text{weight}) \\xi_i^2)\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        A more in-depth documentation of all methods for parameter estimation\n        will be available soon. For now, please refer to the mentioned references.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[\n                i, 0\n            ] * (self.weight + (1 - self.weight) * self.xi[i, 0] ** 2)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquareMixedNorm.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Mixed-norm LMS filter.</p> <p>The LMS-MN algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i (\\text{weight}    + (1 - \\text{weight}) \\xi_i^2)    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Notes <p>A more in-depth documentation of all methods for parameter estimation will be available soon. For now, please refer to the mentioned references.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n    The LMS-MN algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i (\\text{weight}\n       + (1 - \\text{weight}) \\xi_i^2)\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    A more in-depth documentation of all methods for parameter estimation\n    will be available soon. For now, please refer to the mentioned references.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[\n            i, 0\n        ] * (self.weight + (1 - self.weight) * self.xi[i, 0] ** 2)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares","title":"<code>LeastMeanSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares (LMS) filter for parameter estimation in adaptive filtering.</p> <p>The LMS algorithm is an adaptive filter used to estimate the parameters of a model by minimizing the mean square error between the observed and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMS filter.</p> References <ul> <li>Haykin, S., &amp; Widrow, B. (Eds.). (2003). Least-mean-square adaptive filters (Vol. 31). John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquares(BaseEstimator):\n    \"\"\"Least Mean Squares (LMS) filter for parameter estimation in adaptive filtering.\n\n    The LMS algorithm is an adaptive filter used to estimate the parameters of a model\n    by minimizing the mean square error between the observed and predicted values.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n\n    References\n    ----------\n    - Haykin, S., &amp; Widrow, B. (Eds.). (2003). Least-mean-square adaptive filters\n    (Vol. 31). John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n    algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Least Mean Squares filter.\n\n        The LMS algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\psi_i\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * psi_tmp\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Least Mean Squares filter.</p> <p>The LMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\psi_i    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Least Mean Squares filter.\n\n    The LMS algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\psi_i\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * psi_tmp\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth","title":"<code>LeastMeanSquaresFourth</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares Fourth (LMSF) filter for parameter estimation.</p> <p>The LMSF algorithm is an adaptive filter used to estimate the parameters of a model by using the mean fourth error cost function to eliminate the noise effectively.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.5</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMSF filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Gui, G., Mehbodniya, A., &amp; Adachi, F. (2013). Least mean square/fourth algorithm   with application to sparse channel estimation. arXiv preprint arXiv:1304.3911.   https://arxiv.org/pdf/1304.3911.pdf</li> <li>Nascimento, V. H., &amp; Bermudez, J. C. M. (2005, March). When is the least-mean   fourth algorithm mean-square stable? In Proceedings.(ICASSP'05). IEEE   International Conference on Acoustics, Speech, and Signal Processing, 2005.   (Vol. 4, pp. iv-341). IEEE. http://www.lps.usp.br/vitor/artigos/icassp05.pdf</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresFourth(BaseEstimator):\n    \"\"\"Least Mean Squares Fourth (LMSF) filter for parameter estimation.\n\n    The LMSF algorithm is an adaptive filter used to estimate the parameters of a model\n    by using the mean fourth error cost function to eliminate the noise effectively.\n\n    Parameters\n    ----------\n    mu : float, default=0.5\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMSF filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Gui, G., Mehbodniya, A., &amp; Adachi, F. (2013). Least mean square/fourth algorithm\n      with application to sparse channel estimation. arXiv preprint arXiv:1304.3911.\n      https://arxiv.org/pdf/1304.3911.pdf\n    - Nascimento, V. H., &amp; Bermudez, J. C. M. (2005, March). When is the least-mean\n      fourth algorithm mean-square stable? In Proceedings.(ICASSP'05). IEEE\n      International Conference on Acoustics, Speech, and Signal Processing, 2005.\n      (Vol. 4, pp. iv-341). IEEE. http://www.lps.usp.br/vitor/artigos/icassp05.pdf\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.5, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the LMS Fourth filter.\n\n        The LMSF algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i^3\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : ndarray of floats of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[i, 0] ** 3\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresFourth.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the LMS Fourth filter.</p> <p>The LMSF algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i^3    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>ndarray of floats of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the LMS Fourth filter.\n\n    The LMSF algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\psi_i \\xi_i^3\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : ndarray of floats of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1) + self.mu * psi_tmp * self.xi[i, 0] ** 3\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky","title":"<code>LeastMeanSquaresLeaky</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares Leaky (LMSL) filter for parameter estimation.</p> <p>The LMSL algorithm is an adaptive filter used to estimate the parameters of a model by minimizing the mean square error between the observed and predicted values. The leakage factor helps to prevent coefficient drift.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>gama</code> <code>float</code> <p>The leakage factor of the Leaky LMS method.</p> <code>0.2</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>gama</code> <code>float, default=0.2</code> <p>The leakage factor of the Leaky LMS method.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMSL filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresLeaky(BaseEstimator):\n    \"\"\"Least Mean Squares Leaky (LMSL) filter for parameter estimation.\n\n    The LMSL algorithm is an adaptive filter used to estimate the parameters of a model\n    by minimizing the mean square error between the observed and predicted values. The\n    leakage factor helps to prevent coefficient drift.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMSL filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        gama: float = 0.001,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.gama = gama\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Leaky LMS filter.\n\n        The LMSL algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\xi_i \\psi_i\n           $$\n\n        When the leakage factor, $\\gamma$, is set to 0, there is no leakage in the\n        estimation process.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n          John Wiley &amp; Sons.\n        - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n          de algoritmos LMS de passo vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1) * (1 - self.mu * self.gama)\n                + self.mu * self.xi[i, 0] * psi_tmp\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresLeaky.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Leaky LMS filter.</p> <p>The LMSL algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\xi_i \\psi_i    $$</p> <p>When the leakage factor, \\(\\gamma\\), is set to 0, there is no leakage in the estimation process.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias   de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Leaky LMS filter.\n\n    The LMSL algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\xi_i \\psi_i\n       $$\n\n    When the leakage factor, $\\gamma$, is set to 0, there is no leakage in the\n    estimation process.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n      de algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1) * (1 - self.mu * self.gama)\n            + self.mu * self.xi[i, 0] * psi_tmp\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky","title":"<code>LeastMeanSquaresNormalizedLeaky</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares Leaky (NLMSL) filter for parameter estimation.</p> <p>The NLMSL algorithm is an adaptive filter used to estimate the parameters of a model by minimizing the mean square error between the observed and predicted values. The normalization is used to avoid numerical instability when updating the estimated parameters, and the leakage factor helps to prevent coefficient drift.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <code>gama</code> <code>float</code> <p>The leakage factor of the Leaky LMS method.</p> <code>0.2</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float, default=np.finfo(np.float64).eps</code> <p>Normalization factor of the normalized filters.</p> <code>gama</code> <code>float, default=0.2</code> <p>The leakage factor of the Leaky LMS method.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the NLMSL filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresNormalizedLeaky(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares Leaky (NLMSL) filter for parameter estimation.\n\n    The NLMSL algorithm is an adaptive filter used to estimate the parameters of a model\n    by minimizing the mean square error between the observed and predicted values. The\n    normalization is used to avoid numerical instability when updating the estimated\n    parameters, and the leakage factor helps to prevent coefficient drift.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    gama : float, default=0.2\n        The leakage factor of the Leaky LMS method.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the NLMSL filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        gama: float = 0.2,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.gama = gama\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Leaky LMS filter.\n\n        The NLMSL algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\frac{\\xi_i \\psi_i}{\\epsilon\n           + \\psi_i^T \\psi_i}\n           $$\n\n        When the leakage factor, $\\gamma$, is set to 0, there is no leakage in the\n        estimation process.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n          John Wiley &amp; Sons.\n        - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n          de algoritmos LMS de passo vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) * (\n                1 - self.mu * self.gama\n            ) + self.mu * self.xi[i, 0] * psi_tmp / (\n                self.eps + np.dot(psi_tmp.T, psi_tmp)\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedLeaky.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Leaky LMS filter.</p> <p>The NLMSL algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\frac{\\xi_i \\psi_i}{\\epsilon    + \\psi_i^T \\psi_i}    $$</p> <p>When the leakage factor, \\(\\gamma\\), is set to 0, there is no leakage in the estimation process.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias   de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Leaky LMS filter.\n\n    The NLMSL algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} (1 - \\mu \\gamma) + \\mu \\frac{\\xi_i \\psi_i}{\\epsilon\n       + \\psi_i^T \\psi_i}\n       $$\n\n    When the leakage factor, $\\gamma$, is set to 0, there is no leakage in the\n    estimation process.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n      de algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) * (\n            1 - self.mu * self.gama\n        ) + self.mu * self.xi[i, 0] * psi_tmp / (\n            self.eps + np.dot(psi_tmp.T, psi_tmp)\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor","title":"<code>LeastMeanSquaresNormalizedSignRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares SignRegressor filter for parameter estimation.</p> <p>The Normalized Sign-Regressor LMS algorithm updates the parameter estimates recursively by normalizing the input signal to avoid numerical instability and using the sign of the information matrix to adjust the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float, default=np.finfo(np.float64).eps</code> <p>Normalization factor of the normalized filters.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the Normalized Sign-Regressor LMS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares   https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresNormalizedSignRegressor(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares SignRegressor filter for parameter estimation.\n\n    The Normalized Sign-Regressor LMS algorithm updates the parameter estimates\n    recursively by normalizing the input signal to avoid numerical instability\n    and using the sign of the information matrix to adjust the filter coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the Normalized Sign-Regressor LMS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares\n      https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Sign-Regressor LMS filter.\n\n        The Normalized Sign-Regressor LMS algorithm updates the parameter estimates\n        recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot\n           \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}\n           $$\n\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters and the sign of the information matrix is\n        used to change the filter coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[i, 0] * (\n                np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignRegressor.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Sign-Regressor LMS filter.</p> <p>The Normalized Sign-Regressor LMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot    \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}    $$</p> <p>The normalization is used to avoid numerical instability when updating the estimated parameters and the sign of the information matrix is used to change the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Sign-Regressor LMS filter.\n\n    The Normalized Sign-Regressor LMS algorithm updates the parameter estimates\n    recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot\n       \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}\n       $$\n\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters and the sign of the information matrix is\n    used to change the filter coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[i, 0] * (\n            np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign","title":"<code>LeastMeanSquaresNormalizedSignSign</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares SignSign (NLMSSS) filter for parameter estimation.</p> <p>The NLMSSS algorithm updates the parameter estimates recursively by normalizing the input signal to avoid numerical instability and using both the sign of the information matrix and the sign of the error vector to adjust the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the NLMSSS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresNormalizedSignSign(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares SignSign (NLMSSS) filter for parameter estimation.\n\n    The NLMSSS algorithm updates the parameter estimates recursively by normalizing\n    the input signal to avoid numerical instability and using both the sign of the\n    information matrix and the sign of the error vector to adjust the filter\n    coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the NLMSSS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Sign-Sign LMS filter.\n\n        The NLMSSS algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot\n           \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}\n           $$\n\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters and both the sign of the information matrix\n        and the sign of the error vector are used to change the filter\n        coefficients.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n          John Wiley &amp; Sons.\n        - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n        de algoritmos LMS de passo vari\u00e1vel.\n        - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n                self.xi[i, 0]\n            ) * (np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresNormalizedSignSign.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Sign-Sign LMS filter.</p> <p>The NLMSSS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot    \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}    $$</p> <p>The normalization is used to avoid numerical instability when updating the estimated parameters and both the sign of the information matrix and the sign of the error vector are used to change the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Sign-Sign LMS filter.\n\n    The NLMSSS algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot\n       \\frac{\\text{sign}(\\psi_i)}{\\epsilon + \\psi_i^T \\psi_i}\n       $$\n\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters and both the sign of the information matrix\n    and the sign of the error vector are used to change the filter\n    coefficients.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias\n    de algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n            self.xi[i, 0]\n        ) * (np.sign(psi_tmp) / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError","title":"<code>LeastMeanSquaresSignError</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares (LMS) filter for parameter estimation using sign-error.</p> <p>The sign-error LMS algorithm uses the sign of the error vector to update the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling. John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresSignError(BaseEstimator):\n    \"\"\"Least Mean Squares (LMS) filter for parameter estimation using sign-error.\n\n    The sign-error LMS algorithm uses the sign of the error vector to update the filter\n    coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n    John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n    algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.uiter = uiter\n        self.unbiased = unbiased\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Sign-Error Least Mean Squares filter.\n\n        The sign-error LMS algorithm updates the parameter estimates recursively as\n        follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\cdot \\text{sign}(\\xi_i) \\cdot \\psi_i\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = (\n                theta[:, i - 1].reshape(-1, 1)\n                + self.mu * np.sign(self.xi[i, 0]) * psi_tmp\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignError.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Sign-Error Least Mean Squares filter.</p> <p>The sign-error LMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\cdot \\text{sign}(\\xi_i) \\cdot \\psi_i    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Sign-Error Least Mean Squares filter.\n\n    The sign-error LMS algorithm updates the parameter estimates recursively as\n    follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\cdot \\text{sign}(\\xi_i) \\cdot \\psi_i\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = (\n            theta[:, i - 1].reshape(-1, 1)\n            + self.mu * np.sign(self.xi[i, 0]) * psi_tmp\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor","title":"<code>LeastMeanSquaresSignRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares (LMSSR) filter for parameter estimation.</p> <p>The sign-regressor LMS algorithm uses the sign of the matrix information to change the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresSignRegressor(BaseEstimator):\n    \"\"\"Least Mean Squares (LMSSR) filter for parameter estimation.\n\n    The sign-regressor LMS algorithm uses the sign of the matrix\n    information to change the filter coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Sign-Regressor LMS filter.\n\n        The sign-regressor LMS algorithm updates the parameter estimates recursively\n        as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot \\text{sign}(\\psi_i)\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[\n                i, 0\n            ] * np.sign(psi_tmp)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignRegressor.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Sign-Regressor LMS filter.</p> <p>The sign-regressor LMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot \\text{sign}(\\psi_i)    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Sign-Regressor LMS filter.\n\n    The sign-regressor LMS algorithm updates the parameter estimates recursively\n    as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + \\mu \\cdot \\xi_i \\cdot \\text{sign}(\\psi_i)\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + self.mu * self.xi[\n            i, 0\n        ] * np.sign(psi_tmp)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign","title":"<code>LeastMeanSquaresSignSign</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Least Mean Squares Sign-Sign (LMSSS) filter for parameter estimation.</p> <p>The LMSSS algorithm uses both the sign of the matrix information and the sign of the error vector to update the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the LMSSS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling. John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastMeanSquaresSignSign(BaseEstimator):\n    \"\"\"Least Mean Squares Sign-Sign (LMSSS) filter for parameter estimation.\n\n    The LMSSS algorithm uses both the sign of the matrix information and the sign of\n    the error vector to update the filter coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the LMSSS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n    John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n    algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(self, *, mu: float = 0.01, unbiased: bool = False, uiter: int = 30):\n        self.mu = mu\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Sign-Sign LMS filter.\n\n        The LMSSS algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2* \\mu \\cdot \\text{sign}(\\xi_i)\n           \\cdot \\text{sign}(\\psi_i)\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n                self.xi[i, 0]\n            ) * np.sign(psi_tmp)\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastMeanSquaresSignSign.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Sign-Sign LMS filter.</p> <p>The LMSSS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2* \\mu \\cdot \\text{sign}(\\xi_i)    \\cdot \\text{sign}(\\psi_i)    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Sign-Sign LMS filter.\n\n    The LMSSS algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2* \\mu \\cdot \\text{sign}(\\xi_i)\n       \\cdot \\text{sign}(\\psi_i)\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n            self.xi[i, 0]\n        ) * np.sign(psi_tmp)\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquares","title":"<code>LeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Ordinary Least Squares for linear parameter estimation.</p> <p>The Least Squares method minimizes the sum of the squared differences between the observed and predicted values. It is used to estimate the parameters of a linear model.</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 20.</p> <code>20</code> References <ul> <li>Sorenson, H. W. (1970). Least-squares estimation: from Gauss to Kalman.   IEEE spectrum, 7(7), 63-68.   http://pzs.dstu.dp.ua/DataMining/mls/bibl/Gauss2Kalman.pdf</li> <li>Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o de sistemas: t\u00e9cnicas   lineares e n\u00e3o-lineares aplicadas a sistemas reais. Editora da UFMG. 3a edi\u00e7\u00e3o.</li> <li>Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least-squares methods.   Signal processing, 87(10), 2283-2302.   https://eprints.soton.ac.uk/263855/1/tls_overview.pdf</li> <li>Wikipedia entry on Least Squares   https://en.wikipedia.org/wiki/Least_squares</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastSquares(BaseEstimator):\n    \"\"\"Ordinary Least Squares for linear parameter estimation.\n\n    The Least Squares method minimizes the sum of the squared differences\n    between the observed and predicted values. It is used to estimate the\n    parameters of a linear model.\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 20.\n\n    References\n    ----------\n    - Sorenson, H. W. (1970). Least-squares estimation: from Gauss to Kalman.\n      IEEE spectrum, 7(7), 63-68.\n      http://pzs.dstu.dp.ua/DataMining/mls/bibl/Gauss2Kalman.pdf\n    - Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o de sistemas: t\u00e9cnicas\n      lineares e n\u00e3o-lineares aplicadas a sistemas reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n    - Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least-squares methods.\n      Signal processing, 87(10), 2283-2302.\n      https://eprints.soton.ac.uk/263855/1/tls_overview.pdf\n    - Wikipedia entry on Least Squares\n      https://en.wikipedia.org/wiki/Least_squares\n    \"\"\"\n\n    def __init__(self, *, unbiased: bool = False, uiter: int = 20):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Least Squares method.\n\n        The Least Squares method solves the following optimization problem:\n\n        $$\n        \\min_{\\theta} \\| \\psi \\theta - y \\|_2^2\n        $$\n\n        where $\\psi$ is the information matrix, $y$ is the observed data,\n        and $\\theta$ are the model parameters to be estimated.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        check_linear_dependence_rows(psi)\n        theta = np.linalg.lstsq(psi, y, rcond=None)[0]\n        return theta\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Least Squares method.</p> <p>The Least Squares method solves the following optimization problem:</p> \\[ \\min_{\\theta} \\| \\psi \\theta - y \\|_2^2 \\] <p>where \\(\\psi\\) is the information matrix, \\(y\\) is the observed data, and \\(\\theta\\) are the model parameters to be estimated.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Least Squares method.\n\n    The Least Squares method solves the following optimization problem:\n\n    $$\n    \\min_{\\theta} \\| \\psi \\theta - y \\|_2^2\n    $$\n\n    where $\\psi$ is the information matrix, $y$ is the observed data,\n    and $\\theta$ are the model parameters to be estimated.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    check_linear_dependence_rows(psi)\n    theta = np.linalg.lstsq(psi, y, rcond=None)[0]\n    return theta\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual","title":"<code>LeastSquaresMinimalResidual</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Iterative solver for least-squares minimal residual problems.</p> <p>This is a wrapper class for the <code>scipy.sparse.linalg.lsmr</code> method.</p> <p>lsmr solves the system of linear equations <code>Ax = b</code>. If the system is inconsistent, it solves the least-squares problem <code>min ||b - Ax||_2</code>. <code>A</code> is a rectangular matrix of dimension m-by-n, where all cases are allowed: m = n, m &gt; n, or m &lt; n. <code>b</code> is a vector of length m. The matrix A may be dense or sparse (usually sparse).</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>damp</code> <code>float</code> <p>Damping factor for regularized least-squares. <code>lsmr</code> solves the regularized least-squares problem::</p> <p>min ||(b) - (  A   )x||      ||(0)   (damp*I) ||_2</p> <p>where damp is a scalar.  If damp is None or 0, the system is solved without regularization. Default is 0.</p> <code>atol, btol</code> <code>(float, optional)</code> <p>Stopping tolerances. <code>lsmr</code> continues iterations until a certain backward error estimate is smaller than some quantity depending on atol and btol.  Let <code>r = b - Ax</code> be the residual vector for the current approximate solution <code>x</code>. If <code>Ax = b</code> seems to be consistent, <code>lsmr</code> terminates when <code>norm(r) &lt;= atol * norm(A) * norm(x) + btol * norm(b)</code>. Otherwise, <code>lsmr</code> terminates when <code>norm(A^H r) &lt;= atol * norm(A) * norm(r)</code>.  If both tolerances are 1.0e-6 (default), the final <code>norm(r)</code> should be accurate to about 6 digits. (The final <code>x</code> will usually have fewer correct digits, depending on <code>cond(A)</code> and the size of LAMBDA.)  If <code>atol</code> or <code>btol</code> is None, a default value of 1.0e-6 will be used. Ideally, they should be estimates of the relative error in the entries of <code>A</code> and <code>b</code> respectively.  For example, if the entries of <code>A</code> have 7 correct digits, set <code>atol = 1e-7</code>. This prevents the algorithm from doing unnecessary work beyond the uncertainty of the input data.</p> <code>conlim</code> <code>(float, optional)</code> <p><code>lsmr</code> terminates if an estimate of <code>cond(A)</code> exceeds <code>conlim</code>.  For compatible systems <code>Ax = b</code>, conlim could be as large as 1.0e+12 (say).  For least-squares problems, <code>conlim</code> should be less than 1.0e+8. If <code>conlim</code> is None, the default value is 1e+8.  Maximum precision can be obtained by setting <code>atol = btol = conlim = 0</code>, but the number of iterations may then be excessive. Default is 1e8.</p> <code>maxiter</code> <code>(int, optional)</code> <p><code>lsmr</code> terminates if the number of iterations reaches <code>maxiter</code>.  The default is <code>maxiter = min(m, n)</code>.  For ill-conditioned systems, a larger value of <code>maxiter</code> may be needed. Default is False.</p> <code>show</code> <code>(bool, optional)</code> <p>Print iterations logs if <code>show=True</code>. Default is False.</p> <code>x0</code> <code>(array_like, shape(n), optional)</code> <p>Initial guess of <code>x</code>, if None zeros are used. Default is None.</p> References <p>.. [1] D. C.-L. Fong and M. A. Saunders,        \"LSMR: An iterative algorithm for sparse least-squares problems\",        SIAM J. Sci. Comput., vol. 33, pp. 2950-2971, 2011.        :arxiv:<code>1006.0758</code> .. [2] LSMR Software, https://web.stanford.edu/group/SOL/software/lsmr/</p> Notes <p>This docstring is adapted from the <code>scipy.sparse.linalg.lsmr</code> method.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class LeastSquaresMinimalResidual(BaseEstimator):\n    \"\"\"Iterative solver for least-squares minimal residual problems.\n\n    This is a wrapper class for the `scipy.sparse.linalg.lsmr` method.\n\n    lsmr solves the system of linear equations ``Ax = b``. If the system\n    is inconsistent, it solves the least-squares problem ``min ||b - Ax||_2``.\n    ``A`` is a rectangular matrix of dimension m-by-n, where all cases are\n    allowed: m = n, m &gt; n, or m &lt; n. ``b`` is a vector of length m.\n    The matrix A may be dense or sparse (usually sparse).\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    damp : float\n        Damping factor for regularized least-squares. `lsmr` solves\n        the regularized least-squares problem::\n\n         min ||(b) - (  A   )x||\n             ||(0)   (damp*I) ||_2\n\n        where damp is a scalar.  If damp is None or 0, the system\n        is solved without regularization. Default is 0.\n    atol, btol : float, optional\n        Stopping tolerances. `lsmr` continues iterations until a\n        certain backward error estimate is smaller than some quantity\n        depending on atol and btol.  Let ``r = b - Ax`` be the\n        residual vector for the current approximate solution ``x``.\n        If ``Ax = b`` seems to be consistent, `lsmr` terminates\n        when ``norm(r) &lt;= atol * norm(A) * norm(x) + btol * norm(b)``.\n        Otherwise, `lsmr` terminates when ``norm(A^H r) &lt;=\n        atol * norm(A) * norm(r)``.  If both tolerances are 1.0e-6 (default),\n        the final ``norm(r)`` should be accurate to about 6\n        digits. (The final ``x`` will usually have fewer correct digits,\n        depending on ``cond(A)`` and the size of LAMBDA.)  If `atol`\n        or `btol` is None, a default value of 1.0e-6 will be used.\n        Ideally, they should be estimates of the relative error in the\n        entries of ``A`` and ``b`` respectively.  For example, if the entries\n        of ``A`` have 7 correct digits, set ``atol = 1e-7``. This prevents\n        the algorithm from doing unnecessary work beyond the\n        uncertainty of the input data.\n    conlim : float, optional\n        `lsmr` terminates if an estimate of ``cond(A)`` exceeds\n        `conlim`.  For compatible systems ``Ax = b``, conlim could be\n        as large as 1.0e+12 (say).  For least-squares problems,\n        `conlim` should be less than 1.0e+8. If `conlim` is None, the\n        default value is 1e+8.  Maximum precision can be obtained by\n        setting ``atol = btol = conlim = 0``, but the number of\n        iterations may then be excessive. Default is 1e8.\n    maxiter : int, optional\n        `lsmr` terminates if the number of iterations reaches\n        `maxiter`.  The default is ``maxiter = min(m, n)``.  For\n        ill-conditioned systems, a larger value of `maxiter` may be\n        needed. Default is False.\n    show : bool, optional\n        Print iterations logs if ``show=True``. Default is False.\n    x0 : array_like, shape (n,), optional\n        Initial guess of ``x``, if None zeros are used. Default is None.\n\n    References\n    ----------\n    .. [1] D. C.-L. Fong and M. A. Saunders,\n           \"LSMR: An iterative algorithm for sparse least-squares problems\",\n           SIAM J. Sci. Comput., vol. 33, pp. 2950-2971, 2011.\n           :arxiv:`1006.0758`\n    .. [2] LSMR Software, https://web.stanford.edu/group/SOL/software/lsmr/\n\n    Notes\n    -----\n    This docstring is adapted from the `scipy.sparse.linalg.lsmr` method.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        unbiased: bool = False,\n        uiter: int = 30,\n        damp=0.0,\n        atol=1e-6,\n        btol=1e-6,\n        conlim=1e8,\n        maxiter=None,\n        show=False,\n        x0=None,\n    ):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self.damp = damp\n        self.atol = atol\n        self.btol = btol\n        self.conlim = conlim\n        self.maxiter = maxiter\n        self.show = show\n        self.x0 = x0\n\n    def optimize(self, psi, y):\n        \"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        This is a wrapper class for the `scipy.sparse.linalg.lsmr` method.\n\n        References\n        ----------\n        .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html\n        \"\"\"\n        theta = lsmr(\n            psi,\n            y.ravel(),\n            damp=self.damp,\n            atol=self.atol,\n            btol=self.btol,\n            conlim=self.conlim,\n            maxiter=self.maxiter,\n            show=self.show,\n            x0=self.x0,\n        )[0]\n        return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.LeastSquaresMinimalResidual.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Mixed-norm LMS filter.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> Notes <p>This is a wrapper class for the <code>scipy.sparse.linalg.lsmr</code> method.</p> References <p>.. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi, y):\n    \"\"\"Parameter estimation using the Mixed-norm LMS filter.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    This is a wrapper class for the `scipy.sparse.linalg.lsmr` method.\n\n    References\n    ----------\n    .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.lsmr.html\n    \"\"\"\n    theta = lsmr(\n        psi,\n        y.ravel(),\n        damp=self.damp,\n        atol=self.atol,\n        btol=self.btol,\n        conlim=self.conlim,\n        maxiter=self.maxiter,\n        show=self.show,\n        x0=self.x0,\n    )[0]\n    return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares","title":"<code>NonNegativeLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Solve <code>argmin_x || Ax - b ||_2</code> for <code>x &gt;= 0</code>.</p> <p>This is a wrapper class for the <code>scipy.optimize.nnls</code> method.</p> <p>This problem, often called NonNegative Least Squares (NNLS), is a convex optimization problem with convex constraints. It typically arises when the <code>x</code> models quantities for which only nonnegative values are attainable; such as weights of ingredients, component costs, and so on.</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <code>maxiter</code> <code>int</code> <p>Maximum number of iterations. Default value is <code>3 * n</code> where <code>n</code> is the number of features.</p> <code>None</code> <code>atol</code> <code>float</code> <p>Tolerance value used in the algorithm to assess closeness to zero in the projected residual <code>(A.T @ (A x - b))</code> entries. Increasing this value relaxes the solution constraints. A typical relaxation value can be selected as <code>max(m, n) * np.linalg.norm(A, 1) * np.spacing(1.)</code>. Default is None.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>unbiased</code> <code>bool</code> <p>Indicates whether an unbiased estimator is applied.</p> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator.</p> <code>maxiter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>atol</code> <code>float</code> <p>Tolerance value for the algorithm.</p> References <p>Lawson C., Hanson R.J., \"Solving Least Squares Problems\", SIAM,    1995, :doi:<code>10.1137/1.9781611971217</code> Bro, Rasmus and de Jong, Sijmen, \"A Fast Non-Negativity-Constrained Least     Squares Algorithm\", Journal Of Chemometrics, 1997,     :doi:<code>10.1002/(SICI)1099-128X(199709/10)11:5&lt;393::AID-CEM483&gt;3.0.CO;2-L</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sysidentpy.parameter_estimation import NonNegativeLeastSquares\n...\n&gt;&gt;&gt; A = np.array([[1, 0], [1, 0], [0, 1]])\n&gt;&gt;&gt; b = np.array([2, 1, 1])\n&gt;&gt;&gt; nnls_solver = NonNegativeLeastSquares()\n&gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n&gt;&gt;&gt; print(x)\n[[1.5]\n [1. ]]\n</code></pre> <pre><code>&gt;&gt;&gt; b = np.array([-1, -1, -1])\n&gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n&gt;&gt;&gt; print(x)\n[[0.]\n [0.]]\n</code></pre> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class NonNegativeLeastSquares(BaseEstimator):\n    \"\"\"Solve ``argmin_x || Ax - b ||_2`` for ``x &gt;= 0``.\n\n    This is a wrapper class for the `scipy.optimize.nnls` method.\n\n    This problem, often called NonNegative Least Squares (NNLS), is a convex\n    optimization problem with convex constraints. It typically arises when\n    the ``x`` models quantities for which only nonnegative values are\n    attainable; such as weights of ingredients, component costs, and so on.\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n    maxiter : int, optional\n        Maximum number of iterations. Default value is ``3 * n`` where ``n``\n        is the number of features.\n    atol : float, optional\n        Tolerance value used in the algorithm to assess closeness to zero in\n        the projected residual ``(A.T @ (A x - b))`` entries. Increasing this\n        value relaxes the solution constraints. A typical relaxation value can\n        be selected as ``max(m, n) * np.linalg.norm(A, 1) * np.spacing(1.)``.\n        Default is None.\n\n    Attributes\n    ----------\n    unbiased : bool\n        Indicates whether an unbiased estimator is applied.\n    uiter : int\n        Number of iterations for the unbiased estimator.\n    maxiter : int\n        Maximum number of iterations.\n    atol : float\n        Tolerance value for the algorithm.\n\n    References\n    ----------\n    Lawson C., Hanson R.J., \"Solving Least Squares Problems\", SIAM,\n       1995, :doi:`10.1137/1.9781611971217`\n    Bro, Rasmus and de Jong, Sijmen, \"A Fast Non-Negativity-Constrained Least\n        Squares Algorithm\", Journal Of Chemometrics, 1997,\n        :doi:`10.1002/(SICI)1099-128X(199709/10)11:5&lt;393::AID-CEM483&gt;3.0.CO;2-L`\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sysidentpy.parameter_estimation import NonNegativeLeastSquares\n    ...\n    &gt;&gt;&gt; A = np.array([[1, 0], [1, 0], [0, 1]])\n    &gt;&gt;&gt; b = np.array([2, 1, 1])\n    &gt;&gt;&gt; nnls_solver = NonNegativeLeastSquares()\n    &gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n    &gt;&gt;&gt; print(x)\n    [[1.5]\n     [1. ]]\n\n    &gt;&gt;&gt; b = np.array([-1, -1, -1])\n    &gt;&gt;&gt; x = nnls_solver.optimize(A, b)\n    &gt;&gt;&gt; print(x)\n    [[0.]\n     [0.]]\n    \"\"\"\n\n    def __init__(\n        self, unbiased: bool = False, uiter: int = 30, maxiter=None, atol=None\n    ):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        self.maxiter = maxiter\n        self.atol = atol\n\n    def optimize(self, psi, y):\n        \"\"\"Parameter estimation using the NonNegativeLeastSquares algorithm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : ndarray of floats of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        This is a wrapper class for the `scipy.optimize.nnls` method.\n\n        References\n        ----------\n        .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html\n        \"\"\"\n        theta, _ = nnls(psi, y.ravel(), maxiter=self.maxiter, atol=self.atol)\n        return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NonNegativeLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the NonNegativeLeastSquares algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>ndarray of floats of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> Notes <p>This is a wrapper class for the <code>scipy.optimize.nnls</code> method.</p> References <p>.. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi, y):\n    \"\"\"Parameter estimation using the NonNegativeLeastSquares algorithm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : ndarray of floats of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    This is a wrapper class for the `scipy.optimize.nnls` method.\n\n    References\n    ----------\n    .. [1] scipy, https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html\n    \"\"\"\n    theta, _ = nnls(psi, y.ravel(), maxiter=self.maxiter, atol=self.atol)\n    return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares","title":"<code>NormalizedLeastMeanSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares (NLMS) filter for parameter estimation.</p> <p>The NLMS algorithm is an adaptive filter used to estimate the parameters of a model by minimizing the mean square error between the observed and predicted values. The normalization is used to avoid numerical instability when updating the estimated parameters.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the NLMS filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class NormalizedLeastMeanSquares(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares (NLMS) filter for parameter estimation.\n\n    The NLMS algorithm is an adaptive filter used to estimate the parameters of a model\n    by minimizing the mean square error between the observed and predicted values. The\n    normalization is used to avoid numerical instability when updating the estimated\n    parameters.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the NLMS filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Least Mean Squares filter.\n\n        The NLMS algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\frac{\\psi_i}{\\epsilon +\n           \\psi_i^T \\psi_i}\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * (\n                psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n            )\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Least Mean Squares filter.</p> <p>The NLMS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\frac{\\psi_i}{\\epsilon +    \\psi_i^T \\psi_i}    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Least Mean Squares filter.\n\n    The NLMS algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2 \\mu \\xi_i \\frac{\\psi_i}{\\epsilon +\n       \\psi_i^T \\psi_i}\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * self.xi[i, 0] * (\n            psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp))\n        )\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError","title":"<code>NormalizedLeastMeanSquaresSignError</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Normalized Least Mean Squares SignError (NLMSSE) filter for parameter estimation.</p> <p>The NLMSSE algorithm updates the parameter estimates recursively by normalizing the input signal to avoid numerical instability and using the sign of the error vector to adjust the filter coefficients.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>0.01</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>mu</code> <code>float</code> <p>The learning rate or step size for the LMS algorithm.</p> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>xi</code> <code>ndarray or None</code> <p>The estimation error at each iteration. Initialized as None and updated during optimization.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the NLMSSE filter.</p> References <ul> <li>Hayes, M. H. (2009). Statistical digital signal processing and modeling.   John Wiley &amp; Sons.</li> <li>Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de   algoritmos LMS de passo vari\u00e1vel.</li> <li>Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class NormalizedLeastMeanSquaresSignError(BaseEstimator):\n    \"\"\"Normalized Least Mean Squares SignError (NLMSSE) filter for parameter estimation.\n\n    The NLMSSE algorithm updates the parameter estimates recursively by normalizing\n    the input signal to avoid numerical instability and using the sign of the error\n    vector to adjust the filter coefficients.\n\n    Parameters\n    ----------\n    mu : float, default=0.01\n        The learning rate or step size for the LMS algorithm.\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n\n    Attributes\n    ----------\n    mu : float\n        The learning rate or step size for the LMS algorithm.\n    eps : float\n        Normalization factor of the normalized filters.\n    xi : np.ndarray or None\n        The estimation error at each iteration. Initialized as None and updated during\n        optimization.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the NLMSSE filter.\n\n    References\n    ----------\n    - Hayes, M. H. (2009). Statistical digital signal processing and modeling.\n      John Wiley &amp; Sons.\n    - Zipf, J. G. F. (2011). Classifica\u00e7\u00e3o, an\u00e1lise estat\u00edstica e novas estrat\u00e9gias de\n      algoritmos LMS de passo vari\u00e1vel.\n    - Wikipedia entry on Least Mean Squares: https://en.wikipedia.org/wiki/Least_mean_squares_filter\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        mu: float = 0.01,\n        eps: np.float64 = np.finfo(np.float64).eps,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.mu = mu\n        self.eps = eps\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Parameter estimation using the Normalized Sign-Error LMS filter.\n\n        The NLMSSE algorithm updates the parameter estimates recursively as follows:\n\n        1. Compute the estimation error:\n\n           $$\n           \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n           $$\n\n        2. Update the parameter vector:\n\n           $$\n           \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot\n           \\frac{\\psi_i}{\\epsilon + \\psi_i^T \\psi_i}\n           $$\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        The normalization is used to avoid numerical instability when updating\n        the estimated parameters and the sign of the error vector is used to\n        change the filter coefficients.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n\n        for i in range(n_theta, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n                self.xi[i, 0]\n            ) * (psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n            theta[:, i] = tmp_list.flatten()\n\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.NormalizedLeastMeanSquaresSignError.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Parameter estimation using the Normalized Sign-Error LMS filter.</p> <p>The NLMSSE algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Compute the estimation error:</li> </ol> <p>$$    \\xi_i = y_i - \\psi_i^T \\theta_{i-1}    $$</p> <ol> <li>Update the parameter vector:</li> </ol> <p>$$    \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot    \\frac{\\psi_i}{\\epsilon + \\psi_i^T \\psi_i}    $$</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Notes <p>The normalization is used to avoid numerical instability when updating the estimated parameters and the sign of the error vector is used to change the filter coefficients.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Parameter estimation using the Normalized Sign-Error LMS filter.\n\n    The NLMSSE algorithm updates the parameter estimates recursively as follows:\n\n    1. Compute the estimation error:\n\n       $$\n       \\xi_i = y_i - \\psi_i^T \\theta_{i-1}\n       $$\n\n    2. Update the parameter vector:\n\n       $$\n       \\theta_i = \\theta_{i-1} + 2 \\mu \\cdot \\text{sign}(\\xi_i) \\cdot\n       \\frac{\\psi_i}{\\epsilon + \\psi_i^T \\psi_i}\n       $$\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    The normalization is used to avoid numerical instability when updating\n    the estimated parameters and the sign of the error vector is used to\n    change the filter coefficients.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n\n    for i in range(n_theta, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + 2 * self.mu * np.sign(\n            self.xi[i, 0]\n        ) * (psi_tmp / (self.eps + np.dot(psi_tmp.T, psi_tmp)))\n        theta[:, i] = tmp_list.flatten()\n\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares","title":"<code>RecursiveLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Recursive Least Squares (RLS) filter for parameter estimation.</p> <p>The Recursive Least Squares method is used to estimate the parameters of a model by minimizing the sum of the squares of the differences between the observed and predicted values. This method incorporates a forgetting factor to give more weight to recent observations.</p> <p>Parameters:</p> Name Type Description Default <code>lam</code> <code>float</code> <p>Forgetting factor of the Recursive Least Squares method.</p> <code>0.98</code> <code>delta</code> <code>float</code> <p>Normalization factor of the P matrix.</p> <code>0.01</code> <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> <p>Attributes:</p> Name Type Description <code>lam</code> <code>float</code> <p>Forgetting factor of the Recursive Least Squares method.</p> <code>delta</code> <code>float</code> <p>Normalization factor of the P matrix.</p> <code>xi</code> <code>ndarray</code> <p>The estimation error at each iteration.</p> <code>theta_evolution</code> <code>ndarray</code> <p>Evolution of the estimated parameters over iterations.</p> <p>Methods:</p> Name Description <code>optimize</code> <p>Estimate the model parameters using the Recursive Least Squares method.</p> References <ul> <li>Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o    de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas    reais. Editora da UFMG. 3a edi\u00e7\u00e3o.</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class RecursiveLeastSquares(BaseEstimator):\n    \"\"\"Recursive Least Squares (RLS) filter for parameter estimation.\n\n    The Recursive Least Squares method is used to estimate the parameters of a model\n    by minimizing the sum of the squares of the differences between the observed and\n    predicted values. This method incorporates a forgetting factor to give more weight\n    to recent observations.\n\n    Parameters\n    ----------\n    lam : float, default=0.98\n        Forgetting factor of the Recursive Least Squares method.\n    delta : float, default=0.01\n        Normalization factor of the P matrix.\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    Attributes\n    ----------\n    lam : float\n        Forgetting factor of the Recursive Least Squares method.\n    delta : float\n        Normalization factor of the P matrix.\n    xi : np.ndarray\n        The estimation error at each iteration.\n    theta_evolution : np.ndarray\n        Evolution of the estimated parameters over iterations.\n\n    Methods\n    -------\n    optimize(psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray\n        Estimate the model parameters using the Recursive Least Squares method.\n\n    References\n    ----------\n    - Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o\n       de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas\n       reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        delta: float = 0.01,\n        lam: float = 0.98,\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.delta = delta\n        self.lam = lam\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n        self.xi: Optional[np.ndarray] = None\n        self.theta_evolution: Optional[np.ndarray] = None\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Recursive Least Squares method.\n\n        The implementation considers the forgetting factor.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        Notes\n        -----\n        The RLS algorithm updates the parameter estimates recursively as follows:\n\n        1. Initialize the parameter vector `theta` and the covariance matrix `P`:\n\n           $$\n           \\\\theta_0 = \\\\mathbf{0}, \\\\quad P_0 = \\\\frac{1}{\\\\delta} I\n           $$\n\n        2. For each new observation `(psi_i, y_i)`, update the estimates:\n\n           $$\n           k_i = \\\\frac{\\\\lambda^{-1} P_{i-1} \\\\psi_i}{1 +\n           \\\\lambda^{-1} \\\\psi_i^T P_{i-1} \\\\psi_i}\n           $$\n\n           $$\n           \\\\theta_i = \\\\theta_{i-1} + k_i (y_i - \\\\psi_i^T \\\\theta_{i-1})\n           $$\n\n           $$\n           P_i = \\\\lambda^{-1} (P_{i-1} - k_i \\\\psi_i^T P_{i-1})\n           $$\n\n        References\n        ----------\n        - Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o\n           de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas\n           reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n        \"\"\"\n        n_theta, n, theta, self.xi = _initial_values(psi)\n        p = np.eye(n_theta) / self.delta\n\n        for i in range(2, n):\n            psi_tmp = psi[i, :].reshape(-1, 1)\n            k_numerator = self.lam ** (-1) * p.dot(psi_tmp)\n            k_denominator = 1 + self.lam ** (-1) * psi_tmp.T.dot(p).dot(psi_tmp)\n            k = np.divide(k_numerator, k_denominator)\n            self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n            tmp_list = theta[:, i - 1].reshape(-1, 1) + k.dot(self.xi[i, 0])\n            theta[:, i] = tmp_list.flatten()\n\n            p1 = p.dot(psi[i, :].reshape(-1, 1)).dot(psi[i, :].reshape(-1, 1).T).dot(p)\n            p2 = (\n                psi[i, :].reshape(-1, 1).T.dot(p).dot(psi[i, :].reshape(-1, 1))\n                + self.lam\n            )\n\n            p_numerator = p - np.divide(p1, p2)\n            p = np.divide(p_numerator, self.lam)\n\n        self.theta_evolution = theta.copy()\n        return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RecursiveLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Recursive Least Squares method.</p> <p>The implementation considers the forgetting factor.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape = y_training</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> Notes <p>The RLS algorithm updates the parameter estimates recursively as follows:</p> <ol> <li>Initialize the parameter vector <code>theta</code> and the covariance matrix <code>P</code>:</li> </ol> <p>$$    \\theta_0 = \\mathbf{0}, \\quad P_0 = \\frac{1}{\\delta} I    $$</p> <ol> <li>For each new observation <code>(psi_i, y_i)</code>, update the estimates:</li> </ol> <p>$$    k_i = \\frac{\\lambda^{-1} P_{i-1} \\psi_i}{1 +    \\lambda^{-1} \\psi_i^T P_{i-1} \\psi_i}    $$</p> <p>$$    \\theta_i = \\theta_{i-1} + k_i (y_i - \\psi_i^T \\theta_{i-1})    $$</p> <p>$$    P_i = \\lambda^{-1} (P_{i-1} - k_i \\psi_i^T P_{i-1})    $$</p> References <ul> <li>Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o    de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas    reais. Editora da UFMG. 3a edi\u00e7\u00e3o.</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Recursive Least Squares method.\n\n    The implementation considers the forgetting factor.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    Notes\n    -----\n    The RLS algorithm updates the parameter estimates recursively as follows:\n\n    1. Initialize the parameter vector `theta` and the covariance matrix `P`:\n\n       $$\n       \\\\theta_0 = \\\\mathbf{0}, \\\\quad P_0 = \\\\frac{1}{\\\\delta} I\n       $$\n\n    2. For each new observation `(psi_i, y_i)`, update the estimates:\n\n       $$\n       k_i = \\\\frac{\\\\lambda^{-1} P_{i-1} \\\\psi_i}{1 +\n       \\\\lambda^{-1} \\\\psi_i^T P_{i-1} \\\\psi_i}\n       $$\n\n       $$\n       \\\\theta_i = \\\\theta_{i-1} + k_i (y_i - \\\\psi_i^T \\\\theta_{i-1})\n       $$\n\n       $$\n       P_i = \\\\lambda^{-1} (P_{i-1} - k_i \\\\psi_i^T P_{i-1})\n       $$\n\n    References\n    ----------\n    - Book (Portuguese): Aguirre, L. A. (2007). Introdu\u00e7\u00e3o identifica\u00e7\u00e3o\n       de sistemas: t\u00e9cnicas lineares e n\u00e3o-lineares aplicadas a sistemas\n       reais. Editora da UFMG. 3a edi\u00e7\u00e3o.\n    \"\"\"\n    n_theta, n, theta, self.xi = _initial_values(psi)\n    p = np.eye(n_theta) / self.delta\n\n    for i in range(2, n):\n        psi_tmp = psi[i, :].reshape(-1, 1)\n        k_numerator = self.lam ** (-1) * p.dot(psi_tmp)\n        k_denominator = 1 + self.lam ** (-1) * psi_tmp.T.dot(p).dot(psi_tmp)\n        k = np.divide(k_numerator, k_denominator)\n        self.xi[i, 0] = y[i, 0] - np.dot(psi_tmp.T, theta[:, i - 1])[0]\n        tmp_list = theta[:, i - 1].reshape(-1, 1) + k.dot(self.xi[i, 0])\n        theta[:, i] = tmp_list.flatten()\n\n        p1 = p.dot(psi[i, :].reshape(-1, 1)).dot(psi[i, :].reshape(-1, 1).T).dot(p)\n        p2 = (\n            psi[i, :].reshape(-1, 1).T.dot(p).dot(psi[i, :].reshape(-1, 1))\n            + self.lam\n        )\n\n        p_numerator = p - np.divide(p1, p2)\n        p = np.divide(p_numerator, self.lam)\n\n    self.theta_evolution = theta.copy()\n    return theta[:, -1].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression","title":"<code>RidgeRegression</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Ridge Regression estimator using classic and SVD methods.</p> <p>This class implements Ridge Regression, a type of linear regression that includes an L2 penalty to prevent overfitting. The implementation offers two methods for parameter estimation: a classic approach and an approach based on Singular Value Decomposition (SVD).</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>(float64, optional(default=eps))</code> <p>Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. If the input is a noisy signal, the ridge parameter is likely to be set close to the noise level, at least as a starting point. Entered through the self data structure.</p> <code>eps</code> <code>solver</code> <code>(str, optional(default=svd))</code> <p>Solver to use in the parameter estimation procedure.</p> <code>'svd'</code> <p>Methods:</p> Name Description <code>ridge_regression_classic</code> <p>Estimate the model parameters using the classic ridge regression method.</p> <code>ridge_regression</code> <p>Estimate the model parameters using the SVD-based ridge regression method.</p> <code>optimize</code> <p>Optimize the model parameters using the chosen method (SVD or classic).</p> References <ul> <li>Wikipedia entry on ridge regression   https://en.wikipedia.org/wiki/Ridge_regression</li> <li>D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation   reservoir computing,' Nat. Commun. 12, 5564 (2021).   https://www.nature.com/articles/s41467-021-25801-2</li> <li>Hoerl, A. E.; Kennard, R. W. Ridge regression: applications to nonorthogonal   problems. Technometrics, Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.</li> <li>StackExchange: whuber. The proof of shrinking coefficients using ridge regression   through \"spectral decomposition\".   Cross Validated, accessed 21 September 2023,   https://stats.stackexchange.com/q/220324</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>@deprecated(\n    version=\"v0.6.0\",\n    future_version=\"v1.0.0\",\n    message=(\n        \" `solver` is deprecated in v0.5.4 and will be removed in v1.0.0.\"\n        \" A single solver option will be retained moving forward.\"\n    ),\n)\nclass RidgeRegression(BaseEstimator):\n    \"\"\"Ridge Regression estimator using classic and SVD methods.\n\n    This class implements Ridge Regression, a type of linear regression that includes\n    an L2 penalty to prevent overfitting. The implementation offers two methods for\n    parameter estimation: a classic approach and an approach based on Singular Value\n    Decomposition (SVD).\n\n    Parameters\n    ----------\n    alpha : np.float64, optional (default=np.finfo(np.float64).eps)\n        Regularization strength; must be a positive float. Regularization improves the\n        conditioning of the problem and reduces the variance of the estimates. Larger\n        values specify stronger regularization. If the input is a noisy signal,\n        the ridge parameter is likely to be set close to the noise level, at least as\n        a starting point. Entered through the self data structure.\n    solver : str, optional (default=\"svd\")\n        Solver to use in the parameter estimation procedure.\n\n    Methods\n    -------\n    ridge_regression_classic(psi, y)\n        Estimate the model parameters using the classic ridge regression method.\n    ridge_regression(psi, y)\n        Estimate the model parameters using the SVD-based ridge regression method.\n    optimize(psi, y)\n        Optimize the model parameters using the chosen method (SVD or classic).\n\n    References\n    ----------\n    - Wikipedia entry on ridge regression\n      https://en.wikipedia.org/wiki/Ridge_regression\n    - D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation\n      reservoir computing,' Nat. Commun. 12, 5564 (2021).\n      https://www.nature.com/articles/s41467-021-25801-2\n    - Hoerl, A. E.; Kennard, R. W. Ridge regression: applications to nonorthogonal\n      problems. Technometrics, Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.\n    - StackExchange: whuber. The proof of shrinking coefficients using ridge regression\n      through \"spectral decomposition\".\n      Cross Validated, accessed 21 September 2023,\n      https://stats.stackexchange.com/q/220324\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        alpha: np.float64 = np.finfo(np.float64).eps,\n        solver: str = \"svd\",\n        unbiased: bool = False,\n        uiter: int = 30,\n    ):\n        self.alpha = alpha\n        self.solver = solver\n        self.uiter = uiter\n        self.unbiased = unbiased\n        _validate_params(vars(self))\n\n    def ridge_regression_classic(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using ridge regression.\n\n           Based on the least_squares module and uses the same data format but you need\n           to pass alpha in the call to FROLS.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Wikipedia entry on ridge regression\n          https://en.wikipedia.org/wiki/Ridge_regression\n\n        alpha multiplied by the identity matrix (np.eye) favors models (theta) that\n        have small size using an L2 norm.  This prevents over fitting of the model.\n        For applications where preventing overfitting is important, see, for example,\n        D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation\n        reservoir computing,' Nat. Commun. 12, 5564 (2021).\n        https://www.nature.com/articles/s41467-021-25801-2\n\n        \"\"\"\n        check_linear_dependence_rows(psi)\n\n        theta = (\n            np.linalg.pinv(psi.T @ psi + self.alpha * np.eye(psi.shape[1])) @ psi.T @ y\n        )\n        return theta\n\n    def ridge_regression(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Estimate the model parameters using SVD and Ridge Regression method.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape = y_training\n            The data used to training the model.\n\n        Returns\n        -------\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n\n        References\n        ----------\n        - Manuscript: Hoerl, A. E.; Kennard, R. W. Ridge regression:\n                      applications to nonorthogonal problems. Technometrics,\n                      Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.\n\n        - StackExchange: whuber. The proof of shrinking coefficients using ridge\n                         regression through \"spectral decomposition\".\n                         Cross Validated, accessed 21 September 2023,\n                         https://stats.stackexchange.com/q/220324\n        \"\"\"\n        check_linear_dependence_rows(psi)\n        try:\n            U, S, Vh = np.linalg.svd(psi, full_matrices=False)\n            S = np.diag(S)\n            i = np.identity(len(S))\n            theta = Vh.T @ np.linalg.inv(S**2 + self.alpha * i) @ S @ U.T @ y\n        except EstimatorError:\n            warnings.warn(\n                \"The SVD computation did not converge.\"\n                \"Theta values will be calculated with the classic algorithm.\",\n                stacklevel=2,\n            )\n\n            theta = self.ridge_regression_classic(psi, y)\n\n        return theta\n\n    def optimize(self, psi: np.ndarray, y):\n        if self.solver == \"svd\":\n            return self.ridge_regression(psi, y)\n\n        return self.ridge_regression_classic(psi, y)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression","title":"<code>ridge_regression(psi, y)</code>","text":"<p>Estimate the model parameters using SVD and Ridge Regression method.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape = y_training</code> <p>The data used to training the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> References <ul> <li> <p>Manuscript: Hoerl, A. E.; Kennard, R. W. Ridge regression:               applications to nonorthogonal problems. Technometrics,               Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.</p> </li> <li> <p>StackExchange: whuber. The proof of shrinking coefficients using ridge                  regression through \"spectral decomposition\".                  Cross Validated, accessed 21 September 2023,                  https://stats.stackexchange.com/q/220324</p> </li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def ridge_regression(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using SVD and Ridge Regression method.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Manuscript: Hoerl, A. E.; Kennard, R. W. Ridge regression:\n                  applications to nonorthogonal problems. Technometrics,\n                  Taylor &amp; Francis, v. 12, n. 1, p. 69-82, 1970.\n\n    - StackExchange: whuber. The proof of shrinking coefficients using ridge\n                     regression through \"spectral decomposition\".\n                     Cross Validated, accessed 21 September 2023,\n                     https://stats.stackexchange.com/q/220324\n    \"\"\"\n    check_linear_dependence_rows(psi)\n    try:\n        U, S, Vh = np.linalg.svd(psi, full_matrices=False)\n        S = np.diag(S)\n        i = np.identity(len(S))\n        theta = Vh.T @ np.linalg.inv(S**2 + self.alpha * i) @ S @ U.T @ y\n    except EstimatorError:\n        warnings.warn(\n            \"The SVD computation did not converge.\"\n            \"Theta values will be calculated with the classic algorithm.\",\n            stacklevel=2,\n        )\n\n        theta = self.ridge_regression_classic(psi, y)\n\n    return theta\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.RidgeRegression.ridge_regression_classic","title":"<code>ridge_regression_classic(psi, y)</code>","text":"<p>Estimate the model parameters using ridge regression.</p> <p>Based on the least_squares module and uses the same data format but you need    to pass alpha in the call to FROLS.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape = y_training</code> <p>The data used to training the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> References <ul> <li>Wikipedia entry on ridge regression   https://en.wikipedia.org/wiki/Ridge_regression</li> </ul> <p>alpha multiplied by the identity matrix (np.eye) favors models (theta) that have small size using an L2 norm.  This prevents over fitting of the model. For applications where preventing overfitting is important, see, for example, D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation reservoir computing,' Nat. Commun. 12, 5564 (2021). https://www.nature.com/articles/s41467-021-25801-2</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def ridge_regression_classic(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Estimate the model parameters using ridge regression.\n\n       Based on the least_squares module and uses the same data format but you need\n       to pass alpha in the call to FROLS.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape = y_training\n        The data used to training the model.\n\n    Returns\n    -------\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n\n    References\n    ----------\n    - Wikipedia entry on ridge regression\n      https://en.wikipedia.org/wiki/Ridge_regression\n\n    alpha multiplied by the identity matrix (np.eye) favors models (theta) that\n    have small size using an L2 norm.  This prevents over fitting of the model.\n    For applications where preventing overfitting is important, see, for example,\n    D. J. Gauthier, E. Bollt, A. Griffith, W. A. S. Barbosa, 'Next generation\n    reservoir computing,' Nat. Commun. 12, 5564 (2021).\n    https://www.nature.com/articles/s41467-021-25801-2\n\n    \"\"\"\n    check_linear_dependence_rows(psi)\n\n    theta = (\n        np.linalg.pinv(psi.T @ psi + self.alpha * np.eye(psi.shape[1])) @ psi.T @ y\n    )\n    return theta\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares","title":"<code>TotalLeastSquares</code>","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Estimate the model parameters using the Total Least Squares (TLS) method.</p> <p>The Total Least Squares method is used to solve the problem of fitting a model to data when both the independent variables (psi) and the dependent variable (y) are subject to errors. This method minimizes the orthogonal distances from the data points to the fitted model, which is more appropriate when errors are present in all variables.</p> <p>Parameters:</p> Name Type Description Default <code>unbiased</code> <code>bool</code> <p>If True, applies an unbiased estimator. Default is False.</p> <code>False</code> <code>uiter</code> <code>int</code> <p>Number of iterations for the unbiased estimator. Default is 30.</p> <code>30</code> References <ul> <li>Golub, G. H., &amp; Van Loan, C. F. (1980). An analysis of the total least squares problem.   SIAM journal on numerical analysis, 17(6), 883-893.</li> <li>Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least-squares methods.   Signal processing, 87(10), 2283-2302. https://eprints.soton.ac.uk/263855/1/tls_overview.pdf</li> <li>Wikipedia entry on Total Least Squares: https://en.wikipedia.org/wiki/Total_least_squares</li> </ul> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>class TotalLeastSquares(BaseEstimator):\n    \"\"\"Estimate the model parameters using the Total Least Squares (TLS) method.\n\n    The Total Least Squares method is used to solve the problem of fitting a model\n    to data when both the independent variables (psi) and the dependent variable (y)\n    are subject to errors. This method minimizes the orthogonal distances from the\n    data points to the fitted model, which is more appropriate when errors are present\n    in all variables.\n\n    Parameters\n    ----------\n    unbiased : bool, optional\n        If True, applies an unbiased estimator. Default is False.\n    uiter : int, optional\n        Number of iterations for the unbiased estimator. Default is 30.\n\n    References\n    ----------\n    - Golub, G. H., &amp; Van Loan, C. F. (1980). An analysis of the total least squares\n    problem.\n      SIAM journal on numerical analysis, 17(6), 883-893.\n    - Markovsky, I., &amp; Van Huffel, S. (2007). Overview of total least-squares methods.\n      Signal processing, 87(10), 2283-2302. https://eprints.soton.ac.uk/263855/1/tls_overview.pdf\n    - Wikipedia entry on Total Least Squares: https://en.wikipedia.org/wiki/Total_least_squares\n    \"\"\"\n\n    def __init__(self, *, unbiased: bool = False, uiter: int = 30):\n        self.unbiased = unbiased\n        self.uiter = uiter\n        _validate_params(vars(self))\n\n    def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        r\"\"\"Estimate the model parameters using the Total Least Squares method.\n\n        The TLS method solves the following problem:\n\n        $$\n            \\min_{E, f} \\| [E, f] \\|_F \\quad \\text{subject to}\n            \\quad (psi + E) \\theta = y + f\n        $$\n\n        where $E$ and $f$ are the error matrices for $psi$ and $y$ respectively,\n        and $\\| \\cdot \\|_F$ denotes the Frobenius norm.\n\n        Parameters\n        ----------\n        psi : ndarray of floats\n            The information matrix of the model.\n        y : array-like of shape (n_samples, 1)\n            The data used to train the model.\n\n        Returns\n        -------\n        theta : array-like of shape (n_features, 1)\n            The estimated parameters of the model.\n        \"\"\"\n        check_linear_dependence_rows(psi)\n        full = np.hstack((psi, y))\n        n = psi.shape[1]\n        _, _, v = np.linalg.svd(full, full_matrices=True)\n        theta = -v.T[:n, n:] / v.T[n:, n:]\n        return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/parameter-estimation/#sysidentpy.parameter_estimation.estimators.TotalLeastSquares.optimize","title":"<code>optimize(psi, y)</code>","text":"<p>Estimate the model parameters using the Total Least Squares method.</p> <p>The TLS method solves the following problem:</p> \\[     \\min_{E, f} \\| [E, f] \\|_F \\quad \\text{subject to}     \\quad (psi + E) \\theta = y + f \\] <p>where \\(E\\) and \\(f\\) are the error matrices for \\(psi\\) and \\(y\\) respectively, and \\(\\| \\cdot \\|_F\\) denotes the Frobenius norm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array-like of shape (n_samples, 1)</code> <p>The data used to train the model.</p> required <p>Returns:</p> Name Type Description <code>theta</code> <code>array-like of shape (n_features, 1)</code> <p>The estimated parameters of the model.</p> Source code in <code>sysidentpy/parameter_estimation/estimators.py</code> <pre><code>def optimize(self, psi: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    r\"\"\"Estimate the model parameters using the Total Least Squares method.\n\n    The TLS method solves the following problem:\n\n    $$\n        \\min_{E, f} \\| [E, f] \\|_F \\quad \\text{subject to}\n        \\quad (psi + E) \\theta = y + f\n    $$\n\n    where $E$ and $f$ are the error matrices for $psi$ and $y$ respectively,\n    and $\\| \\cdot \\|_F$ denotes the Frobenius norm.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n    y : array-like of shape (n_samples, 1)\n        The data used to train the model.\n\n    Returns\n    -------\n    theta : array-like of shape (n_features, 1)\n        The estimated parameters of the model.\n    \"\"\"\n    check_linear_dependence_rows(psi)\n    full = np.hstack((psi, y))\n    n = psi.shape[1]\n    _, _, v = np.linalg.svd(full, full_matrices=True)\n    theta = -v.T[:n, n:] / v.T[n:, n:]\n    return theta.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/API/residues/","title":"Documentation for <code>Residual Analysis</code>","text":""},{"location":"user-guide/API/residues/#sysidentpy.residues.residues_correlation.calculate_residues","title":"<code>calculate_residues(y, yhat)</code>","text":"<p>Calculate the residues (errors) between true and predicted values.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape (n_samples,)</code> <p>True values.</p> required <code>yhat</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>residues</code> <code>ndarray of shape (n_samples,)</code> <p>Residues (errors) between true and predicted values.</p> Source code in <code>sysidentpy/residues/residues_correlation.py</code> <pre><code>def calculate_residues(y, yhat):\n    \"\"\"Calculate the residues (errors) between true and predicted values.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        True values.\n    yhat : array-like of shape (n_samples,)\n        Predicted values.\n\n    Returns\n    -------\n    residues : ndarray of shape (n_samples,)\n        Residues (errors) between true and predicted values.\n    \"\"\"\n    return (y - yhat).flatten()\n</code></pre>"},{"location":"user-guide/API/residues/#sysidentpy.residues.residues_correlation.compute_cross_correlation","title":"<code>compute_cross_correlation(y, yhat, arr)</code>","text":"<p>Compute the cross-correlation between the residues and another array.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape (n_samples,)</code> <p>True values.</p> required <code>yhat</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <code>arr</code> <code>array-like of shape (n_samples,)</code> <p>Another array to compute the cross-correlation with.</p> required <p>Returns:</p> Name Type Description <code>ccf</code> <code>ndarray of shape (n_samples,)</code> <p>Cross-correlation function.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound for the confidence interval.</p> <code>lower_bound</code> <code>float</code> <p>Lower bound for the confidence interval.</p> Source code in <code>sysidentpy/residues/residues_correlation.py</code> <pre><code>def compute_cross_correlation(y, yhat, arr):\n    \"\"\"Compute the cross-correlation between the residues and another array.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        True values.\n    yhat : array-like of shape (n_samples,)\n        Predicted values.\n    arr : array-like of shape (n_samples,)\n        Another array to compute the cross-correlation with.\n\n    Returns\n    -------\n    ccf : ndarray of shape (n_samples,)\n        Cross-correlation function.\n    upper_bound : float\n        Upper bound for the confidence interval.\n    lower_bound : float\n        Lower bound for the confidence interval.\n    \"\"\"\n    e = calculate_residues(y, yhat)\n    n = len(e) * 2 - 1\n    ccf, upper_bound, lower_bound = _input_ccf(e, arr, n)\n    return ccf, upper_bound, lower_bound\n</code></pre>"},{"location":"user-guide/API/residues/#sysidentpy.residues.residues_correlation.compute_residues_autocorrelation","title":"<code>compute_residues_autocorrelation(y, yhat)</code>","text":"<p>Compute the autocorrelation of the residues.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array-like of shape (n_samples,)</code> <p>True values.</p> required <code>yhat</code> <code>array-like of shape (n_samples,)</code> <p>Predicted values.</p> required <p>Returns:</p> Name Type Description <code>e_acf</code> <code>ndarray of shape (n_samples,)</code> <p>Autocorrelation of the residues.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound for the confidence interval.</p> <code>lower_bound</code> <code>float</code> <p>Lower bound for the confidence interval.</p> Source code in <code>sysidentpy/residues/residues_correlation.py</code> <pre><code>def compute_residues_autocorrelation(y, yhat):\n    \"\"\"Compute the autocorrelation of the residues.\n\n    Parameters\n    ----------\n    y : array-like of shape (n_samples,)\n        True values.\n    yhat : array-like of shape (n_samples,)\n        Predicted values.\n\n    Returns\n    -------\n    e_acf : ndarray of shape (n_samples,)\n        Autocorrelation of the residues.\n    upper_bound : float\n        Upper bound for the confidence interval.\n    lower_bound : float\n        Lower bound for the confidence interval.\n    \"\"\"\n    e = calculate_residues(y, yhat)\n    unnormalized_e_acf = get_unnormalized_e_acf(e)\n    half_of_symmetry_autocorr = int(np.floor(unnormalized_e_acf.size / 2))\n\n    e_acf = (\n        unnormalized_e_acf[half_of_symmetry_autocorr:]\n        / unnormalized_e_acf[half_of_symmetry_autocorr]\n    )\n\n    upper_bound = 1.96 / np.sqrt(len(unnormalized_e_acf))\n    lower_bound = upper_bound * (-1)\n    return e_acf, upper_bound, lower_bound\n</code></pre>"},{"location":"user-guide/API/residues/#sysidentpy.residues.residues_correlation.get_unnormalized_e_acf","title":"<code>get_unnormalized_e_acf(e)</code>","text":"<p>Compute the unnormalized autocorrelation function of the residues.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>array-like of shape (n_samples,)</code> <p>Residues (errors).</p> required <p>Returns:</p> Name Type Description <code>unnormalized_e_acf</code> <code>ndarray of shape (2*n_samples-1,)</code> <p>Unnormalized autocorrelation function of the residues.</p> Source code in <code>sysidentpy/residues/residues_correlation.py</code> <pre><code>def get_unnormalized_e_acf(e):\n    \"\"\"Compute the unnormalized autocorrelation function of the residues.\n\n    Parameters\n    ----------\n    e : array-like of shape (n_samples,)\n        Residues (errors).\n\n    Returns\n    -------\n    unnormalized_e_acf : ndarray of shape (2*n_samples-1,)\n        Unnormalized autocorrelation function of the residues.\n    \"\"\"\n    return np.correlate(e, e, mode=\"full\")\n</code></pre>"},{"location":"user-guide/API/simulation/","title":"Documentation for <code>Simulation</code>","text":"<p>Simulation methods for NARMAX models.</p>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX","title":"<code>SimulateNARMAX</code>","text":"<p>               Bases: <code>BaseMSS</code></p> <p>Simulates a Polynomial NARMAX model.</p> <p>The NARMAX (Nonlinear AutoRegressive Moving Average with eXogenous inputs) model is described as:</p> \\[ y_k = \\mathcal{F}^\\ell \\Big[y_{k-1}, \\dotsc, y_{k-n_y}, x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e} \\Big] + e_k \\] <p>where:</p> <ul> <li>$ n_y \\in \\mathbb{N}^* $, $ n_x \\in \\mathbb{N} $, and $ n_e \\in \\mathbb{N} $ are     the maximum lags for the system output, input, and noise, respectively.</li> <li>$ x_k \\in \\mathbb{R}^{n_x} $ is the system input, and $ y_k \\in \\mathbb{R}^{n_y} $     is the system output at discrete time $ k \\in \\mathbb{N} $.</li> <li>$ e_k \\in \\mathbb{R}^{n_e} $ represents uncertainties and possible noise at     discrete time $ k $.</li> <li>$ \\mathcal{F}^\\ell $ is a nonlinear function of the input and output regressors     with nonlinearity degree $ \\ell \\in \\mathbb{N} $.</li> <li>$ d $ is a time delay, typically set to $ d=1 $.</li> </ul> <p>This class provides tools for simulating NARMAX models using a chosen basis function and estimation method.</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>Estimators</code> <p>The parameter estimation method used for model identification.</p> <code>RecursiveLeastSquares()</code> <code>elag</code> <code>int or list</code> <p>Specifies the maximum lags for the error variables. If an integer, it applies to both input and output lags. If a list, it should contain specific lag values for different variables.</p> <code>2</code> <code>estimate_parameter</code> <code>bool</code> <p>Whether to estimate model parameters. Set to <code>True</code> unless pre-estimated parameters are provided.</p> <code>True</code> <code>calculate_err</code> <code>bool</code> <p>If <code>True</code>, uses the Error Reduction Ratio (ERR) algorithm to select regressors.</p> <code>False</code> <code>model_type</code> <code>str</code> <p>Defines the model type. Supported values: <code>\"NARMAX\"</code>, <code>\"ARX\"</code>, <code>\"OE\"</code>, etc.</p> <code>\"NARMAX\"</code> <code>basis_function</code> <code>Polynomial or Fourier</code> <p>The basis function used to define the model's nonlinear terms.</p> <code>Polynomial()</code> <code>eps</code> <code>float</code> <p>A small numerical constant used for normalization.</p> <code>np.finfo(np.float64).eps</code> <p>Attributes:</p> Name Type Description <code>n_inputs</code> <code>int</code> <p>Number of input variables.</p> <code>xlag</code> <code>int or list</code> <p>Lags for the input variables.</p> <code>ylag</code> <code>int or list</code> <p>Lags for the output variables.</p> <code>n_terms</code> <code>int</code> <p>Number of terms in the final model.</p> <code>err</code> <code>array - like</code> <p>Error Reduction Ratio (ERR) values for the selected regressors.</p> <code>final_model</code> <code>array - like</code> <p>The structure of the identified model.</p> <code>theta</code> <code>array - like</code> <p>Estimated parameters of the model.</p> <code>pivv</code> <code>array - like</code> <p>Pivot vector for variable selection.</p> <code>non_degree</code> <code>int</code> <p>Degree of nonlinearity used in the model.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; x_train = np.random.rand(1000, 1)\n&gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n&gt;&gt;&gt; model = np.array([\n...     [1001, 0],       # y(k-1)\n...     [2001, 1001],    # x1(k-1)y(k-1)\n...     [2002, 0]        # x1(k-2)\n... ])\n&gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Model parameters\n&gt;&gt;&gt; y_pred = simulator.simulate(\n...     X_test=x_train, y_test=y_train,\n...     model_code=model, theta=theta\n... )\n</code></pre> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>class SimulateNARMAX(BaseMSS):\n    r\"\"\"Simulates a Polynomial NARMAX model.\n\n    The NARMAX (Nonlinear AutoRegressive Moving Average with eXogenous inputs) model\n    is described as:\n\n    $$\n    y_k = \\mathcal{F}^\\ell \\Big[y_{k-1}, \\dotsc, y_{k-n_y}, x_{k-d}, x_{k-d-1},\n    \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e} \\Big] + e_k\n    $$\n\n    where:\n\n    - $ n_y \\in \\mathbb{N}^* $, $ n_x \\in \\mathbb{N} $, and $ n_e \\in \\mathbb{N} $ are\n        the maximum lags for the system output, input, and noise, respectively.\n    - $ x_k \\in \\mathbb{R}^{n_x} $ is the system input, and $ y_k \\in \\mathbb{R}^{n_y} $\n        is the system output at discrete time $ k \\in \\mathbb{N} $.\n    - $ e_k \\in \\mathbb{R}^{n_e} $ represents uncertainties and possible noise at\n        discrete time $ k $.\n    - $ \\mathcal{F}^\\ell $ is a nonlinear function of the input and output regressors\n        with nonlinearity degree $ \\ell \\in \\mathbb{N} $.\n    - $ d $ is a time delay, typically set to $ d=1 $.\n\n    This class provides tools for simulating NARMAX models using a chosen basis function\n    and estimation method.\n\n    Parameters\n    ----------\n    estimator : Estimators, default=RecursiveLeastSquares()\n        The parameter estimation method used for model identification.\n    elag : int or list, default=2\n        Specifies the maximum lags for the error variables.\n        If an integer, it applies to both input and output lags.\n        If a list, it should contain specific lag values for different variables.\n    estimate_parameter : bool, default=True\n        Whether to estimate model parameters. Set to `True` unless pre-estimated\n        parameters are provided.\n    calculate_err : bool, default=False\n        If `True`, uses the Error Reduction Ratio (ERR) algorithm to select regressors.\n    model_type : str, default=\"NARMAX\"\n        Defines the model type. Supported values: `\"NARMAX\"`, `\"ARX\"`, `\"OE\"`, etc.\n    basis_function : Polynomial or Fourier, default=Polynomial()\n        The basis function used to define the model's nonlinear terms.\n    eps : float, default=np.finfo(np.float64).eps\n        A small numerical constant used for normalization.\n\n    Attributes\n    ----------\n    n_inputs : int\n        Number of input variables.\n    xlag : int or list\n        Lags for the input variables.\n    ylag : int or list\n        Lags for the output variables.\n    n_terms : int\n        Number of terms in the final model.\n    err : array-like\n        Error Reduction Ratio (ERR) values for the selected regressors.\n    final_model : array-like\n        The structure of the identified model.\n    theta : array-like\n        Estimated parameters of the model.\n    pivv : array-like\n        Pivot vector for variable selection.\n    non_degree : int\n        Degree of nonlinearity used in the model.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; x_train = np.random.rand(1000, 1)\n    &gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n    &gt;&gt;&gt; model = np.array([\n    ...     [1001, 0],       # y(k-1)\n    ...     [2001, 1001],    # x1(k-1)y(k-1)\n    ...     [2002, 0]        # x1(k-2)\n    ... ])\n    &gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Model parameters\n    &gt;&gt;&gt; y_pred = simulator.simulate(\n    ...     X_test=x_train, y_test=y_train,\n    ...     model_code=model, theta=theta\n    ... )\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        estimator: Estimators = RecursiveLeastSquares(),\n        elag: Union[int, list] = 2,\n        estimate_parameter: bool = True,\n        calculate_err: bool = False,\n        model_type: str = \"NARMAX\",\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        eps: np.float64 = np.finfo(np.float64).eps,\n    ):\n        self.elag = elag\n        self.model_type = model_type\n        self.basis_function = basis_function\n        self.estimator = estimator\n        self.estimate_parameter = estimate_parameter\n        self.calculate_err = calculate_err\n        self.eps = eps\n        self.n_inputs = None\n        self.xlag = None\n        self.ylag = None\n        self.n_terms = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n        self.non_degree = None\n        self._validate_simulate_params()\n\n    def _validate_simulate_params(self):\n        if not isinstance(self.estimate_parameter, bool):\n            raise TypeError(\n                \"estimate_parameter must be False or True. Got\"\n                f\" {self.estimate_parameter}\"\n            )\n\n        if not isinstance(self.calculate_err, bool):\n            raise TypeError(\n                f\"calculate_err must be False or True. Got {self.calculate_err}\"\n            )\n\n        if self.basis_function is None:\n            raise TypeError(f\"basis_function can't be. Got {self.basis_function}\")\n\n        if self.model_type not in [\"NARMAX\", \"NAR\", \"NFIR\"]:\n            raise ValueError(\n                f\"model_type must be NARMAX, NAR, or NFIR. Got {self.model_type}\"\n            )\n\n    def _check_simulate_params(self, y_train, y_test, model_code, steps_ahead, theta):\n        if not isinstance(self.basis_function, Polynomial):\n            raise NotImplementedError(\n                \"Currently, SimulateNARMAX only works for polynomial models.\"\n            )\n\n        if y_test is None:\n            raise ValueError(\"y_test cannot be None\")\n\n        if not isinstance(model_code, np.ndarray):\n            raise TypeError(f\"model_code must be an np.np.ndarray. Got {model_code}\")\n\n        if not isinstance(steps_ahead, (int, type(None))):\n            raise ValueError(\n                f\"steps_ahead must be None or integer &gt; zero. Got {steps_ahead}\"\n            )\n\n        if not isinstance(theta, np.ndarray) and not self.estimate_parameter:\n            raise TypeError(\n                \"If estimate_parameter is False, theta must be an np.ndarray. Got\"\n                f\" {theta}\"\n            )\n\n        if self.estimate_parameter:\n            if not all(isinstance(i, np.ndarray) for i in [y_train]):\n                raise TypeError(\n                    \"If estimate_parameter is True, X_train and y_train must be an\"\n                    f\" np.ndarray. Got {type(y_train)}\"\n                )\n\n    def simulate(\n        self,\n        *,\n        X_train=None,\n        y_train=None,\n        X_test=None,\n        y_test=None,\n        model_code=None,\n        steps_ahead=None,\n        theta=None,\n        forecast_horizon=None,\n    ):\n        \"\"\"Simulate the response of a NARMAX model based on user-defined parameters.\n\n        This method simulates the system's response using a predefined model structure\n        (`model_code`) and estimated parameters (`theta`). It allows for both\n        training-based parameter estimation and direct simulation using precomputed\n        parameters.\n\n        Parameters\n        ----------\n        X_train : array-like, shape (n_samples, n_features), optional\n            Input data used for parameter estimation during training.\n            Required if `estimate_parameter=True`.\n        y_train : array-like, shape (n_samples, 1), optional\n            Output (target) data used for parameter estimation during training.\n            Required if `estimate_parameter=True`.\n        X_test : array-like, shape (n_samples, n_features), optional\n            Input data used for simulation (prediction).\n        y_test : array-like, shape (n_samples, 1), optional\n            Output data used as initial conditions for simulation.\n        model_code : array-like, shape (n_terms, n_columns)\n            Encoded representation of the model's regressors, defining\n            the input-output relationships in the system.\n        steps_ahead : int, optional\n            Number of steps ahead for multi-step prediction. If `None`, defaults to\n            one-step-ahead prediction.\n        theta : array-like, shape (n_terms, 1), optional\n            Precomputed model parameters. Required if `estimate_parameter=False`.\n        forecast_horizon : int, optional\n            Number of time steps to predict in open-loop forecasting.\n            Used mainly for NAR and NARMA-type models.\n\n        Returns\n        -------\n        yhat : array-like, shape (n_samples, 1)\n            Predicted output values of the system based on the given inputs.\n\n        Raises\n        ------\n        ValueError\n            If necessary parameters are missing, such as `y_train` when\n            `estimate_parameter=True` or `theta` when `estimate_parameter=False`.\n\n        Notes\n        -----\n        - If `estimate_parameter=True`, the method first estimates the parameters using\n        the provided training data (`X_train`, `y_train`) and the chosen basis function.\n        - If `estimate_parameter=False`, the method assumes `theta` contains the model\n            parameters.\n        - The forecast horizon is automatically adjusted for NAR models if not provided.\n        - The method internally computes the lag structure based on `model_code` to\n            define regressors.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n        &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n        &gt;&gt;&gt; X_train = np.random.rand(1000, 1)\n        &gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n        &gt;&gt;&gt; X_test = np.random.rand(200, 1)\n        &gt;&gt;&gt; y_test = np.random.rand(200, 1)\n        &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n        &gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n        &gt;&gt;&gt; model = np.array([\n        ...     [1001, 0],       # y(k-1)\n        ...     [2001, 1001],    # x1(k-1)y(k-1)\n        ...     [2002, 0]        # x1(k-2)\n        ... ])\n        &gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Precomputed model parameters\n        &gt;&gt;&gt; y_pred = simulator.simulate(\n        ...     X_train=X_train, y_train=y_train,\n        ...     X_test=X_test, y_test=y_test,\n        ...     model_code=model, theta=theta\n        ... )\n\n        \"\"\"\n        self._check_simulate_params(y_train, y_test, model_code, steps_ahead, theta)\n\n        if X_test is not None:\n            self.n_inputs = num_features(X_test)\n        else:\n            self.n_inputs = 1  # just to create the regressor space base\n\n        xlag_code = list_input_regressor_code(model_code)\n        ylag_code = list_output_regressor_code(model_code)\n        self.xlag = get_lag_from_regressor_code(xlag_code)\n        self.ylag = get_lag_from_regressor_code(ylag_code)\n        self.max_lag = max(self.xlag, self.ylag)\n        if self.n_inputs != 1:\n            self.xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n        # for MetaMSS NAR modelling\n        if self.model_type == \"NAR\" and forecast_horizon is None:\n            forecast_horizon = y_test.shape[0] - self.max_lag\n\n        self.non_degree = model_code.shape[1]\n        regressor_code = self.regressor_space(self.n_inputs)\n\n        self.pivv = get_index_from_regressor_code(regressor_code, model_code)\n        self.final_model = regressor_code[self.pivv]\n        # to use in the predict function\n        self.n_terms = self.final_model.shape[0]\n        if self.estimate_parameter and not self.calculate_err:\n            self.max_lag = self._get_max_lag()\n            lagged_data = build_lagged_matrix(\n                X_train, y_train, self.xlag, self.ylag, self.model_type\n            )\n            psi = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=self.pivv,\n            )\n\n            self.theta = self.estimator.optimize(\n                psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n            )\n            if self.estimator.unbiased is True:\n                self.theta = self.estimator.unbiased_estimator(\n                    psi,\n                    y_train[self.max_lag :, 0].reshape(-1, 1),\n                    self.theta,\n                    self.elag,\n                    self.max_lag,\n                    self.estimator,\n                    self.basis_function,\n                    self.estimator.uiter,\n                )\n\n            self.err = self.n_terms * [0]\n        elif not self.estimate_parameter:\n            self.theta = theta\n            self.err = self.n_terms * [0]\n        else:\n            self.max_lag = self._get_max_lag()\n            lagged_data = build_lagged_matrix(\n                X_train, y_train, self.xlag, self.ylag, self.model_type\n            )\n            psi = self.basis_function.fit(\n                lagged_data,\n                self.max_lag,\n                self.ylag,\n                self.xlag,\n                self.model_type,\n                predefined_regressors=self.pivv,\n            )\n\n            _, self.err, _, _ = self.error_reduction_ratio(\n                psi, y_train, self.n_terms, self.final_model\n            )\n            self.theta = self.estimator.optimize(\n                psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n            )\n            if self.estimator.unbiased is True:\n                self.theta = self.estimator.unbiased_estimator(\n                    psi,\n                    y_train[self.max_lag :, 0].reshape(-1, 1),\n                    self.theta,\n                    self.elag,\n                    self.max_lag,\n                    self.estimator,\n                    self.basis_function,\n                    self.estimator.uiter,\n                )\n\n        return self.predict(\n            X=X_test,\n            y=y_test,\n            steps_ahead=steps_ahead,\n            forecast_horizon=forecast_horizon,\n        )\n\n    def error_reduction_ratio(self, psi, y, process_term_number, regressor_code):\n        \"\"\"Perform the Error Reduction Ration algorithm.\n\n        Parameters\n        ----------\n        psi : array_like\n            The information matrix of the model.\n        y : array-like\n            The target data used in the identification process.\n        process_term_number : int\n            Number of Process Terms defined by the user.\n        regressor_code : array_like\n            The regressor code list given the xlag and ylag for a MISO model.\n\n        Returns\n        -------\n        model_code : array_like\n            Model defined by the user to simulate.\n        err : array-like\n            The respective ERR calculated for each regressor.\n        piv : array-like\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        psi_orthogonal : array_like\n            The updated and orthogonal information matrix.\n\n        References\n        ----------\n        - Manuscript: Orthogonal least squares methods and their application\n           to non-linear system identification\n           https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n        - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n           Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n           e Novos Resultados\n\n        \"\"\"\n        squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n        tmp_psi = psi.copy()\n        y = y[self.max_lag :, 0].reshape(-1, 1)\n        tmp_y = y.copy()\n        dimension = tmp_psi.shape[1]\n        piv = np.arange(dimension)\n        tmp_err = np.zeros(dimension)\n        err = np.zeros(dimension)\n\n        for i in np.arange(0, dimension):\n            for j in np.arange(i, dimension):\n                # Add `eps` in the denominator to omit division by zero if\n                # denominator is zero\n                tmp_err[j] = (\n                    (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                    / (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) * squared_y + self.eps)\n                )[0, 0]\n\n            if i == process_term_number:\n                break\n\n            piv_index = np.argmax(tmp_err[i:]) + i\n            err[i] = tmp_err[piv_index]\n            tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n            piv[[piv_index, i]] = piv[[i, piv_index]]\n\n            v = house(tmp_psi[i:, i])\n\n            row_result = rowhouse(tmp_psi[i:, i:], v)\n\n            tmp_y[i:] = rowhouse(tmp_y[i:], v)\n\n            tmp_psi[i:, i:] = np.copy(row_result)\n\n        tmp_piv = piv[0:process_term_number]\n        psi_orthogonal = psi[:, tmp_piv]\n        model_code = regressor_code[tmp_piv, :].copy()\n        return model_code, err, piv, psi_orthogonal\n\n    def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n        \"\"\"Return the predicted values given an input.\n\n        The predict function allows a friendly usage by the user.\n        Given a previously trained model, predict values given\n        a new set of data.\n\n        This method accept y values mainly for prediction n-steps ahead\n        (to be implemented in the future)\n\n        Parameters\n        ----------\n        X : array_like\n            The input data to be used in the prediction process.\n        y : array_like\n            The output data to be used in the prediction process.\n        steps_ahead : int\n            The user can use free run simulation, one-step ahead prediction\n            and n-step ahead prediction. The default is None\n        forecast_horizon : int\n            The number of predictions over the time. The default is None\n\n        Returns\n        -------\n        yhat : array_like\n            The predicted values of the model.\n\n        \"\"\"\n        if isinstance(self.basis_function, Polynomial):\n            if steps_ahead is None:\n                yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n            if steps_ahead == 1:\n                yhat = self._one_step_ahead_prediction(X, y)\n                yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n                return yhat\n\n            check_positive_int(steps_ahead, \"steps_ahead\")\n            yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        if steps_ahead is None:\n            yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        yhat = self._basis_function_n_step_prediction(\n            X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    def _one_step_ahead_prediction(self, x, y):\n        \"\"\"Perform the 1-step-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : array_like of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : array_like\n               The 1-step-ahead predicted values of the model.\n\n        \"\"\"\n        lagged_data = build_lagged_matrix(x, y, self.xlag, self.ylag, self.model_type)\n        x_base = self.basis_function.transform(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv[: len(self.final_model)],\n        )\n\n        yhat = super()._one_step_ahead_prediction(x_base)\n        return yhat.reshape(-1, 1)\n\n    def _n_step_ahead_prediction(self, x, y, steps_ahead):\n        \"\"\"Perform the n-steps-ahead prediction of a model.\n\n        Parameters\n        ----------\n        y : array-like of shape = max_lag\n            Initial conditions values of the model\n            to start recursive process.\n        x : array_like of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : array_like\n               The n-steps-ahead predicted values of the model.\n\n        \"\"\"\n        yhat = super()._n_step_ahead_prediction(x, y, steps_ahead)\n        return yhat\n\n    def _model_prediction(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Perform the infinity steps-ahead simulation of a model.\n\n        Parameters\n        ----------\n        y_initial : array-like of shape = max_lag\n            Number of initial conditions values of output\n            to start recursive process.\n        x : array_like of shape = n_samples\n            Vector with input values to be used in model simulation.\n\n        Returns\n        -------\n        yhat : array_like\n               The predicted values of the model.\n\n        \"\"\"\n        if self.model_type in [\"NARMAX\", \"NAR\"]:\n            return self._narmax_predict(x, y_initial, forecast_horizon)\n        if self.model_type == \"NFIR\":\n            return self._nfir_predict(x, y_initial)\n\n        raise ValueError(\n            f\"model_type must be NARMAX, NAR or NFIR. Got {self.model_type}\"\n        )\n\n    def _narmax_predict(self, x, y_initial, forecast_horizon):\n        if len(y_initial) &lt; self.max_lag:\n            raise ValueError(\n                \"Insufficient initial condition elements! Expected at least\"\n                f\" {self.max_lag} elements.\"\n            )\n\n        if x is not None:\n            forecast_horizon = x.shape[0]\n        else:\n            forecast_horizon = forecast_horizon + self.max_lag\n\n        if self.model_type == \"NAR\":\n            self.n_inputs = 0\n\n        y_output = super()._narmax_predict(x, y_initial, forecast_horizon)\n        return y_output\n\n    def _nfir_predict(self, x, y_initial):\n        y_output = super()._nfir_predict(x, y_initial)\n        return y_output\n\n    def _basis_function_predict(self, x, y_initial, forecast_horizon=None):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in SimulateNARMAX for now.\"\n        )\n\n    def _basis_function_n_step_prediction(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in SimulateNARMAX for now.\"\n        )\n\n    def _basis_function_n_steps_horizon(self, x, y, steps_ahead, forecast_horizon):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"You can only use Polynomial Basis Function in SimulateNARMAX for now.\"\n        )\n\n    def fit(self, *, X=None, y=None):\n        \"\"\"Not implemented.\"\"\"\n        raise NotImplementedError(\n            \"There is no fit method in Simulate because the model is predefined.\"\n        )\n</code></pre>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.error_reduction_ratio","title":"<code>error_reduction_ratio(psi, y, process_term_number, regressor_code)</code>","text":"<p>Perform the Error Reduction Ration algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>array_like</code> <p>The information matrix of the model.</p> required <code>y</code> <code>array - like</code> <p>The target data used in the identification process.</p> required <code>process_term_number</code> <code>int</code> <p>Number of Process Terms defined by the user.</p> required <code>regressor_code</code> <code>array_like</code> <p>The regressor code list given the xlag and ylag for a MISO model.</p> required <p>Returns:</p> Name Type Description <code>model_code</code> <code>array_like</code> <p>Model defined by the user to simulate.</p> <code>err</code> <code>array - like</code> <p>The respective ERR calculated for each regressor.</p> <code>piv</code> <code>array - like</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>psi_orthogonal</code> <code>array_like</code> <p>The updated and orthogonal information matrix.</p> References <ul> <li>Manuscript: Orthogonal least squares methods and their application    to non-linear system identification    https://eprints.soton.ac.uk/251147/1/778742007_content.pdf</li> <li>Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares    Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o    e Novos Resultados</li> </ul> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def error_reduction_ratio(self, psi, y, process_term_number, regressor_code):\n    \"\"\"Perform the Error Reduction Ration algorithm.\n\n    Parameters\n    ----------\n    psi : array_like\n        The information matrix of the model.\n    y : array-like\n        The target data used in the identification process.\n    process_term_number : int\n        Number of Process Terms defined by the user.\n    regressor_code : array_like\n        The regressor code list given the xlag and ylag for a MISO model.\n\n    Returns\n    -------\n    model_code : array_like\n        Model defined by the user to simulate.\n    err : array-like\n        The respective ERR calculated for each regressor.\n    piv : array-like\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    psi_orthogonal : array_like\n        The updated and orthogonal information matrix.\n\n    References\n    ----------\n    - Manuscript: Orthogonal least squares methods and their application\n       to non-linear system identification\n       https://eprints.soton.ac.uk/251147/1/778742007_content.pdf\n    - Manuscript (portuguese): Identifica\u00e7\u00e3o de Sistemas n\u00e3o Lineares\n       Utilizando Modelos NARMAX Polinomiais - Uma Revis\u00e3o\n       e Novos Resultados\n\n    \"\"\"\n    squared_y = np.dot(y[self.max_lag :].T, y[self.max_lag :])\n    tmp_psi = psi.copy()\n    y = y[self.max_lag :, 0].reshape(-1, 1)\n    tmp_y = y.copy()\n    dimension = tmp_psi.shape[1]\n    piv = np.arange(dimension)\n    tmp_err = np.zeros(dimension)\n    err = np.zeros(dimension)\n\n    for i in np.arange(0, dimension):\n        for j in np.arange(i, dimension):\n            # Add `eps` in the denominator to omit division by zero if\n            # denominator is zero\n            tmp_err[j] = (\n                (np.dot(tmp_psi[i:, j].T, tmp_y[i:]) ** 2)\n                / (np.dot(tmp_psi[i:, j].T, tmp_psi[i:, j]) * squared_y + self.eps)\n            )[0, 0]\n\n        if i == process_term_number:\n            break\n\n        piv_index = np.argmax(tmp_err[i:]) + i\n        err[i] = tmp_err[piv_index]\n        tmp_psi[:, [piv_index, i]] = tmp_psi[:, [i, piv_index]]\n        piv[[piv_index, i]] = piv[[i, piv_index]]\n\n        v = house(tmp_psi[i:, i])\n\n        row_result = rowhouse(tmp_psi[i:, i:], v)\n\n        tmp_y[i:] = rowhouse(tmp_y[i:], v)\n\n        tmp_psi[i:, i:] = np.copy(row_result)\n\n    tmp_piv = piv[0:process_term_number]\n    psi_orthogonal = psi[:, tmp_piv]\n    model_code = regressor_code[tmp_piv, :].copy()\n    return model_code, err, piv, psi_orthogonal\n</code></pre>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.fit","title":"<code>fit(*, X=None, y=None)</code>","text":"<p>Not implemented.</p> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def fit(self, *, X=None, y=None):\n    \"\"\"Not implemented.\"\"\"\n    raise NotImplementedError(\n        \"There is no fit method in Simulate because the model is predefined.\"\n    )\n</code></pre>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.predict","title":"<code>predict(*, X=None, y=None, steps_ahead=None, forecast_horizon=None)</code>","text":"<p>Return the predicted values given an input.</p> <p>The predict function allows a friendly usage by the user. Given a previously trained model, predict values given a new set of data.</p> <p>This method accept y values mainly for prediction n-steps ahead (to be implemented in the future)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array_like</code> <p>The input data to be used in the prediction process.</p> <code>None</code> <code>y</code> <code>array_like</code> <p>The output data to be used in the prediction process.</p> <code>None</code> <code>steps_ahead</code> <code>int</code> <p>The user can use free run simulation, one-step ahead prediction and n-step ahead prediction. The default is None</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>The number of predictions over the time. The default is None</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>array_like</code> <p>The predicted values of the model.</p> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def predict(self, *, X=None, y=None, steps_ahead=None, forecast_horizon=None):\n    \"\"\"Return the predicted values given an input.\n\n    The predict function allows a friendly usage by the user.\n    Given a previously trained model, predict values given\n    a new set of data.\n\n    This method accept y values mainly for prediction n-steps ahead\n    (to be implemented in the future)\n\n    Parameters\n    ----------\n    X : array_like\n        The input data to be used in the prediction process.\n    y : array_like\n        The output data to be used in the prediction process.\n    steps_ahead : int\n        The user can use free run simulation, one-step ahead prediction\n        and n-step ahead prediction. The default is None\n    forecast_horizon : int\n        The number of predictions over the time. The default is None\n\n    Returns\n    -------\n    yhat : array_like\n        The predicted values of the model.\n\n    \"\"\"\n    if isinstance(self.basis_function, Polynomial):\n        if steps_ahead is None:\n            yhat = self._model_prediction(X, y, forecast_horizon=forecast_horizon)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n        if steps_ahead == 1:\n            yhat = self._one_step_ahead_prediction(X, y)\n            yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n            return yhat\n\n        check_positive_int(steps_ahead, \"steps_ahead\")\n        yhat = self._n_step_ahead_prediction(X, y, steps_ahead=steps_ahead)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    if steps_ahead is None:\n        yhat = self._basis_function_predict(X, y, forecast_horizon=forecast_horizon)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n    if steps_ahead == 1:\n        yhat = self._one_step_ahead_prediction(X, y)\n        yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n        return yhat\n\n    yhat = self._basis_function_n_step_prediction(\n        X, y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n    )\n    yhat = np.concatenate([y[: self.max_lag], yhat], axis=0)\n    return yhat\n</code></pre>"},{"location":"user-guide/API/simulation/#sysidentpy.simulation._simulation.SimulateNARMAX.simulate","title":"<code>simulate(*, X_train=None, y_train=None, X_test=None, y_test=None, model_code=None, steps_ahead=None, theta=None, forecast_horizon=None)</code>","text":"<p>Simulate the response of a NARMAX model based on user-defined parameters.</p> <p>This method simulates the system's response using a predefined model structure (<code>model_code</code>) and estimated parameters (<code>theta</code>). It allows for both training-based parameter estimation and direct simulation using precomputed parameters.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Input data used for parameter estimation during training. Required if <code>estimate_parameter=True</code>.</p> <code>None</code> <code>y_train</code> <code>(array - like, shape(n_samples, 1))</code> <p>Output (target) data used for parameter estimation during training. Required if <code>estimate_parameter=True</code>.</p> <code>None</code> <code>X_test</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Input data used for simulation (prediction).</p> <code>None</code> <code>y_test</code> <code>(array - like, shape(n_samples, 1))</code> <p>Output data used as initial conditions for simulation.</p> <code>None</code> <code>model_code</code> <code>(array - like, shape(n_terms, n_columns))</code> <p>Encoded representation of the model's regressors, defining the input-output relationships in the system.</p> <code>None</code> <code>steps_ahead</code> <code>int</code> <p>Number of steps ahead for multi-step prediction. If <code>None</code>, defaults to one-step-ahead prediction.</p> <code>None</code> <code>theta</code> <code>(array - like, shape(n_terms, 1))</code> <p>Precomputed model parameters. Required if <code>estimate_parameter=False</code>.</p> <code>None</code> <code>forecast_horizon</code> <code>int</code> <p>Number of time steps to predict in open-loop forecasting. Used mainly for NAR and NARMA-type models.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>yhat</code> <code>(array - like, shape(n_samples, 1))</code> <p>Predicted output values of the system based on the given inputs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If necessary parameters are missing, such as <code>y_train</code> when <code>estimate_parameter=True</code> or <code>theta</code> when <code>estimate_parameter=False</code>.</p> Notes <ul> <li>If <code>estimate_parameter=True</code>, the method first estimates the parameters using the provided training data (<code>X_train</code>, <code>y_train</code>) and the chosen basis function.</li> <li>If <code>estimate_parameter=False</code>, the method assumes <code>theta</code> contains the model     parameters.</li> <li>The forecast horizon is automatically adjusted for NAR models if not provided.</li> <li>The method internally computes the lag structure based on <code>model_code</code> to     define regressors.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; X_train = np.random.rand(1000, 1)\n&gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n&gt;&gt;&gt; X_test = np.random.rand(200, 1)\n&gt;&gt;&gt; y_test = np.random.rand(200, 1)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n&gt;&gt;&gt; model = np.array([\n...     [1001, 0],       # y(k-1)\n...     [2001, 1001],    # x1(k-1)y(k-1)\n...     [2002, 0]        # x1(k-2)\n... ])\n&gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Precomputed model parameters\n&gt;&gt;&gt; y_pred = simulator.simulate(\n...     X_train=X_train, y_train=y_train,\n...     X_test=X_test, y_test=y_test,\n...     model_code=model, theta=theta\n... )\n</code></pre> Source code in <code>sysidentpy/simulation/_simulation.py</code> <pre><code>def simulate(\n    self,\n    *,\n    X_train=None,\n    y_train=None,\n    X_test=None,\n    y_test=None,\n    model_code=None,\n    steps_ahead=None,\n    theta=None,\n    forecast_horizon=None,\n):\n    \"\"\"Simulate the response of a NARMAX model based on user-defined parameters.\n\n    This method simulates the system's response using a predefined model structure\n    (`model_code`) and estimated parameters (`theta`). It allows for both\n    training-based parameter estimation and direct simulation using precomputed\n    parameters.\n\n    Parameters\n    ----------\n    X_train : array-like, shape (n_samples, n_features), optional\n        Input data used for parameter estimation during training.\n        Required if `estimate_parameter=True`.\n    y_train : array-like, shape (n_samples, 1), optional\n        Output (target) data used for parameter estimation during training.\n        Required if `estimate_parameter=True`.\n    X_test : array-like, shape (n_samples, n_features), optional\n        Input data used for simulation (prediction).\n    y_test : array-like, shape (n_samples, 1), optional\n        Output data used as initial conditions for simulation.\n    model_code : array-like, shape (n_terms, n_columns)\n        Encoded representation of the model's regressors, defining\n        the input-output relationships in the system.\n    steps_ahead : int, optional\n        Number of steps ahead for multi-step prediction. If `None`, defaults to\n        one-step-ahead prediction.\n    theta : array-like, shape (n_terms, 1), optional\n        Precomputed model parameters. Required if `estimate_parameter=False`.\n    forecast_horizon : int, optional\n        Number of time steps to predict in open-loop forecasting.\n        Used mainly for NAR and NARMA-type models.\n\n    Returns\n    -------\n    yhat : array-like, shape (n_samples, 1)\n        Predicted output values of the system based on the given inputs.\n\n    Raises\n    ------\n    ValueError\n        If necessary parameters are missing, such as `y_train` when\n        `estimate_parameter=True` or `theta` when `estimate_parameter=False`.\n\n    Notes\n    -----\n    - If `estimate_parameter=True`, the method first estimates the parameters using\n    the provided training data (`X_train`, `y_train`) and the chosen basis function.\n    - If `estimate_parameter=False`, the method assumes `theta` contains the model\n        parameters.\n    - The forecast horizon is automatically adjusted for NAR models if not provided.\n    - The method internally computes the lag structure based on `model_code` to\n        define regressors.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sysidentpy.simulation import SimulateNARMAX\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; X_train = np.random.rand(1000, 1)\n    &gt;&gt;&gt; y_train = np.random.rand(1000, 1)\n    &gt;&gt;&gt; X_test = np.random.rand(200, 1)\n    &gt;&gt;&gt; y_test = np.random.rand(200, 1)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; simulator = SimulateNARMAX(basis_function=basis_function)\n    &gt;&gt;&gt; model = np.array([\n    ...     [1001, 0],       # y(k-1)\n    ...     [2001, 1001],    # x1(k-1)y(k-1)\n    ...     [2002, 0]        # x1(k-2)\n    ... ])\n    &gt;&gt;&gt; theta = np.array([[0.2, 0.9, 0.1]]).T  # Precomputed model parameters\n    &gt;&gt;&gt; y_pred = simulator.simulate(\n    ...     X_train=X_train, y_train=y_train,\n    ...     X_test=X_test, y_test=y_test,\n    ...     model_code=model, theta=theta\n    ... )\n\n    \"\"\"\n    self._check_simulate_params(y_train, y_test, model_code, steps_ahead, theta)\n\n    if X_test is not None:\n        self.n_inputs = num_features(X_test)\n    else:\n        self.n_inputs = 1  # just to create the regressor space base\n\n    xlag_code = list_input_regressor_code(model_code)\n    ylag_code = list_output_regressor_code(model_code)\n    self.xlag = get_lag_from_regressor_code(xlag_code)\n    self.ylag = get_lag_from_regressor_code(ylag_code)\n    self.max_lag = max(self.xlag, self.ylag)\n    if self.n_inputs != 1:\n        self.xlag = self.n_inputs * [list(range(1, self.max_lag + 1))]\n\n    # for MetaMSS NAR modelling\n    if self.model_type == \"NAR\" and forecast_horizon is None:\n        forecast_horizon = y_test.shape[0] - self.max_lag\n\n    self.non_degree = model_code.shape[1]\n    regressor_code = self.regressor_space(self.n_inputs)\n\n    self.pivv = get_index_from_regressor_code(regressor_code, model_code)\n    self.final_model = regressor_code[self.pivv]\n    # to use in the predict function\n    self.n_terms = self.final_model.shape[0]\n    if self.estimate_parameter and not self.calculate_err:\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(\n            X_train, y_train, self.xlag, self.ylag, self.model_type\n        )\n        psi = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv,\n        )\n\n        self.theta = self.estimator.optimize(\n            psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if self.estimator.unbiased is True:\n            self.theta = self.estimator.unbiased_estimator(\n                psi,\n                y_train[self.max_lag :, 0].reshape(-1, 1),\n                self.theta,\n                self.elag,\n                self.max_lag,\n                self.estimator,\n                self.basis_function,\n                self.estimator.uiter,\n            )\n\n        self.err = self.n_terms * [0]\n    elif not self.estimate_parameter:\n        self.theta = theta\n        self.err = self.n_terms * [0]\n    else:\n        self.max_lag = self._get_max_lag()\n        lagged_data = build_lagged_matrix(\n            X_train, y_train, self.xlag, self.ylag, self.model_type\n        )\n        psi = self.basis_function.fit(\n            lagged_data,\n            self.max_lag,\n            self.ylag,\n            self.xlag,\n            self.model_type,\n            predefined_regressors=self.pivv,\n        )\n\n        _, self.err, _, _ = self.error_reduction_ratio(\n            psi, y_train, self.n_terms, self.final_model\n        )\n        self.theta = self.estimator.optimize(\n            psi, y_train[self.max_lag :, 0].reshape(-1, 1)\n        )\n        if self.estimator.unbiased is True:\n            self.theta = self.estimator.unbiased_estimator(\n                psi,\n                y_train[self.max_lag :, 0].reshape(-1, 1),\n                self.theta,\n                self.elag,\n                self.max_lag,\n                self.estimator,\n                self.basis_function,\n                self.estimator.uiter,\n            )\n\n    return self.predict(\n        X=X_test,\n        y=y_test,\n        steps_ahead=steps_ahead,\n        forecast_horizon=forecast_horizon,\n    )\n</code></pre>"},{"location":"user-guide/API/uofr/","title":"Documentation for <code>UOFR</code>","text":"<p>Build NARMAX Models using UOFR algorithm.</p>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR","title":"<code>UOFR</code>","text":"<p>               Bases: <code>OFRBase</code></p> <p>Ultra Orthogonal Forward Regression algorithm.</p> <p>This class uses the UOFR algorithm ([1]) to build NARMAX models. The NARMAX model is described as:</p> \\[     y_k= F[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},     \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_y\\in \\mathbb{N}^*\\), \\(n_x \\in \\mathbb{N}\\), \\(n_e \\in \\mathbb{N}\\), are the maximum lags for the system output and input respectively; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input and \\(y_k \\in \\mathbb{R}^{n_y}\\) is the system output at discrete time \\(k \\in \\mathbb{N}^n\\); $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}\\) is some nonlinear function of the input and output regressors and \\(d\\) is a time delay typically set to  \\(d=1\\).</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>int</code> <p>The maximum lag of the output.</p> <code>2</code> <code>xlag</code> <code>int</code> <p>The maximum lag of the input.</p> <code>2</code> <code>elag</code> <code>int</code> <p>The maximum lag of the residues regressors.</p> <code>2</code> <code>order_selection</code> <code>bool</code> <p>Whether to use information criteria for order selection.</p> <code>True</code> <code>info_criteria</code> <code>str</code> <p>The information criteria method to be used.</p> <code>\"aic\"</code> <code>n_terms</code> <code>int</code> <p>The number of the model terms to be selected. Note that n_terms overwrite the information criteria values.</p> <code>None</code> <code>n_info_values</code> <code>int</code> <p>The number of iterations of the information criteria method.</p> <code>10</code> <code>estimator</code> <code>str</code> <p>The parameter estimation method.</p> <code>\"least_squares\"</code> <code>model_type</code> <code>str</code> <p>The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models</p> <code>'NARMAX'</code> <code>eps</code> <code>float</code> <p>Normalization factor of the normalized filters.</p> <code>np.finfo(np.float64).eps</code> <code>alpha</code> <code>float</code> <p>Regularization parameter used in ridge regression. Ridge regression parameter that regularizes the algorithm to prevent over fitting. If the input is a noisy signal, the ridge parameter is likely to be set close to the noise level, at least as a starting point. Entered through the self data structure.</p> <code>np.finfo(np.float64).eps</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from sysidentpy.model_structure_selection import FROLS\n&gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n&gt;&gt;&gt; from sysidentpy.utils.display_results import results\n&gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n&gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n&gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n...                                                    colored_noise=True,\n...                                                    sigma=0.2,\n...                                                    train_percentage=90)\n&gt;&gt;&gt; basis_function = Polynomial(degree=2)\n&gt;&gt;&gt; model = UOFR(basis_function=basis_function,\n...               order_selection=True,\n...               n_info_values=10,\n...               extended_least_squares=False,\n...               ylag=2,\n...               xlag=2,\n...               info_criteria='aic',\n...               )\n&gt;&gt;&gt; model.fit(x_train, y_train)\n&gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n&gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n&gt;&gt;&gt; print(rrse)\n0.001993603325328823\n&gt;&gt;&gt; r = pd.DataFrame(\n...     results(\n...         model.final_model, model.theta, model.err,\n...         model.n_terms, err_precision=8, dtype='sci'\n...         ),\n...     columns=['Regressors', 'Parameters', 'ERR'])\n&gt;&gt;&gt; print(r)\n    Regressors Parameters         ERR\n0        x1(k-2)     0.9000       0.0\n1         y(k-1)     0.1999       0.0\n2  x1(k-1)y(k-1)     0.1000       0.0\n</code></pre> References <ul> <li>Manuscript: Ultra-Orthogonal Forward Regression Algorithms for the     Identification of Non-Linear Dynamic Systems    https://eprints.whiterose.ac.uk/107310/1/UOFR%20Algorithms%20R1.pdf</li> </ul> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>class UOFR(OFRBase):\n    r\"\"\"Ultra Orthogonal Forward Regression algorithm.\n\n    This class uses the UOFR algorithm ([1]) to build NARMAX models.\n    The NARMAX model is described as:\n\n    $$\n        y_k= F[y_{k-1}, \\dotsc, y_{k-n_y},x_{k-d}, x_{k-d-1},\n        \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k\n    $$\n\n    where $n_y\\in \\mathbb{N}^*$, $n_x \\in \\mathbb{N}$, $n_e \\in \\mathbb{N}$,\n    are the maximum lags for the system output and input respectively;\n    $x_k \\in \\mathbb{R}^{n_x}$ is the system input and $y_k \\in \\mathbb{R}^{n_y}$\n    is the system output at discrete time $k \\in \\mathbb{N}^n$;\n    $e_k \\in \\mathbb{R}^{n_e}4 stands for uncertainties and possible noise\n    at discrete time $k$. In this case, $\\mathcal{F}$ is some nonlinear function\n    of the input and output regressors and $d$ is a time delay typically set to\n     $d=1$.\n\n    Parameters\n    ----------\n    ylag : int, default=2\n        The maximum lag of the output.\n    xlag : int, default=2\n        The maximum lag of the input.\n    elag : int, default=2\n        The maximum lag of the residues regressors.\n    order_selection: bool, default=False\n        Whether to use information criteria for order selection.\n    info_criteria : str, default=\"aic\"\n        The information criteria method to be used.\n    n_terms : int, default=None\n        The number of the model terms to be selected.\n        Note that n_terms overwrite the information criteria\n        values.\n    n_info_values : int, default=10\n        The number of iterations of the information\n        criteria method.\n    estimator : str, default=\"least_squares\"\n        The parameter estimation method.\n    model_type: str, default=\"NARMAX\"\n        The user can choose \"NARMAX\", \"NAR\" and \"NFIR\" models\n    eps : float, default=np.finfo(np.float64).eps\n        Normalization factor of the normalized filters.\n    alpha : float, default=np.finfo(np.float64).eps\n        Regularization parameter used in ridge regression.\n        Ridge regression parameter that regularizes the algorithm to prevent over\n        fitting. If the input is a noisy signal, the ridge parameter is likely to be\n        set close to the noise level, at least as a starting point.\n        Entered through the self data structure.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; import matplotlib.pyplot as plt\n    &gt;&gt;&gt; from sysidentpy.model_structure_selection import FROLS\n    &gt;&gt;&gt; from sysidentpy.basis_function import Polynomial\n    &gt;&gt;&gt; from sysidentpy.utils.display_results import results\n    &gt;&gt;&gt; from sysidentpy.metrics import root_relative_squared_error\n    &gt;&gt;&gt; from sysidentpy.utils.generate_data import get_miso_data, get_siso_data\n    &gt;&gt;&gt; x_train, x_valid, y_train, y_valid = get_siso_data(n=1000,\n    ...                                                    colored_noise=True,\n    ...                                                    sigma=0.2,\n    ...                                                    train_percentage=90)\n    &gt;&gt;&gt; basis_function = Polynomial(degree=2)\n    &gt;&gt;&gt; model = UOFR(basis_function=basis_function,\n    ...               order_selection=True,\n    ...               n_info_values=10,\n    ...               extended_least_squares=False,\n    ...               ylag=2,\n    ...               xlag=2,\n    ...               info_criteria='aic',\n    ...               )\n    &gt;&gt;&gt; model.fit(x_train, y_train)\n    &gt;&gt;&gt; yhat = model.predict(x_valid, y_valid)\n    &gt;&gt;&gt; rrse = root_relative_squared_error(y_valid, yhat)\n    &gt;&gt;&gt; print(rrse)\n    0.001993603325328823\n    &gt;&gt;&gt; r = pd.DataFrame(\n    ...     results(\n    ...         model.final_model, model.theta, model.err,\n    ...         model.n_terms, err_precision=8, dtype='sci'\n    ...         ),\n    ...     columns=['Regressors', 'Parameters', 'ERR'])\n    &gt;&gt;&gt; print(r)\n        Regressors Parameters         ERR\n    0        x1(k-2)     0.9000       0.0\n    1         y(k-1)     0.1999       0.0\n    2  x1(k-1)y(k-1)     0.1000       0.0\n\n    References\n    ----------\n    - Manuscript: Ultra-Orthogonal Forward Regression Algorithms for the\n        Identification of Non-Linear Dynamic Systems\n       https://eprints.whiterose.ac.uk/107310/1/UOFR%20Algorithms%20R1.pdf\n\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        ylag: Union[int, list] = 2,\n        xlag: Union[int, list] = 2,\n        elag: Union[int, list] = 2,\n        order_selection: bool = True,\n        info_criteria: str = \"aic\",\n        n_terms: Union[int, None] = None,\n        n_info_values: int = 15,\n        estimator: Estimators = RecursiveLeastSquares(),\n        basis_function: Union[Polynomial, Fourier] = Polynomial(),\n        model_type: str = \"NARMAX\",\n        eps: np.float64 = np.finfo(np.float64).eps,\n        alpha: float = 0,\n        err_tol: Optional[float] = None,\n    ):\n        self.order_selection = order_selection\n        self.ylag = ylag\n        self.xlag = xlag\n        self.max_lag = self._get_max_lag()\n        self.info_criteria = info_criteria\n        self.info_criteria_function = get_info_criteria(info_criteria)\n        self.n_info_values = n_info_values\n        self.n_terms = n_terms\n        self.estimator = estimator\n        self.elag = elag\n        self.model_type = model_type\n        self.basis_function = basis_function\n        self.eps = eps\n        if isinstance(self.estimator, RidgeRegression):\n            self.alpha = self.estimator.alpha\n        else:\n            self.alpha = alpha\n\n        self.err_tol = err_tol\n        self._validate_params()\n        self.n_inputs = None\n        self.regressor_code = None\n        self.info_values = None\n        self.err = None\n        self.final_model = None\n        self.theta = None\n        self.pivv = None\n\n    def gaussian_test_function(self, t: np.ndarray, order: int) -&gt; np.ndarray:\n        \"\"\"Generate Gaussian-like test function derivatives.\"\"\"\n        sigma = 1.0  # Adjust based on signal characteristics\n        gaussian = np.exp(-(t**2) / (2 * sigma**2))\n        derivative = np.gradient(gaussian, t)\n        for _ in range(order - 1):\n            derivative = np.gradient(derivative, t)\n        return derivative\n\n    def normalize_test_function(self, phi_j: np.ndarray) -&gt; np.ndarray:\n        \"\"\"Normalize derivatives.\"\"\"\n        norm = np.linalg.norm(phi_j, ord=2)\n        return phi_j / norm if norm != 0 else phi_j\n\n    def compute_modulated_signal(\n        self, signal: np.ndarray, phi_bar_j: np.ndarray\n    ) -&gt; np.ndarray:\n        modulated = np.convolve(signal.flatten(), phi_bar_j, mode=\"valid\")\n        return modulated  # Length = len(signal) - len(phi_bar_j) + 1\n\n    def augment_uls_terms(\n        self, y: np.ndarray, psi: np.ndarray, m: int = 2, test_support: int = 5\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Augment signals for ULS with matching row counts.\"\"\"\n        modulated_length = len(y) - test_support + 1\n        num_terms = psi.shape[1]\n        t = np.linspace(-3, 3, test_support)\n\n        # Initialize y_augmented and psi_augmented with original truncated signals\n        y_augmented = y[:modulated_length].reshape(-1, 1)\n        psi_augmented = psi[:modulated_length, :]\n\n        for j in range(1, m + 1):\n            phi_j = self.gaussian_test_function(t, order=j)\n            phi_bar_j = self.normalize_test_function(phi_j)\n            y_j = self.compute_modulated_signal(y, phi_bar_j).reshape(-1, 1)\n            y_augmented = np.vstack([y_augmented, y_j])\n            modulated_terms = np.zeros((modulated_length, num_terms))\n            for term in range(num_terms):\n                x_j = self.compute_modulated_signal(psi[:, term], phi_bar_j)\n                modulated_terms[:, term] = x_j\n\n            psi_augmented = np.vstack([psi_augmented, modulated_terms])\n\n        return y_augmented, psi_augmented\n\n    def sobolev_error_reduction_ratio(\n        self,\n        psi: np.ndarray,\n        y: np.ndarray,\n        process_term_number: int,\n        m: int = 2,\n        test_support: int = 5,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Define Ultra Orthogonal Least Squares.\"\"\"\n        y = y[self.max_lag :, 0].reshape(-1, 1)\n        y_augmented, psi_augmented = self.augment_uls_terms(y, psi, m, test_support)\n        y_augmented = y_augmented.reshape(-1, 1)\n        # Compute ERR on the augmented ULS matrix\n        squared_y = np.dot(y_augmented.T, y_augmented)\n        psi_working = psi_augmented.copy()\n        y_working = y_augmented.copy()\n        num_terms = psi_working.shape[1]\n        piv = np.arange(num_terms)\n        candidate_err = np.zeros(num_terms)\n        err = np.zeros(num_terms)\n\n        for step_idx in np.arange(0, num_terms):\n            for term_idx in np.arange(step_idx, num_terms):\n                candidate_err[term_idx] = (\n                    (\n                        np.dot(psi_working[step_idx:, term_idx].T, y_working[step_idx:])\n                        ** 2\n                    )\n                    / (\n                        (\n                            np.dot(\n                                psi_working[step_idx:, term_idx].T,\n                                psi_working[step_idx:, term_idx],\n                            )\n                            + self.alpha\n                        )\n                        * squared_y\n                    )\n                    + self.eps\n                )[0, 0]\n\n            max_err_idx = np.argmax(candidate_err[step_idx:]) + step_idx\n            err[step_idx] = candidate_err[max_err_idx]\n            if step_idx == process_term_number:\n                break\n\n            if (self.err_tol is not None) and (err.cumsum()[step_idx] &gt;= self.err_tol):\n                self.n_terms = step_idx + 1\n                process_term_number = step_idx + 1\n                break\n\n            psi_working[:, [max_err_idx, step_idx]] = psi_working[\n                :, [step_idx, max_err_idx]\n            ]\n            piv[[max_err_idx, step_idx]] = piv[[step_idx, max_err_idx]]\n            reflector = house(psi_working[step_idx:, step_idx])\n            row_result = rowhouse(psi_working[step_idx:, step_idx:], reflector)\n            y_working[step_idx:] = rowhouse(y_working[step_idx:], reflector)\n            psi_working[step_idx:, step_idx:] = np.copy(row_result)\n\n        tmp_piv = piv[0:process_term_number]\n        psi_orthogonal = psi[:, tmp_piv]\n        return err, tmp_piv, psi_orthogonal\n\n    def run_mss_algorithm(\n        self, psi: np.ndarray, y: np.ndarray, process_term_number: int\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        return self.sobolev_error_reduction_ratio(psi, y, process_term_number)\n\n    def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n        \"\"\"Fit polynomial NARMAX model.\n\n        This is an 'alpha' version of the 'fit' function which allows\n        a friendly usage by the user. Given two arguments, x and y, fit\n        training data.\n\n        Parameters\n        ----------\n        X : ndarray of floats\n            The input data to be used in the training process.\n        y : ndarray of floats\n            The output data to be used in the training process.\n\n        Returns\n        -------\n        model : ndarray of int\n            The model code representation.\n        piv : array-like of shape = number_of_model_elements\n            Contains the index to put the regressors in the correct order\n            based on err values.\n        theta : array-like of shape = number_of_model_elements\n            The estimated parameters of the model.\n        err : array-like of shape = number_of_model_elements\n            The respective ERR calculated for each regressor.\n        info_values : array-like of shape = n_regressor\n            Vector with values of akaike's information criterion\n            for models with N terms (where N is the\n            vector position + 1).\n\n        \"\"\"\n        super().fit(X=X, y=y)\n        return self\n\n    def predict(\n        self,\n        *,\n        X: Optional[np.ndarray] = None,\n        y: np.ndarray,\n        steps_ahead: Optional[int] = None,\n        forecast_horizon: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        yhat = super().predict(\n            X=X, y=y, steps_ahead=steps_ahead, forecast_horizon=forecast_horizon\n        )\n        return yhat\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.augment_uls_terms","title":"<code>augment_uls_terms(y, psi, m=2, test_support=5)</code>","text":"<p>Augment signals for ULS with matching row counts.</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def augment_uls_terms(\n    self, y: np.ndarray, psi: np.ndarray, m: int = 2, test_support: int = 5\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Augment signals for ULS with matching row counts.\"\"\"\n    modulated_length = len(y) - test_support + 1\n    num_terms = psi.shape[1]\n    t = np.linspace(-3, 3, test_support)\n\n    # Initialize y_augmented and psi_augmented with original truncated signals\n    y_augmented = y[:modulated_length].reshape(-1, 1)\n    psi_augmented = psi[:modulated_length, :]\n\n    for j in range(1, m + 1):\n        phi_j = self.gaussian_test_function(t, order=j)\n        phi_bar_j = self.normalize_test_function(phi_j)\n        y_j = self.compute_modulated_signal(y, phi_bar_j).reshape(-1, 1)\n        y_augmented = np.vstack([y_augmented, y_j])\n        modulated_terms = np.zeros((modulated_length, num_terms))\n        for term in range(num_terms):\n            x_j = self.compute_modulated_signal(psi[:, term], phi_bar_j)\n            modulated_terms[:, term] = x_j\n\n        psi_augmented = np.vstack([psi_augmented, modulated_terms])\n\n    return y_augmented, psi_augmented\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.fit","title":"<code>fit(*, X=None, y)</code>","text":"<p>Fit polynomial NARMAX model.</p> <p>This is an 'alpha' version of the 'fit' function which allows a friendly usage by the user. Given two arguments, x and y, fit training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray of floats</code> <p>The input data to be used in the training process.</p> <code>None</code> <code>y</code> <code>ndarray of floats</code> <p>The output data to be used in the training process.</p> required <p>Returns:</p> Name Type Description <code>model</code> <code>ndarray of int</code> <p>The model code representation.</p> <code>piv</code> <code>array-like of shape = number_of_model_elements</code> <p>Contains the index to put the regressors in the correct order based on err values.</p> <code>theta</code> <code>array-like of shape = number_of_model_elements</code> <p>The estimated parameters of the model.</p> <code>err</code> <code>array-like of shape = number_of_model_elements</code> <p>The respective ERR calculated for each regressor.</p> <code>info_values</code> <code>array-like of shape = n_regressor</code> <p>Vector with values of akaike's information criterion for models with N terms (where N is the vector position + 1).</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def fit(self, *, X: Optional[np.ndarray] = None, y: np.ndarray):\n    \"\"\"Fit polynomial NARMAX model.\n\n    This is an 'alpha' version of the 'fit' function which allows\n    a friendly usage by the user. Given two arguments, x and y, fit\n    training data.\n\n    Parameters\n    ----------\n    X : ndarray of floats\n        The input data to be used in the training process.\n    y : ndarray of floats\n        The output data to be used in the training process.\n\n    Returns\n    -------\n    model : ndarray of int\n        The model code representation.\n    piv : array-like of shape = number_of_model_elements\n        Contains the index to put the regressors in the correct order\n        based on err values.\n    theta : array-like of shape = number_of_model_elements\n        The estimated parameters of the model.\n    err : array-like of shape = number_of_model_elements\n        The respective ERR calculated for each regressor.\n    info_values : array-like of shape = n_regressor\n        Vector with values of akaike's information criterion\n        for models with N terms (where N is the\n        vector position + 1).\n\n    \"\"\"\n    super().fit(X=X, y=y)\n    return self\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.gaussian_test_function","title":"<code>gaussian_test_function(t, order)</code>","text":"<p>Generate Gaussian-like test function derivatives.</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def gaussian_test_function(self, t: np.ndarray, order: int) -&gt; np.ndarray:\n    \"\"\"Generate Gaussian-like test function derivatives.\"\"\"\n    sigma = 1.0  # Adjust based on signal characteristics\n    gaussian = np.exp(-(t**2) / (2 * sigma**2))\n    derivative = np.gradient(gaussian, t)\n    for _ in range(order - 1):\n        derivative = np.gradient(derivative, t)\n    return derivative\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.normalize_test_function","title":"<code>normalize_test_function(phi_j)</code>","text":"<p>Normalize derivatives.</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def normalize_test_function(self, phi_j: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Normalize derivatives.\"\"\"\n    norm = np.linalg.norm(phi_j, ord=2)\n    return phi_j / norm if norm != 0 else phi_j\n</code></pre>"},{"location":"user-guide/API/uofr/#sysidentpy.model_structure_selection.sobolev_orthogonal_forward_regression.UOFR.sobolev_error_reduction_ratio","title":"<code>sobolev_error_reduction_ratio(psi, y, process_term_number, m=2, test_support=5)</code>","text":"<p>Define Ultra Orthogonal Least Squares.</p> Source code in <code>sysidentpy/model_structure_selection/sobolev_orthogonal_forward_regression.py</code> <pre><code>def sobolev_error_reduction_ratio(\n    self,\n    psi: np.ndarray,\n    y: np.ndarray,\n    process_term_number: int,\n    m: int = 2,\n    test_support: int = 5,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Define Ultra Orthogonal Least Squares.\"\"\"\n    y = y[self.max_lag :, 0].reshape(-1, 1)\n    y_augmented, psi_augmented = self.augment_uls_terms(y, psi, m, test_support)\n    y_augmented = y_augmented.reshape(-1, 1)\n    # Compute ERR on the augmented ULS matrix\n    squared_y = np.dot(y_augmented.T, y_augmented)\n    psi_working = psi_augmented.copy()\n    y_working = y_augmented.copy()\n    num_terms = psi_working.shape[1]\n    piv = np.arange(num_terms)\n    candidate_err = np.zeros(num_terms)\n    err = np.zeros(num_terms)\n\n    for step_idx in np.arange(0, num_terms):\n        for term_idx in np.arange(step_idx, num_terms):\n            candidate_err[term_idx] = (\n                (\n                    np.dot(psi_working[step_idx:, term_idx].T, y_working[step_idx:])\n                    ** 2\n                )\n                / (\n                    (\n                        np.dot(\n                            psi_working[step_idx:, term_idx].T,\n                            psi_working[step_idx:, term_idx],\n                        )\n                        + self.alpha\n                    )\n                    * squared_y\n                )\n                + self.eps\n            )[0, 0]\n\n        max_err_idx = np.argmax(candidate_err[step_idx:]) + step_idx\n        err[step_idx] = candidate_err[max_err_idx]\n        if step_idx == process_term_number:\n            break\n\n        if (self.err_tol is not None) and (err.cumsum()[step_idx] &gt;= self.err_tol):\n            self.n_terms = step_idx + 1\n            process_term_number = step_idx + 1\n            break\n\n        psi_working[:, [max_err_idx, step_idx]] = psi_working[\n            :, [step_idx, max_err_idx]\n        ]\n        piv[[max_err_idx, step_idx]] = piv[[step_idx, max_err_idx]]\n        reflector = house(psi_working[step_idx:, step_idx])\n        row_result = rowhouse(psi_working[step_idx:, step_idx:], reflector)\n        y_working[step_idx:] = rowhouse(y_working[step_idx:], reflector)\n        psi_working[step_idx:, step_idx:] = np.copy(row_result)\n\n    tmp_piv = piv[0:process_term_number]\n    psi_orthogonal = psi[:, tmp_piv]\n    return err, tmp_piv, psi_orthogonal\n</code></pre>"},{"location":"user-guide/API/utils/","title":"Documentation for <code>Neural NARX</code>","text":"<p>Utilities fo data validation.</p> <p>Display results formatted for the user.</p> <p>Utilities for data generation.</p> <p>Utils methods for NARMAX modeling.</p> <p>Plotting methods.</p>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_dimension","title":"<code>check_dimension(x, y)</code>","text":"<p>Check if x and y have only real values.</p> <p>If there is any string or object samples a ValueError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_dimension(x, y):\n    \"\"\"Check if x and y have only real values.\n\n    If there is any string or object samples a ValueError is raised.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if x.ndim == 0:\n        raise ValueError(\n            \"Input must be a 2d array, got scalar instead. Reshape your data using\"\n            \" array.reshape(-1, 1)\"\n        )\n\n    if x.ndim == 1:\n        raise ValueError(\n            \"Input must be a 2d array, got 1d array instead. \"\n            \"Reshape your data using array.reshape(-1, 1)\"\n        )\n\n    if y.ndim == 0:\n        raise ValueError(\n            \"Output must be a 2d array, got scalar instead. \"\n            \"Reshape your data using array.reshape(-1, 1)\"\n        )\n\n    if y.ndim == 1:\n        raise ValueError(\n            \"Output must be a 2d array, got 1d array instead. \"\n            \"Reshape your data using array.reshape(-1, 1)\"\n        )\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_infinity","title":"<code>check_infinity(x, y)</code>","text":"<p>Check that x and y have no NaN or Inf samples.</p> <p>If there is any NaN or Inf samples a ValueError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_infinity(x, y):\n    \"\"\"Check that x and y have no NaN or Inf samples.\n\n    If there is any NaN or Inf samples a ValueError is raised.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if np.isinf(x).any():\n        msg_error = (\n            \"Input contains invalid values (e.g. NaN, Inf) on \"\n            f\"index {np.argwhere(np.isinf(x))}\"\n        )\n        raise ValueError(msg_error)\n\n    if np.isinf(y).any():\n        msg_error = (\n            \"Output contains invalid values (e.g Inf) on \"\n            f\"index {np.argwhere(np.isinf(y))}\"\n        )\n        raise ValueError(msg_error)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_length","title":"<code>check_length(x, y)</code>","text":"<p>Check that x and y have the same number of samples.</p> <p>If the length of x and y are different a ValueError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_length(x, y):\n    \"\"\"Check that x and y have the same number of samples.\n\n    If the length of x and y are different a ValueError is raised.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if x.shape[0] != y.shape[0]:\n        msg_error = (\n            \"Input and output data must have the same number of \"\n            f\"samples. x has dimension {x.shape} and \"\n            f\"y has dimension {y.shape}\"\n        )\n        raise ValueError(msg_error)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_linear_dependence_rows","title":"<code>check_linear_dependence_rows(psi)</code>","text":"<p>Check for linear dependence in the rows of the Psi matrix.</p> <p>Parameters:</p> Name Type Description Default <code>psi</code> <code>ndarray of floats</code> <p>The information matrix of the model.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the Psi matrix has linearly dependent rows.</p> Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_linear_dependence_rows(psi):\n    \"\"\"Check for linear dependence in the rows of the Psi matrix.\n\n    Parameters\n    ----------\n    psi : ndarray of floats\n        The information matrix of the model.\n\n    Warns\n    -----\n    UserWarning\n        If the Psi matrix has linearly dependent rows.\n    \"\"\"\n    if np.linalg.matrix_rank(psi) != psi.shape[1]:\n        warn(\n            \"Psi matrix might have linearly dependent rows.\"\n            \"Be careful and check your data\",\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_nan","title":"<code>check_nan(x, y)</code>","text":"<p>Check that x and y have no NaN or Inf samples.</p> <p>If there is any NaN or Inf samples a ValueError is raised.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_nan(x, y):\n    \"\"\"Check that x and y have no NaN or Inf samples.\n\n    If there is any NaN or Inf samples a ValueError is raised.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    if np.isnan(x).any():\n        msg_error = (\n            \"Input contains invalid values (e.g. NaN, Inf) on \"\n            f\"index {np.argwhere(np.isnan(x))}\"\n        )\n        raise ValueError(msg_error)\n\n    if not ~np.isnan(y).any():\n        msg_error = (\n            \"Output contains invalid values (e.g. NaN, Inf) on \"\n            f\"index {np.argwhere(np.isnan(y))}\"\n        )\n        raise ValueError(msg_error)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_random_state","title":"<code>check_random_state(seed)</code>","text":"<p>Turn <code>seed</code> into a <code>np.random.RandomState</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>{None, int, `numpy.random.Generator`,</code> <pre><code>`numpy.random.RandomState`}, optional\n</code></pre> <p>If <code>seed</code> is None (or <code>np.random</code>), the <code>numpy.random.RandomState</code> singleton is used. If <code>seed</code> is an int, a new <code>RandomState</code> instance is used, seeded with <code>seed</code>. If <code>seed</code> is already a <code>Generator</code> or <code>RandomState</code> instance then that instance is used.</p> required <p>Returns:</p> Name Type Description <code>seed</code> <code>{`numpy.random.Generator`, `numpy.random.RandomState`}</code> <p>Random number generator.</p> Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_random_state(seed):\n    \"\"\"Turn `seed` into a `np.random.RandomState` instance.\n\n    Parameters\n    ----------\n    seed : {None, int, `numpy.random.Generator`,\n            `numpy.random.RandomState`}, optional\n        If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n        singleton is used.\n        If `seed` is an int, a new ``RandomState`` instance is used,\n        seeded with `seed`.\n        If `seed` is already a ``Generator`` or ``RandomState`` instance then\n        that instance is used.\n\n    Returns\n    -------\n    seed : {`numpy.random.Generator`, `numpy.random.RandomState`}\n        Random number generator.\n\n    \"\"\"\n    if seed is None or seed is np.random:\n        return np.random.mtrand._rand\n    if isinstance(seed, (numbers.Integral, np.integer)):\n        return np.random.default_rng(seed)\n    if isinstance(seed, (np.random.RandomState, np.random.Generator)):\n        return seed\n\n    raise ValueError(\n        \"%r cannot be used to seed a numpy.random.RandomState instance\" % seed\n    )\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.check_arrays.check_x_y","title":"<code>check_x_y(x, y)</code>","text":"<p>Validate input and output data using some crucial tests.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray of floats</code> <p>The input data.</p> required <code>y</code> <code>ndarray of floats</code> <p>The output data.</p> required Source code in <code>sysidentpy/utils/check_arrays.py</code> <pre><code>def check_x_y(x, y):\n    \"\"\"Validate input and output data using some crucial tests.\n\n    Parameters\n    ----------\n    x : ndarray of floats\n        The input data.\n    y : ndarray of floats\n        The output data.\n\n    \"\"\"\n    check_length(x, y)\n    check_dimension(x, y)\n    check_infinity(x, y)\n    check_nan(x, y)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.deprecation.deprecated","title":"<code>deprecated(version, future_version=None, message=None, alternative=None, **kwargs)</code>","text":"<p>Decorate deprecated methods.</p> <p>This decorator is adapted from astroML decorator: https://github.com/astroML/astroML/blob/f66558232f6d33cb34ecd1bed8a80b9db7ae1c30/astroML/utils/decorators.py#L120</p> Source code in <code>sysidentpy/utils/deprecation.py</code> <pre><code>def deprecated(version, future_version=None, message=None, alternative=None, **kwargs):\n    \"\"\"Decorate deprecated methods.\n\n    This decorator is adapted from astroML decorator:\n    https://github.com/astroML/astroML/blob/f66558232f6d33cb34ecd1bed8a80b9db7ae1c30/astroML/utils/decorators.py#L120\n\n    \"\"\"\n\n    def deprecate_function(\n        func,\n        version=version,\n        future_version=future_version,\n        message=message,\n        alternative=alternative,\n    ):\n        if message is None:\n            message = f\"Function {func.__name__} has been deprecated since {version}.\"\n            if alternative is not None:\n                message += (\n                    f\"\\n You'll have to use {alternative} instead.\"\n                    \"This module was deprecated in favor of \"\n                    f\"{alternative} module into which all the refactored \"\n                    \"classes and functions are moved.\"\n                )\n            if future_version is not None:\n                message += (\n                    f\"\\n This change will be applied in version {future_version}.\"\n                )\n\n        @functools.wraps(func)\n        def deprecated_func(*args, **kwargs):\n            warnings.warn(message, FutureWarning, stacklevel=1)\n            return func(*args, **kwargs)\n\n        return deprecated_func\n\n    def deprecate_class(\n        cls,\n        version=version,\n        future_version=future_version,\n        message=message,\n        alternative=alternative,\n    ):\n        if message is None:\n            message = f\"Class {cls.__name__} has been deprecated since {version}.\"\n            if alternative is not None:\n                message += alternative\n            if future_version is not None:\n                message += (\n                    f\"\\n This change will be applied in version {future_version}.\"\n                )\n\n        cls.__init__ = deprecate_function(cls.__init__, message=message)\n\n        return cls\n\n    def deprecate_warning(obj):\n        if isinstance(obj, type):\n            return deprecate_class(obj)\n\n        return deprecate_function(obj)\n\n    return deprecate_warning\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.display_results.results","title":"<code>results(final_model=None, theta=None, err=None, n_terms=None, theta_precision=4, err_precision=8, dtype='dec')</code>","text":"<p>Return the model regressors, parameters and ERR values.</p> <p>Generates a formatted string matrix containing model regressors, their corresponding parameters, and error reduction ratio (ERR) values.</p> <p>This function constructs a structured output where each row represents a model regressor, its estimated parameter, and the associated ERR value. The numerical values can be displayed in either decimal or scientific notation.</p> <p>Parameters:</p> Name Type Description Default <code>final_model</code> <code>array - like</code> <p>The identified model structure, where each row corresponds to a regressor represented by numerical codes.</p> <code>None</code> <code>theta</code> <code>array - like</code> <p>A column vector containing the estimated parameters for each regressor.</p> <code>None</code> <code>err</code> <code>array - like</code> <p>A vector containing the error reduction ratio (ERR) values for each regressor.</p> <code>None</code> <code>n_terms</code> <code>int</code> <p>Number of terms (regressors) in the model.</p> <code>None</code> <code>theta_precision</code> <code>int</code> <p>Number of decimal places for displaying parameter values.</p> <code>4</code> <code>err_precision</code> <code>int</code> <p>Number of decimal places for displaying ERR values.</p> <code>8</code> <code>dtype</code> <code>(dec, sci)</code> <p>Format for displaying numerical values: - 'dec' : Decimal notation. - 'sci' : Scientific notation.</p> <code>'dec'</code> <p>Returns:</p> Name Type Description <code>output_matrix</code> <code>list of lists</code> <p>A structured matrix where: - The first column contains regressor representations as strings. - The second column contains the corresponding estimated parameter values. - The third column contains the associated ERR values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>theta_precision</code> or <code>err_precision</code> is not a positive integer. If <code>dtype</code> is not 'dec' or 'sci'.</p> Source code in <code>sysidentpy/utils/display_results.py</code> <pre><code>def results(\n    final_model=None,\n    theta=None,\n    err=None,\n    n_terms=None,\n    theta_precision=4,\n    err_precision=8,\n    dtype=\"dec\",\n):\n    \"\"\"Return the model regressors, parameters and ERR values.\n\n    Generates a formatted string matrix containing model regressors,\n    their corresponding parameters, and error reduction ratio (ERR) values.\n\n    This function constructs a structured output where each row represents\n    a model regressor, its estimated parameter, and the associated ERR value.\n    The numerical values can be displayed in either decimal or scientific notation.\n\n    Parameters\n    ----------\n    final_model : array-like, optional\n        The identified model structure, where each row corresponds to\n        a regressor represented by numerical codes.\n\n    theta : array-like, optional\n        A column vector containing the estimated parameters for each regressor.\n\n    err : array-like, optional\n        A vector containing the error reduction ratio (ERR) values for each regressor.\n\n    n_terms : int, optional\n        Number of terms (regressors) in the model.\n\n    theta_precision : int, default=4\n        Number of decimal places for displaying parameter values.\n\n    err_precision : int, default=8\n        Number of decimal places for displaying ERR values.\n\n    dtype : {'dec', 'sci'}, default='dec'\n        Format for displaying numerical values:\n        - 'dec' : Decimal notation.\n        - 'sci' : Scientific notation.\n\n    Returns\n    -------\n    output_matrix : list of lists\n        A structured matrix where:\n        - The first column contains regressor representations as strings.\n        - The second column contains the corresponding estimated parameter values.\n        - The third column contains the associated ERR values.\n\n    Raises\n    ------\n    ValueError\n        If `theta_precision` or `err_precision` is not a positive integer.\n        If `dtype` is not 'dec' or 'sci'.\n\n    \"\"\"\n    if not isinstance(theta_precision, int) or theta_precision &lt; 1:\n        raise ValueError(\n            f\"theta_precision must be integer and &gt; zero. Got {theta_precision}.\"\n        )\n\n    if not isinstance(err_precision, int) or err_precision &lt; 1:\n        raise ValueError(\n            f\"err_precision must be integer and &gt; zero. Got {err_precision}.\"\n        )\n\n    if dtype not in (\"dec\", \"sci\"):\n        raise ValueError(f\"dtype must be dec or sci. Got {dtype}.\")\n\n    output_matrix = []\n    theta_output_format = \"{:.\" + str(theta_precision)\n    err_output_format = \"{:.\" + str(err_precision)\n\n    if dtype == \"dec\":\n        theta_output_format = theta_output_format + \"f}\"\n        err_output_format = err_output_format + \"f}\"\n    else:\n        theta_output_format = theta_output_format + \"E}\"\n        err_output_format = err_output_format + \"E}\"\n\n    for i in range(n_terms):\n        if np.max(final_model[i]) &lt; 1:\n            tmp_regressor = str(1)\n        else:\n            regressor_dic = Counter(final_model[i])\n            regressor_string = []\n            for j in range(len(list(regressor_dic.keys()))):\n                regressor_key = list(regressor_dic.keys())[j]\n                if regressor_key &lt; 1:\n                    translated_key = \"\"\n                    translated_exponent = \"\"\n                else:\n                    delay_string = str(\n                        int(regressor_key - np.floor(regressor_key / 1000) * 1000)\n                    )\n                    if int(regressor_key / 1000) &lt; 2:\n                        translated_key = \"y(k-\" + delay_string + \")\"\n                    else:\n                        translated_key = (\n                            \"x\"\n                            + str(int(regressor_key / 1000) - 1)\n                            + \"(k-\"\n                            + delay_string\n                            + \")\"\n                        )\n                    if regressor_dic[regressor_key] &lt; 2:\n                        translated_exponent = \"\"\n                    else:\n                        translated_exponent = \"^\" + str(regressor_dic[regressor_key])\n                regressor_string.append(translated_key + translated_exponent)\n            tmp_regressor = \"\".join(regressor_string)\n\n        current_parameter = theta_output_format.format(theta[i, 0])\n        current_err = err_output_format.format(err[i])\n        current_output = [tmp_regressor, current_parameter, current_err]\n        output_matrix.append(current_output)\n\n    return output_matrix\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.generate_data.get_miso_data","title":"<code>get_miso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90)</code>","text":"<p>Generate synthetic data for Multiple-Input Single-Output system identification.</p> <p>This function simulates input-output data for a nonlinear MISO system using two input signals. The system output is influenced by both inputs and can be affected by either white or colored (autoregressive) noise based on the <code>colored_noise</code> flag.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>(int, optional(default=5000))</code> <p>Number of samples to generate.</p> <code>5000</code> <code>colored_noise</code> <code>(bool, optional(default=False))</code> <p>If True, adds colored (autoregressive) noise to the system; otherwise, white noise is used.</p> <code>False</code> <code>sigma</code> <code>(float, optional(default=0.05))</code> <p>Standard deviation of the noise distribution.</p> <code>0.05</code> <code>train_percentage</code> <code>(int, optional(default=90))</code> <p>Percentage of the dataset allocated for training. The remainder is used for validation.</p> <code>90</code> <p>Returns:</p> Name Type Description <code>x_train</code> <code>ndarray</code> <p>Input data matrix (features) for system identification (training).</p> <code>x_valid</code> <code>ndarray</code> <p>Input data matrix (features) for system validation (testing).</p> <code>y_train</code> <code>ndarray</code> <p>Output data corresponding to <code>x_train</code>.</p> <code>y_valid</code> <code>ndarray</code> <p>Output data corresponding to <code>x_valid</code>.</p> Notes <ul> <li>The system follows the nonlinear difference equation:</li> </ul> <p>y[k] = 0.4 * y[k-1]\u00b2 + 0.1 * y[k-1] * x1[k-1] + 0.6 * x2[k-1]          - 0.3 * x1[k-1] * x2[k-2] + e[k]</p> <p>where <code>e[k]</code> is either white or colored noise.</p> <ul> <li>The inputs <code>x1</code> and <code>x2</code> are independently sampled from a uniform distribution in   the range [-1, 1].</li> <li>The dataset is split into training and validation sets based on <code>train_percentage</code>   , ensuring a clear separation between them.</li> <li>The function returns <code>x_train</code> and <code>x_valid</code> as stacked arrays, where each row   represents a sample and each column corresponds to an input variable   (<code>x1</code> or <code>x2</code>).</li> </ul> Source code in <code>sysidentpy/utils/generate_data.py</code> <pre><code>def get_miso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90):\n    \"\"\"Generate synthetic data for Multiple-Input Single-Output system identification.\n\n    This function simulates input-output data for a nonlinear MISO system using two\n    input signals. The system output is influenced by both inputs and can be affected\n    by either white or colored (autoregressive) noise based on the `colored_noise` flag.\n\n    Parameters\n    ----------\n    n : int, optional (default=5000)\n        Number of samples to generate.\n    colored_noise : bool, optional (default=False)\n        If True, adds colored (autoregressive) noise to the system; otherwise, white\n        noise is used.\n    sigma : float, optional (default=0.05)\n        Standard deviation of the noise distribution.\n    train_percentage : int, optional (default=90)\n        Percentage of the dataset allocated for training. The remainder is used\n        for validation.\n\n    Returns\n    -------\n    x_train : ndarray\n        Input data matrix (features) for system identification (training).\n    x_valid : ndarray\n        Input data matrix (features) for system validation (testing).\n    y_train : ndarray\n        Output data corresponding to `x_train`.\n    y_valid : ndarray\n        Output data corresponding to `x_valid`.\n\n    Notes\n    -----\n    - The system follows the nonlinear difference equation:\n\n      y[k] = 0.4 * y[k-1]\u00b2 + 0.1 * y[k-1] * x1[k-1] + 0.6 * x2[k-1]\n             - 0.3 * x1[k-1] * x2[k-2] + e[k]\n\n      where `e[k]` is either white or colored noise.\n\n    - The inputs `x1` and `x2` are independently sampled from a uniform distribution in\n      the range [-1, 1].\n    - The dataset is split into training and validation sets based on `train_percentage`\n      , ensuring a clear separation between them.\n    - The function returns `x_train` and `x_valid` as stacked arrays, where each row\n      represents a sample and each column corresponds to an input variable\n      (`x1` or `x2`).\n\n    \"\"\"\n    if train_percentage &lt; 0 or train_percentage &gt; 100:\n        raise ValueError(\"train_percentage must be smaller than 100\")\n\n    mu = 0  # mean of the distribution\n    nu = np.random.normal(mu, sigma, n).T\n    e = np.zeros((n, 1))\n\n    lag = 2\n    if colored_noise is True:\n        for k in range(lag, len(e)):\n            e[k] = 0.8 * nu[k - 1] + nu[k]\n    else:\n        e = nu\n\n    x1 = np.random.uniform(-1, 1, n).T\n    x2 = np.random.uniform(-1, 1, n).T\n    y = np.zeros((n, 1))\n    theta = np.array([[0.4], [0.1], [0.6], [-0.3]])\n\n    lag = 2\n    for k in range(lag, len(e)):\n        y[k] = (\n            theta[0] * y[k - 1] ** 2\n            + theta[1] * y[k - 1] * x1[k - 1]\n            + theta[2] * x2[k - 1]\n            + theta[3] * x1[k - 1] * x2[k - 2]\n            + e[k]\n        )\n\n    split_data = int(len(x1) * (train_percentage / 100))\n    x1_train = x1[0:split_data].reshape(-1, 1)\n    x2_train = x2[0:split_data].reshape(-1, 1)\n    x1_valid = x1[split_data::].reshape(-1, 1)\n    x2_valid = x2[split_data::].reshape(-1, 1)\n\n    x_train = np.hstack([x1_train, x2_train])\n    x_valid = np.hstack([x1_valid, x2_valid])\n\n    y_train = y[0:split_data].reshape(-1, 1)\n    y_valid = y[split_data::].reshape(-1, 1)\n\n    return x_train, x_valid, y_train, y_valid\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.generate_data.get_siso_data","title":"<code>get_siso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90)</code>","text":"<p>Generate synthetic data for Single-Input Single-Output system identification.</p> <p>This function simulates input-output data for a SISO system based on a predefined nonlinear difference equation. The system output is affected by either white noise or colored noise (autoregressive noise) depending on the <code>colored_noise</code> flag.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>(int, optional(default=5000))</code> <p>Number of samples to generate.</p> <code>5000</code> <code>colored_noise</code> <code>(bool, optional(default=False))</code> <p>If True, adds colored (autoregressive) noise to the system; otherwise, white noise is used.</p> <code>False</code> <code>sigma</code> <code>(float, optional(default=0.05))</code> <p>Standard deviation of the noise distribution.</p> <code>0.05</code> <code>train_percentage</code> <code>(int, optional(default=90))</code> <p>Percentage of the dataset allocated for training. The rest is used for validation.</p> <code>90</code> <p>Returns:</p> Name Type Description <code>x_train</code> <code>ndarray</code> <p>Input data for system identification (training).</p> <code>x_valid</code> <code>ndarray</code> <p>Input data for system validation (testing).</p> <code>y_train</code> <code>ndarray</code> <p>Output data corresponding to <code>x_train</code>.</p> <code>y_valid</code> <code>ndarray</code> <p>Output data corresponding to <code>x_valid</code>.</p> Notes <ul> <li>The system follows the nonlinear difference equation:</li> </ul> <p>y[k] = 0.2 * y[k-1] + 0.1 * y[k-1] * x[k-1] + 0.9 * x[k-2] + e[k]</p> <p>where <code>e[k]</code> is either white or colored noise.</p> <ul> <li>The input <code>x</code> is uniformly sampled from the range [-1, 1].</li> <li>The dataset is split based on <code>train_percentage</code>, ensuring a clear separation   between training and validation data.</li> </ul> Source code in <code>sysidentpy/utils/generate_data.py</code> <pre><code>def get_siso_data(n=5000, colored_noise=False, sigma=0.05, train_percentage=90):\n    r\"\"\"Generate synthetic data for Single-Input Single-Output system identification.\n\n    This function simulates input-output data for a SISO system based on a predefined\n    nonlinear difference equation. The system output is affected by either white noise\n    or colored noise (autoregressive noise) depending on the `colored_noise` flag.\n\n    Parameters\n    ----------\n    n : int, optional (default=5000)\n        Number of samples to generate.\n    colored_noise : bool, optional (default=False)\n        If True, adds colored (autoregressive) noise to the system; otherwise, white\n        noise is used.\n    sigma : float, optional (default=0.05)\n        Standard deviation of the noise distribution.\n    train_percentage : int, optional (default=90)\n        Percentage of the dataset allocated for training. The rest is used for\n        validation.\n\n    Returns\n    -------\n    x_train : ndarray\n        Input data for system identification (training).\n    x_valid : ndarray\n        Input data for system validation (testing).\n    y_train : ndarray\n        Output data corresponding to `x_train`.\n    y_valid : ndarray\n        Output data corresponding to `x_valid`.\n\n    Notes\n    -----\n    - The system follows the nonlinear difference equation:\n\n      y[k] = 0.2 * y[k-1] + 0.1 * y[k-1] * x[k-1] + 0.9 * x[k-2] + e[k]\n\n      where `e[k]` is either white or colored noise.\n\n    - The input `x` is uniformly sampled from the range [-1, 1].\n    - The dataset is split based on `train_percentage`, ensuring a clear separation\n      between training and validation data.\n\n    \"\"\"\n    if train_percentage &lt; 0 or train_percentage &gt; 100:\n        raise ValueError(\"train_percentage must be smaller than 100\")\n\n    mu = 0  # mean of the distribution\n    nu = np.random.normal(mu, sigma, n).T\n    e = np.zeros((n, 1))\n\n    lag = 2\n    if colored_noise is True:\n        for k in range(lag, len(e)):\n            e[k] = 0.8 * nu[k - 1] + nu[k]\n    else:\n        e = nu\n\n    x = np.random.uniform(-1, 1, n).T\n    y = np.zeros((n, 1))\n    theta = np.array([[0.2], [0.1], [0.9]])\n    lag = 2\n    for k in range(lag, len(x)):\n        y[k] = (\n            theta[0] * y[k - 1]\n            + theta[1] * y[k - 1] * x[k - 1]\n            + theta[2] * x[k - 2]\n            + e[k]\n        )\n\n    split_data = int(len(x) * (train_percentage / 100))\n\n    x_train = x[0:split_data].reshape(-1, 1)\n    x_valid = x[split_data::].reshape(-1, 1)\n\n    y_train = y[0:split_data].reshape(-1, 1)\n    y_valid = y[split_data::].reshape(-1, 1)\n\n    return x_train, x_valid, y_train, y_valid\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.build_input_matrix","title":"<code>build_input_matrix(x, xlag)</code>","text":"<p>Build the information matrix of input values.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree entered by the user.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>Input data used during the training phase.</p> required <code>xlag</code> <code>int, list of int, or nested list of int</code> <p>Input that can be a single integer, a list, or a nested list.</p> required <p>Returns:</p> Type Description <code>data = ndarray of floats</code> <p>The lagged matrix built in respect with each lag and column.</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def build_input_matrix(x, xlag: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the information matrix of input values.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree entered by the user.\n\n    Parameters\n    ----------\n    x : array-like\n        Input data used during the training phase.\n    xlag : int, list of int, or nested list of int\n        Input that can be a single integer, a list, or a nested list.\n\n    Returns\n    -------\n    data = ndarray of floats\n        The lagged matrix built in respect with each lag and column.\n\n    \"\"\"\n    # Generate a lagged data which each column is a input or output\n    # related to its respective lags. With this approach we can create\n    # the information matrix by using all possible combination of\n    # the columns as a product in the iterations\n\n    n_inputs, xlag = _process_xlag(x, xlag)\n    x_lagged = _create_lagged_x(x, n_inputs, xlag)\n    constant = np.ones([x_lagged.shape[0], 1])\n    data = np.concatenate([constant, x_lagged], axis=1)\n    return data\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.build_input_output_matrix","title":"<code>build_input_output_matrix(x, y, xlag, ylag)</code>","text":"<p>Build the information matrix.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree entered by the user.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>Input data used on training phase.</p> required <code>y</code> <code>array - like</code> <p>Target data used on training phase.</p> required <code>xlag</code> <code>int, list of int, or nested list of int</code> <p>Input that can be a single integer, a list, or a nested list.</p> required <code>ylag</code> <code>int or list of int</code> <p>The range of lags according to user definition.</p> required <p>Returns:</p> Type Description <code>data = ndarray of floats</code> <p>The constructed information matrix.</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def build_input_output_matrix(x: np.ndarray, y: np.ndarray, xlag, ylag) -&gt; np.ndarray:\n    \"\"\"Build the information matrix.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree entered by the user.\n\n    Parameters\n    ----------\n    x : array-like\n        Input data used on training phase.\n    y : array-like\n        Target data used on training phase.\n    xlag : int, list of int, or nested list of int\n        Input that can be a single integer, a list, or a nested list.\n    ylag : int or list of int\n        The range of lags according to user definition.\n\n    Returns\n    -------\n    data = ndarray of floats\n        The constructed information matrix.\n\n    \"\"\"\n    # Generate a lagged data which each column is a input or output\n    # related to its respective lags. With this approach we can create\n    # the information matrix by using all possible combination of\n    # the columns as a product in the iterations\n    lagged_data = initial_lagged_matrix(x, y, xlag, ylag)\n    constant = np.ones([lagged_data.shape[0], 1])\n    data = np.concatenate([constant, lagged_data], axis=1)\n    return data\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.build_output_matrix","title":"<code>build_output_matrix(y, ylag)</code>","text":"<p>Build the information matrix of output values.</p> <p>Each column of the information matrix represents a candidate regressor. The set of candidate regressors are based on xlag, ylag, and degree entered by the user.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>array - like</code> <p>Output data used during the training phase.</p> required <code>ylag</code> <code>int or list of int</code> <p>The range of lags according to user definition.</p> required <p>Returns:</p> Type Description <code>data = ndarray of floats</code> <p>The constructed output regressor matrix.</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def build_output_matrix(y, ylag: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build the information matrix of output values.\n\n    Each column of the information matrix represents a candidate\n    regressor. The set of candidate regressors are based on xlag,\n    ylag, and degree entered by the user.\n\n    Parameters\n    ----------\n    y : array-like\n        Output data used during the training phase.\n    ylag : int or list of int\n        The range of lags according to user definition.\n\n    Returns\n    -------\n    data = ndarray of floats\n        The constructed output regressor matrix.\n\n    \"\"\"\n    # Generate a lagged data which each column is an input or output\n    # related to its respective lags. With this approach we can create\n    # the information matrix by using all possible combination of\n    # the columns as a product in the iterations\n    ylag = _process_ylag(ylag)\n    y_lagged = _create_lagged_y(y, ylag)\n    constant = np.ones([y_lagged.shape[0], 1])\n    data = np.concatenate([constant, y_lagged], axis=1)\n    return data\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.count_model_regressors","title":"<code>count_model_regressors(*, x, y, xlag, ylag, model_type, basis_function, is_neural_narx=False)</code>","text":"<p>Compute the number of model regressors after applying the basis function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input data.</p> required <code>y</code> <code>ndarray</code> <p>Output data.</p> required <code>xlag</code> <code>int</code> <p>Number of lags for input variables.</p> required <code>ylag</code> <code>int</code> <p>Number of lags for output variables.</p> required <code>model_type</code> <code>str</code> <p>The type of model ('NARMAX', 'NAR', 'NFIR', etc.).</p> required <code>basis_function</code> <code>object</code> <p>The basis function used for feature transformation.</p> required <code>is_neural_narx</code> <code>bool</code> <p>Whether to adjust for a neural NARX model, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of regressors/features after transformation.</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def count_model_regressors(\n    *,\n    x: np.ndarray,\n    y: np.ndarray,\n    xlag: int,\n    ylag: int,\n    model_type: str,\n    basis_function,\n    is_neural_narx: bool = False,\n) -&gt; int:\n    \"\"\"\n    Compute the number of model regressors after applying the basis function.\n\n    Parameters\n    ----------\n    x : np.ndarray\n        Input data.\n    y : np.ndarray\n        Output data.\n    xlag : int\n        Number of lags for input variables.\n    ylag : int\n        Number of lags for output variables.\n    model_type : str\n        The type of model ('NARMAX', 'NAR', 'NFIR', etc.).\n    basis_function : object\n        The basis function used for feature transformation.\n    is_neural_narx : bool, optional\n        Whether to adjust for a neural NARX model, by default False.\n\n    Returns\n    -------\n    int\n        The number of regressors/features after transformation.\n    \"\"\"\n    data = build_lagged_matrix(x, y, xlag, ylag, model_type)\n    n_features = basis_function.fit(data[:3, :]).shape[1]\n    if is_neural_narx:\n        return n_features - 1\n\n    return n_features\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.get_build_io_method","title":"<code>get_build_io_method(model_type)</code>","text":"<p>Get info criteria method.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <p>The type of the model (NARMAX, NAR or NFIR)</p> required <p>Returns:</p> Type Description <code>build_method = Self</code> <p>Method to build the input-output matrix</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def get_build_io_method(model_type):\n    \"\"\"Get info criteria method.\n\n    Parameters\n    ----------\n    model_type = str\n        The type of the model (NARMAX, NAR or NFIR)\n\n    Returns\n    -------\n    build_method = Self\n        Method to build the input-output matrix\n    \"\"\"\n    build_matrix_options = {\n        \"NARMAX\": build_input_output_matrix,\n        \"NFIR\": build_input_matrix,\n        \"NAR\": build_output_matrix,\n    }\n    return build_matrix_options.get(model_type, None)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.initial_lagged_matrix","title":"<code>initial_lagged_matrix(x, y, xlag, ylag)</code>","text":"<p>Construct a matrix with lagged versions of input and output variables.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>Input data used during the training phase.</p> required <code>y</code> <code>array - like</code> <p>Output data used during the training phase.</p> required <code>xlag</code> <code>int, list of int, or nested list of int</code> <p>Input that can be a single integer, a list, or a nested list.</p> required <code>ylag</code> <code>int or list of int</code> <p>The range of lags according to user definition.</p> required <p>Returns:</p> Name Type Description <code>lagged_data</code> <code>ndarray</code> <p>The combined matrix containing lagged input and output values.</p> <p>Examples:</p> <p>If <code>xlag=2</code> and <code>ylag=2</code>, the resulting matrix will contain columns: Y[k-1], Y[k-2], x[k-1], x[k-2].</p> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def initial_lagged_matrix(x: np.ndarray, y: np.ndarray, xlag, ylag) -&gt; np.ndarray:\n    \"\"\"Construct a matrix with lagged versions of input and output variables.\n\n    Parameters\n    ----------\n    x : array-like\n        Input data used during the training phase.\n    y : array-like\n        Output data used during the training phase.\n    xlag : int, list of int, or nested list of int\n        Input that can be a single integer, a list, or a nested list.\n    ylag : int or list of int\n        The range of lags according to user definition.\n\n    Returns\n    -------\n    lagged_data : ndarray\n        The combined matrix containing lagged input and output values.\n\n    Examples\n    --------\n    If `xlag=2` and `ylag=2`, the resulting matrix will contain columns:\n    Y[k-1], Y[k-2], x[k-1], x[k-2].\n\n    \"\"\"\n    n_inputs, xlag = _process_xlag(x, xlag)\n    ylag = _process_ylag(ylag)\n    x_lagged = _create_lagged_x(x, n_inputs, xlag)\n    y_lagged = _create_lagged_y(y, ylag)\n    lagged_data = np.concatenate([y_lagged, x_lagged], axis=1)\n    return lagged_data\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.information_matrix.shift_column","title":"<code>shift_column(col_to_shift, lag)</code>","text":"<p>Shift an array by a specified lag, introducing zeros for missing values.</p> <p>Parameters:</p> Name Type Description Default <code>col_to_shift</code> <code>array-like of shape (n_samples,)</code> <p>The input or output time-series data to be lagged.</p> required <code>lag</code> <code>int</code> <p>The number of time steps to shift the data.</p> required <p>Returns:</p> Name Type Description <code>tmp_column</code> <code>ndarray of shape (n_samples, 1)</code> <p>The shifted array, where the first <code>lag</code> values are replaced with zeros.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n&gt;&gt;&gt; shift_column(y, 1)\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])\n</code></pre> Source code in <code>sysidentpy/utils/information_matrix.py</code> <pre><code>def shift_column(col_to_shift: np.ndarray, lag: int) -&gt; np.ndarray:\n    \"\"\"Shift an array by a specified lag, introducing zeros for missing values.\n\n    Parameters\n    ----------\n    col_to_shift : array-like of shape (n_samples,)\n        The input or output time-series data to be lagged.\n    lag : int\n        The number of time steps to shift the data.\n\n    Returns\n    -------\n    tmp_column : ndarray of shape (n_samples, 1)\n        The shifted array, where the first `lag` values are replaced with zeros.\n\n    Examples\n    --------\n    &gt;&gt;&gt; y = np.array([1, 2, 3, 4, 5])\n    &gt;&gt;&gt; shift_column(y, 1)\n    array([[0],\n           [1],\n           [2],\n           [3],\n           [4]])\n\n    \"\"\"\n    n_samples = col_to_shift.shape[0]\n    tmp_column = np.zeros((n_samples, 1))\n    aux = col_to_shift[0 : n_samples - lag].reshape(-1, 1)\n    tmp_column[lag:, 0] = aux[:, 0]\n    return tmp_column\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.lags.get_lag_from_regressor_code","title":"<code>get_lag_from_regressor_code(regressors)</code>","text":"<p>Get the maximum lag from array of regressors.</p> <p>Parameters:</p> Name Type Description Default <code>regressors</code> <code>ndarray of int</code> <p>Flattened list of input or output regressors.</p> required <p>Returns:</p> Name Type Description <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> Source code in <code>sysidentpy/utils/lags.py</code> <pre><code>def get_lag_from_regressor_code(regressors):\n    \"\"\"Get the maximum lag from array of regressors.\n\n    Parameters\n    ----------\n    regressors : ndarray of int\n        Flattened list of input or output regressors.\n\n    Returns\n    -------\n    max_lag : int\n        Maximum lag of list of regressors.\n\n    \"\"\"\n    lag_list = [int(i) for i in regressors.astype(\"str\") for i in [np.sum(int(i[2:]))]]\n    if len(lag_list) != 0:\n        return max(lag_list)\n\n    return 1\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.lags.get_max_lag_from_model_code","title":"<code>get_max_lag_from_model_code(model_code)</code>","text":"<p>Create a flattened array of input regressors.</p> <p>Parameters:</p> Name Type Description Default <code>model_code</code> <code>ndarray of int</code> <p>Model defined by the user to simulate.</p> required <p>Returns:</p> Name Type Description <code>max_lag</code> <code>int</code> <p>Maximum lag of list of regressors.</p> Source code in <code>sysidentpy/utils/lags.py</code> <pre><code>def get_max_lag_from_model_code(model_code: List[int]) -&gt; int:\n    \"\"\"Create a flattened array of input regressors.\n\n    Parameters\n    ----------\n    model_code : ndarray of int\n        Model defined by the user to simulate.\n\n    Returns\n    -------\n    max_lag : int\n        Maximum lag of list of regressors.\n\n    \"\"\"\n    xlag_code = list_input_regressor_code(model_code)\n    ylag_code = list_output_regressor_code(model_code)\n    xlag = get_lag_from_regressor_code(xlag_code)\n    ylag = get_lag_from_regressor_code(ylag_code)\n    return max(xlag, ylag)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.lags.get_max_xlag","title":"<code>get_max_xlag(xlag=1)</code>","text":"<p>Get maximum value from various xlag structures.</p> <p>Parameters:</p> Name Type Description Default <code>xlag</code> <code>int, list of int, or nested list of int</code> <p>Input that can be a single integer, a list, or a nested list.</p> <code>1</code> <p>Returns:</p> Type Description <code>int</code> <p>Maximum value found.</p> Source code in <code>sysidentpy/utils/lags.py</code> <pre><code>def get_max_xlag(xlag: int = 1):\n    \"\"\"Get maximum value from various xlag structures.\n\n    Parameters\n    ----------\n    xlag : int, list of int, or nested list of int\n        Input that can be a single integer, a list, or a nested list.\n\n    Returns\n    -------\n    int\n        Maximum value found.\n    \"\"\"\n    if isinstance(xlag, int):  # Case 1: Single integer\n        return xlag\n\n    if isinstance(xlag, list):\n        # Case 2: Flat list of integers\n        if all(isinstance(i, int) for i in xlag):\n            return max(xlag)\n        # Case 3: Nested list\n        return max(chain.from_iterable(xlag))\n\n    raise ValueError(\"Unsupported data type for xlag\")\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.lags.get_max_ylag","title":"<code>get_max_ylag(ylag=1)</code>","text":"<p>Get maximum ylag.</p> <p>Parameters:</p> Name Type Description Default <code>ylag</code> <code>ndarray of int</code> <p>The range of lags according to user definition.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>ny</code> <code>list</code> <p>Maximum value of ylag.</p> Source code in <code>sysidentpy/utils/lags.py</code> <pre><code>def get_max_ylag(ylag: int = 1):\n    \"\"\"Get maximum ylag.\n\n    Parameters\n    ----------\n    ylag : ndarray of int\n        The range of lags according to user definition.\n\n    Returns\n    -------\n    ny : list\n        Maximum value of ylag.\n\n    \"\"\"\n    ny = np.max(list(chain.from_iterable([[ylag]])))\n    return ny\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.narmax_tools.regressor_code","title":"<code>regressor_code(*, X=None, xlag=2, ylag=2, model_type='NARMAX', model_representation=None, basis_function=Polynomial())</code>","text":"<p>Generate a regressor code based on the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input feature matrix.</p> <code>None</code> <code>xlag</code> <code>int</code> <p>The number of lags for the input features.</p> <code>2</code> <code>ylag</code> <code>int</code> <p>The number of lags for the target variable.</p> <code>2</code> <code>model_type</code> <code>str</code> <p>The type of model to be used. Default is \"NARMAX\".</p> <code>'NARMAX'</code> <code>model_representation</code> <code>str</code> <p>The model representation to be used.</p> <code>None</code> <code>basis_function</code> <code>object</code> <p>The basis function object used to transform the regressor space.</p> <code>Polynomial()</code> <p>Returns:</p> Name Type Description <code>encoding</code> <code>ndarray</code> <p>The generated regressor encoding.</p> Source code in <code>sysidentpy/utils/narmax_tools.py</code> <pre><code>@deprecated(\n    version=\"v0.6.0\",\n    future_version=\"v1.0.0\",\n    message=(\n        \" `regressor_code` is deprecated in v0.6.0 and will be removed in v1.0.0.\"\n        \" Use the `count_model_regressors` from sysidentpy.utils instead.\"\n    ),\n)\ndef regressor_code(\n    *,\n    X: Optional[np.ndarray] = None,\n    xlag: int = 2,\n    ylag: int = 2,\n    model_type: str = \"NARMAX\",\n    model_representation: Optional[str] = None,\n    basis_function: Polynomial = Polynomial(),\n) -&gt; np.ndarray:\n    \"\"\"Generate a regressor code based on the provided parameters.\n\n    Parameters\n    ----------\n    X : np.ndarray, optional\n        The input feature matrix.\n    xlag : int, optional\n        The number of lags for the input features.\n    ylag : int, optional\n        The number of lags for the target variable.\n    model_type : str, optional\n        The type of model to be used. Default is \"NARMAX\".\n    model_representation : str, optional\n        The model representation to be used.\n    basis_function : object, optional\n        The basis function object used to transform the regressor space.\n\n    Returns\n    -------\n    encoding : np.ndarray\n        The generated regressor encoding.\n    \"\"\"\n    if X is not None:\n        n_inputs = num_features(X)\n    else:\n        n_inputs = 1  # only used to create the regressor space base\n\n    encoding = RegressorDictionary(\n        xlag=xlag, ylag=ylag, model_type=model_type, basis_function=basis_function\n    ).regressor_space(n_inputs)\n\n    if not isinstance(basis_function, Polynomial) and basis_function.ensemble:\n        repetition = basis_function.n * 2\n        basis_code = np.sort(\n            np.tile(encoding[1:, :], (repetition, 1)),\n            axis=0,\n        )\n        encoding = np.concatenate([encoding[1:], basis_code])\n    elif (\n        not isinstance(basis_function, Polynomial) and basis_function.ensemble is False\n    ):\n        repetition = basis_function.n * 2\n        encoding = np.sort(\n            np.tile(encoding[1:, :], (repetition, 1)),\n            axis=0,\n        )\n\n    if (\n        isinstance(basis_function, Polynomial)\n        and model_representation == \"neural_network\"\n    ):\n        return encoding[1:]\n    if isinstance(basis_function, Polynomial) and model_representation is None:\n        return encoding\n\n    return encoding\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.narmax_tools.set_weights","title":"<code>set_weights(*, static_function=True, static_gain=True, start=-0.01, stop=-5, num=50, base=2.71)</code>","text":"<p>Set log-spaced weights assigned to each objective in the MO optimization.</p> <p>Parameters:</p> Name Type Description Default <code>static_function</code> <code>bool</code> <p>Indicator for the presence of static function data. Default is True.</p> <code>True</code> <code>static_gain</code> <code>bool</code> <p>Indicator for the presence of static gain data. Default is True.</p> <code>True</code> <code>start</code> <code>float</code> <p>The starting exponent for the log-spaced weights. Default is -0.01.</p> <code>-0.01</code> <code>stop</code> <code>float</code> <p>The stopping exponent for the log-spaced weights. Default is -5.</p> <code>-5</code> <code>num</code> <code>int</code> <p>The number of weights to generate. Default is 50.</p> <code>50</code> <code>base</code> <code>float</code> <p>The base of the logarithm used to generate weights. Default is 2.71.</p> <code>2.71</code> <p>Returns:</p> Name Type Description <code>weights</code> <code>ndarray of floats</code> <p>An array containing the weights for each objective.</p> Notes <p>This method calculates the weights to be assigned to different objectives in multi-objective optimization. The choice of weights depends on the presence of static function and static gain data. If both are present, a set of weights for dynamic, gain, and static objectives is computed. If either static function or static gain is absent, a simplified set of weights is generated.</p> Source code in <code>sysidentpy/utils/narmax_tools.py</code> <pre><code>def set_weights(\n    *,\n    static_function: bool = True,\n    static_gain: bool = True,\n    start: float = -0.01,\n    stop: float = -5,\n    num: int = 50,\n    base: float = 2.71,\n) -&gt; np.ndarray:\n    \"\"\"Set log-spaced weights assigned to each objective in the MO optimization.\n\n    Parameters\n    ----------\n    static_function : bool, optional\n        Indicator for the presence of static function data. Default is True.\n    static_gain : bool, optional\n        Indicator for the presence of static gain data. Default is True.\n    start : float, optional\n        The starting exponent for the log-spaced weights. Default is -0.01.\n    stop : float, optional\n        The stopping exponent for the log-spaced weights. Default is -5.\n    num : int, optional\n        The number of weights to generate. Default is 50.\n    base : float, optional\n        The base of the logarithm used to generate weights. Default is 2.71.\n\n    Returns\n    -------\n    weights : ndarray of floats\n        An array containing the weights for each objective.\n\n    Notes\n    -----\n    This method calculates the weights to be assigned to different objectives in\n    multi-objective optimization. The choice of weights depends on the presence\n    of static function and static gain data. If both are present, a set of weights\n    for dynamic, gain, and static objectives is computed. If either static function\n    or static gain is absent, a simplified set of weights is generated.\n\n    \"\"\"\n    w1 = np.logspace(start=start, stop=stop, num=num, base=base)\n    if static_function is False or static_gain is False:\n        w2 = 1 - w1\n        return np.vstack([w1, w2])\n\n    w2 = w1[::-1]\n    w1_grid, w2_grid = np.meshgrid(w1, w2)\n    w3_grid = 1 - (w1_grid + w2_grid)\n    mask = w1_grid + w2_grid &lt;= 1\n    dynamic_weight = np.flip(w1_grid[mask])\n    gain_weight = np.flip(w2_grid[mask])\n    static_weight = np.flip(w3_grid[mask])\n    return np.vstack([dynamic_weight, gain_weight, static_weight])\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.narmax_tools.train_test_split","title":"<code>train_test_split(X, y, test_size=0.25)</code>","text":"<p>Split the time series dataset into training and testing sets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature matrix. Can be None if there are no features.</p> required <code>y</code> <code>ndarray</code> <p>The target vector.</p> required <code>test_size</code> <code>float</code> <p>The proportion of the dataset to include in the test split. Default is 0.25.</p> <code>0.25</code> <p>Returns:</p> Name Type Description <code>X_train</code> <code>ndarray or None</code> <p>The training set feature matrix, or None if x is None.</p> <code>X_test</code> <code>ndarray or None</code> <p>The testing set feature matrix, or None if x is None.</p> <code>y_train</code> <code>ndarray</code> <p>The training set target vector.</p> <code>y_test</code> <code>ndarray</code> <p>The testing set target vector.</p> Source code in <code>sysidentpy/utils/narmax_tools.py</code> <pre><code>def train_test_split(\n    X: Optional[np.ndarray], y: np.ndarray, test_size: float = 0.25\n) -&gt; Tuple[Optional[np.ndarray], Optional[np.ndarray], np.ndarray, np.ndarray]:\n    \"\"\"Split the time series dataset into training and testing sets.\n\n    Parameters\n    ----------\n    X : np.ndarray, optional\n        The feature matrix. Can be None if there are no features.\n    y : np.ndarray\n        The target vector.\n    test_size : float, optional\n        The proportion of the dataset to include in the test split. Default is 0.25.\n\n    Returns\n    -------\n    X_train : np.ndarray or None\n        The training set feature matrix, or None if x is None.\n    X_test : np.ndarray or None\n        The testing set feature matrix, or None if x is None.\n    y_train : np.ndarray\n        The training set target vector.\n    y_test : np.ndarray\n        The testing set target vector.\n    \"\"\"\n    if not 0 &lt; test_size &lt; 1:\n        raise ValueError(\"test_size should be between 0 and 1\")\n\n    # Determine the split index\n    split_index = int(len(y) * (1 - test_size))\n\n    y_train, y_test = y[:split_index], y[split_index:]\n\n    if X is None:\n        return None, None, y_train, y_test\n\n    X_train, X_test = X[:split_index], X[split_index:]\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.plotting.plot_residues_correlation","title":"<code>plot_residues_correlation(data=None, *, figsize=(10, 6), n=100, style='default', facecolor='white', title='Residual Analysis', ylabel='Correlation')</code>","text":"<p>Plot the residual validation.</p> Source code in <code>sysidentpy/utils/plotting.py</code> <pre><code>def plot_residues_correlation(\n    data=None,\n    *,\n    figsize: Tuple[int, int] = (10, 6),\n    n: int = 100,\n    style: str = \"default\",\n    facecolor: str = \"white\",\n    title: str = \"Residual Analysis\",\n    ylabel: str = \"Correlation\",\n) -&gt; None:\n    \"\"\"Plot the residual validation.\"\"\"\n    plt.style.use(style)\n    plt.rcParams[\"axes.facecolor\"] = facecolor\n    _, ax = plt.subplots(figsize=figsize, facecolor=facecolor)\n    ax.plot(data[0][:n], color=\"#1f77b4\")\n    ax.axhspan(data[1], data[2], color=\"#ccd9ff\", alpha=0.5, lw=0)\n    ax.set_xlabel(\"Lag\", fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    ax.tick_params(labelsize=14)\n    ax.set_ylim([-1, 1])\n    ax.set_title(title, fontsize=18)\n    plt.show()\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.plotting.plot_results","title":"<code>plot_results(y, *, yhat, n=100, title='Free run simulation', xlabel='Samples', ylabel='y, $\\\\hat{y}$', data_color='#1f77b4', model_color='#ff7f0e', marker='o', model_marker='*', linewidth=1.5, figsize=(10, 6), style='default', facecolor='white')</code>","text":"<p>Plot the results of a simulation.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ndarray</code> <p>True data values.</p> required <code>yhat</code> <code>ndarray</code> <p>Model predictions.</p> required <code>n</code> <code>int</code> <p>Number of samples to plot.</p> <code>100</code> <code>title</code> <code>str</code> <p>Plot title.</p> <code>'Free run simulation'</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis.</p> <code>'Samples'</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis.</p> <code>'y, $\\\\hat{y}$'</code> <code>data_color</code> <code>str</code> <p>Color for the data line.</p> <code>'#1f77b4'</code> <code>model_color</code> <code>str</code> <p>Color for the model line.</p> <code>'#ff7f0e'</code> <code>marker</code> <code>str</code> <p>Marker style for the data line.</p> <code>'o'</code> <code>model_marker</code> <code>str</code> <p>Marker style for the model line.</p> <code>'*'</code> <code>linewidth</code> <code>float</code> <p>Line width for both lines.</p> <code>1.5</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size (width, height).</p> <code>(10, 6)</code> <code>style</code> <code>str</code> <p>Matplotlib style.</p> <code>'default'</code> <code>facecolor</code> <code>str</code> <p>Figure facecolor.</p> <code>'white'</code> Source code in <code>sysidentpy/utils/plotting.py</code> <pre><code>def plot_results(\n    y: np.ndarray,\n    *,\n    yhat: np.ndarray,\n    n: int = 100,\n    title: str = \"Free run simulation\",\n    xlabel: str = \"Samples\",\n    ylabel: str = r\"y, $\\hat{y}$\",\n    data_color: str = \"#1f77b4\",\n    model_color: str = \"#ff7f0e\",\n    marker: str = \"o\",\n    model_marker: str = \"*\",\n    linewidth: float = 1.5,\n    figsize: Tuple[int, int] = (10, 6),\n    style: str = \"default\",\n    facecolor: str = \"white\",\n) -&gt; None:\n    \"\"\"Plot the results of a simulation.\n\n    Parameters\n    ----------\n    y : np.ndarray\n        True data values.\n    yhat : np.ndarray\n        Model predictions.\n    n : int\n        Number of samples to plot.\n    title : str\n        Plot title.\n    xlabel : str\n        Label for the x-axis.\n    ylabel : str\n        Label for the y-axis.\n    data_color : str\n        Color for the data line.\n    model_color : str\n        Color for the model line.\n    marker : str\n        Marker style for the data line.\n    model_marker : str\n        Marker style for the model line.\n    linewidth : float\n        Line width for both lines.\n    figsize : Tuple[int, int]\n        Figure size (width, height).\n    style : str\n        Matplotlib style.\n    facecolor : str\n        Figure facecolor.\n\n    \"\"\"\n    if len(y) == 0 or len(yhat) == 0:\n        raise ValueError(\"Arrays must have at least 1 samples.\")\n\n    # Set Matplotlib style and figure properties\n    plt.style.use(style)\n    plt.rcParams[\"axes.facecolor\"] = facecolor\n\n    _, ax = plt.subplots(figsize=figsize, facecolor=facecolor)\n    ax.plot(\n        y[:n], c=data_color, alpha=1, marker=marker, label=\"Data\", linewidth=linewidth\n    )\n    ax.plot(\n        yhat[:n], c=model_color, marker=model_marker, label=\"Model\", linewidth=linewidth\n    )\n\n    # Customize plot properties\n    ax.set_title(title, fontsize=18)\n    ax.legend()\n    ax.tick_params(labelsize=14)\n    ax.set_xlabel(xlabel, fontsize=14)\n    ax.set_ylabel(ylabel, fontsize=14)\n    plt.show()\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.save_load.load_model","title":"<code>load_model(*, file_name='model', path=None)</code>","text":"<p>Load the model from file \"file_name.syspy\" located at path \"path\".</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <p>model to be loaded</p> <code>'model'</code> <code>path</code> <code>None</code> <p>Returns:</p> Name Type Description <code>model_loaded</code> <code>model loaded, as a variable, containing model and its attributes</code> Source code in <code>sysidentpy/utils/save_load.py</code> <pre><code>def load_model(\n    *,\n    file_name=\"model\",\n    path=None,\n):\n    \"\"\"Load the model from file \"file_name.syspy\" located at path \"path\".\n\n    Parameters\n    ----------\n    file_name: file name (str), along with .syspy extension of the file containing\n        model to be loaded\n    path: location where \"file_name.syspy\" is (optional).\n\n    Returns\n    -------\n    model_loaded: model loaded, as a variable, containing model and its attributes\n\n    \"\"\"\n    # Checking if path is provided\n    if path is not None:\n\n        # Composing file_name with path\n        file_name = os.path.join(path, file_name)\n\n    # Loading the model\n    with open(file_name, \"rb\") as fp:\n        model_loaded = pk.load(fp)\n\n    return model_loaded\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.save_load.save_model","title":"<code>save_model(*, model=None, file_name='model', path=None)</code>","text":"<p>Save the model \"model\" in folder \"folder\" using an extension .syspy.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>None</code> <code>file_name</code> <code>'model'</code> <code>path</code> <code>None</code> <p>Returns:</p> Type Description <code>file file_name.syspy located at \"path\", containing the estimated model.</code> Source code in <code>sysidentpy/utils/save_load.py</code> <pre><code>def save_model(\n    *,\n    model=None,\n    file_name=\"model\",\n    path=None,\n):\n    \"\"\"Save the model \"model\" in folder \"folder\" using an extension .syspy.\n\n    Parameters\n    ----------\n    model: the model variable to be saved\n    file_name: file name, along with .syspy extension\n    path: location where the model will be saved (optional)\n\n    Returns\n    -------\n    file file_name.syspy located at \"path\", containing the estimated model.\n\n    \"\"\"\n    if model is None:\n        raise TypeError(\"model cannot be None.\")\n\n    # Checking if path is provided\n    if path is not None:\n\n        # Composing file_name with path\n        file_name = os.path.join(path, file_name)\n\n    # Saving model\n    with open(file_name, \"wb\") as fp:\n        pk.dump(model, fp)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.simulation.get_index_from_regressor_code","title":"<code>get_index_from_regressor_code(regressor_code, model_code)</code>","text":"<p>Get the index of user regressor in regressor space.</p> <p>Took from: https://stackoverflow.com/questions/38674027/find-the-row-indexes-of-several-values-in-a-numpy-array/38674038#38674038</p> <p>Parameters:</p> Name Type Description Default <code>regressor_code</code> <code>ndarray of int</code> <p>Matrix codification of all possible regressors.</p> required <code>model_code</code> <code>ndarray of int</code> <p>Model defined by the user to simulate.</p> required <p>Returns:</p> Name Type Description <code>model_index</code> <code>ndarray of int</code> <p>Index of model code in the regressor space.</p> Source code in <code>sysidentpy/utils/simulation.py</code> <pre><code>def get_index_from_regressor_code(regressor_code: np.ndarray, model_code: List[int]):\n    \"\"\"Get the index of user regressor in regressor space.\n\n    Took from: https://stackoverflow.com/questions/38674027/find-the-row-indexes-of-several-values-in-a-numpy-array/38674038#38674038\n\n    Parameters\n    ----------\n    regressor_code : ndarray of int\n        Matrix codification of all possible regressors.\n    model_code : ndarray of int\n        Model defined by the user to simulate.\n\n    Returns\n    -------\n    model_index : ndarray of int\n        Index of model code in the regressor space.\n\n    \"\"\"\n    dims = regressor_code.max(0) + 1\n    model_index = np.where(\n        np.in1d(\n            np.ravel_multi_index(regressor_code.T, dims),\n            np.ravel_multi_index(model_code.T, dims),\n        )\n    )[0]\n    return model_index\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.simulation.list_input_regressor_code","title":"<code>list_input_regressor_code(model_code)</code>","text":"<p>Create a flattened array of input regressors.</p> <p>Parameters:</p> Name Type Description Default <code>model_code</code> <code>ndarray of int</code> <p>Model defined by the user to simulate.</p> required <p>Returns:</p> Name Type Description <code>regressor_code</code> <code>ndarray of int</code> <p>Flattened list of output regressors.</p> Source code in <code>sysidentpy/utils/simulation.py</code> <pre><code>def list_input_regressor_code(model_code: List[int]) -&gt; np.ndarray:\n    \"\"\"Create a flattened array of input regressors.\n\n    Parameters\n    ----------\n    model_code : ndarray of int\n        Model defined by the user to simulate.\n\n    Returns\n    -------\n    regressor_code : ndarray of int\n        Flattened list of output regressors.\n\n    \"\"\"\n    regressor_code = [\n        code for code in model_code.ravel() if (code != 0) and (str(code)[0] != \"1\")\n    ]\n    return np.asarray(regressor_code)\n</code></pre>"},{"location":"user-guide/API/utils/#sysidentpy.utils.simulation.list_output_regressor_code","title":"<code>list_output_regressor_code(model_code)</code>","text":"<p>Create a flattened array of output regressors.</p> <p>Parameters:</p> Name Type Description Default <code>model_code</code> <code>ndarray of int</code> <p>Model defined by the user to simulate.</p> required <p>Returns:</p> Name Type Description <code>regressor_code</code> <code>ndarray of int</code> <p>Flattened list of output regressors.</p> Source code in <code>sysidentpy/utils/simulation.py</code> <pre><code>def list_output_regressor_code(model_code: List[int]) -&gt; np.ndarray:\n    \"\"\"Create a flattened array of output regressors.\n\n    Parameters\n    ----------\n    model_code : ndarray of int\n        Model defined by the user to simulate.\n\n    Returns\n    -------\n    regressor_code : ndarray of int\n        Flattened list of output regressors.\n\n    \"\"\"\n    regressor_code = [\n        code for code in model_code.ravel() if (code != 0) and (str(code)[0] == \"1\")\n    ]\n\n    return np.asarray(regressor_code)\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/","title":"Create a NARX Neural Network","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#series-parallel-training-and-parallel-prediction","title":"Series-Parallel Training and Parallel prediction","text":"<p>Currently SysIdentPy support a Series-Parallel (open-loop) Feedforward Network training process, which make the training process easier. We convert the NARX network from Series-Parallel to the Parallel (closed-loop) configuration for prediction. </p> <p>Series-Parallel allows us to use Pytorch directly for training, so we can use all the power of the Pytorch library to build our NARX Neural Network model! </p> <p></p> <p>The reader is referred to the following paper for a more in depth discussion about Series-Parallel and Parallel configurations regarding NARX neural network:</p> <p>Parallel Training Considered Harmful?: Comparing series-parallel and parallel feedforward network training</p>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#building-a-narx-neural-network","title":"Building a NARX Neural Network","text":"<p>First, just import the necessary packages</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>from torch import nn\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.utils.narmax_tools import regressor_code\nimport torch\n</code></pre> <pre><code>torch.cuda.is_available()\n</code></pre> <pre><code>False\n</code></pre> <pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n</code></pre> <pre><code>Using cpu device\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#getting-the-data","title":"Getting the data","text":"<p>The data is generated by simulating the following model:</p> <p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\).</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\),</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#choosing-the-narx-parameters-loss-function-and-optimizer","title":"Choosing the NARX parameters, loss function and optimizer","text":"<p>One can create a NARXNN object and choose the maximum lag of both input and output for building the regressor matrix to serve as input of the network.</p> <p>In addition, you can choose the loss function, the optimizer, the optional parameters of the optimizer, the number of epochs.</p> <p>Because we built this feature on top of Pytorch, you can choose any of the loss function of the torch.nn.functional. Click here for a list of the loss functions you can use. You just need to pass the name of the loss function you want.</p> <p>Similarly, you can choose any of the optimizers of the torch.optim. Click here for a list of optimizers available.</p> <pre><code>basis_function = Polynomial(degree=1)\n\nnarx_net = NARXNN(\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    device=\"cuda\",\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\neural_network\\narx_nn.py:156: UserWarning: No CUDA available. We set the device as CPU\n  self.device = self._check_cuda(device)\n</code></pre> <p>Since we have defined our NARXNN using \\(ylag=2\\), \\(xlag=2\\) and a polynomial basis function with \\(degree=1\\), we have a regressor matrix with 4 features. We need the size of the regressor matrix to build the layers of our network. Our input data(x_train) have only one feature, but since we are creating an NARX network, a regressor matrix is built behind the scenes with new features based on the xlag and ylag.</p> <p>If you need help finding how many regressors are created behind the scenes you can use the narmax_tools function regressor_code and take the size of the regressor code generated:</p> <pre><code>basis_function = Polynomial(degree=1)\n\nregressors = regressor_code(\n    X=x_train,\n    xlag=2,\n    ylag=2,\n    model_type=\"NARMAX\",\n    model_representation=\"neural_network\",\n    basis_function=basis_function,\n)\n</code></pre> <pre><code>n_features = regressors.shape[0]  # the number of features of the NARX net\nn_features\n</code></pre> <pre><code>4\n</code></pre> <pre><code>regressors\n</code></pre> <pre><code>array([[1001],\n       [1002],\n       [2001],\n       [2002]])\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#building-the-narx-neural-network","title":"Building the NARX Neural Network","text":"<p>The configuration of your network follows exactly the same pattern of a network defined in Pytorch. The following representing our NARX neural network.</p> <pre><code>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n</code></pre> <p>We have to pass the defined network to our NARXNN estimator.</p> <pre><code>narx_net.net = NARX()\n</code></pre> <pre><code>if device == \"cuda\":\n    narx_net.net.to(torch.device(\"cuda\"))\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#fit-and-predict","title":"Fit and Predict","text":"<p>Because we have a fit (for training) and predict function for Polynomial NARMAX, we create the same pattern for the NARX net. So, you only have to fit and predict using the following:</p> <pre><code>narx_net.fit(X=x_train, y=y_train, X_test=x_valid, y_test=y_valid)\n</code></pre> <pre><code>&lt;sysidentpy.neural_network.narx_nn.NARXNN at 0x19ddfff3890&gt;\n</code></pre> <pre><code>yhat = narx_net.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#results","title":"Results","text":"<p>Now we show the results</p> <pre><code>print(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>MSE:  0.00013103585914746256\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#note","title":"Note","text":"<p>If you built the net configuration before calling the NARXNN, you can just pass the model to the NARXNN as follows:</p> <pre><code>class NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(n_features, 30)\n        self.lin2 = nn.Linear(30, 30)\n        self.lin3 = nn.Linear(30, 1)\n        self.tanh = nn.Tanh()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.tanh(z)\n        z = self.lin2(z)\n        z = self.tanh(z)\n        z = self.lin3(z)\n        return z\n\n\nnarx_net2 = NARXNN(\n    net=NARX(),\n    ylag=2,\n    xlag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    loss_func=\"mse_loss\",\n    optimizer=\"Adam\",\n    epochs=2000,\n    verbose=False,\n    optim_params={\n        \"betas\": (0.9, 0.999),\n        \"eps\": 1e-05,\n    },  # optional parameters of the optimizer\n)\n\nnarx_net2.fit(X=x_train, y=y_train)\nyhat = narx_net2.predict(X=x_valid, y=y_valid)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>MSE:  0.00010086796658327408\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/how-to/create-a-narx-neural-network/#note_1","title":"Note","text":"<p>Remember you can use n-steps-ahead prediction and NAR and NFIR models. Check how to use it in their respective examples.</p>"},{"location":"user-guide/how-to/save-and-load-models/","title":"Save and Load Models","text":"<p>Example created by Samir Angelo Milani Martins</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/how-to/save-and-load-models/#obtaining-the-model-using-frols","title":"Obtaining the model using FROLS.","text":"<pre><code>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.save_load import save_model, load_model\n\n# Generating 1 input 1 output sample data from a benchmark system\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nyhat = model.predict(X=x_valid, y=y_valid)\n\n# Gathering results\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n</code></pre>"},{"location":"user-guide/how-to/save-and-load-models/#saving-obtained-model-in-file-model_namesyspy","title":"Saving obtained model in file \"model_name.syspy\"","text":"<pre><code># save_model(model_variable, file_name.syspy, path (optional))\nsave_model(model=model, file_name=\"model_name.syspy\")\n</code></pre>"},{"location":"user-guide/how-to/save-and-load-models/#loading-model-and-checking-if-everything-went-smoothly","title":"Loading model and checking if everything went smoothly","text":"<pre><code># load_model(file_name.syspy, path (optional))\nloaded_model = load_model(file_name=\"model_name.syspy\")\n\n# Predicting output with loaded_model\nyhat_loaded = loaded_model.predict(X=x_valid, y=y_valid)\n\nr_loaded = pd.DataFrame(\n    results(\n        loaded_model.final_model,\n        loaded_model.theta,\n        loaded_model.err,\n        loaded_model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\n\n# Printing both: original model and model loaded from file\nprint(\"\\n Original model \\n\", r)\nprint(\"\\n Model Loaded from file \\n\", r_loaded)\n\n# Checking predictions from both: original model and model loaded from file\nif (yhat == yhat_loaded).all():\n    print(\"\\n Predictions are the same!\")\n\n# Ploting results\nplot_results(y=y_valid, yhat=yhat_loaded, n=1000)\n</code></pre> <pre><code> Original model \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56631676E-01\n1         y(k-1)  1.9999E-01  3.99688899E-02\n2  x1(k-1)y(k-1)  1.0000E-01  3.39940092E-03\n\n Model Loaded from file \n       Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56631676E-01\n1         y(k-1)  1.9999E-01  3.99688899E-02\n2  x1(k-1)y(k-1)  1.0000E-01  3.39940092E-03\n\n Predictions are the same!\n</code></pre>"},{"location":"user-guide/how-to/set-specific-lags/","title":"Set Specific Lags","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Different ways to set the maximum lag for input and output</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n</code></pre>"},{"location":"user-guide/how-to/set-specific-lags/#setting-lags-using-a-range-of-values","title":"Setting lags using a range of values","text":"<p>If you pass int values for ylag and xlag, the lags are defined as a range from 1-ylag and 1-xlag. </p> <p>For example: if ylag=4 then the candidate regressors are \\(y_{k-1}, y_{k-2}, y_{k-3}, y_{k-4}\\)</p> <pre><code>basis_function = Polynomial(degree=1)\n\nmodel = FROLS(\n    order_selection=True,\n    ylag=4,\n    xlag=4,\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n</code></pre>"},{"location":"user-guide/how-to/set-specific-lags/#setting-specific-lags-using-lists","title":"Setting specific lags using lists","text":"<p>If you pass the ylag and xlag as a list, only the lags related to values in the list will be created. \\(y_{k-1}, y_{k-4}\\),  \\(x_{k-1}, x_{k-4}\\)</p> <pre><code>model = FROLS(\n    order_selection=True,\n    ylag=[1, 4],\n    xlag=[1, 4],\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n</code></pre>"},{"location":"user-guide/how-to/set-specific-lags/#setting-lags-for-multiple-input-single-output-miso-models","title":"Setting lags for Multiple Input Single Output (MISO) models","text":"<p>The following example shows how to define specific lags for each input. One should notice that we have to use a nested list in that case.</p> <pre><code># The example considers a model with 2 inputs, but you can use the same for any amount of inputs.\n\nmodel = FROLS(\n    order_selection=True,\n    ylag=[1, 4],\n    xlag=[[1, 2, 3, 4], [1, 7]],\n    info_criteria=\"aic\",\n    basis_function=basis_function,\n)\n# The lags defined are:\n# x1(k-1), x1(k-2), x(k-3), x(k-4)\n# x2(k-1), x1(k-7)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/","title":"Simulate Existing Models","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.simulation import SimulateNARMAX\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data","text":""},{"location":"user-guide/how-to/simulating-existing-models/#the-data-is-generated-by-simulating-the-following-model","title":"The data is generated by simulating the following model:","text":"<p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/#defining-the-model","title":"Defining the model","text":"<p>We already know that the generated data is a result of the model  \\(\ud835\udc66_\ud835\udc58=0.2\ud835\udc66_{\ud835\udc58\u22121}+0.1\ud835\udc66_{\ud835\udc58\u22121}\ud835\udc65_{\ud835\udc58\u22121}+0.9\ud835\udc65_{\ud835\udc58\u22122}+\ud835\udc52_\ud835\udc58\\) . Thus, we can create a model with those regressors follwing a codification pattern: - \\(0\\) is the constant term, - \\([1001] = y_{k-1}\\) - \\([100n] = y_{k-n}\\) - \\([200n] = x1_{k-n}\\) - \\([300n] = x2_{k-n}\\) - \\([1011, 1001] = y_{k-11} \\times y_{k-1}\\) - \\([100n, 100m] = y_{k-n} \\times y_{k-m}\\) - \\([12001, 1003, 1001] = x11_{k-1} \\times y_{k-3} \\times y_{k-1}\\) - and so on</p>"},{"location":"user-guide/how-to/simulating-existing-models/#important-note","title":"Important Note","text":"<p>The order of the arrays matter. </p> <p>If you use [2001, 1001], it will work, but [1001, 2001] will not (the regressor will be ignored). Always put the highest value first: - \\([2003, 2001]\\) works - \\([2001, 2003]\\) do not work</p> <p>We will handle this limitation in upcoming update.</p> <pre><code>s = SimulateNARMAX(\n    basis_function=Polynomial(), calculate_err=True, estimate_parameter=False\n)\n\n# the model must be a numpy array\nmodel = np.array(\n    [\n        [1001, 0],  # y(k-1)\n        [2001, 1001],  # x1(k-1)y(k-1)\n        [2002, 0],  # x1(k-2)\n    ]\n)\n# theta must be a numpy array of shape (n, 1) where n is the number of regressors\ntheta = np.array([[0.2, 0.9, 0.1]]).T\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/#simulating-the-model","title":"Simulating the model","text":"<p>After defining the model and theta we just need to use the simulate method.</p> <p>The simulate method returns the predicted values and the results where we can look at regressors, parameters and ERR values.</p> <pre><code>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0         y(k-1)  2.0000E-01  0.00000000E+00\n1        x1(k-2)  9.0000E-01  0.00000000E+00\n2  x1(k-1)y(k-1)  1.0000E-01  0.00000000E+00\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/how-to/simulating-existing-models/#options","title":"Options","text":"<p>You can set the <code>steps_ahead</code> to run the prediction/simulation:</p> <pre><code>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=1,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.001980394341423956\n</code></pre> <pre><code>yhat = s.simulate(\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    theta=theta,\n    steps_ahead=21,\n)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.0019394741034286557\n</code></pre>"},{"location":"user-guide/how-to/simulating-existing-models/#estimating-the-parameters","title":"Estimating the parameters","text":"<p>If you have only the model strucuture, you can create an object with <code>estimate_parameter=True</code> and choose the methed for estimation using <code>estimator</code>. In this case, you have to pass the training data for parameters estimation. </p> <p>When <code>estimate_parameter=True</code>, we also computate the ERR considering only the regressors defined by the user. </p> <pre><code>s = SimulateNARMAX(\n    basis_function=Polynomial(),\n    estimate_parameter=True,\n    estimator=LeastSquares(),\n    calculate_err=True,\n)\n\nyhat = s.simulate(\n    X_train=x_train,\n    y_train=y_train,\n    X_test=x_test,\n    y_test=y_test,\n    model_code=model,\n    # theta will be estimated using the defined estimator\n)\n\nr = pd.DataFrame(\n    results(s.final_model, s.theta, s.err, s.n_terms, err_precision=8, dtype=\"sci\"),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0         y(k-1)  1.9999E-01  9.57682046E-01\n1        x1(k-2)  9.0003E-01  3.87716434E-02\n2  x1(k-1)y(k-1)  1.0009E-01  3.54306118E-03\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/how-to/use-extended-least-squares/","title":"Use Extended Least Squares","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>To use the Extended Least Squares (ELS) algorithm, set the <code>unbiased</code> parameter to <code>True</code> when defining the parameter estimator algorithm.</p> <pre><code>from sysidentpy.parameter_estimation import LeastSquares\n\nestimator = LeastSquares(unbiased=True)\n</code></pre> <p>The <code>unbiased</code> hyperparameter is available in all parameter estimation algorithms, with a default value of <code>False</code>.</p> <p>Additionally, the Extended Least Squares algorithm is iterative. In SysIdentPy, the default number of iterations is set to 20 (<code>uiter=20</code>), as studies in the literature indicate that the algorithm typically converges within 10 to 20 iterations. However, you can adjust this value to any number of iterations you prefer.</p> <pre><code>from sysidentpy.parameter_estimation import LeastSquares\n\nestimator = LeastSquares(unbiased=True, uiter=40)\n</code></pre> <p>A simple yet complete code example demonstrating parameter estimation using the Extended Least Squares (ELS) algorithm is shown below.</p> <p>(Simulated data is used for illustrative purposes.)</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.generate_data import get_siso_data\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model = FROLS(\n        order_selection=False,\n        n_terms=3,\n        ylag=2,\n        xlag=2,\n        elag=2,\n        info_criteria=\"aic\",\n        estimator=estimator,\n        basis_function=basis_function,\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\nplt.figure(figsize=(14, 4))\n\n# Compute and plot KDE for each parameter using scipy's gaussian_kde\nx_grid = np.linspace(np.min(parameters), np.max(parameters), 1000)\n\nfor i, label in enumerate([\"Parameter 1\", \"Parameter 2\", \"Parameter 3\"]):\n    kde = gaussian_kde(parameters[i, :])\n    plt.plot(x_grid, kde(x_grid), label=label)\n\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters (Matplotlib only)\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/NFIR-model-overview/","title":"NFIR Model - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>This example shows how to use SysIdentPy to build NFIR models. NFIR models are models with no output feedback. In other words, there are no \\(y(k-n_y)\\) regressors, only \\(x(k-n_x)\\).</p> <p>The NFIR model can be described as:</p> \\[     y_k= F^\\ell[x_{k-d}, x_{k-d-1}, \\dotsc, x_{k-d-n_x}, e_{k-1}, \\dotsc, e_{k-n_e}] + e_k \\] <p>where \\(n_x \\in \\mathbb{N}\\) is the maximum lag for the system input; \\(x_k \\in \\mathbb{R}^{n_x}\\) is the system input at discrete time \\(k \\in \\mathbb{N}^n\\); \\(e_k \\in \\mathbb{R}^{n_e}\\) stands for uncertainties and possible noise at discrete time \\(k\\). In this case, \\(\\mathcal{F}^\\ell\\) is some nonlinear function of the input regressors with nonlinearity degree \\(\\ell \\in \\mathbb{N}\\) and \\(d\\) is a time delay typically set to \\(d=1\\).</p> <p>It is important to note that NFIR model size is generally significantly higher compared to their counterpart NARMAX model size. This drawback can be noted in linear models dimensionality and it leads to even more complex scenarios in the nonlinear case.</p> <p>So, if you are looking for parsimonious and compact models, consider using NARMAX models. However, when comparing NFIR and NARMAX models, it's generally more challenging to establish stability, particularly in a control-oriented context, with NARMAX models than with NFIR models.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import pandas as pd\nimport numpy as np\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.model_structure_selection import AOLS, FROLS\n</code></pre> <p>NFIR x NARMAX</p> <p>We will reproduce the same example provided in the \"presenting main functionality section\". In that example, we use a NARX model with xlag and ylag equal to 2 and a nonlinearity degree equal to 2. That resulted in a model with 3 regressors and a RRSE (validation metric) equal to \\(0.000184\\)</p> Regressors Parameters ERR x1(k-2) 9.0001E-01 9.57505011E-01 y(k-1) 2.0001E-01 3.89117583E-02 x1(k-1)y(k-1) 9.9992E-02 3.58319976E-03"},{"location":"user-guide/tutorials/NFIR-model-overview/#so-what-happens-if-i-use-a-nfir-model-with-the-same-configuration","title":"So, what happens if I use a NFIR model with the same configuration?","text":"<pre><code>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:618: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 6\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n0.2129700627690414\n  Regressors  Parameters             ERR\n0    x1(k-2)  8.9017E-01  9.55432286E-01\n</code></pre> <p>In the NFIR case, we got a model with 1 regressor, but with significantly worse RRSE (\\(0.21\\))</p> Regressors Parameters ERR x1(k-2) 8.9017E-01 9.55432286E-01 <p>So, to get a better NFIR model, we have to set a higher order model. In other words, we have to set a higher maximum lag to build the model.</p> <p>Lets set xlag=3.</p> <pre><code>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=3,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.04314951932710626\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9980E-01  9.55367779E-01\n1         x1(k-3)  1.7832E-01  3.94348076E-02\n2  x1(k-3)x1(k-1)  9.1104E-02  3.33315478E-03\n</code></pre> <p></p> <p>Now, the model have 3 regressors, but the RRSE is still worse (\\(0.04\\)).</p> Regressors Parameters ERR x1(k-2) 8.9980E-01 9.55367779E-01 x1(k-3) 1.7832E-01 3.94348076E-02 x1(k-3)x1(k-1) 9.1104E-02 3.33315478E-03 <p>Lets set xlag=5.</p> <pre><code>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=5,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.004209451216121233\n       Regressors  Parameters             ERR\n0         x1(k-2)  8.9978E-01  9.55485306E-01\n1         x1(k-3)  1.7979E-01  3.93181813E-02\n2  x1(k-3)x1(k-1)  8.9706E-02  3.33141271E-03\n3         x1(k-4)  3.5772E-02  1.54789285E-03\n4  x1(k-4)x1(k-2)  1.7615E-02  1.09675506E-04\n5  x1(k-4)x1(k-1)  1.7871E-02  1.13215338E-04\n6         x1(k-5)  6.9594E-03  6.23773643E-05\n7  x1(k-5)x1(k-1)  4.1353E-03  6.10794551E-06\n8  x1(k-5)x1(k-3)  3.4007E-03  3.98364615E-06\n9  x1(k-5)x1(k-2)  2.9798E-03  3.42693984E-06\n</code></pre> <p></p> <p>Now the RRSE is closer to the NARMAX model, but the NFIR model have 10 regressors. So, as mentioned before, the order of NFIR models is generally higher than NARMAX model to get comparable results.</p> Regressors Parameters ERR x1(k-2) 8.9978E-01 9.55485306E-01 x1(k-3) 1.7979E-01 3.93181813E-02 x1(k-3)x1(k-1) 8.9706E-02 3.33141271E-03 x1(k-4) 3.5772E-02 1.54789285E-03 x1(k-4)x1(k-2) 1.7615E-02 1.09675506E-04 x1(k-4)x1(k-1) 1.7871E-02 1.13215338E-04 x1(k-5) 6.9594E-03 6.23773643E-05 x1(k-5)x1(k-1) 4.1353E-03 6.10794551E-06 x1(k-5)x1(k-3) 3.4007E-03 3.98364615E-06 x1(k-5)x1(k-2) 2.9798E-03 3.42693984E-06 <p>xlag = 35</p> <pre><code>np.random.seed(seed=42)\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n\nbasis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    xlag=35,\n    n_info_values=200,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    model_type=\"NFIR\",\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.0033427508754120074\n        Regressors  Parameters             ERR\n0          x1(k-2)  9.0009E-01  9.55386378E-01\n1          x1(k-3)  1.8001E-01  3.94178379E-02\n2   x1(k-3)x1(k-1)  9.0886E-02  3.32874170E-03\n3          x1(k-4)  3.5412E-02  1.54540871E-03\n4   x1(k-4)x1(k-2)  1.8743E-02  1.12751104E-04\n5   x1(k-4)x1(k-1)  1.8378E-02  1.13878189E-04\n6          x1(k-5)  6.7236E-03  6.27406151E-05\n7   x1(k-5)x1(k-1)  4.4974E-03  6.32909200E-06\n8   x1(k-5)x1(k-3)  3.5420E-03  3.95779051E-06\n9   x1(k-5)x1(k-2)  5.5656E-03  3.39231220E-06\n10         x1(k-6)  1.5079E-03  2.37202762E-06\n11  x1(k-6)x1(k-2)  1.8768E-03  3.65196792E-07\n12  x1(k-6)x1(k-3)  1.0685E-03  2.92529290E-07\n13  x1(k-6)x1(k-1)  6.3191E-04  2.55676107E-07\n</code></pre> <p></p> <p>Now the RRSE is closer to the NARMAX model, but the NFIR model have 14 regressors, with <code>RRSE=0.0033</code>. So, as you can check in these examples, the order of NFIR models is generally higher than NARMAX model to get comparable results, even trying different parameter estimation algorithms</p> Regressors Parameters ERR x1(k-2) 9.0009E-01 9.55386378E-01 x1(k-3) 1.8001E-01 3.94178379E-02 x1(k-3)x1(k-1) 9.0886E-02 3.32874170E-03 x1(k-4) 3.5412E-02 1.54540871E-03 x1(k-4)x1(k-2) 1.8743E-02 1.12751104E-04 x1(k-4)x1(k-1) 1.8378E-02 1.13878189E-04 x1(k-5) 6.7236E-03 6.27406151E-05 x1(k-5)x1(k-1) 4.4974E-03 6.32909200E-06 x1(k-5)x1(k-3) 3.5420E-03 3.95779051E-06 x1(k-5)x1(k-2) 5.5656E-03 3.39231220E-06 x1(k-6) 1.5079E-03 2.37202762E-06 x1(k-6)x1(k-2) 1.8768E-03 3.65196792E-07 x1(k-6)x1(k-3) 1.0685E-03 2.92529290E-07 x1(k-6)x1(k-1) 6.3191E-04 2.55676107E-07"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/","title":"PV Forecasting Benchmark","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#note","title":"Note","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained against neural prophet library.</p> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS and FROLS (with polynomial base function) methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p> <p>We will compare a 1-step ahead forecaster on solar irradiance data (that can be a proxy for solar PV production). The config of the neuralprophet model was taken from the neuralprophet documentation (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>The training will occur on 80% of the data, reserving the last 20% for the validation.</p> <p>Note: the data used in this example can be found in neuralprophet github.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</code></pre>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#frols","title":"FROLS","text":"<pre><code>raw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_pv_ghi/SanFrancisco_PV_GHI.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    ylag=24,\n    xlag=24,\n    info_criteria=\"bic\",\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    estimator=LeastSquares(),\n)\n\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(sysidentpy_loss)\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <pre><code>2204.333646698544\n</code></pre>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#metamss","title":"MetaMSS","text":"<pre><code>set_random_seed(42)\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_pv_ghi/SanFrancisco_PV_GHI.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nestimator = LeastSquares()\nsysidentpy_metamss = MetaMSS(\n    basis_function=basis_function,\n    xlag=24,\n    ylag=24,\n    estimator=estimator,\n    maxiter=10,\n    steps_ahead=1,\n    n_agents=15,\n    loss_func=\"metamss_loss\",\n    model_type=\"NARMAX\",\n    random_state=42,\n)\nsysidentpy_metamss.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\n\nyhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1)\nmetamss_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <pre><code>2157.7700127350877\n</code></pre>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#aols","title":"AOLS","text":"<pre><code>set_random_seed(42)\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_pv_ghi/SanFrancisco_PV_GHI.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(\n    ylag=24, xlag=24, k=2, L=1, model_type=\"NARMAX\", basis_function=basis_function\n)\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\n\nplot_results(y=y_test[-104:], yhat=yhat[-104:])\n</code></pre> <pre><code>2361.561682547365\n</code></pre>"},{"location":"user-guide/tutorials/PV-forecasting-benchmark/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\n\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_pv_ghi/SanFrancisco_PV_GHI.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n    n_lags=24,\n    ar_sparsity=0.5,\n    # num_hidden_layers = 2,\n    # d_hidden=20,\n)\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\n\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\n# fig = m.plot(forecast)\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n</code></pre> <pre><code>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\nINFO: nprophet.utils - set_auto_seasonalities: Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 32\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 7\n 87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 87/100 [00:00&lt;00:00, 644.82it/s]\nINFO: nprophet - _lr_range_test: learning rate range test found optimal lr: 1.23E-01\nEpoch[7/7]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:02&lt;00:00,  2.58it/s, SmoothL1Loss=0.00415, MAE=58.8, RegLoss=0.0112]\nINFO: nprophet - _evaluate: Validation metrics:    SmoothL1Loss    MAE\n1         0.003 48.746\n\n\n4642.234763049609\n</code></pre> <pre><code>plt.plot(forecast[\"y\"][-104:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-104:], \"k*-\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x2618e76ebe0&gt;]\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/","title":"Air Passenger Benchmark","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#note","title":"Note","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained using the sktime and neural prophet library.</p> <p>From sktime, the following models will be used:</p> <ul> <li> <p>AutoARIMA</p> </li> <li> <p>BATS</p> </li> <li> <p>TBATS</p> </li> <li> <p>Exponential Smoothing</p> </li> <li> <p>Prophet</p> </li> <li> <p>AutoETS</p> </li> </ul> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS, FROLS (with polynomial base function) and NARXNN methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p> <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport scipy.signal.signaltools\n\n\ndef _centered(arr, newsize):\n    # Return the center newsize portion of the array.\n    newsize = np.asarray(newsize)\n    currsize = np.array(arr.shape)\n    startind = (currsize - newsize) // 2\n    endind = startind + newsize\n    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n    return arr[tuple(myslice)]\n\n\nscipy.signal.signaltools._centered = _centered\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom torch import nn\n\n# from sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.neural_network import NARXNN\n\nfrom sktime.datasets import load_airline\nfrom sktime.forecasting.ets import AutoETS\nfrom sktime.forecasting.arima import ARIMA, AutoARIMA\nfrom sktime.forecasting.base import ForecastingHorizon\nfrom sktime.forecasting.exp_smoothing import ExponentialSmoothing\nfrom sktime.forecasting.fbprophet import Prophet\nfrom sktime.forecasting.tbats import TBATS\nfrom sktime.forecasting.bats import BATS\n\n# from sktime.forecasting.model_evaluation import evaluate\nfrom sktime.forecasting.model_selection import temporal_train_test_split\nfrom sktime.performance_metrics.forecasting import mean_squared_error\nfrom sktime.utils.plotting import plot_series\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_series\\_check.py:43: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:45: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\datatypes\\_panel\\_check.py:46: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\utils\\validation\\series.py:18: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\forecasting\\base\\_fh.py:18: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  RELATIVE_TYPES = (pd.Int64Index, pd.RangeIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\sktime\\forecasting\\base\\_fh.py:19: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  ABSOLUTE_TYPES = (pd.Int64Index, pd.RangeIndex, pd.DatetimeIndex, pd.PeriodIndex)\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\nc:\\Users\\wilso\\miniconda3\\envs\\neural_prophet\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#air-passengers-data","title":"Air passengers data","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</code></pre> <pre><code>121 23\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#results","title":"Results","text":"No. Package Mean Squared Error 1 SysIdentPy (Neural Model) 316.54 2 SysIdentPy (MetaMSS) 450.99 3 SysIdentPy (AOLS) 476.64 4 NeuralProphet 501.24 5 SysIdentPy (FROLS) 805.95 6 Exponential Smoothing 910.52 7 Prophet 1186.00 8 AutoArima 1714.47 9 Manual Arima 2085.42 10 ETS 2590.05 11 BATS 7286.64 12 TBATS 7448.43"},{"location":"user-guide/tutorials/air-passenger-benchmark/#sysidentpy-frols","title":"SysIdentPy FROLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    ylag=13,  # the lags for all models will be 13\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\nsysidentpy.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(y=y_test, forecast_horizon=23)\nfrols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:619: UserWarning:\n\nn_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 14\n\n\n\n805.9521186338106\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#sysidentpy-aols","title":"SysIdentPy AOLS","text":"<pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\ndf_train, df_test = temporal_train_test_split(y, test_size=23)\ndf_train = df_train.reset_index()\ndf_train.columns = [\"ds\", \"y\"]\ndf_train[\"ds\"] = pd.to_datetime(df_train[\"ds\"].astype(str))\ndf_test = df_test.reset_index()\ndf_test.columns = [\"ds\", \"y\"]\ndf_test[\"ds\"] = pd.to_datetime(df_test[\"ds\"].astype(str))\n\nsysidentpy_AOLS = AOLS(\n    ylag=13, k=2, L=1, model_type=\"NAR\", basis_function=basis_function\n)\nsysidentpy_AOLS.fit(y=y_train)\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\nplot_results(y=y_test[sysidentpy_AOLS.max_lag :], yhat=yhat[sysidentpy_AOLS.max_lag :])\n</code></pre> <pre><code>476.64996316992523\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#sysidentpy-metamss","title":"SysIdentPy MetaMSS","text":"<pre><code>set_random_seed(42)\n\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\n\nsysidentpy_metamss = MetaMSS(\n    basis_function=basis_function, ylag=13, model_type=\"NAR\", test_size=0.17\n)\nsysidentpy_metamss.fit(y=y_train)\n\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\n\nyhat = sysidentpy_metamss.predict(y=y_test, steps_ahead=None, forecast_horizon=23)\nmetamss_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\nplot_results(\n    y=y_test[sysidentpy_metamss.max_lag :], yhat=yhat[sysidentpy_metamss.max_lag :]\n)\n</code></pre> <pre><code>450.992127624293\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#sysidentpy-neural-narx","title":"SysIdentPy Neural NARX","text":"<pre><code>import torch\n\ntorch.manual_seed(42)\n\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=36)\ny_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)\nx_train = np.zeros_like(y_train)\nx_test = np.zeros_like(y_test)\n\n\nclass NARX(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(13, 20)\n        self.lin2 = nn.Linear(20, 20)\n        self.lin3 = nn.Linear(20, 20)\n        self.lin4 = nn.Linear(20, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, xb):\n        z = self.lin(xb)\n        z = self.relu(z)\n        z = self.lin2(z)\n        z = self.relu(z)\n        z = self.lin3(z)\n        z = self.relu(z)\n        z = self.lin4(z)\n        return z\n\n\nnarx_net = NARXNN(\n    net=NARX(),\n    ylag=13,\n    model_type=\"NAR\",\n    basis_function=Polynomial(degree=1),\n    epochs=900,\n    verbose=False,\n    learning_rate=2.5e-02,\n    optim_params={},  # optional parameters of the optimizer\n)\n\nnarx_net.fit(y=y_train)\nyhat = narx_net.predict(y=y_test, forecast_horizon=23)\nnarxnet_loss = loss(\n    pd.Series(y_test.flatten()[narx_net.max_lag :]),\n    pd.Series(yhat.flatten()[narx_net.max_lag :]),\n)\nprint(narxnet_loss)\nplot_results(y=y_test[narx_net.max_lag :], yhat=yhat[narx_net.max_lag :])\n</code></pre> <pre><code>316.54086775668776\n</code></pre> <pre><code>y = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)  # 23 samples for testing\nplot_series(y_train, y_test, labels=[\"y_train\", \"y_test\"])\nfh = ForecastingHorizon(y_test.index, is_relative=False)\nprint(y_train.shape[0], y_test.shape[0])\n</code></pre> <pre><code>121 23\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#exponential-smoothing","title":"Exponential Smoothing","text":"<pre><code>es = ExponentialSmoothing(trend=\"add\", seasonal=\"multiplicative\", sp=12)\ny = load_airline()\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nes.fit(y_train)\ny_pred_es = es.predict(fh)\n\nplot_series(y_test, y_pred_es, labels=[\"y_test\", \"y_pred\"])\nes_loss = loss(y_test, y_pred_es)\nes_loss\n</code></pre> <pre><code>910.462659260655\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#autoets","title":"AutoETS","text":"<pre><code>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nets = AutoETS(auto=True, sp=12, n_jobs=-1)\nets.fit(y_train)\ny_pred_ets = ets.predict(fh)\n\nplot_series(y_test, y_pred_ets, labels=[\"y_test\", \"y_pred\"])\nets_loss = loss(y_test, y_pred_ets)\nets_loss\n</code></pre> <pre><code>1739.117296439066\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#autoarima","title":"AutoArima","text":"<pre><code>auto_arima = AutoARIMA(sp=12, suppress_warnings=True)\ny = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nauto_arima.fit(y_train)\ny_pred_auto_arima = auto_arima.predict(fh)\n\nplot_series(y_test, y_pred_auto_arima, labels=[\"y_test\", \"y_pred\"])\nautoarima_loss = loss(y_test, y_pred_auto_arima)\nautoarima_loss\n</code></pre> <pre><code>1714.4753226965322\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#arima","title":"Arima","text":"<pre><code>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nmanual_arima = ARIMA(\n    order=(13, 1, 0), suppress_warnings=True\n)  # seasonal_order=(0, 1, 0, 12)\nmanual_arima.fit(y_train)\ny_pred_manual_arima = manual_arima.predict(fh)\nplot_series(y_test, y_pred_manual_arima, labels=[\"y_test\", \"y_pred\"])\nmanualarima_loss = loss(y_test, y_pred_manual_arima)\nmanualarima_loss\n</code></pre> <pre><code>2085.425167938668\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#bats","title":"BATS","text":"<pre><code>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nbats = BATS(sp=12, use_trend=True, use_box_cox=False)\nbats.fit(y_train)\ny_pred_bats = bats.predict(fh)\n\nplot_series(y_test, y_pred_bats, labels=[\"y_test\", \"y_pred\"])\nbats_loss = loss(y_test, y_pred_bats)\nbats_loss\n</code></pre> <pre><code>7286.6484525676415\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#tbats","title":"TBATS","text":"<pre><code>y = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\ntbats = TBATS(sp=12, use_trend=True, use_box_cox=False)\ntbats.fit(y_train)\ny_pred_tbats = tbats.predict(fh)\nplot_series(y_test, y_pred_tbats, labels=[\"y_test\", \"y_pred\"])\ntbats_loss = loss(y_test, y_pred_tbats)\ntbats_loss\n</code></pre> <pre><code>7448.434672875093\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#prophet","title":"Prophet","text":"<pre><code>set_random_seed(42)\n\ny = load_airline()\n\ny_train, y_test = temporal_train_test_split(y, test_size=23)\nz = y.copy()\nz = z.to_timestamp(freq=\"M\")\nz_train, z_test = temporal_train_test_split(z, test_size=23)\n\n\nprophet = Prophet(\n    seasonality_mode=\"multiplicative\",\n    n_changepoints=int(len(y_train) / 12),\n    add_country_holidays={\"country_name\": \"Germany\"},\n    yearly_seasonality=True,\n    weekly_seasonality=False,\n    daily_seasonality=False,\n)\nprophet.fit(z_train)\ny_pred_prophet = prophet.predict(fh.to_relative(cutoff=y_train.index[-1]))\n\ny_pred_prophet.index = y_test.index\nplot_series(y_test, y_pred_prophet, labels=[\"y_test\", \"y_pred\"])\nprophet_loss = loss(y_test, y_pred_prophet)\nprophet_loss\n</code></pre> <pre><code>1186.0045566050442\n</code></pre>"},{"location":"user-guide/tutorials/air-passenger-benchmark/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\n\ndf = pd.read_csv(r\".\\datasets\\air_passengers.csv\")\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\ndf_train = df.iloc[:-23, :].copy()\ndf_test = df.iloc[-23:, :].copy()\n\nm = NeuralProphet(seasonality_mode=\"multiplicative\")\n\nmetrics = m.fit(df_train, freq=\"MS\")\n\nfuture = m.make_future_dataframe(\n    df_train, periods=23, n_historic_predictions=len(df_train)\n)\n\nforecast = m.predict(future)\nplt.plot(forecast[\"yhat1\"].values[-23:])\nplt.plot(df_test[\"y\"].values)\nneuralprophet_loss = loss(forecast[\"yhat1\"].values[-23:], df_test[\"y\"].values)\nneuralprophet_loss\n</code></pre> <pre><code>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n\n\n11-21 20:57:55 - WARNING - Parts of code may break if using other than daily data.\n\n\nINFO: nprophet.utils - set_auto_seasonalities: Disabling weekly seasonality. Run NeuralProphet with weekly_seasonality=True to override this.\n\n\n11-21 20:57:55 - INFO - Disabling weekly seasonality. Run NeuralProphet with weekly_seasonality=True to override this.\n\n\nINFO: nprophet.utils - set_auto_seasonalities: Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n\n\n11-21 20:57:55 - INFO - Disabling daily seasonality. Run NeuralProphet with daily_seasonality=True to override this.\n\n\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 8\n\n\n11-21 20:57:55 - INFO - Auto-set batch_size to 8\n\n\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 264\n\n\n11-21 20:57:55 - INFO - Auto-set epochs to 264\n\n\n 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 83/100 [00:00&lt;00:00, 1034.46it/s]\nINFO: nprophet - _lr_range_test: learning rate range test found optimal lr: 1.87E-01\n\n\n11-21 20:57:55 - INFO - learning rate range test found optimal lr: 1.87E-01\n\n\nEpoch[264/264]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 264/264 [00:03&lt;00:00, 66.42it/s, SmoothL1Loss=0.000325, MAE=6.38, RegLoss=0]\n\n\n\n\n\n501.24794023767436\n</code></pre> <pre><code>results = {\n    \"Exponential Smoothing\": es_loss,\n    \"ETS\": ets_loss,\n    \"AutoArima\": autoarima_loss,\n    \"Manual Arima\": manualarima_loss,\n    \"BATS\": bats_loss,\n    \"TBATS\": tbats_loss,\n    \"Prophet\": prophet_loss,\n    \"SysIdentPy (Polynomial Model)\": frols_loss,\n    \"SysIdentPy (Neural Model)\": narxnet_loss,\n    \"SysIdentPy (AOLS)\": aols_loss,\n    \"SysIdentPy (MetaMSS)\": metamss_loss,\n    \"NeuralProphet\": neuralprophet_loss,\n}\n\nsorted(results.items(), key=lambda result: result[1])\n</code></pre> <pre><code>[('SysIdentPy (Neural Model)', 316.54086775668776),\n ('SysIdentPy (MetaMSS)', 450.992127624293),\n ('SysIdentPy (AOLS)', 476.64996316992523),\n ('NeuralProphet', 501.24794023767436),\n ('SysIdentPy (Polynomial Model)', 805.9521186338106),\n ('Exponential Smoothing', 910.462659260655),\n ('Prophet', 1186.0045566050442),\n ('AutoArima', 1714.4753226965322),\n ('ETS', 1739.117296439066),\n ('Manual Arima', 2085.425167938668),\n ('BATS', 7286.6484525676415),\n ('TBATS', 7448.434672875093)]\n</code></pre>"},{"location":"user-guide/tutorials/aols-overview/","title":"AOLS Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <pre><code>import pandas as pd\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\nfrom sysidentpy.model_structure_selection import AOLS\n\n# generating simulated data\nx_train, x_test, y_train, y_test = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre> <pre><code>basis_function = Polynomial(degree=2)\nmodel = AOLS(xlag=3, ylag=3, k=5, L=1, basis_function=basis_function)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.accelerated_orthogonal_least_squares.AOLS at 0x25cd3b406d0&gt;\n</code></pre> <pre><code>yhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0018996279285613828\n      Regressors   Parameters             ERR\n0         y(k-1)   1.9999E-01  0.00000000E+00\n1        x1(k-2)   9.0003E-01  0.00000000E+00\n2  x1(k-1)y(k-1)   9.9954E-02  0.00000000E+00\n3  x1(k-3)y(k-1)  -2.1442E-04  0.00000000E+00\n4      x1(k-1)^2   3.3714E-04  0.00000000E+00\n</code></pre> <pre><code>plot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/tutorials/basis-function-overview/","title":"Basis Functions - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>This notebook is not intended to find the best possible models for system identification. Instead, it serves as a simple demonstration of the basis functions available in SysIdentPy. The goal is to showcase each basis function with minimal code to illustrate how to use them within the SysIdentPy framework.</p> <p>We use basic settings for model structure selection and parameter estimation, but for real-world applications, you may need to fine-tune the hyperparameters and explore more advanced methods to achieve optimal results.</p> <p>For more details on SysIdentPy and how to fully leverage its capabilities, please refer to the official documentation and the companion book.</p>"},{"location":"user-guide/tutorials/basis-function-overview/#introduction","title":"Introduction","text":"<p>In this example, we'll explore how to use SysIdentPy to apply various basis functions for system identification and model structure selection. We'll use a simulated dataset and apply the FROLS algorithm with different basis functions. Each basis function will be evaluated, and results will be plotted to compare their performance.</p> <p>You can learn more about SysIdentPy's basis functions by referring to the official documentation.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy import basis_function\n</code></pre>"},{"location":"user-guide/tutorials/basis-function-overview/#generating-simulated-data","title":"Generating Simulated Data","text":"<p>We begin by generating simulated Single-Input Single-Output (SISO) data using the get_siso_data function. This utility allows us to create realistic data for system identification tasks. For more details about how to customize the data generation process, visit the data utilities documentation.</p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n</code></pre>"},{"location":"user-guide/tutorials/basis-function-overview/#basis-functions-in-sysidentpy","title":"Basis Functions in SysIdentPy","text":"<p>SysIdentPy provides several basis functions that can be used in system identification. Basis functions transform input data into a feature space, enabling the identification of non-linear systems.</p> <p>The following code snippet dynamically loads and instantiates each available basis function. You can explore the full list of basis functions available in SysIdentPy by visiting the basis functions documentation.</p> <pre><code>basis_function.__all__\n</code></pre> <pre><code>['Bernstein',\n 'Bilinear',\n 'Fourier',\n 'Hermite',\n 'HermiteNormalized',\n 'Laguerre',\n 'Legendre',\n 'Polynomial']\n</code></pre> <pre><code>import inspect\nfrom sysidentpy import basis_function\n\nfor basis_name, bf in inspect.getmembers(basis_function):\n    if inspect.isclass(bf):\n        estimator = LeastSquares()\n        model = FROLS(\n            order_selection=True,\n            n_info_values=15,\n            ylag=2,\n            xlag=2,\n            info_criteria=\"aic\",\n            estimator=estimator,\n            err_tol=None,\n            basis_function=bf(degree=5),\n        )\n\n        model.fit(X=x_train, y=y_train)\n        yhat = model.predict(X=x_valid, y=y_valid)\n\n        plot_results(\n            y=y_valid,\n            yhat=yhat,\n            n=100,\n            title=f\"{basis_name}\",\n            xlabel=\"Samples\",\n            ylabel=r\"y, $\\hat{y}$\",\n            data_color=\"#1f77b4\",\n            model_color=\"#ff7f0e\",\n            marker=\"o\",\n            model_marker=\"*\",\n            linewidth=1.5,\n            figsize=(10, 6),\n            style=\"seaborn-v0_8-notebook\",\n            facecolor=\"white\",\n        )\n</code></pre> <pre><code>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning:  `bias` and `n` are deprecated in 0.5.0 and will be removed in 1.0.0. Use `include_bias` and `degree`, respectively, instead.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"user-guide/tutorials/coupled-eletric-device/","title":"Coupled Eletric Device","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The CE8 coupled electric drives dataset - Nonlinear Benchmark presents a compelling use case for demonstrating the performance of SysIdentPy. This system involves two electric motors driving a pulley with a flexible belt, creating a dynamic environment ideal for testing system identification tools.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#system-overview","title":"System Overview","text":"<p>The CE8 system, illustrated in Figure 1, features: - Two Electric Motors: These motors independently control the tension and speed of the belt, providing symmetrical control around zero. This enables both clockwise and counterclockwise movements. - Pulley Mechanism: The pulley is supported by a spring, introducing a lightly damped dynamic mode that adds complexity to the system. - Speed Control Focus: The primary focus is on the speed control system. The pulley\u2019s angular speed is measured using a pulse counter, which is insensitive to the direction of the velocity.</p> <p></p> <p>Figure 1. CE8 system design.</p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#sensor-and-filtering","title":"Sensor and Filtering","text":"<p>The measurement process involves: - Pulse Counter: This sensor measures the angular speed of the pulley without regard to the direction. - Analogue Low Pass Filtering: This reduces high-frequency noise, followed by anti-aliasing filtering to prepare the signal for digital processing. The dynamic effects are mainly influenced by the electric drive time constants and the spring, with the low pass filtering having a minimal impact on the output.</p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#sota-results","title":"SOTA Results","text":"<p>SysIdentPy can be used to build robust models for identifying and modeling the complex dynamics of the CE8 system. The performance will be compared against a benchmark provided by Max D. Champneys, Gerben I. Beintema, Roland T\u00f3th, Maarten Schoukens, and Timothy J. Rogers -\u00a0Baselines for Nonlinear Benchmarks,\u00a0Workshop on Nonlinear System Identification Benchmarks, 2024.</p> <p></p> <p>The benchmark evaluate the average metric between the two experiments. That's why the SOTA method do not have the better metric for <code>test 1</code>, but it is still the best overall.  The goal of this case study is not only to showcase the robustness of SysIdentPy but also provides valuable insights into its practical applications in real-world dynamic systems.</p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/coupled-eletric-device/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the CE8 coupled electric drives dataset. This example showcases the robust performance of SysIdentPy in modeling and identifying complex dynamic systems. The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for CE8 system.</p> <p>This practical example will help users understand how to effectively utilize SysIdentPy for their own system identification tasks, leveraging its advanced features to handle the complexities of real-world dynamic systems. Let's dive into the code and explore the capabilities of SysIdentPy.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.CED(atleast_2d=True)\ndata_train_1, data_train_2 = train_val\ndata_test_1, data_test_2 = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation GerbenBeintema - nonlinear_benchmarks: The official dataload for nonlinear benchmark datasets to check the details of how to use it.</p> <p>The following plot detail the training and testing data of both experiments. Here we are trying to get two models, one for each experiment, that have a better performance than the mentioned baselines.</p> <pre><code>plt.plot(data_train_1.u)\nplt.plot(data_train_1.y)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(data_test_1.u)\nplt.plot(data_test_1.y)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(data_train_2.u)\nplt.plot(data_train_2.y)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(data_test_2.u)\nplt.plot(data_test_2.y)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"user-guide/tutorials/coupled-eletric-device/#results","title":"Results","text":"<p>First, we will set the exactly same configuration to built models for both experiments. We can have better models by optimizing the configurations individually, but we will start simple.</p> <p>A basic configuration of FROLS using a polynomial basis function with degree equal 2 is defined. The information criteria will be the default one, the <code>aic</code>. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example.</p> <p>Model for experiment 1:</p> <pre><code>y_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n    n_info_values=120,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\parameter_estimation\\estimators.py:75: UserWarning: Psi matrix might have linearly dependent rows.Be careful and check your data\n  self._check_linear_dependence_rows(psi)\n</code></pre> <p></p> <p>Model for experiment 2:</p> <pre><code>y_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n    n_info_values=120,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\parameter_estimation\\estimators.py:75: UserWarning: Psi matrix might have linearly dependent rows.Be careful and check your data\n  self._check_linear_dependence_rows(psi)\n</code></pre> <p></p> <p>The first configuration for experiment 1 is already better than the LTI ARX, LTI SS, GRU, LSTM, MLP NARX, MLP FIR, OLSTM, and the SOTA models shown in the benchmark table. Better than 8 out 11 models shown in the benchmark. For experiment 2, its better than LTI ARX, LTI SS, GRU, RNN, LSTM, OLSTM, and pNARX (7 out 11). It's a good start, but let's check if the performance improves if we set a higher lag for both <code>xlag</code> and <code>ylag</code>.</p> <p>The average metric is \\((0.1131 + 0.1059)/2 = 0.1095\\), which is very good, but worse than the SOTA (\\(0.0945\\)). We will now increase the lags for <code>x</code> and <code>y</code> to check if we get a better model. Before increasing the lags, the information criteria is shown:</p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <p></p> <p>It can be observed that after 22 regressors, adding new regressors do not improve the model performance (considering the configuration defined for that model). Because we want to try models with higher lags and higher nonlinearity degree, the stopping criteria will be changed to <code>err_tol</code> instead of information criteria. This will made the algorithm runs considerably faster.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9996,\n    n_terms=22,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>(22, 2) 0.9970964868326048\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aicc\",\n    err_tol=0.9996,\n    n_terms=22,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <p></p> <p>In the first experiment, the model showed a slight improvement, while the performance of the second experiment experienced a minor decline. Increasing the lag settings with these configurations did not result in significant changes. Therefore, let's set the polynomial degree to \\(3\\) and increase the number of terms to build the model to <code>n_terms=40</code> if the <code>err_tol</code> is not reached. It's important to note that these values are chosen empirically. We could also adjust the parameter estimation technique, the <code>err_tol</code>, the model structure selection algorithm, and the basis function, among other factors. Users are encouraged to employ hyperparameter tuning techniques to find the optimal combinations of hyperparameters.</p> <pre><code># experiment 1\ny_train = data_train_1.y\ny_test = data_test_1.y\nx_train = data_train_1.u\nx_test = data_test_1.u\n\nn = data_test_1.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9996,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\nprint(model.final_model.shape, model.err.sum())\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 1 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <pre><code>(40, 3) 0.9982136069197526\n</code></pre> <p></p> <pre><code># experiment 2\ny_train = data_train_2.y\ny_test = data_test_2.y\nx_train = data_train_2.u\nx_test = data_test_2.u\n\nn = data_test_2.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aicc\",\n    err_tol=0.9996,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\n\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=f\"Free Run simulation. Model 2 -&gt; RMSE: {round(rmse, 4)}\",\n)\n</code></pre> <p></p> <p>As shown in the plot, we have surpassed the state-of-the-art (SOTA) results with an average metric of \\((0.0969 + 0.0731)/2 = 0.0849\\). Additionally, the metric for the first experiment matches the best model in the benchmark, and the metric for the second experiment slightly exceeds the benchmark's best model. Using the same configuration for both models, we achieved the best overall results!</p>"},{"location":"user-guide/tutorials/electromechanical-system-identification-entropic-regression/","title":"Electromechanical System Identification - Entropic Regression","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>More details about this data can be found in the following paper (in Portuguese): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>df1 = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/generator/x_cc.csv\"\n)\ndf2 = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/generator/y_cc.csv\"\n)\n</code></pre> <pre><code>df2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code># we will decimate the data using d=500 in this example\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-entropic-regression/#building-a-polynomial-narx-model-using-entropic-regression-algorithm","title":"Building a Polynomial NARX model using Entropic Regression Algorithm","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = ER(\n    ylag=6,\n    xlag=6,\n    n_perm=2,\n    k=2,\n    skip_forward=True,\n    estimator=estimator,\n    basis_function=basis_function,\n)\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\utils\\deprecation.py:40: FutureWarning: Passing a string to define the estimator will rise an error in v0.4.0. \n You'll have to use ER(estimator=LeastSquares()) instead. \n The only change is that you'll have to define the estimator first instead of passing a string like 'least_squares'. \n This change will make easier to implement new estimators and it'll improve code readability.\n  warnings.warn(message, FutureWarning, stacklevel=1)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_20912\\4260657624.py:1: UserWarning: Given the higher number of possible regressors (91), the Entropic Regression algorithm may take long time to run. Consider reducing the number of regressors \n  model.fit(X=x_train, y=y_train)\n\n\n0.03276775133089435\n        Regressors   Parameters             ERR\n0                1  -6.7052E+02  0.00000000E+00\n1           y(k-1)   9.6022E-01  0.00000000E+00\n2           y(k-5)  -3.0769E-02  0.00000000E+00\n3          x1(k-2)   7.3733E+02  0.00000000E+00\n4         y(k-1)^2   1.5897E-04  0.00000000E+00\n5     y(k-2)y(k-1)  -2.2080E-04  0.00000000E+00\n6     y(k-3)y(k-1)   2.9946E-06  0.00000000E+00\n7     y(k-5)y(k-1)   4.9779E-06  0.00000000E+00\n8    x1(k-1)y(k-1)  -1.7036E-01  0.00000000E+00\n9    x1(k-2)y(k-1)  -2.0748E-01  0.00000000E+00\n10   x1(k-4)y(k-1)   8.3724E-03  0.00000000E+00\n11        y(k-2)^2   7.3635E-05  0.00000000E+00\n12   x1(k-1)y(k-2)   1.2028E-01  0.00000000E+00\n13   x1(k-2)y(k-2)   8.0270E-02  0.00000000E+00\n14   x1(k-3)y(k-2)  -3.0208E-03  0.00000000E+00\n15   x1(k-4)y(k-2)  -8.8307E-03  0.00000000E+00\n16   x1(k-1)y(k-3)  -4.9095E-02  0.00000000E+00\n17   x1(k-1)y(k-4)   1.2375E-02  0.00000000E+00\n18       x1(k-1)^2   1.1682E+02  0.00000000E+00\n19  x1(k-3)x1(k-2)   5.2777E+00  0.00000000E+00\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-metamss/","title":"Electromechanical System Identification - MetaMSS","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import MetaMSS, FROLS\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>df1 = pd.read_csv(\"./datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"./datasets/y_cc.csv\")\n\ndf2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code>df1.iloc[::500].values.shape\n</code></pre> <pre><code>(1000, 1)\n</code></pre> <p>We will decimate the data using d=500 in this example. Besides, we separate the MetaMSS data to use the same amount of samples in the prediction validation. Because MetaMSS need a train and test data to optimize the parameters of the model, in this case, we'll use 400 samples to train instead of 500 samples used for the other models. </p> <pre><code># we will decimate the data using d=500 in this example\nx_train, x_test = np.split(df1.iloc[::500].values, 2)\ny_train, y_test = np.split(df2.iloc[::500].values, 2)\n</code></pre> <pre><code>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = MetaMSS(\n    xlag=5,\n    ylag=5,\n    estimator=estimator,\n    maxiter=5,\n    n_agents=15,\n    basis_function=basis_function,\n    random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:455: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:465: RuntimeWarning: invalid value encountered in sqrt\n  se_theta = np.sqrt(var_e)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n\n\n\n\n\n&lt;sysidentpy.model_structure_selection.meta_model_structure_selection.MetaMSS at 0x229e13e3150&gt;\n</code></pre> <pre><code>yhat = model.predict(X=x_test, y=y_test, steps_ahead=None)\nrrse = root_relative_squared_error(y_test[model.max_lag :, :], yhat[model.max_lag :, :])\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_test, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_test, yhat, x_test)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.035919583498004094\n        Regressors   Parameters             ERR\n0                1  -6.1606E+02  0.00000000E+00\n1           y(k-1)   1.3117E+00  0.00000000E+00\n2           y(k-2)  -3.0579E-01  0.00000000E+00\n3          x1(k-1)   5.7920E+02  0.00000000E+00\n4          x1(k-3)  -1.8750E-01  0.00000000E+00\n5    x1(k-1)y(k-1)  -1.7305E-01  0.00000000E+00\n6    x1(k-2)y(k-1)  -1.1660E-01  0.00000000E+00\n7    x1(k-1)y(k-2)   1.2182E-01  0.00000000E+00\n8    x1(k-2)y(k-2)   3.4112E-02  0.00000000E+00\n9    x1(k-1)y(k-3)  -4.8970E-02  0.00000000E+00\n10   x1(k-1)y(k-4)   1.3846E-02  0.00000000E+00\n11       x1(k-2)^2   1.0290E+02  0.00000000E+00\n12  x1(k-3)x1(k-2)   8.6745E-01  0.00000000E+00\n13  x1(k-4)x1(k-2)   3.4336E-01  0.00000000E+00\n14  x1(k-5)x1(k-2)   2.7815E-01  0.00000000E+00\n15       x1(k-3)^2  -9.3749E-01  0.00000000E+00\n16  x1(k-4)x1(k-3)   6.1039E-01  0.00000000E+00\n17  x1(k-5)x1(k-3)   3.9361E-02  0.00000000E+00\n18       x1(k-4)^2  -4.6335E-01  0.00000000E+00\n19  x1(k-5)x1(k-4)  -9.5668E-02  0.00000000E+00\n20       x1(k-5)^2   3.6922E-01  0.00000000E+00\n</code></pre> <p></p> <p></p> <p></p> <pre><code># Plotting the evolution of the agents\nplt.plot(model.best_by_iter)\nmodel.best_by_iter[-1]\n</code></pre> <pre><code>0.0017530517788608157\n</code></pre> <p></p> <pre><code># You have access to all tested models\n# model.tested_models\n</code></pre> <pre><code>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nxlag = ylag = 5\n\nestimators = [\n    (\n        \"NARX_KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX_Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX_ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            ylag=ylag,\n            xlag=xlag,\n            basis_function=basis_function,\n            info_criteria=\"bic\",\n            err_tol=None,\n        ),\n    ),\n    (\n        \"MetaMSS\",\n        MetaMSS(\n            norm=-2,\n            xlag=xlag,\n            ylag=ylag,\n            estimator=estimator,\n            maxiter=5,\n            n_agents=15,\n            loss_func=\"metamss_loss\",\n            basis_function=basis_function,\n            random_state=42,\n        ),\n    ),\n]\n\n\nall_results = {}\nfor model_name, modelo in estimators:\n    all_results[\"%s\" % model_name] = []\n    modelo.fit(X=x_train, y=y_train)\n    yhat = modelo.predict(X=x_test, y=y_test)\n    if model_name in [\"FROLS-Polynomial_NARX\", \"MetaMSS\"]:\n        result = root_relative_squared_error(\n            y_test[modelo.max_lag :], yhat[modelo.max_lag :]\n        )\n    else:\n        result = root_relative_squared_error(y_test, yhat)\n    all_results[\"%s\" % model_name].append(result)\n    print(model_name, \"%.3f\" % np.mean(result))\n</code></pre> <pre><code>NARX_KNeighborsRegressor 1.158\nNARX_DecisionTreeRegressor 0.203\nNARX_RandomForestRegressor 0.146\nNARX_Catboost 0.120\nNARX_ARD 0.083\nFROLS-Polynomial_NARX 0.057\n\n\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:455: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:465: RuntimeWarning: invalid value encountered in sqrt\n  se_theta = np.sqrt(var_e)\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nC:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpy334\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n\n\nMetaMSS 0.036\n</code></pre> <pre><code>for model_name, metric in sorted(\n    all_results.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(model_name, np.mean(metric))\n</code></pre> <pre><code>MetaMSS 0.035919583498004094\nFROLS-Polynomial_NARX 0.05729765719062527\nNARX_ARD 0.08265856190495872\nNARX_Catboost 0.12034851661643597\nNARX_RandomForestRegressor 0.14557973585496042\nNARX_DecisionTreeRegressor 0.203057724881072\nNARX_KNeighborsRegressor 1.157787546845798\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-overview/","title":"Electromechanical System Identification - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>More details about this data can be found in the following paper (in Portuguese): https://www.researchgate.net/publication/320418710_Identificacao_de_um_motorgerador_CC_por_meio_de_modelos_polinomiais_autorregressivos_e_redes_neurais_artificiais</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import RecursiveLeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>df1 = pd.read_csv(\"../examples/datasets/x_cc.csv\")\ndf2 = pd.read_csv(\"../examples/datasets/y_cc.csv\")\n</code></pre> <pre><code>df2[5000:80000].plot(figsize=(10, 4))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <p></p> <pre><code># we will decimate the data using d=500 in this example\nx_train, x_valid = np.split(df1.iloc[::500].values, 2)\ny_train, y_valid = np.split(df2.iloc[::500].values, 2)\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-overview/#building-a-polynomial-narx-model","title":"Building a Polynomial NARX model","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=100,\n    ylag=5,\n    xlag=5,\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>C:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:618: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 66\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n0.05681502501595064\n        Regressors   Parameters             ERR\n0           y(k-1)   1.5935E+00  9.86000310E-01\n1        x1(k-1)^2   1.1202E+02  7.94813324E-03\n2         y(k-2)^2  -1.7469E-05  2.50921747E-03\n3    x1(k-1)y(k-1)  -1.5994E-01  1.43297462E-03\n4           y(k-2)  -7.4013E-01  1.02774988E-03\n5    x1(k-1)y(k-2)   1.0771E-01  5.35195948E-04\n6     y(k-3)y(k-1)   4.2578E-05  3.46258211E-04\n7        x1(k-4)^2  -6.1823E+00  6.91218347E-05\n8    x1(k-1)y(k-3)  -3.0064E-02  2.83751722E-05\n9     y(k-4)y(k-1)  -1.4505E-05  2.01620114E-05\n10  x1(k-4)x1(k-1)  -2.7490E+00  1.09189469E-05\n11    y(k-4)y(k-2)   7.2062E-06  1.27131624E-05\n12   x1(k-5)y(k-1)  -8.5557E-04  6.53111914E-06\n13  x1(k-3)x1(k-2)  -9.8645E-01  4.24331903E-06\n14  x1(k-2)x1(k-1)  -2.3609E+00  6.41299982E-06\n15         x1(k-3)  -2.0121E+02  6.43059002E-06\n16   x1(k-1)y(k-5)   3.0338E-03  2.76577885E-06\n17   x1(k-3)y(k-1)   3.2426E-02  2.79523223E-06\n18   x1(k-4)y(k-1)   5.9510E-03  1.62218750E-06\n19               1  -4.2071E+01  1.13359933E-06\n</code></pre>"},{"location":"user-guide/tutorials/electromechanical-system-identification-overview/#testing-different-autoregressive-models","title":"Testing different autoregressive models","text":"<pre><code>from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    AdaBoostRegressor,\n    GradientBoostingRegressor,\n)\nfrom sklearn.naive_bayes import GaussianNB\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sysidentpy.general_estimators import NARX\n\nbasis_function = Polynomial(degree=2)\nxlag = 5\nylag = 5\n\nestimators = [\n    (\n        \"KNeighborsRegressor\",\n        NARX(\n            base_estimator=KNeighborsRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            model_type=\"NARMAX\",\n        ),\n    ),\n    (\n        \"NARX-DecisionTreeRegressor\",\n        NARX(\n            base_estimator=DecisionTreeRegressor(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-RandomForestRegressor\",\n        NARX(\n            base_estimator=RandomForestRegressor(n_estimators=200),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"NARX-Catboost\",\n        NARX(\n            base_estimator=CatBoostRegressor(\n                iterations=800, learning_rate=0.1, depth=8\n            ),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n            fit_params={\"verbose\": False},\n        ),\n    ),\n    (\n        \"NARX-ARD\",\n        NARX(\n            base_estimator=ARDRegression(),\n            xlag=xlag,\n            ylag=ylag,\n            basis_function=basis_function,\n        ),\n    ),\n    (\n        \"FROLS-Polynomial_NARX\",\n        FROLS(\n            order_selection=True,\n            n_info_values=50,\n            ylag=xlag,\n            xlag=ylag,\n            info_criteria=\"bic\",\n            estimator=estimator,\n            basis_function=basis_function,\n            err_tol=None,\n        ),\n    ),\n]\n\nall_results = {}\nfor model_name, modelo in estimators:\n    all_results[\"%s\" % model_name] = []\n    modelo.fit(X=x_train, y=y_train)\n    yhat = modelo.predict(X=x_valid, y=y_valid)\n    result = root_relative_squared_error(\n        y_valid[modelo.max_lag :], yhat[modelo.max_lag :]\n    )\n    all_results[\"%s\" % model_name].append(result)\n    print(model_name, \"%.3f\" % np.mean(result))\n</code></pre> <pre><code>KNeighborsRegressor 1.168\nNARX-DecisionTreeRegressor 0.190\nNARX-RandomForestRegressor 0.151\nNARX-Catboost 0.121\nNARX-ARD 0.083\nFROLS-Polynomial_NARX 0.057\n</code></pre> <pre><code>for model_name, metric in sorted(\n    all_results.items(), key=lambda x: np.mean(x[1]), reverse=False\n):\n    print(model_name, np.mean(metric))\n</code></pre> <pre><code>FROLS-Polynomial_NARX 0.05729765719062527\nNARX-ARD 0.08336072971138789\nNARX-Catboost 0.12137085298392238\nNARX-RandomForestRegressor 0.15102205613876338\nNARX-DecisionTreeRegressor 0.19018792321900427\nKNeighborsRegressor 1.1676227184643708\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/","title":"F-16 Aircraft - N-steps Ahead Prediction","text":"<p>Note: The following examples do not try to replicate the results of the cited manuscripts. Even the model parameters such as ylag and xlag and size of identification and validation data are not the same of the cited papers. Moreover, sampling rate adjustment and other different data preparation are not handled here.</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>The following text was taken from the link http://www.nonlinearbenchmark.org/#F16. </p> <p>Note: The reader is reffered to the mentioned website for a complete reference concerning the experiment. For now, this notebook is just a simple example of the performance of SysIdentPy on a real world dataset. A more detailed study of this system will be published in the future.  </p> <p>The F-16 Ground Vibration Test benchmark features a high order system with clearance and friction nonlinearities at the mounting interface of the payloads.</p> <p>The experimental data made available to the Workshop participants were acquired on a full-scale F-16 aircraft on the occasion of the Siemens LMS Ground Vibration Testing Master Class, held in September 2014 at the Saffraanberg military basis, Sint-Truiden, Belgium.</p> <p>During the test campaign, two dummy payloads were mounted at the wing tips to simulate the mass and inertia properties of real devices typically equipping an F-16 in \ufb02ight. The aircraft structure was instrumented with accelerometers. One shaker was attached underneath the right wing to apply input signals. The dominant source of nonlinearity in the structural dynamics was expected to originate from the mounting interfaces of the two payloads. These interfaces consist of T-shaped connecting elements on the payload side, slid through a rail attached to the wing side. A preliminary investigation showed that the back connection of the right-wing-to-payload interface was the predominant source of nonlinear distortions in the aircraft dynamics, and is therefore the focus of this benchmark study.</p> <p>A detailed formulation of the identification problem can be found here. All the provided files and information on the F-16 aircraft benchmark system are available for download here. This zip-file contains a detailed system description, the estimation and test data sets, and some pictures of the setup. The data is available in the .csv and .mat file format.</p> <p>Please refer to the F16 benchmark as:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/#preparing-the-data","title":"Preparing the data","text":"<pre><code>f_16 = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/f_16_vibration_test/f-16.txt\",\n    header=None,\n    names=[\"x1\", \"x2\", \"y\"],\n)\n</code></pre> <pre><code>f_16.shape\n</code></pre> <pre><code>(32768, 3)\n</code></pre> <pre><code>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</code></pre> <pre><code>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</code></pre> <pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/#building-the-model","title":"Building the model","text":"<pre><code>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x25d368a8910&gt;\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft-n-steps-ahead-prediction/#defining-the-forecasting-horizon","title":"Defining the forecasting horizon","text":"<p>To perform a n-steps-ahead prediction you just need to set the \"steps_ahead\" argument.</p> <p>Note The default value for steps_ahead is None and it performs a infinity-steps-ahead prediction</p> <pre><code>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=1)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</code></pre> <pre><code>0.09610207940697202\n   Regressors   Parameters             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</code></pre> <p></p> <pre><code>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=5)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</code></pre> <pre><code>0.2168472873799118\n</code></pre> <p></p> <pre><code>y_hat = model.predict(X=x_val, y=y_val, steps_ahead=None)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nplot_results(y=y_val, yhat=y_hat, n=1000)\n</code></pre> <pre><code>0.2910089654603829\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/f-16-aircraft/","title":"F-16 Aircraft","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Note: The following examples do not try to replicate the results of the cited manuscripts. Even the model parameters such as ylag and xlag and size of identification and validation data are not the same of the cited papers. Moreover, sampling rate adjustment and other different data preparation are not handled here.</p>"},{"location":"user-guide/tutorials/f-16-aircraft/#reference","title":"Reference","text":"<p>The following text was taken from the link http://www.nonlinearbenchmark.org/#F16. </p> <p>Note: The reader is referred to the mentioned website for a complete reference concerning the experiment. For now, this notebook is just a simple example of the performance of SysIdentPy on a real world dataset. A more detailed study of this system will be published in the future.  </p> <p>The F-16 Ground Vibration Test benchmark features a high order system with clearance and friction nonlinearities at the mounting interface of the payloads.</p> <p>The experimental data made available to the Workshop participants were acquired on a full-scale F-16 aircraft on the occasion of the Siemens LMS Ground Vibration Testing Master Class, held in September 2014 at the Saffraanberg military basis, Sint-Truiden, Belgium.</p> <p>During the test campaign, two dummy payloads were mounted at the wing tips to simulate the mass and inertia properties of real devices typically equipping an F-16 in \ufb02ight. The aircraft structure was instrumented with accelerometers. One shaker was attached underneath the right wing to apply input signals. The dominant source of nonlinearity in the structural dynamics was expected to originate from the mounting interfaces of the two payloads. These interfaces consist of T-shaped connecting elements on the payload side, slid through a rail attached to the wing side. A preliminary investigation showed that the back connection of the right-wing-to-payload interface was the predominant source of nonlinear distortions in the aircraft dynamics, and is therefore the focus of this benchmark study.</p> <p>A detailed formulation of the identification problem can be found here. All the provided files and information on the F-16 aircraft benchmark system are available for download here. This zip-file contains a detailed system description, the estimation and test data sets, and some pictures of the setup. The data is available in the .csv and .mat file format.</p> <p>Please refer to the F16 benchmark as:</p> <p>J.P. No\u00ebl and M. Schoukens, F-16 aircraft benchmark based on ground vibration test data, 2017 Workshop on Nonlinear System Identification Benchmarks, pp. 19-23, Brussels, Belgium, April 24-26, 2017.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code>f_16 = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/f_16_vibration_test/f-16.txt\",\n    header=None,\n    names=[\"x1\", \"x2\", \"y\"],\n)\n</code></pre> <pre><code>f_16.shape\n</code></pre> <pre><code>(32768, 3)\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#visualizating-the-data","title":"Visualizating the data","text":"<pre><code>f_16[[\"x1\", \"x2\"]][0:500].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre> <pre><code>f_16[\"y\"][0:2000].plot(figsize=(12, 8))\n</code></pre> <pre><code>&lt;Axes: &gt;\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#spliting-the-data","title":"Spliting the data","text":"<pre><code>x1_id, x1_val = f_16[\"x1\"][0:16384].values.reshape(-1, 1), f_16[\"x1\"][\n    16384::\n].values.reshape(-1, 1)\nx2_id, x2_val = f_16[\"x2\"][0:16384].values.reshape(-1, 1), f_16[\"x2\"][\n    16384::\n].values.reshape(-1, 1)\nx_id = np.concatenate([x1_id, x2_id], axis=1)\nx_val = np.concatenate([x1_val, x2_val], axis=1)\n\ny_id, y_val = f_16[\"y\"][0:16384].values.reshape(-1, 1), f_16[\"y\"][\n    16384::\n].values.reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#setting-the-input-lags","title":"Setting the input lags","text":"<pre><code>x1lag = list(range(1, 10))\nx2lag = list(range(1, 10))\nx2lag\n</code></pre> <pre><code>[1, 2, 3, 4, 5, 6, 7, 8, 9]\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#model-training-and-evalutation","title":"Model training and evalutation","text":"<pre><code>basis_function = Polynomial(degree=1)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=39,\n    ylag=20,\n    xlag=[x1lag, x2lag],\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_id, y=y_id)\ny_hat = model.predict(X=x_val, y=y_val)\nrrse = root_relative_squared_error(y_val, y_hat)\nprint(rrse)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.2910089654603829\n   Regressors   Parameters             ERR\n0      y(k-1)   1.8387E+00  9.43378253E-01\n1      y(k-2)  -1.8938E+00  1.95167599E-02\n2      y(k-3)   1.3337E+00  1.02432261E-02\n3      y(k-6)  -1.6038E+00  8.03485985E-03\n4      y(k-9)   2.6776E-01  9.27874557E-04\n5     x2(k-7)  -2.2385E+01  3.76837313E-04\n6     x1(k-1)   8.2709E+00  6.81508210E-04\n7     x2(k-3)   1.0587E+02  1.57459800E-03\n8     x1(k-8)  -3.7975E+00  7.35086279E-04\n9     x2(k-1)   8.5725E+01  4.85358786E-04\n10     y(k-7)   1.3955E+00  2.77245281E-04\n11     y(k-5)   1.3219E+00  8.64120037E-04\n12    y(k-10)  -2.9306E-01  8.51717688E-04\n13     y(k-4)  -9.5479E-01  7.23623116E-04\n14     y(k-8)  -7.1309E-01  4.44988077E-04\n15    y(k-12)  -3.0437E-01  1.49743148E-04\n16    y(k-11)   4.8602E-01  3.34613282E-04\n17    y(k-13)  -8.2442E-02  1.43738964E-04\n18    y(k-15)  -1.6762E-01  1.25546584E-04\n19    x1(k-2)  -8.9698E+00  9.76699739E-05\n20    y(k-17)   2.2036E-02  4.55983807E-05\n21    y(k-14)   2.4900E-01  1.10314107E-04\n22    y(k-19)  -6.8239E-03  1.99734771E-05\n23    x2(k-9)  -9.6265E+01  2.98523208E-05\n24    x2(k-8)   2.2620E+02  2.34402543E-04\n25    x2(k-2)  -2.3609E+02  1.04172323E-04\n26    y(k-20)  -5.4663E-02  5.37895336E-05\n27    x2(k-6)  -2.3651E+02  2.11392628E-05\n28    x2(k-4)   1.7378E+02  2.18396315E-05\n29    x1(k-7)   4.9862E+00  2.03811842E-05\n</code></pre> <pre><code>plot_results(y=y_val, yhat=y_hat, n=1000)\nee = compute_residues_autocorrelation(y_val, y_hat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_val, y_hat, x_val[:, 0])\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre>"},{"location":"user-guide/tutorials/f-16-aircraft/#information-criteria-plot","title":"Information Criteria plot","text":"<pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n\n# You can use the plot below to choose the \"n_terms\" and run the model again with the most adequate value of terms.\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/","title":"Fourier NARX - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>This example shows how changing or adding a new basis function could improve the model</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares, RecursiveLeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\n\nnp.seterr(all=\"ignore\")\nnp.random.seed(1)\n\n%matplotlib inline\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#defining-the-system","title":"Defining the system","text":"<pre><code># Simulated system\ndef system_equation(y, u):\n    yk = (\n        (0.2 - 0.75 * np.cos(-y[0] ** 2)) * np.cos(y[0])\n        - (0.15 + 0.45 * np.cos(-y[0] ** 2)) * np.cos(y[1])\n        + np.cos(u[0])\n        + 0.2 * u[1]\n        + 0.7 * u[0] * u[1]\n    )\n    return yk\n\n\nrepetition = 5\nrandom_samples = 200\ntotal_time = repetition * random_samples\nn = np.arange(0, total_time)\n\n# Generating input\nx = np.random.normal(size=(random_samples,)).repeat(repetition)\n\n\n_, ax = plt.subplots(figsize=(12, 6))\nax.step(n, x)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$x[n]$\", fontsize=18)\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#simulate-the-system","title":"Simulate the system","text":"<pre><code>y = np.empty_like(x)\n# Initial Conditions\ny0 = [0, 0]\n\n# Simulate it\ny[0:2] = y0\nfor i in range(2, len(y)):\n    y[i] = system_equation(\n        [y[i - 1], y[i - 2]], [x[i - 1], x[i - 2]]\n    ) + np.random.normal(scale=0.1)\n\n# Plot\n_, ax = plt.subplots(figsize=(12, 6))\nax.plot(n, y)\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.grid()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#adding-noise-to-the-system","title":"Adding noise to the system","text":"<pre><code># Noise free data\nynoise_free = y.copy()\n\n# Generate noise\nv = np.random.normal(scale=0.5, size=y.shape)\n\n# Data corrupted with noise\nynoisy = ynoise_free + v\n\n# Plot\n_, ax = plt.subplots(figsize=(14, 8))\nax.plot(n, ynoise_free, label=\"Noise-free data\")\nax.plot(n, ynoisy, label=\"Corrupted data\")\nax.set_xlabel(\"$n$\", fontsize=18)\nax.set_ylabel(\"$y[n]$\", fontsize=18)\nax.legend(fontsize=18)\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#generating-training-and-test-data","title":"Generating training and test data","text":"<pre><code>n_train = 700\n\n# Identification data\ny_train = ynoisy[:n_train].reshape(-1, 1)\nx_train = x[:n_train].reshape(-1, 1)\n\n# Validation data\ny_test = ynoise_free[n_train:].reshape(-1, 1)\nx_test = x[n_train:].reshape(-1, 1)\n</code></pre>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#polynomial-basis-function","title":"Polynomial Basis Function","text":"<p>As you can see bellow, using only the polynomial basis function with the following parameters do not result in a bad model. However, lets check how is the performance using the Fourier Basis Function.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    estimator=estimator,\n    err_tol=None,\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</code></pre> <pre><code>0.6768251106751224\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#ensembling-a-fourier-basis-function","title":"Ensembling a Fourier Basis Function","text":"<p>In this case, adding the Fourier Basis Function solves the problem and returns a model capable to predict the defined system</p> <pre><code>basis_function = Fourier(degree=2, n=2, p=2 * np.pi, ensemble=True)\nsysidentpy = FROLS(\n    order_selection=True,\n    n_info_values=70,\n    xlag=2,\n    ylag=2,  # the lags for all models will be 13\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    err_tol=None,\n)\nsysidentpy.fit(X=x_train, y=y_train)\n\nyhat = sysidentpy.predict(X=x_test, y=y_test)\nfrols_loss = root_relative_squared_error(\n    y_test[sysidentpy.max_lag :], yhat[sysidentpy.max_lag :]\n)\nprint(frols_loss)\n\nplot_results(y=y_test[sysidentpy.max_lag :], yhat=yhat[sysidentpy.max_lag :])\n</code></pre> <pre><code>0.3742244715879492\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/fourier-NARX-overview/#important","title":"Important","text":"<p>Currently you can't get the model representation using <code>sysidentpy.regressor_code</code> for Fourier NARX models. Actually, you can use the method, but the representation is not accurate because we don't make clear what are the regressors related to the polynomial or related to the fourier basis function. This is a improvement to be done in future updates!</p>"},{"location":"user-guide/tutorials/general-NARX-models/","title":"General NARX Models","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>In this example we will create NARX models using different estimator like GradientBoostingRegressor, Bayesian Regression, Automatic Relevance Determination (ARD) Regression and Catboost</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import matplotlib.pyplot as plt\nfrom sysidentpy.metrics import mean_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.general_estimators import NARX\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre> <pre><code># simulated dataset\nx_train, x_valid, y_train, y_valid = get_siso_data(\n    n=10000, colored_noise=False, sigma=0.01, train_percentage=80\n)\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#importance-of-the-narx-architecture","title":"Importance of the NARX architecture","text":"<p>To get an idea of the importance of the NARX architecture, lets take a look in the performance of the models without the NARX configuration.</p> <pre><code>catboost = CatBoostRegressor(iterations=300, learning_rate=0.1, depth=6)\n</code></pre> <pre><code>gb = GradientBoostingRegressor(\n    loss=\"quantile\",\n    alpha=0.90,\n    n_estimators=250,\n    max_depth=10,\n    learning_rate=0.1,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\n</code></pre> <pre><code>def plot_results_tmp(y_valid, yhat):\n    _, ax = plt.subplots(figsize=(14, 8))\n    ax.plot(y_valid[:200], label=\"Data\", marker=\"o\")\n    ax.plot(yhat[:200], label=\"Prediction\", marker=\"*\")\n    ax.set_xlabel(\"$n$\", fontsize=18)\n    ax.set_ylabel(\"$y[n]$\", fontsize=18)\n    ax.grid()\n    ax.legend(fontsize=18)\n    plt.show()\n</code></pre> <pre><code>catboost.fit(x_train, y_train, verbose=False)\nplot_results_tmp(y_valid, catboost.predict(x_valid))\n</code></pre> <p></p> <pre><code>gb.fit(x_train, y_train.ravel())\nplot_results_tmp(y_valid, gb.predict(x_valid))\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/general-NARX-models/#introducing-the-narx-configuration-using-sysidentpy","title":"Introducing the NARX configuration using SysIdentPy","text":"<p>As you can see, you just need to pass the base estimator you want to the NARX class from SysIdentPy do build the NARX model! You can choose the lags of the input and output variables to build the regressor matrix.</p> <p>We keep the fit/predict method to make the process straightforward.</p>"},{"location":"user-guide/tutorials/general-NARX-models/#narx-with-catboost","title":"NARX with Catboost","text":"<pre><code>basis_function = Fourier(degree=1)\n\ncatboost_narx = NARX(\n    base_estimator=CatBoostRegressor(iterations=300, learning_rate=0.1, depth=8),\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n    fit_params={\"verbose\": False},\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid, steps_ahead=1)\nprint(\"MSE: \", mean_squared_error(y_valid, yhat))\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>MSE:  0.00024145290395678653\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#narx-with-gradient-boosting","title":"NARX with Gradient Boosting","text":"<pre><code>basis_function = Fourier(degree=1)\n\ngb_narx = NARX(\n    base_estimator=GradientBoostingRegressor(\n        loss=\"quantile\",\n        alpha=0.90,\n        n_estimators=250,\n        max_depth=10,\n        learning_rate=0.1,\n        min_samples_leaf=9,\n        min_samples_split=9,\n    ),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\ngb_narx.fit(X=x_train, y=y_train)\nyhat = gb_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.0011824693986863938\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#narx-with-ard","title":"NARX with ARD","text":"<pre><code>from sysidentpy.general_estimators import NARX\n\nARD_narx = NARX(\n    base_estimator=ARDRegression(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nARD_narx.fit(X=x_train, y=y_train)\nyhat = ARD_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.0011058934497373794\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#narx-with-bayesian-ridge","title":"NARX with Bayesian Ridge","text":"<pre><code>from sysidentpy.general_estimators import NARX\n\nBayesianRidge_narx = NARX(\n    base_estimator=BayesianRidge(),\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    model_type=\"NARMAX\",\n)\n\nBayesianRidge_narx.fit(X=x_train, y=y_train)\nyhat = BayesianRidge_narx.predict(X=x_valid, y=y_valid)\nprint(mean_squared_error(y_valid, yhat))\n\nplot_results(y=y_valid, yhat=yhat, n=200)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Residues\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Residues\", ylabel=\"$x_1e$\")\n</code></pre> <pre><code>0.0011077874945734536\n</code></pre>"},{"location":"user-guide/tutorials/general-NARX-models/#note","title":"Note","text":"<p>Remember you can use n-steps-ahead prediction and NAR and NFIR models now. Check how to use it in their respective examples. </p>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/","title":"Importance of Extended Least Squares","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n</code></pre>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data","text":"<p>The data is generated by simulating the following model: \\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-2} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma\\) is defined by the user.</p> <p>In the next example we will generate a data with 3000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=True, sigma=0.2, train_percentage=90\n)\n</code></pre>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#build-the-model","title":"Build the model","text":"<p>First we will train a model without the Extended Least Squares Algorithm for comparison purpose.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=False)\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.5499799245432233\n</code></pre> <p>Clearly we have something wrong with the obtained model. See the basic_steps notebook to compare the results obtained using the same data but without colored noise. But let take a look in whats is wrong.</p> <pre><code>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0        x1(k-2)  8.9976E-01  7.41682256E-01\n1         y(k-1)  2.8734E-01  8.33321202E-02\n2  x1(k-1)y(k-1)  1.2348E-01  5.10334067E-03\n</code></pre>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#biased-parameter-estimation","title":"Biased parameter estimation","text":"<p>As we can observe above, the model structure is exact the same the one that generate the data. You can se that the ERR ordered the terms in the correct way. And this is an important note regarding the Error Reduction Ratio algorithm used here: it is very robust to colored noise!! </p> <p>That is a great feature! However, although the structure is correct, the model parameters are not ok! Here we have a biased estimation! The real parameter for \\(y_{k-1}\\) is \\(0.2\\), not \\(0.3\\).</p> <p>In this case, we are actually modeling using a NARX model, not a NARMAX. The MA part exists to allow a unbiased estimation of the parameters. To achieve a unbiased estimation of the parameters we have the Extend Least Squares algorithm. Remember, if the data have only white noise, NARX is fine. </p> <p>Before applying the Extended Least Squares Algorithm we will run several NARX models to check how different the estimated parameters are from the real ones.</p> <pre><code>parameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\n# Set the theme for seaborn (optional)\nsns.set_theme()\n\nplt.figure(figsize=(14, 4))\n\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\")\nsns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\")\nsns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")\n\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#using-the-extended-least-squares-algorithm","title":"Using the Extended Least Squares algorithm","text":"<p>As shown in figure above, we have a problem to estimate the parameter for \\(y_{k-1}\\). Now we will use the Extended Least Squares Algorithm.</p> <p>In SysIdentPy, just set extended_least_squares to True and the algorithm will be applied.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares(unbiased=True)\nparameters = np.zeros([3, 50])\n\nfor i in range(50):\n    x_train, x_valid, y_train, y_valid = get_siso_data(\n        n=3000, colored_noise=True, train_percentage=90\n    )\n\n    model = FROLS(\n        order_selection=False,\n        n_terms=3,\n        ylag=2,\n        xlag=2,\n        elag=2,\n        info_criteria=\"aic\",\n        estimator=estimator,\n        basis_function=basis_function,\n    )\n\n    model.fit(X=x_train, y=y_train)\n    parameters[:, i] = model.theta.flatten()\n\n\nplt.figure(figsize=(14, 4))\n\n# Plot KDE for each parameter\nsns.kdeplot(parameters.T[:, 0], label=\"Parameter 1\")\nsns.kdeplot(parameters.T[:, 1], label=\"Parameter 2\")\nsns.kdeplot(parameters.T[:, 2], label=\"Parameter 3\")\n\n# Plot vertical lines where the real values must lie\nplt.axvline(x=0.1, color=\"k\", linestyle=\"--\", label=\"Real Value 0.1\")\nplt.axvline(x=0.2, color=\"k\", linestyle=\"--\", label=\"Real Value 0.2\")\nplt.axvline(x=0.9, color=\"k\", linestyle=\"--\", label=\"Real Value 0.9\")\n\nplt.xlabel(\"Parameter Value\")\nplt.ylabel(\"Density\")\nplt.title(\"Kernel Density Estimate of Parameters\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Great! Now we have an unbiased estimation of the parameters!</p>"},{"location":"user-guide/tutorials/importance-of-extended-least-squares/#note","title":"Note","text":"<p>Note: The Extended Least Squares is an iterative algorithm. In SysIdentpy the default is 30 iterations (<code>uiter=30</code>) because it is known from literature that the algorithm converges quickly (about 10 or 20 iterations).</p>"},{"location":"user-guide/tutorials/information-criteria-overview/","title":"Information Criteria - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#comparing-different-information-criteria-methods","title":"Comparing different information criteria methods","text":"<p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\n</code></pre>"},{"location":"user-guide/tutorials/information-criteria-overview/#generating-sample-data","title":"Generating sample data","text":"<p>The data is generated by simulating the following model: \\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <p>In the next example we will generate a data with 3000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_test, y_train, y_test = get_siso_data(\n    n=100, colored_noise=False, sigma=0.1, train_percentage=70\n)\n</code></pre> <p>The idea is to show the impact of the information criteria to select the number of terms to compose the final model. You will se why it is an auxiliary tool and let the algorithm select the number of terms based on the minimum value is not a good idea when dealing with data highly corrupted by noise (even white noise) </p> <p>Note: You may find different results when running the examples. This is due the fact we are not setting a fixed random generator for the sample data. However, the main analysis remain.</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#aic","title":"AIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    # estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\nmodel.fit(X=x_train, y=y_train)\n\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.1681621129389993\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2076E-01  9.41001395E-01\n1          y(k-1)   1.7063E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7342E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -9.7902E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   4.9319E-02  1.85339089E-03\n5        y(k-2)^2  -5.6743E-02  1.02439383E-03\n6         x1(k-1)  -2.0179E-02  6.78305323E-04\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-273.81858224, -311.60797635, -331.34011486, -338.49936124,\n       -342.10339048, -342.27073244, -342.82764626, -342.16492383,\n       -341.04704839, -339.58437034, -337.79642875, -336.20531349,\n       -333.72427584, -331.48645717, -329.53042523])\n</code></pre> <p>As can be seen above, the minimum value make the algorithm choose a model with 4 terms. However, if you check the plot, 3 terms is the best choice. Increasing the number of terms from 3 upwards do not lead to a better model since the difference is very small.</p> <p>In this case, you should run the model again with the parameters n_terms=3! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again!</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#aicc","title":"AICc","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aicc\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.1697650652880654\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2659E-01  9.41001395E-01\n1          y(k-1)   1.7219E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03\n4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03\n5        y(k-2)^2  -4.8117E-02  1.02439383E-03\n6         x1(k-1)  -2.4728E-02  6.78305323E-04\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-273.99834706, -311.49708248, -331.28179881, -338.08184328,\n       -341.32119362, -341.3373076 , -341.51818541, -340.50119461,\n       -339.16231408, -336.96924137, -334.34018578, -331.63849434,\n       -328.92685178, -325.8181513 , -322.55039655])\n</code></pre> <p>As can be seen above, the minimum value make the algorithm choose a model with 4 terms. AICc, however, have major differences compared with AIC when the number of samples is small.</p> <p>In this case, you should run the model again with the parameters n_terms=3! The ERR algorithm ordered the terms in a correct way, so you will get the exact model structure again! </p>"},{"location":"user-guide/tutorials/information-criteria-overview/#bic","title":"BIC","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"bic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.16868631871856155\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.3050E-01  9.41001395E-01\n1          y(k-1)   1.7980E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-271.83944541, -307.24268246, -324.99827569, -329.8387331 ,\n       -331.19139703, -329.39731055, -327.84829815, -325.18581094,\n       -322.29019301, -318.63381344, -314.63988674, -310.67712915,\n       -306.81399235, -302.66957173, -298.4885502 ])\n</code></pre> <p>BIC did a better job in this case! The way it penalizes the model regarding the number of terms ensure that the minimum value here was exact the number of expected terms to compose the model. Good, but not always the best method!</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#lilc","title":"LILC","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"lilc\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_test, yhat=yhat, n=1000)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.16868631871856155\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.3050E-01  9.41001395E-01\n1          y(k-1)   1.7980E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7026E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -8.5581E-02  2.75495842E-03\n4  x1(k-2)x1(k-1)   7.0047E-02  1.85339089E-03\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-273.17951619, -309.92282401, -329.01848803, -335.19901621,\n       -337.89175092, -337.43773522, -337.22879359, -335.90637716,\n       -334.35083001, -332.03452122, -329.3806653 , -326.75797849,\n       -324.23491246, -321.43056262, -318.58961187])\n</code></pre> <p>LILC also includes spurious terms. Like AIC, it fails to automatically select the correct terms but you could select the right number based on the plot above!</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#fpe","title":"FPE","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"fpe\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test)\nrrse = root_relative_squared_error(y_test, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>0.1697650652880654\n       Regressors   Parameters             ERR\n0         x1(k-2)   9.2659E-01  9.41001395E-01\n1          y(k-1)   1.7219E-01  2.71018399E-02\n2   x1(k-1)y(k-1)   1.7454E-01  8.79812755E-03\n3   x1(k-1)y(k-2)  -1.0170E-01  2.75495842E-03\n4  x1(k-2)x1(k-1)   5.7955E-02  1.85339089E-03\n5        y(k-2)^2  -4.8117E-02  1.02439383E-03\n6         x1(k-1)  -2.4728E-02  6.78305323E-04\n\n\n\n\n\nText(0, 0.5, 'Information Criteria')\n</code></pre> <pre><code>model.info_values\n</code></pre> <pre><code>array([-274.05880892, -311.68054386, -331.65290152, -338.70751749,\n       -342.27085495, -342.68306863, -343.33508312, -342.86743567,\n       -342.15953986, -340.68281499, -338.85950374, -337.05732543,\n       -335.3437066 , -333.33668596, -331.27985457])\n</code></pre> <p>FPE also failed to automatically select the right number of terms! But, as we pointed out before, Information Criteria is an auxiliary tool! If you look at the plots, all the methods allows you to choose the right numbers of terms!</p>"},{"location":"user-guide/tutorials/information-criteria-overview/#important-note","title":"Important Note","text":"<p>Here we are dealing with a known model structure! Concerning real data, we do not know the right number of terms so the methods above stands as excellent tools to help you out!</p> <p>If you check the metrics above, even with the models with more terms, you will see excellent metrics! But System Identification always search for the best model structure! Model Structure Selection is the core of NARMAX methods! In this respect, the examples are to show basic concepts and how the algorithms work!</p>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/","title":"Load Forecasting Benchmark","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#note","title":"Note","text":"<p>The following example is not intended to say that one library is better than another. The main focus of these examples is to show that SysIdentPy can be a good alternative for people looking to model time series.</p> <p>We will compare the results obtained against neural prophet library.</p> <p>For the sake of brevity, from SysIdentPy only the MetaMSS, AOLS and FROLS (with polynomial base function) methods will be used. See the SysIdentPy documentation to learn other ways of modeling with the library.</p> <p>We will compare a 1-step ahead forecaster on electricity consumption of a building. The config of the neuralprophet model was taken from the neuralprophet documentation (https://neuralprophet.com/html/example_links/energy_data_example.html)</p> <p>The training will occur on 80% of the data, reserving the last 20% for the validation.</p> <p>Note: the data used in this example can be found in neuralprophet github.</p>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#benchmark-results","title":"Benchmark results:","text":"No. Package Mean Squared Error 1 SysIdentPy (FROLS) 4183 2 SysIdentPy (MetaMSS) 5264 3 SysIdentPy (AOLS) 5264 4 NeuralProphet 11471 <pre><code>from warnings import simplefilter\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import mean_squared_error\n\nfrom sktime.datasets import load_airline\nfrom neuralprophet import NeuralProphet\nfrom neuralprophet import set_random_seed\n\nsimplefilter(\"ignore\", FutureWarning)\nnp.seterr(all=\"ignore\")\n\n%matplotlib inline\n\nloss = mean_squared_error\n</code></pre>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#frols","title":"FROLS","text":"<pre><code>raw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_hospital/SanFrancisco_Hospital.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy = FROLS(\n    order_selection=True,\n    info_criteria=\"bic\",\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n)\nsysidentpy.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy.max_lag :], y_test])\n\nyhat = sysidentpy.predict(X=x_test, y=y_test, steps_ahead=1)\nsysidentpy_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy.max_lag :]),\n)\nprint(sysidentpy_loss)\n\n\nplot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8))\n</code></pre> <pre><code>c:\\Users\\wilso\\Desktop\\projects\\GitHub\\sysidentpy\\sysidentpy\\model_structure_selection\\ofr_base.py:537: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 5\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n4183.359498155755\n</code></pre>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#metamss","title":"MetaMSS","text":"<pre><code>raw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_hospital/SanFrancisco_Hospital.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nsysidentpy_metamss = MetaMSS(\n    xlag=2,\n    ylag=2,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    steps_ahead=1,\n    n_agents=15,\n    random_state=42,\n)\nsysidentpy_metamss.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_metamss.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_metamss.max_lag :], y_test])\n\nyhat = sysidentpy_metamss.predict(X=x_test, y=y_test, steps_ahead=1)\nmetamss_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_metamss.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_metamss.max_lag :]),\n)\nprint(metamss_loss)\n\n\nplot_results(y=y_test[:700], yhat=yhat[:700], n=504, figsize=(18, 8))\n</code></pre> <pre><code>5264.428783519863\n</code></pre>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#aols","title":"AOLS","text":"<pre><code>set_random_seed(42)\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_hospital/SanFrancisco_Hospital.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\ndf_train, df_val = df.iloc[:7008, :], df.iloc[7008:, :]\n\ny = df[\"y\"].values.reshape(-1, 1)\ny_train = df_train[\"y\"].values.reshape(-1, 1)\ny_test = df_val[\"y\"].values.reshape(-1, 1)\n\nx_train = df_train[\"ds\"].dt.hour.values.reshape(-1, 1)\nx_test = df_val[\"ds\"].dt.hour.values.reshape(-1, 1)\nbasis_function = Polynomial(degree=1)\nsysidentpy_AOLS = AOLS(xlag=2, ylag=2, basis_function=basis_function)\nsysidentpy_AOLS.fit(X=x_train, y=y_train)\nx_test = np.concatenate([x_train[-sysidentpy_AOLS.max_lag :], x_test])\ny_test = np.concatenate([y_train[-sysidentpy_AOLS.max_lag :], y_test])\n\nyhat = sysidentpy_AOLS.predict(X=x_test, y=y_test, steps_ahead=1)\naols_loss = loss(\n    pd.Series(y_test.flatten()[sysidentpy_AOLS.max_lag :]),\n    pd.Series(yhat.flatten()[sysidentpy_AOLS.max_lag :]),\n)\nprint(aols_loss)\n\n\nplot_results(y=y_test[-504:], yhat=yhat[-504:], n=504, figsize=(18, 8))\n</code></pre> <pre><code>5264.42917196841\n</code></pre>"},{"location":"user-guide/tutorials/load-forecasting-benchmark/#neural-prophet","title":"Neural Prophet","text":"<pre><code>set_random_seed(42)\n\nraw = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/san_francisco_hospital/SanFrancisco_Hospital.csv\"\n)\ndf = pd.DataFrame()\ndf[\"ds\"] = pd.date_range(\"1/1/2015 1:00:00\", freq=str(60) + \"Min\", periods=8760)\ndf[\"y\"] = raw.iloc[:, 0].values\n\nm = NeuralProphet(\n    n_lags=24, ar_sparsity=0.5, num_hidden_layers=2, d_hidden=20, learning_rate=0.001\n)\nmetrics = m.fit(df, freq=\"H\", valid_p=0.2)\n\ndf_train, df_val = m.split_df(df, valid_p=0.2)\nm.test(df_val)\n\nfuture = m.make_future_dataframe(df_val, n_historic_predictions=True)\nforecast = m.predict(future)\nprint(loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1]))\n\nneuralprophet_loss = loss(forecast[\"y\"][24:-1], forecast[\"yhat1\"][24:-1])\n</code></pre> <pre><code>WARNING: nprophet - fit: Parts of code may break if using other than daily data.\n\n\nINFO: nprophet.utils - set_auto_seasonalities: Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this.\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set batch_size to 32\nINFO: nprophet.config - set_auto_batch_epoch: Auto-set epochs to 7\nEpoch[7/7]: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7/7 [00:04&lt;00:00,  1.74it/s, SmoothL1Loss=0.0102, MAE=81.6, RegLoss=0.011] \nINFO: nprophet - _evaluate: Validation metrics:    SmoothL1Loss    MAE\n1         0.011 84.733\n\n\n11397.103026422525\n</code></pre> <pre><code>plt.figure(figsize=(18, 8))\nplt.plot(forecast[\"y\"][-504:], \"ro-\")\nplt.plot(forecast[\"yhat1\"][-504:], \"k*-\")\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x237847417f0&gt;]\n</code></pre> <pre><code>results = {\n    \"SysIdentPy - FROLS\": sysidentpy_loss,\n    \"SysIdentPy (AOLS)\": aols_loss,\n    \"SysIdentPy (MetaMSS)\": metamss_loss,\n    \"NeuralProphet\": neuralprophet_loss,\n}\n\nsorted(results.items(), key=lambda result: result[1])\n</code></pre> <pre><code>[('SysIdentPy - FROLS', 4183.359498155755),\n ('SysIdentPy (MetaMSS)', 5264.429171346123),\n ('SysIdentPy (AOLS)', 5264.42917196841),\n ('NeuralProphet', 11397.103026422525)]\n</code></pre>"},{"location":"user-guide/tutorials/m4-benchmark/","title":"M4 Benchmark","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The M4 dataset is a well known resource for time series forecasting, offering a wide range of data series used to test and improve forecasting methods. Created for the M4 competition organized by Spyros Makridakis, this dataset has driven many advancements in forecasting techniques.</p> <p>The M4 dataset includes 100,000 time series from various fields such as demographics, finance, industry, macroeconomics, and microeconomics, which were selected randomly from the ForeDeCk database. The series come in different frequencies (yearly, quarterly, monthly, weekly, daily, and hourly), making it a comprehensive collection for testing forecasting methods.</p> <p>In this case study, we will focus on the hourly subset of the M4 dataset. This subset consists of time series data recorded hourly, providing a detailed and high-frequency look at changes over time. Hourly data presents unique challenges due to its granularity and the potential for capturing short-term fluctuations and patterns.</p> <p>The M4 dataset provides a standard benchmark to compare different forecasting methods, allowing researchers and practitioners to evaluate their models consistently. With series from various domains and frequencies, the M4 dataset represents real-world forecasting challenges, making it valuable for developing robust forecasting techniques. The competition and the dataset itself have led to the creation of new algorithms and methods, significantly improving forecasting accuracy and reliability.</p> <p>We will present a end to end walkthrough using the M4 hourly dataset to demonstrate the capabilities of SysIdentPy. SysIdentPy offers a range of tools and techniques designed to effectively handle the complexities of time series data, but we will focus on fast and easy setup for this case. We will cover model selection and evaluation metrics specific to the hourly dataset.</p> <p>By the end of this case study, you will have a solid understanding of how to use SysIdentPy for forecasting with the M4 hourly dataset, preparing you to tackle similar forecasting challenges in real-world scenarios.</p>"},{"location":"user-guide/tutorials/m4-benchmark/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\ndatasetsforecast==0.0.8\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\ns3fs==2024.6.1\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/m4-benchmark/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import warnings\nimport numpy as np\nimport pandas as pd\nfrom pandas.errors import SettingWithCopyWarning\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import (\n    root_relative_squared_error,\n    symmetric_mean_absolute_percentage_error,\n)\nfrom sysidentpy.utils.plotting import plot_results\n\nfrom datasetsforecast.m4 import M4, M4Evaluation\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=UserWarning)\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\ntrain = pd.read_csv(\"https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\")\ntest = pd.read_csv(\n    \"https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\"\n).rename(columns={\"y\": \"y_test\"})\n</code></pre> <p>The following plots provide a visualization of the training data for a small subset of the time series. The plot shows the raw data, giving you an insight into the patterns and behaviors inherent in each series.</p> <p>By observing the data, you can get a sense of the variety and complexity of the time series we are working with. The plots can reveal important characteristics such as trends, seasonal patterns, and potential anomalies within the time series. Understanding these elements is crucial for the development of accurate forecasting models.</p> <p>However, when dealing with a large number of different time series, it is common to start with broad assumptions rather than detailed individual analysis. In this context, we will adopt a similar approach. Instead of going into the specifics of each dataset, we will make some general assumptions and see how SysIdentPy handles them.</p> <p>This approach provides a practical starting point, demonstrating how SysIdentPy can manage different types of time series data without too much work. As you become more familiar with the tool, you can refine your models with more detailed insights. For now, let's focus on using SysIdentPy to create the forecasts based on these initial assumptions.</p> <p>Our first assumption is that there is a 24-hour seasonal pattern in the series. By examining the plots below, this seems reasonable. Therefore, we'll begin building our models with <code>ylag=24</code>.</p> <pre><code>ax = (\n    train[train[\"unique_id\"] == \"H10\"]\n    .reset_index(drop=True)[\"y\"]\n    .plot(figsize=(15, 2), title=\"H10\")\n)\nxcoords = [a for a in range(24, 24 * 30, 24)]\n\nfor xc in xcoords:\n    plt.axvline(x=xc, color=\"red\", linestyle=\"--\", alpha=0.5)\n</code></pre> <p></p> <p>Lets check build a model for the <code>H20</code> group before we extrapolate the settings for every group. Because there are no input features, we will be using a <code>NAR</code> model type in SysIdentPy. To keep things simple and fast, we will start with Polynomial basis function with degree \\(1\\).</p> <pre><code>unique_id = \"H20\"\ny_id = train[train[\"unique_id\"] == unique_id][\"y\"].values.reshape(-1, 1)\ny_val = test[test[\"unique_id\"] == unique_id][\"y_test\"].values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n    order_selection=True,\n    ylag=24,\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\n\nmodel.fit(y=y_id)\ny_val = np.concatenate([y_id[-model.max_lag :], y_val])\ny_hat = model.predict(y=y_val, forecast_horizon=48)\nsmape = symmetric_mean_absolute_percentage_error(\n    y_val[model.max_lag : :], y_hat[model.max_lag : :]\n)\n\nplot_results(\n    y=y_val[model.max_lag :],\n    yhat=y_hat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Group: {unique_id} - SMAPE {round(smape, 4)}\",\n)\n</code></pre> <p></p> <p>Probably, the result are not optimal and will not work for every group. However, let's check how this setting performs against the winner model \u00a0M4 time series competition: the Exponential Smoothing with Recurrent Neural Networks (ESRNN).</p> <pre><code>esrnn_url = (\n    \"https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-118.zip\"\n)\nesrnn_forecasts = M4Evaluation.load_benchmark(\"data\", \"Hourly\", esrnn_url)\nesrnn_evaluation = M4Evaluation.evaluate(\"data\", \"Hourly\", esrnn_forecasts)\n\nesrnn_evaluation\n</code></pre> SMAPE MASE OWA Hourly 9.328443 0.893046 0.440163 <p>The following code took only 49 seconds to run on my machine (AMD Ryzen 5 5600x processor, 32GB RAM at 3600MHz). Because of its efficiency, I didn't create a parallel version. By the end of this use case, you will see how SysIdentPy can be both fast and effective, delivering good results without too much optimization.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n    y_id = data[\"y\"].values.reshape(-1, 1)\n    basis_function = Polynomial(degree=1)\n    model = FROLS(\n        ylag=24,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        n_info_values=25,\n    )\n    try:\n        model.fit(y=y_id)\n        y_val = y_id[-model.max_lag :].reshape(-1, 1)\n        y_hat = model.predict(y=y_val, forecast_horizon=48)\n        r.append(\n            [\n                u_id * len(y_hat[model.max_lag : :]),\n                ds_test,\n                y_hat[model.max_lag : :].ravel(),\n            ]\n        )\n    except Exception:\n        print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(\n    [\"unique_id\", \"ds\", \"NARMAX_1\"]\n)\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)  # .clip(lower=10)\npivot_df = results_1.pivot(index=\"unique_id\", columns=\"ds\", values=\"NARMAX_1\")\nresults = pivot_df.to_numpy()\n\nM4Evaluation.evaluate(\"data\", \"Hourly\", results)\n</code></pre> SMAPE MASE OWA Hourly 16.034196 0.958083 0.636132 <p>The initial results are reasonable, but they don't quite match the performance of <code>ESRNN</code>. These results are based solely on our first assumption. To better understand the performance, let\u2019s examine the groups with the worst results.</p> <p></p> <p>The following plot illustrates two such groups, <code>H147</code> and <code>H136</code>. Both exhibit a 24-hour seasonal pattern.</p> <p></p> <p></p> <p>However, a closer look reveals an additional insight: in addition to the daily pattern, these series also show a weekly pattern. Observe how the data looks like when we split the series into weekly segments.</p> <p></p> <pre><code>xcoords = list(range(0, 168 * 5, 168))\nfiltered_train = train[train[\"unique_id\"] == \"H147\"].reset_index(drop=True)\n\nfig, ax = plt.subplots(figsize=(10, 1.5 * len(xcoords[1:])))\nfor i, start in enumerate(xcoords[:-1]):\n    end = xcoords[i + 1]\n    ax = fig.add_subplot(len(xcoords[1:]), 1, i + 1)\n    filtered_train[\"y\"].iloc[start:end].plot(ax=ax)\n    ax.set_title(f\"H147 -&gt; Slice {i+1}: Hour {start} to {end-1}\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Therefore, we will build models setting <code>ylag=168</code>.</p> <p>Note that this is a very high number for lags, so be careful if you want to try it with higher polynomial degrees because the time to run the models can increase significantly. I tried some configurations with polynomial degree equal to 2 and only took \\(6\\) minutes to run (even less, using <code>AOLS</code>), without making the code run in parallel. As you can see, SysIdentPy can be very fast and you can make it faster by applying parallelization.</p> <pre><code># this took 2min to run on my computer.\nr = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n    y_id = data[\"y\"].values.reshape(-1, 1)\n    basis_function = Polynomial(degree=1)\n    model = FROLS(\n        ylag=168,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n    )\n    try:\n        model.fit(y=y_id)\n        y_val = y_id[-model.max_lag :].reshape(-1, 1)\n        y_hat = model.predict(y=y_val, forecast_horizon=48)\n        r.append(\n            [\n                u_id * len(y_hat[model.max_lag : :]),\n                ds_test,\n                y_hat[model.max_lag : :].ravel(),\n            ]\n        )\n    except Exception:\n        print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(\n    [\"unique_id\", \"ds\", \"NARMAX_1\"]\n)\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)  # .clip(lower=10)\npivot_df = results_1.pivot(index=\"unique_id\", columns=\"ds\", values=\"NARMAX_1\")\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate(\"data\", \"Hourly\", results)\n</code></pre> SMAPE MASE OWA Hourly 10.475998 0.773749 0.446471 <p>Now, the results are much closer to those of the <code>ESRNN</code> model! While the Symmetric Mean Absolute Percentage Error (<code>SMAPE</code>) is slightly worse, the Mean Absolute Scaled Error (<code>MASE</code>) is better when comparing against <code>ESRNN</code>, leading to a very similar Overall Weighted Average (<code>OWA</code>) metric. Remarkably, these results are achieved using only simple <code>AR</code> models. Next, let's see if the <code>AOLS</code> method can provide even better results.</p> <pre><code>r = []\nds_test = list(range(701, 749))\nfor u_id, data in train.groupby(by=[\"unique_id\"], observed=True):\n    y_id = data[\"y\"].values.reshape(-1, 1)\n    basis_function = Polynomial(degree=1)\n    model = AOLS(\n        ylag=168,\n        basis_function=basis_function,\n        model_type=\"NAR\",\n        # due to high lag settings, k was increased to 6 as an initial guess\n        k=6,\n    )\n    try:\n        model.fit(y=y_id)\n        y_val = y_id[-model.max_lag :].reshape(-1, 1)\n        y_hat = model.predict(y=y_val, forecast_horizon=48)\n        r.append(\n            [\n                u_id * len(y_hat[model.max_lag : :]),\n                ds_test,\n                y_hat[model.max_lag : :].ravel(),\n            ]\n        )\n    except Exception:\n        print(f\"Problem with {u_id}\")\n\nresults_1 = pd.DataFrame(r, columns=[\"unique_id\", \"ds\", \"NARMAX_1\"]).explode(\n    [\"unique_id\", \"ds\", \"NARMAX_1\"]\n)\nresults_1[\"NARMAX_1\"] = results_1[\"NARMAX_1\"].astype(float)  # .clip(lower=10)\npivot_df = results_1.pivot(index=\"unique_id\", columns=\"ds\", values=\"NARMAX_1\")\nresults = pivot_df.to_numpy()\nM4Evaluation.evaluate(\"data\", \"Hourly\", results)\n</code></pre> SMAPE MASE OWA Hourly 9.951141 0.809965 0.439755 <p>The Overall Weighted Average (<code>OWA</code>) is even better than that of the <code>ESRNN</code> model! Additionally, the <code>AOLS</code> method was incredibly efficient, taking only 6 seconds to run. This combination of high performance and rapid execution makes <code>AOLS</code> a compelling alternative for time series forecasting in cases with multiple series.</p> <p>Before we finish, let's verify how the performance of the <code>H147</code> model has improved with the <code>ylag=168</code> setting.</p> <p></p> <p>Based on the M4 benchmark paper, we could also clip the predictions lower than 10 to 10 and the results would be slightly better. But this is left to the user.</p> <p>We could achieve even better performance with some fine-tuning of the model configuration. However, I\u2019ll leave exploring these alternative adjustments as an exercise for the user. However, keep in mind that experimenting with different settings does not always guarantee improved results. A deeper theoretical knowledge can often lead you to better configurations and, hence, better results.</p>"},{"location":"user-guide/tutorials/model-with-multiple-inputs/","title":"Model With Multiple Inputs","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p>"},{"location":"user-guide/tutorials/model-with-multiple-inputs/#generating-2-input-1-output-sample-data","title":"Generating 2 input 1 output sample data","text":"<p>The data is generated by simulating the following model:</p> <p>\\(y_k = 0.4y_{k-1}^2 + 0.1y_{k-1}x1_{k-1} + 0.6x2_{k-1} -0.3x1_{k-1}x2_{k-2} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.001\\)</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.generate_data import get_miso_data\n</code></pre> <pre><code>x_train, x_valid, y_train, y_valid = get_miso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre> <p>There is a specific difference for multiple input data.</p> <ul> <li>You have to pass the lags for each input in a nested list (e.g., [[1, 2], [1, 2]])</li> </ul> <p>The remainder settings remains the same.</p>"},{"location":"user-guide/tutorials/model-with-multiple-inputs/#build-the-model","title":"Build the model","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_terms=4,\n    ylag=2,\n    xlag=[[1, 2], [1, 2]],\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n</code></pre> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1a88cc17350&gt;\n</code></pre>"},{"location":"user-guide/tutorials/model-with-multiple-inputs/#model-evaluation","title":"Model evaluation","text":"<pre><code>yhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <pre><code>0.00314141814133057\n       Regressors   Parameters             ERR\n0         x2(k-1)   5.9999E-01  9.15006949E-01\n1  x2(k-2)x1(k-1)  -3.0010E-01  4.31748224E-02\n2        y(k-1)^2   3.9976E-01  4.15131661E-02\n3   x1(k-1)y(k-1)   1.0028E-01  2.96827987E-04\n</code></pre> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre>"},{"location":"user-guide/tutorials/modeling-a-magneto-rheological-damper-device/","title":"Modeling a Magneto Rheological Damper Device","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The memory effects between quasi-static input and output make the modeling of hysteretic systems very difficult. Physics-based models are often used to describe the hysteresis loops, but these models usually lack the simplicity and efficiency required in practical applications involving system characterization, identification, and control. As detailed in Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models, NARX models have proven to be a feasible choice to describe the hysteresis loops. See Chapter 8 for a detailed background. However, even considering the sufficient conditions for rate independent hysteresis representation, classical structure selection algorithms fails to return a model with decent performance and the user needs to set a multi-valued function to ensure the occurrence of the bounding structure \\(\\mathcal{H}\\) (Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models).</p> <p>Even though some progress has been made, previous work has been limited to models with a single equilibrium point. The present case study aims to present new prospects in the model structure selection of hysteretic systems regarding the cases where the models have multiple inputs and it is not restricted concerning the number of equilibrium points. For that, the MetaMSS algorithm will be used to build a model for a magneto-rheological damper (MRD) considering the mentioned sufficient conditions.</p>"},{"location":"user-guide/tutorials/modeling-a-magneto-rheological-damper-device/#a-brief-description-of-the-bouc-wen-model-of-magneto-rheological-damper-device","title":"A Brief description of the Bouc-Wen model of magneto-rheological damper device","text":"<p>The data used in this study-case is the Bouc-Wen model (Bouc, R - Forced Vibrations of a Mechanical System with Hysteresis), (Wen, Y. X. - Method for Random Vibration of Hysteretic Systems) of an MRD whose schematic diagram is shown in the figure below.</p> <p></p> <p>The model for a magneto-rheological damper proposed by Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback.</p> <p>The general form of the Bouc-Wen model can be described as (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback):</p> \\[ \\begin{equation} \\dfrac{dz}{dt} = g\\left[x,z,sign\\left(\\dfrac{dx}{dt}\\right)\\right]\\dfrac{dx}{dt}, \\end{equation} \\] <p>where \\(z\\) is the hysteretic model output, \\(x\\) the input and \\(g[\\cdot]\\) a nonlinear function of \\(x\\), \\(z\\) and \\(sign (dx/dt)\\). (Spencer, B. F. and Sain, M. K. - Controlling buildings: a new frontier in feedback) proposed the following phenomenological model for the aforementioned device:</p> \\[ \\begin{align} f&amp;= c_1\\dot{\\rho}+k_1(x-x_0),\\nonumber\\\\ \\dot{\\rho}&amp;=\\dfrac{1}{c_0+c_1}[\\alpha z+c_0\\dot{x}+k_0(x-\\rho)],\\nonumber\\\\ \\dot{z}&amp;=-\\gamma|\\dot{x}-\\dot{\\rho}|z|z|^{n-1}-\\beta(\\dot{x}-\\dot{\\rho})|z|^n+A(\\dot{x}-\\dot{\\rho}),\\nonumber\\\\ \\alpha&amp;=\\alpha_a+\\alpha_bu_{bw},\\nonumber\\\\ c_1&amp;=c_{1a}+c_{1b}u_{bw},\\nonumber\\\\ c_0&amp;=c_{0a}+c_{0b}u_{bw},\\nonumber\\\\ \\dot{u}_{bw}&amp;=-\\eta(u_{bw}-E). \\end{align} \\] <p>where \\(f\\) is the damping force, \\(c_1\\) and \\(c_0\\) represent the viscous coefficients, \\(E\\) is the input voltage, \\(x\\) is the displacement and \\(\\dot{x}\\) is the velocity of the model. The parameters of the system (see table below) were taken from Leva, A. and Piroddi, L. - NARX-based technique for the modelling of magneto-rheological damping devices.</p> Parameter Value Parameter Value \\(c_{0_a}\\) \\(20.2 \\, N \\, s/cm\\) \\(\\alpha_{a}\\) \\(44.9 \\, N/cm\\) \\(c_{0_b}\\) \\(2.68 \\, N \\, s/cm \\, V\\) \\(\\alpha_{b}\\) \\(638 \\, N/cm\\) \\(c_{1_a}\\) \\(350 \\, N \\, s/cm\\) \\(\\gamma\\) \\(39.3 \\, cm^{-2}\\) \\(c_{1_b}\\) \\(70.7 \\, N \\, s/cm \\, V\\) \\(\\beta\\) \\(39.3 \\, cm^{-2}\\) \\(k_{0}\\) \\(15 \\, N/cm\\) \\(n\\) \\(2\\) \\(k_{1}\\) \\(5.37 \\, N/cm\\) \\(\\eta\\) \\(251 \\, s^{-1}\\) \\(x_{0}\\) \\(0 \\, cm\\) \\(A\\) \\(47.2\\) <p>For this particular study, both displacement and voltage inputs, \\(x\\) and \\(E\\), respectively, were generated by filtering a white Gaussian noise sequence using a Blackman-Harris FIR filter with \\(6\\)Hz cutoff frequency. The integration step-size was set to \\(h = 0.002\\), following the procedures described in Martins, S. A. M. and Aguirre, L. A. - Sufficient conditions for rate-independent hysteresis in autoregressive identified models. These procedures are for identification purposes only since the inputs of a MRD could have several different characteristics.</p> <p>The data used in this example is provided by the Professor Samir Angelo Milani Martins.</p> <p>The challenges are:</p> <ul> <li>it possesses a nonlinearity featuring memory, i.e. a dynamic nonlinearity;</li> <li>the nonlinearity is governed by an internal variable z(t), which is not measurable;</li> <li>the nonlinear functional form in the Bouc Wen equation is nonlinear in the parameter;</li> <li>the nonlinear functional form in the Bouc Wen equation does not admit a finite Taylor series expansion because of the presence of absolute values</li> </ul>"},{"location":"user-guide/tutorials/modeling-a-magneto-rheological-damper-device/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nscikit-learn==1.4.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/modeling-a-magneto-rheological-damper-device/#sysidentpy-configuration","title":"SysIdentPy Configuration","text":"<pre><code>import numpy as np\nfrom sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\ndf = pd.read_csv(\n    \"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/bouc_wen/boucwen_histeretic_system.csv\"\n)\nscaler_x = MaxAbsScaler()\nscaler_y = MaxAbsScaler()\n\ninit = 400\nx_train = df[[\"E\", \"v\"]].iloc[init : df.shape[0] // 2, :]\nx_train[\"sign_v\"] = np.sign(df[\"v\"])\nx_train = scaler_x.fit_transform(x_train)\n\nx_test = df[[\"E\", \"v\"]].iloc[df.shape[0] // 2 + 1 : df.shape[0] - init, :]\nx_test[\"sign_v\"] = np.sign(df[\"v\"])\nx_test = scaler_x.transform(x_test)\n\ny_train = df[[\"f\"]].iloc[init : df.shape[0] // 2, :].values.reshape(-1, 1)\ny_train = scaler_y.fit_transform(y_train)\n\ny_test = (\n    df[[\"f\"]].iloc[df.shape[0] // 2 + 1 : df.shape[0] - init, :].values.reshape(-1, 1)\n)\ny_test = scaler_y.transform(y_test)\n\n# Plotting the data\nplt.figure(figsize=(10, 8))\nplt.suptitle(\"Identification (training) data\", fontsize=16)\n\nplt.subplot(221)\nplt.plot(y_train, \"k\")\nplt.ylabel(\"Force - Output\")\nplt.xlabel(\"Samples\")\nplt.title(\"y\")\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(222)\nplt.plot(x_train[:, 0], \"k\")\nplt.ylabel(\"Control Voltage\")\nplt.xlabel(\"Samples\")\nplt.title(\"x_1\")\nplt.grid()\nplt.axis([0, 1500, 0, 1])\n\nplt.subplot(223)\nplt.plot(x_train[:, 1], \"k\")\nplt.ylabel(\"Velocity\")\nplt.xlabel(\"Samples\")\nplt.title(\"x_2\")\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.subplot(224)\nplt.plot(x_train[:, 2], \"k\")\nplt.ylabel(\"sign(Velocity)\")\nplt.xlabel(\"Samples\")\nplt.title(\"x_3\")\nplt.grid()\nplt.axis([0, 1500, -1.5, 1.5])\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n</code></pre> <p>Let's check how is the histeretic behavior considering each input:</p> <pre><code>plt.figure()\nplt.plot(x_train[:, 0], y_train)\nplt.xlabel(\"x1 - Voltage\")\nplt.ylabel(\"y - Force\")\n\nplt.figure()\nplt.plot(x_train[:, 1], y_train)\nplt.xlabel(\"x2 - Velocity\")\nplt.ylabel(\"y - Force\")\n\nplt.figure()\nplt.plot(x_train[:, 2], y_train)\nplt.xlabel(\"u3 - sign(Velocity)\")\nplt.ylabel(\"y - Force\")\n</code></pre> <pre><code>Text(0, 0.5, 'y - Force')\n</code></pre> <p></p> <p></p> <p></p> <p>Now, we can just build a NARX model:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=[[1], [1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"FROLS: sign(v) and MaxAbsScaler\",\n)\n</code></pre> <pre><code>0.04510435472905795\n</code></pre> <p></p> <p>If we remove the <code>sign(v)</code> input and try to build a NARX model using the same configuration, the model diverge, as can be seen in the following figure:</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=[[1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    info_criteria=\"aic\",\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"FROLS: MaxAbsScaler, discarding sign(v)\",\n)\n</code></pre> <pre><code>nan\n\n\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\n</code></pre> <p></p> <p>If we use the <code>MetaMSS</code> algorithm instead, the results are better.</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\n\nbasis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n    xlag=[[1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    random_state=42,\n)\n\nmodel.fit(X=x_train[:, :2], y=y_train)\nyhat = model.predict(X=x_test[:, :2], y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"MetaMSS: MaxAbsScaler, discarding sign(v)\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:453: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\metrics\\_regression.py:216: RuntimeWarning: overflow encountered in square\n  numerator = np.sum(np.square((yhat - y)))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: invalid value encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n\n\n0.24685651932553157\n</code></pre> <p></p> <p>However, when the output of the system reach its minimum value, the model oscillate</p> <pre><code>plot_results(\n    y=y_test[1100:1200], yhat=yhat[1100:1200], n=10000, title=\"Unstable region\"\n)\n</code></pre> <p></p> <p>If we add the <code>sign(v)</code> input again and use <code>MetaMSS</code>, the results are very close to the <code>FROLS</code> algorithm with all inputs</p> <pre><code>basis_function = Polynomial(degree=3)\nmodel = MetaMSS(\n    xlag=[[1], [1], [1]],\n    ylag=1,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    random_state=42,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag :, :])\nrrse = root_relative_squared_error(y_test[model.max_lag :], yhat[model.max_lag :])\nprint(rrse)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=10000,\n    title=\"MetaMSS: sign(v) and MaxAbsScaler\",\n)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\parameter_estimation\\estimators.py:75: UserWarning: Psi matrix might have linearly dependent rows.Be careful and check your data\n  self._check_linear_dependence_rows(psi)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\narmax_base.py:724: RuntimeWarning: overflow encountered in power\n  regressor_value[j] = np.prod(np.power(raw_regressor, model_exponent))\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\meta_model_structure_selection.py:453: RuntimeWarning: overflow encountered in square\n  sum_of_squared_residues = np.sum(residues**2)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2590: RuntimeWarning: divide by zero encountered in power\n  absx **= ord\n\n\n0.055422497807759194\n</code></pre> <p></p> <p>This case will also highlight the significance of data scaling. Previously, we used the <code>MaxAbsScaler</code> method, which resulted in great models when using the <code>sign(v)</code> inputs, but also resulted in unstable models when removing that input feature. When scaling is applied using <code>MinMaxScaler</code>, however, the overall stability of the results improves, and the model does not diverge, even when the <code>sign(v)</code> input is removed, using the <code>FROLS</code> algorithm.</p> <p>The user can get the results bellow by just changing the data scaling method using</p> <pre><code>scaler_x = MinMaxScaler()\nscaler_y = MinMaxScaler()\n</code></pre> <p>and running the each model again. That is the only change to improve the results.</p> <p></p> <p>FROLS: with <code>sign(v)</code> and <code>MinMaxScaler</code>. RMSE: 0.1159</p> <p> FROLS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1639</p> <p></p> <p>MetaMSS: discarding <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.1762</p> <p></p> <p>MetaMSS: including <code>sign(v)</code> and using <code>MinMaxScaler</code>. RMSE: 0.0694</p> <p>In contrast, the MetaMSS method returned the best model overall, but not better than the best <code>FROLS</code> method using <code>MaxAbsScaler</code>.</p> <p>Here is the predicted histeretic loop:</p> <pre><code>plt.plot(x_test[:, 1], yhat)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x225ff4f8b00&gt;]\n</code></pre> <p></p> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/","title":"Multiobjective Parameter Estimation - An Overview","text":"<p>Example created by Gabriel Bueno Leandro, Samir Milani Martins and Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Multiobjective parameter estimation represents a fundamental paradigm shift in the way we approach the parameter tuning problem for NARMAX models. Instead of seeking a single set of parameter values that optimally fits the model to the data, multiobjective approaches aim to identify a set of parameter solutions, known as the Pareto front, that provide a trade-off between competing objectives. These objectives often encompass a spectrum of model performance criteria, such as goodness-of-fit, model complexity, and robustness.</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#reference","title":"Reference","text":"<p>For further information, check this reference: https://doi.org/10.1080/00207170601185053.</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#use-case-buck-converter","title":"Use case: Buck converter","text":"A buck converter is a type of DC/DC converter that decreases the voltage (while increasing the current) from its input (power supply) to its output (load). It is similar to a boost converter (elevator) and is a type of switched-mode power supply (SMPS) that typically contains at least two semiconductors (a diode and a transistor, although modern buck converters replace the diode with a second transistor used for synchronous rectification) and at least one energy storage element, a capacitor, inductor or both combined.  <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.multiobjective_parameter_estimation import AILS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.narmax_tools import set_weights\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#dynamic-behavior","title":"Dynamic Behavior","text":"<pre><code>df_train = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/buck/buck_id.csv\"\n)\ndf_valid = pd.read_csv(\n    r\"https://raw.githubusercontent.com/wilsonrljr/sysidentpy-data/refs/heads/main/datasets/buck/buck_valid.csv\"\n)\n\n# Plotting the measured output (identification and validation data)\nplt.figure(1)\nplt.title(\"Output\")\nplt.plot(df_train.sampling_time, df_train.y, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.y, label=\"Validation\", linewidth=1.5)\nplt.xlabel(\"Samples\")\nplt.ylabel(\"Voltage\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Plotting the measured input (identification and validation data)\nplt.figure(2)\nplt.title(\"Input\")\nplt.plot(df_train.sampling_time, df_train.input, label=\"Identification\", linewidth=1.5)\nplt.plot(df_valid.sampling_time, df_valid.input, label=\"Validation\", linewidth=1.5)\nplt.ylim(2.1, 2.6)\nplt.ylabel(\"u\")\nplt.xlabel(\"Samples\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#buck-converter-static-function","title":"Buck Converter Static Function","text":"<p>The duty cycle, represented by the symbol \\(D\\), is defined as the ratio of the time the system is on (\\(T_{on}\\)\u200b) to the total operation cycle time (\\(T\\)). Mathematically, this can be expressed as \\(D=\\frac{T_{on}}{T}\\). The complement of the duty cycle, represented by \\(D'\\), is defined as the ratio of the time the system is off (\\(T_{off}\\)) to the total operation cycle time (\\(T\\)) and can be expressed as \\(D'=\\frac{T_{off}}{T}\\).</p> <p>The load voltage (\\(V_o\\)) is related to the source voltage (\\(V_d\\)) by the equation \\(V_o\u200b=D\u22c5V_d\u200b=(1\u2212D\u2019)\u22c5V_d\\).  For this particular converter, it is known that \\(D\u2032=\\frac{\\bar{u}-1}{3}\u200b\\),\u200b which means that the static function of this system can be derived from theory to be:</p> <p>\\(V_o = \\frac{4V_d}{3} - \\frac{V_d}{3}\\cdot \\bar{u}\\)</p> <p>If we assume that the source voltage \\(V_d\\)\u200b is equal to 24 V, then we can rewrite the above expression as follows:</p> <p>\\(V_o = (4 - \\bar{u})\\cdot 8\\)</p> <pre><code># Static data\nVd = 24\nUo = np.linspace(0, 4, 50)\nYo = (4 - Uo) * Vd / 3\nUo = Uo.reshape(-1, 1)\nYo = Yo.reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Curve\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{y}$\")\nplt.plot(Uo, Yo, linewidth=1.5, linestyle=\"-\", marker=\"o\")\nplt.show()\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#buck-converter-static-gain","title":"Buck converter Static Gain","text":"<p>The gain of a Buck converter is a measure of how its output voltage changes in response to changes in its input voltage. Mathematically, the gain can be calculated as the derivative of the converter\u2019s static function, which describes the relationship between its input and output voltages. In this case, the static function of the Buck converter is given by the equation:</p> <p>\\(V_o = (4 - \\bar{u})\\cdot 8\\)</p> <p>Taking the derivative of this equation with respect to \\(\\hat{u}\\), we find that the gain of the Buck converter is equal to \u22128. In other words, for every unit increase in the input voltage \\(\\hat{u}\\), the output voltage Vo\u200b will decrease by 8 units.</p> <p>so \\(gain=V_o'=-8\\)</p> <pre><code># Defining the gain\ngain = -8 * np.ones(len(Uo)).reshape(-1, 1)\nplt.figure(3)\nplt.title(\"Buck Converter Static Gain\")\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{gain}$\")\nplt.plot(Uo, gain, linewidth=1.5, label=\"gain\", linestyle=\"-\", marker=\"o\")\nplt.legend()\nplt.show()\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#building-a-dynamic-model-using-the-mono-objective-approach","title":"Building a dynamic model using the mono-objective approach","text":"<pre><code>x_train = df_train.input.values.reshape(-1, 1)\ny_train = df_train.y.values.reshape(-1, 1)\nx_valid = df_valid.input.values.reshape(-1, 1)\ny_valid = df_valid.y.values.reshape(-1, 1)\n\nbasis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=8,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n)\n\nmodel.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1ee748da650&gt;\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#affine-information-least-squares-algorithm-ails","title":"Affine Information Least Squares Algorithm (AILS)","text":"<p>AILS is a multiobjective parameter estimation algorithm, based on a set of affine information pairs. The multiobjective approach proposed in the mentioned paper and implemented in SysIdentPy leads to a convex multiobjective optimization problem, which can be solved by AILS. AILS is a LeastSquares-type non-iterative scheme for finding the Pareto-set solutions for the multiobjective problem.</p> <p>So, with the model structure defined (we will be using the one built using the dynamic data above), one can estimate the parameters using the multiobjective approach.</p> <p>The information about static function and static gain, besides the usual dynamic input/output data, can be used to build the pair of affine information to estimate the parameters of the model. We can model the cost function as:</p> <p>$ \\gamma(\\hat\\theta) = w_1\\cdot J_{LS}(\\hat{\\theta})+w_2\\cdot J_{SF}(\\hat{\\theta})+w_3\\cdot J_{SG}(\\hat{\\theta}) $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#multiobjective-parameter-estimation-considering-3-different-objectives-the-prediction-error-the-static-function-and-the-static-gain","title":"Multiobjective parameter estimation considering 3 different objectives: the prediction error, the static function and the static gain","text":"<pre><code># you can use any set of model structure you want in your use case, but in this notebook we will use the one obtained above the compare with other work\nmo_estimator = AILS(final_model=model.final_model)\n\n# setting the log-spaced weights of each objective function\nw = set_weights(static_function=True, static_gain=True)\n\n# you can also use something like\n\n# w = np.array(\n#     [\n#         [0.98, 0.7, 0.5, 0.35, 0.25, 0.01, 0.15, 0.01],\n#         [0.01, 0.1, 0.3, 0.15, 0.25, 0.98, 0.35, 0.01],\n#         [0.01, 0.2, 0.2, 0.50, 0.50, 0.01, 0.50, 0.98],\n#     ]\n# )\n\n# to set the weights. Each row correspond to each objective\n</code></pre> <p>AILS has an <code>estimate</code> method that returns the cost functions (J), the Euclidean norm of the cost functions (E), the estimated parameters referring to each weight (theta), the regressor matrix of the gain and static_function affine information HR and QR, respectively.</p> <pre><code>J, E, theta, HR, QR, position = mo_estimator.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.006842 0.003078 0.990080 0.999970 1.095020e-05 0.000013 0.245244 1 0.007573 0.002347 0.990080 0.999938 2.294665e-05 0.000016 0.245236 2 0.008382 0.001538 0.990080 0.999885 6.504913e-05 0.000018 0.245223 3 0.009277 0.000642 0.990080 0.999717 4.505541e-04 0.000021 0.245182 4 0.006842 0.098663 0.894495 1.000000 7.393246e-08 0.000015 0.245251 ... ... ... ... ... ... ... ... 2290 0.659632 0.333527 0.006842 0.995896 3.965699e-04 1.000000 0.244489 2291 0.730119 0.263039 0.006842 0.995632 5.602981e-04 0.972842 0.244412 2292 0.808139 0.185020 0.006842 0.995364 8.321071e-04 0.868299 0.244300 2293 0.894495 0.098663 0.006842 0.995100 1.364999e-03 0.660486 0.244160 2294 0.990080 0.003078 0.006842 0.992584 9.825987e-02 0.305492 0.261455 <p>2295 rows \u00d7 7 columns</p> <p>Now we can set theta related to any weight results</p> <pre><code>model.theta = theta[-1, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the last combination of the weights\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 2.2930E+00 9.999E-01 1 y(k-1) 2.3307E-01 2.042E-05 2 y(k-2) 6.3209E-01 1.108E-06 3 x1(k-1) -5.9333E-01 4.688E-06 4 y(k-1)^2 2.7673E-01 3.922E-07 5 y(k-2)y(k-1) -5.3228E-01 8.389E-07 6 x1(k-1)y(k-1) 1.6667E-02 5.690E-07 7 y(k-2)^2 2.5766E-01 3.827E-06"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#the-dynamic-results-for-that-chosen-theta-is","title":"The dynamic results for that chosen theta is","text":"<pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#the-static-gain-result-is","title":"The static gain result is","text":"<pre><code>plt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#the-static-function-result-is","title":"The static function result is","text":"<pre><code>plt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#getting-the-best-weight-combination-based-on-the-norm-of-the-cost-function","title":"Getting the best weight combination based on the norm of the cost function","text":"<pre><code># the variable `position` returned in `estimate` method give the position of the best weight combination\nmodel.theta = theta[position, :].reshape(\n    -1, 1\n)  # setting the theta estimated for the best combination of the weights\n# the model structure is exactly the same, but the order of the regressors is changed in estimate method. Thats why you have to change the model.final_model\nmodel.final_model = mo_estimator.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\n# The dynamic results for that chosen theta is\nplot_results(y=y_valid, yhat=yhat, n=1000)\n# The static gain result is\nplt.figure(4)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n# The static function result is\nplt.figure(5)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>      Regressors   Parameters        ERR\n0              1   1.5405E+00  9.999E-01\n1         y(k-1)   2.9687E-01  2.042E-05\n2         y(k-2)   6.4693E-01  1.108E-06\n3        x1(k-1)  -4.1302E-01  4.688E-06\n4       y(k-1)^2   2.7671E-01  3.922E-07\n5   y(k-2)y(k-1)  -5.3474E-01  8.389E-07\n6  x1(k-1)y(k-1)   4.0624E-03  5.690E-07\n7       y(k-2)^2   2.5832E-01  3.827E-06\n</code></pre> <p>You can also plot the pareto-set solutions</p> <pre><code>plt.figure(6)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Pareto-set solutions\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#detailing-ails","title":"Detailing AILS","text":"<p>The polynomial NARX model built using the mono-objective approach has the following structure:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 $</p> <p>The, the goal when using the static function and static gain information in the multiobjective scenario is to estimate the vector \\(\\hat{\\theta}\\) based on:</p> <p>$ \\theta = [w_1\\Psi^T\\Psi + w_2(HR)^T(HR) + w_3(QR)(QR)<sup>T]</sup> [w_1\\Psi^T y + w_2(HR)<sup>T\\overline{g}+w_3(QR)</sup>T\\overline{y}] $</p> <p>The \\(\\Psi\\) matrix is built using the usual mono-objective dynamic modeling approach in SysIdentPy. However, it is still necessary to find the Q, H and R matrices. AILS have the methods to compute all of those matrices. Basically, to do that, \\(q_i^T\\) is first estimated:</p> <p>$ q_i^T = \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_1} &amp; \\overline{y_i}^2 &amp; \\cdots &amp; \\overline{y_i}^l &amp;  F_{yu} &amp; \\overline{u_i}^2 &amp; \\cdots &amp; \\overline{u_i}^l \\end{bmatrix} $</p> <p>where \\(F_{yu}\\) stands for all non-linear monomials in the model that are related to \\(y(k)\\) and \\(u(k)\\), \\(l\\) is the largest non-linearity in the model for input and output terms. For a model with a degree of nonlinearity equal to 2, we can obtain:</p> <p>$ q_i^T =  \\begin{bmatrix} 1 &amp; \\overline{y_i} &amp; \\overline{u_i} &amp; \\overline{y_i}^2 &amp; \\overline{u_i}:\\overline{y_i} &amp; \\overline{u_i}^2  \\end{bmatrix} $</p> <p>It is possible to encode the \\(q_i^T\\) matrix so that it follows the model encoding defined in SysIdentPy. To do this, 0 is considered as a constant, \\(y_i\\) equal to 1 and \\(u_i\\) equal to 2. The number of columns indicates the degree of nonlinearity of the system and the number of rows reflects the number of terms:</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\ 1 &amp; 0\\ 2 &amp; 0\\ 1 &amp; 1\\ 2 &amp; 1\\ 2 &amp; 2\\ \\end{bmatrix} $ $ = $ $  \\begin{bmatrix} 1 \\ \\overline{y_i}\\ \\overline{u_i}\\ \\overline{y_i}^2\\ \\overline{u_i}:\\overline{y_i}\\ \\overline{u_i}^2\\ \\end{bmatrix} $</p> <p>Finally, the result can be easily obtained using the \u2018regressor_space\u2019 method of SysIdentPy</p> <pre><code>from sysidentpy.narmax_base import RegressorDictionary\n\nobject_qit = RegressorDictionary(xlag=1, ylag=1)\nR_example = object_qit.regressor_space(n_inputs=1) // 1000\nprint(f\"R = {R_example}\")\n</code></pre> <pre><code>R = [[0 0]\n [1 0]\n [2 0]\n [1 1]\n [2 1]\n [2 2]]\n</code></pre> <p>such that:</p> <p>$ \\overline{y_i} = q_i^T R\\theta $</p> <p>and:</p> <p>$ \\overline{g_i} = H R\\theta $</p> <p>where \\(R\\) is the linear mapping of the static regressors represented by \\(q_i^T\\). In addition, the \\(H\\) matrix holds affine information regarding \\(\\overline{g_i}\\), which is equal to \\(\\overline{g_i} = \\frac{d\\overline{y}}{d\\overline{u}}{\\big |}_{(\\overline{u_i}\\:\\overline{y_i})}\\).</p> <p>From now on, we will begin to apply the parameter estimation in a multiobjective manner. This will be done with the NARX polynomial model of the BUCK converter in mind. In this context, \\(q_i^T\\) will be generic and will assume a specific format for the problem at hand. For this task, the \\(R_qit\\) method will be used, whose objective is to return the \\(q_i^T\\) related to the model and the matrix of the linear mapping \\(R\\):</p> <pre><code>R, qit = mo_estimator.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> <pre><code>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 0 0 0 0]\n [0 0 0 0 1 1 0 1]\n [0 0 0 0 0 0 1 0]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [2 0]\n [1 1]]\n</code></pre> <p>So</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\ 1 &amp; 0\\ 2 &amp; 0\\ 1 &amp; 1\\ 2 &amp; 1\\  \\end{bmatrix} $ $ =$ $ \\begin{bmatrix} 1\\ \\overline{y}\\ \\overline{u}\\ \\overline{y^2}\\ \\overline{u}:\\overline{y}\\  \\end{bmatrix} $</p> <p>You can notice that the method produces outputs consistent with what is expected:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 u(k-1) y(k-1) + \\theta_4 + \\theta_5 y(k-1)^2 + \\theta_6 u(k-1) + \\theta_7 y(k-2)y(k-1) + \\theta_8 y(k-2)^2 $</p> <p>and:</p> <p>$ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\ \\overline{y^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\ \\overline{y}:\\overline{u} &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\end{bmatrix} $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#validation","title":"Validation","text":"<p>The following model structure will be used to validate the approach:</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 $</p> <p>\\(\\therefore\\)</p> <p>$ final_model =  \\begin{bmatrix} 1001 &amp; 0\\ 1002 &amp; 0\\ 0 &amp; 0\\ 2001 &amp; 0\\ 2001 &amp; 2001\\ 2002 &amp; 2001\\ 2002 &amp; 0\\ 2002 &amp; 2002 \\end{bmatrix} $</p> <p>defining in code:</p> <pre><code>final_model = np.array(\n    [\n        [1001, 0],\n        [1002, 0],\n        [0, 0],\n        [2001, 0],\n        [2001, 2001],\n        [2002, 2001],\n        [2002, 0],\n        [2002, 2002],\n    ]\n)\nfinal_model\n</code></pre> <pre><code>array([[1001,    0],\n       [1002,    0],\n       [   0,    0],\n       [2001,    0],\n       [2001, 2001],\n       [2002, 2001],\n       [2002,    0],\n       [2002, 2002]])\n</code></pre> <pre><code>mult2 = AILS(final_model=final_model)\n</code></pre> <pre><code>def psi(X, Y):\n    PSI = np.zeros((len(X), 8))\n    for k in range(2, len(Y)):\n        PSI[k, 0] = Y[k - 1]\n        PSI[k, 1] = Y[k - 2]\n        PSI[k, 2] = 1\n        PSI[k, 3] = X[k - 1]\n        PSI[k, 4] = X[k - 1] ** 2\n        PSI[k, 5] = X[k - 2] * X[k - 1]\n        PSI[k, 6] = X[k - 2]\n        PSI[k, 7] = X[k - 2] ** 2\n    return np.delete(PSI, [0, 1], axis=0)\n</code></pre> <p>The value of theta with the lowest mean squared error obtained with the same code implemented in Scilab was:</p> <p>$ W_{LS} = 0.3612343 $</p> <p>and:</p> <p>$ W_{SG} = 0.3548699 $</p> <p>and:</p> <p>$ W_{SF} = 0.3548699 $</p> <pre><code>PSI = psi(x_train, y_train)\nw = np.array([[0.3612343], [0.2838959], [0.3548699]])\n</code></pre> <pre><code>C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:4: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 0] = Y[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 1] = Y[k - 2]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:7: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 3] = X[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:8: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 4] = X[k - 1] ** 2\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:9: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 5] = X[k - 2] * X[k - 1]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:10: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 6] = X[k - 2]\nC:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_58792\\3629615894.py:11: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  PSI[k, 7] = X[k - 2] ** 2\n</code></pre> <pre><code>J, E, theta, HR, QR, position = mult2.estimate(\n    y=y_train, X=x_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\n    \"w1\": w[0, :],\n    \"w2\": w[2, :],\n    \"w3\": w[1, :],\n    \"J_ls\": J[0, :],\n    \"J_sg\": J[1, :],\n    \"J_sf\": J[2, :],\n    \"||J||:\": E,\n}\n# the order of the weights is different because the way we implemented in Python, but the results are very close as expected\npd.DataFrame(result)\n</code></pre> w1 w2 w3 J_ls J_sg J_sf ||J||: 0 0.361234 0.35487 0.283896 1.0 1.0 1.0 1.0 <p>Dynamic results</p> <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = mult2.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4287E+00 9.999E-01 1 y(k-1) 5.5147E-01 2.042E-05 2 y(k-2) 4.0449E-01 1.108E-06 3 x1(k-1) -1.2605E+01 4.688E-06 4 x1(k-2) 1.2257E+01 3.922E-07 5 x1(k-1)^2 8.3274E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1416E+01 5.690E-07 7 x1(k-2)^2 3.0846E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <p>Static gain</p> <pre><code>plt.figure(7)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.ylim(-16, 0)\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Static function</p> <pre><code>plt.figure(8)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Pareto-set solutions</p> <pre><code>plt.figure(9)\nax = plt.axes(projection=\"3d\")\nax.plot3D(J[0, :], J[1, :], J[2, :], \"o\", linewidth=0.1)\nax.set_title(\"Optimum pareto-curve\", fontsize=15)\nax.set_xlabel(\"$J_{ls}$\", fontsize=10)\nax.set_ylabel(\"$J_{sg}$\", fontsize=10)\nax.set_zlabel(\"$J_{sf}$\", fontsize=10)\nplt.show()\n</code></pre> <p></p> <pre><code>theta[position, :]\n</code></pre> <pre><code>array([  1.42867821,   0.55147249,   0.40449005, -12.60549001,\n        12.25730092,   8.32739876, -11.41573694,   3.08460955])\n</code></pre> <p>The following table show the results reported in \u2018IniciacaoCientifica2007\u2019 and the ones obtained with SysIdentPy implementation</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.5514725 0.549144 \\(\\theta_2\\) 0.40449005 0.408028 \\(\\theta_3\\) 1.42867821 1.45097 \\(\\theta_4\\) -12.60548863 -12.55788 \\(\\theta_5\\) 8.32740057 8.1516315 \\(\\theta_6\\) -11.41574116 -11.09728 \\(\\theta_7\\) 12.25729955 12.215782 \\(\\theta_8\\) 3.08461195 2.9319577 <p>where:</p> <p>$ E_{Scilab} =    17.426613 $</p> <p>and:</p> <p>$ E_{Python} = 17.474865 $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#note-as-mentioned-before-the-order-of-the-regressors-in-the-model-change-but-it-is-the-same-structure-the-tables-shows-the-respective-regressor-parameter-concerning-sysidentpy-and-iniciacaocientifica2007-but-the-order-1-2-and-so-on-are-not-the-same-of-the-ones-in-modelfinal_model","title":"Note: as mentioned before, the order of the regressors in the model change, but it is the same structure. The tables shows the respective regressor parameter concerning <code>SysIdentPy</code> and <code>IniciacaoCientifica2007</code>,  but the order <code>\u03b8\u2081</code>, <code>\u03b8\u2082</code> and so on are not the same of the ones in model.final_model","text":"<pre><code>R, qit = mult2.build_linear_mapping()\nprint(\"R matrix:\")\nprint(R)\nprint(\"qit matrix:\")\nprint(qit)\n</code></pre> <pre><code>R matrix:\n[[1 0 0 0 0 0 0 0]\n [0 1 1 0 0 0 0 0]\n [0 0 0 1 1 0 0 0]\n [0 0 0 0 0 1 1 1]]\nqit matrix:\n[[0 0]\n [1 0]\n [0 1]\n [0 2]]\n</code></pre> <p>model's structure that will be utilized (\u2018IniciacaoCientifica2007\u2019):</p> <p>$ y(k) = \\theta_1 y(k-1) + \\theta_2 y(k-2) + \\theta_3 + \\theta_4 u(k-1) + \\theta_5 u(k-1)^2 + \\theta_6 u(k-2)u(k-1)+\\theta_7 u(k-2) + \\theta_8 u(k-2)^2 $</p> <p>$ q_i =  \\begin{bmatrix} 0 &amp; 0\\ 1 &amp; 0\\ 2 &amp; 0\\ 2 &amp; 2\\  \\end{bmatrix} $ $ = $ $ \\begin{bmatrix} 1\\ \\overline{y}\\ \\overline{u}\\ \\overline{u^2} \\end{bmatrix} $</p> <p>and:</p> <p>$ R =  \\begin{bmatrix} term/\\theta &amp; \\theta_1 &amp; \\theta_2 &amp; \\theta_3 &amp; \\theta_4 &amp; \\theta_5 &amp; \\theta_6 &amp; \\theta_7 &amp; \\theta_8\\ 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\overline{y} &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ \\overline{u} &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\ \\overline{u^2} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} $</p> <p>consistent with matrix R:</p> <p>R = [0 0 1 0 0 0 0 0;1 1 0 0 0 0 0 0;0 0 0 1 0 0 1 0;0 0 0 0 1 1 0 1]; // R </p> <p>or:</p> <p>$  R =  \\begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 &amp; 1 \\end{bmatrix} $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#biobjective-optimization","title":"Biobjective optimization","text":""},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#an-use-case-applied-to-buck-converter-cc-cc-using-as-objectives-the-static-curve-information-and-the-prediction-error-dynamic","title":"An use case applied to Buck converter CC-CC using as objectives the static curve information and the prediction error (dynamic)","text":"<pre><code>bi_objective = AILS(\n    static_function=True, static_gain=False, final_model=final_model, normalize=True\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> <p>$ W_{LS} = 0.9931126 $</p> <p>and:</p> <p>$ W_{SF} = 0.0068874 $</p> <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(-0.01, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\nJ, E, theta, HR, QR, position = bi_objective.estimate(\n    y=y_train, X=x_train, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg ||J||: 0 0.990080 0.009920 0.990863 1.000000 0.990939 1 0.987127 0.012873 0.990865 0.987032 0.990939 2 0.984182 0.015818 0.990867 0.974307 0.990939 3 0.981247 0.018753 0.990870 0.961803 0.990940 4 0.978320 0.021680 0.990873 0.949509 0.990941 ... ... ... ... ... ... 1995 0.002555 0.997445 0.999993 0.000072 0.999993 1996 0.002547 0.997453 0.999994 0.000072 0.999994 1997 0.002540 0.997460 0.999996 0.000071 0.999996 1998 0.002532 0.997468 0.999998 0.000071 0.999998 1999 0.002525 0.997475 1.000000 0.000070 1.000000 <p>2000 rows \u00d7 5 columns</p> <pre><code>model.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 1.3873E+00 9.999E-01 1 y(k-1) 5.4941E-01 2.042E-05 2 y(k-2) 4.0804E-01 1.108E-06 3 x1(k-1) -1.2515E+01 4.688E-06 4 x1(k-2) 1.2227E+01 3.922E-07 5 x1(k-1)^2 8.1171E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1047E+01 5.690E-07 7 x1(k-2)^2 2.9043E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(10)\nplt.title(\"Static Curve\")\nplt.plot(Uo, Yo, linewidth=1.5, label=\"Static curve\", linestyle=\"-\", marker=\"o\")\nplt.plot(\n    Uo,\n    QR.dot(model.theta),\n    linewidth=1.5,\n    label=\"NARX \u200b\u200bstatic representation\",\n    linestyle=\"-\",\n    marker=\"^\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.xlabel(\"$\\\\bar{y}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Static Curve Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>where the best estimated \\(\\theta\\) is</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54940883 0.5494135 \\(\\theta_2\\) 0.40803995 0.4080312 \\(\\theta_3\\) 1.38725684 3.3857601 \\(\\theta_4\\) -12.51466378 -12.513688 \\(\\theta_5\\) 8.11712897 8.116575 \\(\\theta_6\\) -11.04664789 -11.04592 \\(\\theta_7\\) 12.22693907 12.227184 \\(\\theta_8\\) 2.90425844 2.9038468 <p>where:</p> <p>$ E_{Scilab} = 17.408934 $</p> <p>and:</p> <p>$ E_{Python} = 17.408947 $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#multiobjective-parameter-estimation","title":"Multiobjective parameter estimation","text":""},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#use-case-considering-2-different-objectives-the-prediction-error-and-the-static-gain","title":"Use case considering 2 different objectives: the prediction error and the static gain","text":"<pre><code>bi_objective_gain = AILS(\n    static_function=False, static_gain=True, final_model=final_model, normalize=False\n)\n</code></pre> <p>the value of theta with the lowest mean squared error obtained through the routine in Scilab was:</p> <p>$ W_{LS} = 0.9931126 $</p> <p>and:</p> <p>$ W_{SF} = 0.0068874 $</p> <pre><code>w = np.zeros((2, 2000))\nw[0, :] = np.logspace(0, -6, num=2000, base=2.71)\nw[1, :] = np.ones(2000) - w[0, :]\n# W = np.array([[0.9931126],\n# [0.0068874]])\nJ, E, theta, HR, QR, position = bi_objective_gain.estimate(\n    X=x_train, y=y_train, gain=gain, y_static=Yo, X_static=Uo, weighing_matrix=w\n)\nresult = {\"w1\": w[0, :], \"w2\": w[1, :], \"J_ls\": J[0, :], \"J_sg\": J[1, :], \"||J||:\": E}\npd.DataFrame(result)\n</code></pre> w1 w2 J_ls J_sg ||J||: 0 1.000000 0.000000 17.407256 3.579461e+01 39.802849 1 0.997012 0.002988 17.407528 2.109260e-01 17.408806 2 0.994033 0.005967 17.407540 2.082067e-01 17.408785 3 0.991063 0.008937 17.407559 2.056636e-01 17.408774 4 0.988102 0.011898 17.407585 2.031788e-01 17.408771 ... ... ... ... ... ... 1995 0.002555 0.997445 17.511596 3.340081e-07 17.511596 1996 0.002547 0.997453 17.511596 3.320125e-07 17.511596 1997 0.002540 0.997460 17.511597 3.300289e-07 17.511597 1998 0.002532 0.997468 17.511598 3.280571e-07 17.511598 1999 0.002525 0.997475 17.511599 3.260972e-07 17.511599 <p>2000 rows \u00d7 5 columns</p> <pre><code># Writing the results\nmodel.theta = theta[position, :].reshape(-1, 1)\nmodel.final_model = bi_objective_gain.final_model\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=3,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nr\n</code></pre> Regressors Parameters ERR 0 1 1.4853E+00 9.999E-01 1 y(k-1) 5.4940E-01 2.042E-05 2 y(k-2) 4.0806E-01 1.108E-06 3 x1(k-1) -1.2581E+01 4.688E-06 4 x1(k-2) 1.2210E+01 3.922E-07 5 x1(k-1)^2 8.1686E+00 8.389E-07 6 x1(k-2)x1(k-1) -1.1122E+01 5.690E-07 7 x1(k-2)^2 2.9455E+00 3.827E-06 <pre><code>plot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> <p></p> <pre><code>plt.figure(12)\nplt.title(\"Gain\")\nplt.plot(\n    Uo,\n    gain,\n    linewidth=1.5,\n    linestyle=\"-\",\n    marker=\"o\",\n    label=\"Buck converter static gain\",\n)\nplt.plot(\n    Uo,\n    HR.dot(model.theta),\n    linestyle=\"-\",\n    marker=\"^\",\n    linewidth=1.5,\n    label=\"NARX model gain\",\n)\nplt.xlabel(\"$\\\\bar{u}$\")\nplt.ylabel(\"$\\\\bar{g}$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure(11)\nplt.title(\"Costs Functions\")\nplt.plot(J[1, :], J[0, :], \"o\")\nplt.xlabel(\"Gain Information\")\nplt.ylabel(\"Prediction Error\")\nplt.show()\n</code></pre> <p></p> <p>being the selected \\(\\theta\\):</p> Theta SysIdentPy IniciacaoCientifica2007 \\(\\theta_1\\) 0.54939785 0.54937289 \\(\\theta_2\\) 0.40805603 0.40810168 \\(\\theta_3\\) 1.48525190 1.48663719 \\(\\theta_4\\) -12.58066084 -12.58127183 \\(\\theta_5\\) 8.16862622 8.16780294 \\(\\theta_6\\) -11.12171897 -11.11998621 \\(\\theta_7\\) 12.20954849 12.20927355 \\(\\theta_8\\) 2.94548501 2.9446532 <p>where:</p> <p>$ E_{Scilab} =  17.408997 $</p> <p>and:</p> <p>$ E_{Python} = 17.408781 $</p>"},{"location":"user-guide/tutorials/multiobjective-parameter-estimation-overview/#additional-information","title":"Additional Information","text":"<p>You can also access the matrix Q and H using the following methods</p> <p>Matrix Q:</p> <pre><code>bi_objective_gain.build_static_function_information(Uo, Yo)[1]\n</code></pre> <pre><code>array([[   50.        ,   800.        ,   800.        ,   100.        ,\n          100.        ,   269.3877551 ,   269.3877551 ,   269.3877551 ],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  800.        , 17240.81632653, 17240.81632653,  1044.89795918,\n         1044.89795918,  2089.79591837,  2089.79591837,  2089.79591837],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  100.        ,  1044.89795918,  1044.89795918,   269.3877551 ,\n          269.3877551 ,   816.32653061,   816.32653061,   816.32653061],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407],\n       [  269.3877551 ,  2089.79591837,  2089.79591837,   816.32653061,\n          816.32653061,  2638.54142407,  2638.54142407,  2638.54142407]])\n</code></pre> <p>Matrix H+R:</p> <pre><code>bi_objective_gain.build_static_gain_information(Uo, Yo, gain)[1]\n</code></pre> <pre><code>array([[    0.        ,     0.        ,     0.        ,     0.        ,\n            0.        ,     0.        ,     0.        ,     0.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  3200.        ,  3200.        ,  -400.        ,\n         -400.        , -1600.        , -1600.        , -1600.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        ,  -400.        ,  -400.        ,    50.        ,\n           50.        ,   200.        ,   200.        ,   200.        ],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041],\n       [    0.        , -1600.        , -1600.        ,   200.        ,\n          200.        ,  1077.55102041,  1077.55102041,  1077.55102041]])\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/","title":"Parameter Estimation - Overview","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> <pre><code>import pandas as pd\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import (\n    TotalLeastSquares,\n    RecursiveLeastSquares,\n    NonNegativeLeastSquares,\n    LeastMeanSquares,\n    AffineLeastMeanSquares,\n)\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data","text":"<p>The data is generated by simulating the following model:</p> <p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.001, train_percentage=90\n)\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#there-are-several-method-to-be-used-for-parameter-estimation","title":"There are several method to be used for parameter estimation.","text":"<ul> <li>Least Squares;</li> <li>Total Least Squares;</li> <li>Recursive Least Squares</li> <li>Ridge Regression</li> <li>NonNegative Least Squares</li> <li>Least Squares Minimal Residues</li> <li>Bounded Variable Least Squares</li> <li>Least Mean Squares</li> <li>Affine Least Mean Squares</li> <li>Least Mean Squares Sign Error</li> <li>Normalized Least Mean Squares</li> <li>Least Mean Squares Normalized Sign Error</li> <li>Least Mean Squares Sign Regressor</li> <li>Least Mean Squares Normalized Sign Regressor</li> <li>Least Mean Squares Sign Sign</li> <li>Least Mean Squares Normalized Sign Sign</li> <li>Least Mean Squares Normalized Leaky</li> <li>Least Mean Squares Leaky</li> <li>Least Mean Squares Fourth</li> <li>Least Mean Squares Mixed Norm</li> </ul> <p>Polynomial NARMAX models are linear-in-the-parameter, so Least Squares based methods works well for most cases (using with extended least squares algorithm when dealing with colered noise).</p> <p>However, the user can choose some recursive and stochastic gradient descent methods (in this case, the least mean squares algorithm and its variants) to that task too.</p> <p>Choosing the method is straightforward: pass any of the methods mentioned above on estimator parameters.</p> <ul> <li>Note: Each algorithm have specifc parameter that need to be tunned. In the following examples we will use the default ones. More examples regarding tunned parameter will be available soon. For now, the user can read the method documentation for more information.</li> </ul>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#total-least-squares","title":"Total Least Squares","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = TotalLeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0021167167052431584\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56200123E-01\n1         y(k-1)  1.9995E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  1.0004E-01  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#recursive-least-squares","title":"Recursive Least Squares","text":"<pre><code># recursive least squares\nbasis_function = Polynomial(degree=2)\nestimator = RecursiveLeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0020703083403116164\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0012E-01  9.56200123E-01\n1         y(k-1)  2.0021E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  9.9550E-02  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#least-mean-squares","title":"Least Mean Squares","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastMeanSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.015488793944313425\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9775E-01  9.56200123E-01\n1         y(k-1)  2.0085E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  7.5708E-02  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#affine-least-mean-squares","title":"Affine Least Mean Squares","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = AffineLeastMeanSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0021441596280611167\n      Regressors  Parameters             ERR\n0        x1(k-2)  8.9989E-01  9.56200123E-01\n1         y(k-1)  1.9992E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  1.0003E-01  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/parameter-estimation-overview/#nonnegative-least-squares","title":"NonNegative Least Squares","text":"<pre><code>basis_function = Polynomial(degree=2)\nestimator = NonNegativeLeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    estimator=estimator,\n    basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.0021170157359329173\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0000E-01  9.56200123E-01\n1         y(k-1)  1.9995E-01  4.05078042E-02\n2  x1(k-1)y(k-1)  1.0004E-01  3.28866604E-03\n</code></pre>"},{"location":"user-guide/tutorials/silver-box-system/","title":"Silver Box System","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The description content mainly derives (copy and paste) from the associated paper - Three free data sets for development and benchmarking in nonlinear system identification. For a detailed description, readers are referred to the linked reference.</p> <p>The Silverbox system can be seen as an electronic implementation of the Duffing oscillator. It is build as a 2<sup>nd</sup> order linear time-invariant system with a 3<sup>rd</sup> degree polynomial static nonlinearity around it in feedback. This type of dynamics are, for instance, often encountered in mechanical systems Nonlinear Benchmark - Silverbox.</p> <p>In this case study, we will create a NARX model for the Silver box benchmark. The Silver box represents a simplified version of mechanical oscillating processes, which are a critical category of nonlinear dynamic systems. Examples include vehicle suspensions, where shock absorbers and progressive springs play vital roles. The data generated by the Silver box provides a simplified representation of such combined components. The electrical circuit generating this data closely approximates, but does not perfectly match, the idealized models described below.</p> <p>As described in the original paper, the system was excited using a general waveform generator (HPE1445A). The input signal begins as a discrete-time signal \\(r(k)\\), which is converted to an analog signal \\(r_c(t)\\) using zero-order-hold reconstruction. The actual excitation signal \\(u_0(t)\\) is then obtained by passing \\(r_c(t)\\) through an analog low-pass filter \\(G(p)\\) to eliminate high-frequency content around multiples of the sampling frequency. Here, \\(p\\) denotes the differentiation operator. Thus, the input is given by:</p> \\[ u_0(t) = G(p) r_c(t). \\] <p>The input and output signals were measured using HP1430A data acquisition cards, with synchronized clocks for the acquisition and generator cards. The sampling frequency was:</p> \\[ f_s = \\frac{10^7}{2^{14}} = 610.35 \\, \\text{Hz}. \\] <p>The silver box uses analog electrical circuitry to generate data representing a nonlinear mechanical resonating system with a moving mass \\(m\\), viscous damping \\(d\\), and a nonlinear spring \\(k(y)\\). The electrical circuit is designed to relate the displacement \\(y(t)\\) (the output) to the force \\(u(t)\\) (the input) by the following differential equation:</p> \\[ m \\frac{d^2 y(t)}{dt^2} + d \\frac{d y(t)}{dt} + k(y(t)) y(t) = u(t). \\] <p>The nonlinear progressive spring is described by a static, position-dependent stiffness:</p> \\[ k(y(t)) = a + b y^2(t). \\] <p>The signal-to-noise ratio is sufficiently high to model the system without accounting for measurement noise. However, measurement noise can be included by replacing \\(y(t)\\) with the artificial variable \\(x(t)\\) in the equation above, and introducing disturbances \\(w(t)\\) and \\(e(t)\\) as follows:</p> \\[ \\begin{align} &amp; m \\frac{d^2 x(t)}{dt^2} + d \\frac{d x(t)}{dt} + k(x(t)) x(t) = u(t) + w(t), \\\\ &amp; k(x(t)) = a + b x^2(t), \\\\ &amp; y(t) = x(t) + e(t). \\end{align} \\]"},{"location":"user-guide/tutorials/silver-box-system/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/silver-box-system/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Silver box dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for mentioned system.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.Silverbox(atleast_2d=True)\n\nx_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_multisine.u, test_multisine.y\n\nn = test_multisine.state_initialization_window_length\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation - GerbenBeintema/nonlinear_benchmarks: The official dataload for http://www.nonlinearbenchmark.org/ (github.com) to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plt.plot(x_train)\nplt.plot(y_train, alpha=0.3)\nplt.title(\"Experiment 1: training data\")\nplt.show()\n\nplt.plot(x_test)\nplt.plot(y_test, alpha=0.3)\nplt.title(\"Experiment 1: testing data\")\nplt.show()\n\nplt.plot(test_arrow_full.u)\nplt.plot(test_arrow_full.y, alpha=0.3)\nplt.title(\"Experiment 2: training data\")\nplt.show()\n\nplt.plot(test_arrow_no_extrapolation.u)\nplt.plot(test_arrow_no_extrapolation.y, alpha=0.2)\nplt.title(\"Experiment 2: testing data\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p>Important Note</p> <p>The goal of this benchmark is to develop a model that outperforms the state-of-the-art (SOTA) model presented in the benchmarking paper. However, the results in the paper differ from those provided in the  GitHub repository.</p> nx Set NRMS RMS (mV) 2 Train 0.10653 5.8103295 2 Validation 0.11411 6.1938068 2 Test 0.19151 10.2358533 2 Test (no extra) 0.12284 5.2789727 4 Train 0.03571 1.9478290 4 Validation 0.03922 2.1286373 4 Test 0.12712 6.7943448 4 Test (no extra) 0.05204 2.2365904 8 Train 0.03430 1.8707026 8 Validation 0.03732 2.0254112 8 Test 0.10826 5.7865255 8 Test (no extra) 0.04743 2.0382715 &gt; Table: results presented in the github. <p>It appears that the values shown in the paper actually represent the training time, not the error metrics. I will contact the authors to confirm this information. According to the Nonlinear Benchmark website, the information is as follows:</p> <p></p> <p>where the values in the \"Training time\" column matches the ones presented as error metrics in the paper.</p> <p>While we await confirmation of the correct values for this benchmark, we will demonstrate the performance of SysIdentPy. However, we will refrain from making any comparisons or attempting to improve the model at this stage.</p>"},{"location":"user-guide/tutorials/silver-box-system/#results","title":"Results","text":"<p>We will start (as we did in every other case study) with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=40</code>. Because we dealing with a large training dataset, we will use the <code>err_tol</code> instead of information criteria to have a faster performance. We will also set <code>n_terms=40</code>, which means that the search will stop if the <code>err_tol</code> is reached or 40 regressors is tested in the <code>ERR</code> algorithm. While this approach might result in a sub-optimal model, it is a reasonable starting point for our first attempt. There are three different experiments: multisine, arrow (full), and arrow (no extrapolation).</p> <pre><code>basis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.999,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nnrmse = rmse / y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=300,\n    figsize=(15, 4),\n    title=f\"Multisine. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n</code></pre> <pre><code>0.1423804033714937 7.727682109791501\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_full.u, test_arrow_full.y\n\nn = test_arrow_full.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9999,\n    n_terms=80,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\n# we will not concatente the last values from train data to use as initial condition here because\n# this test data have a very different behavior.\n# However, if you want you can do that and you will see that the model will still perform\n# great after a few iterations\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nnrmse = rmse / y_test.std()\nrmse_mv = 1000 * rmse\n\nprint(nrmse, rmse_mv)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=300,\n    figsize=(15, 4),\n    title=f\"Arrow (full). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n</code></pre> <pre><code>0.07762658947015803 4.14903534238172\n</code></pre> <p></p> <p></p> <pre><code>x_train, y_train = train_val.u, train_val.y\ntest_multisine, test_arrow_full, test_arrow_no_extrapolation = test\nx_test, y_test = test_arrow_no_extrapolation.u, test_arrow_no_extrapolation.y\n\nn = test_arrow_no_extrapolation.state_initialization_window_length\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    xlag=14,\n    ylag=14,\n    basis_function=basis_function,\n    estimator=LeastSquares(),\n    err_tol=0.9999,\n    n_terms=40,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nnrmse = rmse / y_test.std()\nrmse_mv = 1000 * rmse\nprint(nrmse, rmse_mv)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=30000,\n    figsize=(15, 4),\n    title=f\"Arrow (no extrapolation). Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=300,\n    figsize=(15, 4),\n    title=f\"Free Run simulation. Model -&gt; RMSE (x1000) mv: {round(rmse_mv, 4)}\",\n)\n</code></pre> <pre><code>0.05187400789723806 2.2293393254015776\n</code></pre> <p></p> <p></p>"},{"location":"user-guide/tutorials/wiener-hammerstein-system/","title":"Wiener Hammerstein System","text":"<p>Note: The example shown in this notebook is taken from the companion book Nonlinear System Identification and Forecasting: Theory and Practice with SysIdentPy.</p> <p>The description content primarily derives from the benchmark website - Nonlinear Benchmark and associated paper - Wiener-Hammerstein benchmark with process noise. For a detailed description, readers are referred to the linked references.</p> <p>The nonlinear benchmark website stands as a significant contribution to the system identification and machine learning community. The users are encouraged to explore all the papers referenced on the site.</p> <p>This benchmark focuses on a Wiener-Hammerstein electronic circuit where process noise plays a significant role in distorting the output signal.</p> <p>The Wiener-Hammerstein structure is a well-known block-oriented system which contains a static nonlinearity sandwiched between two Linear Time-Invariant (LTI) blocks (Figure 2). This arrangement presents a challenging identification problem due to the presence of these LTI blocks.</p> <p></p> <p>Figure 2: the Wiener-Hammerstein system</p> <p>In Figure 2, the Wiener-Hammerstein system is illustrated with process noise \\(e_x(t)\\) entering before the static nonlinearity \\(f(x)\\), sandwiched between LTI blocks represented by \\(R(s)\\) and \\(S(s)\\) at the input and output, respectively. Additionally, small, negligible noise sources \\(e_u(t)\\) and \\(e_y(t)\\) affect the measurement channels. The measured input and output signals are denoted as \\(u_m(t)\\) and \\(y_m(t)\\).</p> <p>The first LTI block \\(R(s)\\) is effectively modeled as a third-order lowpass filter. The second LTI subsystem \\(S(s)\\) is configured as an inverse Chebyshev filter with a stop-band attenuation of \\(40 dB\\) and a cutoff frequency of \\(5 kHz\\). Notably, \\(S(s)\\) includes a transmission zero within the operational frequency range, complicating its inversion.</p> <p>The static nonlinearity \\(f(x)\\) is implemented using a diode-resistor network, resulting in saturation nonlinearity. Process noise \\(e_x(t)\\) is introduced as filtered white Gaussian noise, generated from a discrete-time third-order lowpass Butterworth filter followed by zero-order hold and analog low-pass reconstruction filtering with a cutoff of \\(20 kHz\\).</p> <p>Measurement noise sources \\(e_u(t)\\) and \\(e_y(t)\\) are minimal compared to \\(e_x(t)\\). The system's inputs and process noise are generated using an Arbitrary Waveform Generator (AWG), specifically the Agilent/HP E1445A, sampling at \\(78125 Hz\\), synchronized with an acquisition system (Agilent/HP E1430A) to ensure phase coherence and prevent leakage errors. Buffering between the acquisition cards and the system's inputs and outputs minimizes measurement equipment distortion.</p> <p>The benchmark provides two standard test signals through the benchmarking website: a random phase multisine and a sine-sweep signal. Both signals have an \\(rms\\) value of \\(0.71 Vrms\\) and cover frequencies from DC to \\(15 kHz\\) (excluding DC). The sine-sweep spans this frequency range at a rate of \\(4.29 MHz/min\\). These test sets serve as targets for evaluating the model's performance, emphasizing accurate representation under varied conditions.</p> <p>The Wiener-Hammerstein benchmark highlights three primary nonlinear system identification challenges:</p> <ol> <li>Process Noise: Significant in the system, influencing output fidelity.</li> <li>Static Nonlinearity: Indirectly accessible from measured data, posing identification challenges.</li> <li>Output Dynamics: Complex inversion due to transmission zero presence in \\(S(s)\\).</li> </ol> <p>The goal of this benchmark is to develop and validate robust models using separate estimation data, ensuring accurate characterization of the Wiener-Hammerstein system's behavior.</p>"},{"location":"user-guide/tutorials/wiener-hammerstein-system/#required-packages-and-versions","title":"Required Packages and Versions","text":"<p>To ensure that you can replicate this case study, it is essential to use specific versions of the required packages. Below is a list of the packages along with their respective versions needed for running the case studies effectively.</p> <p>To install all the required packages, you can create a <code>requirements.txt</code> file with the following content:</p> <pre><code>sysidentpy==0.4.0\npandas==2.2.2\nnumpy==1.26.0\nmatplotlib==3.8.4\nnonlinear_benchmarks==0.1.2\n</code></pre> <p>Then, install the packages using: <pre><code>pip install -r requirements.txt\n</code></pre></p> <ul> <li>Ensure that you use a virtual environment to avoid conflicts between package versions.</li> <li>Versions specified are based on compatibility with the code examples provided. If you are using different versions, some adjustments in the code might be necessary.</li> </ul>"},{"location":"user-guide/tutorials/wiener-hammerstein-system/#sysidentpy-configuration","title":"SysIdentPy configuration","text":"<p>In this section, we will demonstrate the application of SysIdentPy to the Wiener-Hammerstein system dataset.  The following code will guide you through the process of loading the dataset, configuring the SysIdentPy parameters, and building a model for Wiener-Hammerstein system.</p> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sysidentpy.model_structure_selection import FROLS, AOLS, MetaMSS\nfrom sysidentpy.basis_function import Polynomial, Fourier\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.parameter_estimation import (\n    LeastSquares,\n    BoundedVariableLeastSquares,\n    NonNegativeLeastSquares,\n    LeastSquaresMinimalResidual,\n)\n\nfrom sysidentpy.metrics import root_mean_squared_error\nfrom sysidentpy.utils.plotting import plot_results\n\nimport nonlinear_benchmarks\n\ntrain_val, test = nonlinear_benchmarks.WienerHammerBenchMark(atleast_2d=True)\nx_train, y_train = train_val\nx_test, y_test = test\n</code></pre> <p>We used the <code>nonlinear_benchmarks</code> package to load the data. The user is referred to the package documentation to check the details of how to use it.</p> <p>The following plot detail the training and testing data of the experiment.</p> <pre><code>plot_n = 800\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_train[:plot_n])\nplt.plot(y_train[:plot_n])\nplt.title(\"Experiment: training data\")\nplt.legend([\"x_train\", \"y_train\"])\nplt.show()\n\nplt.figure(figsize=(15, 4))\nplt.plot(x_test[:plot_n])\nplt.plot(y_test[:plot_n])\nplt.title(\"Experiment: testing data\")\nplt.legend([\"x_test\", \"y_test\"])\nplt.show()\n</code></pre> <p></p> <p></p> <p>The goal of this benchmark it to get a model that have a better performance than the SOTA model provided in the benchmarking paper.</p> <p></p> <p>State of the art results presented in the benchmarking paper. In this section we are only working with the Wiener-Hammerstein results, which are presented in the \\(W-H\\)  column.</p>"},{"location":"user-guide/tutorials/wiener-hammerstein-system/#results","title":"Results","text":"<p>We will start with a basic configuration of FROLS using a polynomial basis function with degree equal 2. The <code>xlag</code> and <code>ylag</code> are set to \\(7\\) in this first example. Because the dataset is considerably large, we will start with <code>n_info_values=50</code>. This means the FROLS algorithm will not include all regressors when calculating the information criteria used to determine the model order. While this approach might result in a sub-optimal model, it is a reasonable starting point for our first attempt.</p> <pre><code># 3min to run in my machine (amd 5600x, 32gb ram)\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=7,\n    ylag=7,\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nrmse_sota = rmse / y_test.std()\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=1000,\n    title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\",\n)\n</code></pre> <p></p> <p>The first configuration is already better than the SOTA models shown in the benchmark table! We started using <code>xlag=ylag=7</code> to have an idea of how well SysIdentPy would handle this dataset, but the results are pretty good already! However, the benchmarking paper indicates  that they used higher lags for their models. Let's check what happens if we set <code>xlag=ylag=10</code>.</p> <pre><code># 7min to run in my machine (amd 5600x, 32gb ram)\n\nx_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    n_info_values=50,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nrmse_sota = rmse / y_test.std()\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=1000,\n    title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\",\n)\n</code></pre> <p></p> <p>The performance is even better now! For now, we are not worried about the model complexity (even in this case where we are comparing to a deep state neural network...). However, if we check the model order and the <code>AIC</code> plot, we see that the model have 50 regressors , but the <code>AIC</code> values do not change much after each added regression.</p> <pre><code>plt.plot(model.info_values)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x28c0058a450&gt;]\n</code></pre> <p></p> <p>So, what happens if we set a model with half of the regressors?</p> <pre><code># 14 seconds to run\n\nx_train, y_train = train_val\nx_test, y_test = test\n\nn = test.state_initialization_window_length\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n    xlag=10,\n    ylag=10,\n    basis_function=basis_function,\n    estimator=LeastSquares(unbiased=False),\n    n_info_values=50,\n    n_terms=25,\n    order_selection=False,\n)\n\nmodel.fit(X=x_train, y=y_train)\ny_test = np.concatenate([y_train[-model.max_lag :], y_test])\nx_test = np.concatenate([x_train[-model.max_lag :], x_test])\nyhat = model.predict(X=x_test, y=y_test[: model.max_lag, :])\nrmse = root_mean_squared_error(y_test[model.max_lag + n :], yhat[model.max_lag + n :])\nrmse_sota = rmse / y_test.std()\nplot_results(\n    y=y_test[model.max_lag :],\n    yhat=yhat[model.max_lag :],\n    n=1000,\n    title=f\"SysIdentPy -&gt; RMSE: {round(rmse, 4)}, NRMSE: {round(rmse_sota, 4)}\",\n)\n</code></pre> <p></p> <p>As shown in the figure above, the results still outperform the SOTA models presented in the benchmarking paper. The SOTA results from the paper could likely be improved as well. Users are encouraged to explore the deepsysid package, which can be used to build deep state neural networks.</p> <p>This basic configuration can serve as a starting point for users to develop even better models using SysIdentPy. Give it a try!</p>"},{"location":"user-guide/tutorials/your-first-model/","title":"Your First Model","text":"<p>Example created by Wilson Rocha Lacerda Junior</p> <p>Looking for more details on NARMAX models? For comprehensive information on models, methods, and a wide range of examples and benchmarks implemented in SysIdentPy, check out our book: Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy</p> <p>This book provides in-depth guidance to support your work with SysIdentPy.</p> <p>Here we import the NARMAX model, the metric for model evaluation and the methods to generate sample data for tests. Also, we import pandas for specific usage.</p> <pre><code>pip install sysidentpy\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_residues_correlation, plot_results\nfrom sysidentpy.residues.residues_correlation import (\n    compute_residues_autocorrelation,\n    compute_cross_correlation,\n)\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#generating-1-input-1-output-sample-data","title":"Generating 1 input 1 output sample data","text":"<p>The data is generated by simulating the following model:</p> <p>\\(y_k = 0.2y_{k-1} + 0.1y_{k-1}x_{k-1} + 0.9x_{k-1} + e_{k}\\)</p> <p>If colored_noise is set to True:</p> <p>\\(e_{k} = 0.8\\nu_{k-1} + \\nu_{k}\\)</p> <p>where \\(x\\) is a uniformly distributed random variable and \\(\\nu\\) is a gaussian distributed variable with \\(\\mu=0\\) and \\(\\sigma=0.1\\)</p> <p>In the next example we will generate a data with 1000 samples with white noise and selecting 90% of the data to train the model. </p> <pre><code>x_train, x_valid, y_train, y_valid = get_siso_data(\n    n=1000, colored_noise=False, sigma=0.0001, train_percentage=90\n)\n</code></pre> <p>To obtain a NARMAX model we have to choose some values, e.g, the nonlinearity degree (degree), the maximum lag for the inputs and output (xlag and ylag).</p> <p>In addition, you can select the information criteria to be used with the Error Reduction Ratio to select the model order and the method to estimate the model parameters:</p> <ul> <li>Information Criteria: aic, aicc, bic, lilc, fpe</li> <li>Parameter Estimation: LeastSquares, TotalLeastSquares, RecursiveLeastSquares, NonNegativeLeastSquares, LeastMeanSquares and many more (see the docs)</li> </ul> <p>The n_terms values is optional. It refer to the number of terms to include in the final model. You can set this value based on the information criteria (see below) or based on priori information about the model structure. The default value is n_terms=None, so the algorithm will choose the minimum value reached by the information criteria.</p> <p>To use information criteria you have to set order_selection=True. You can also select n_info_values (default = 15).</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    err_tol=None,\n    basis_function=basis_function,\n)\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#model-structure-selection","title":"Model Structure Selection","text":"<p>The fit method executes the Error Reduction Ratio algorithm using Househoulder reflection to select the model structure.</p> <p>Enforcing keyword-only arguments in fit and predict methods as well. This is an effort to promote clear and non-ambiguous use of the library.</p> <pre><code>model.fit(X=x_train, y=y_train)\n</code></pre> <pre><code>&lt;sysidentpy.model_structure_selection.forward_regression_orthogonal_least_squares.FROLS at 0x1db932f5090&gt;\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#free-run-simulation","title":"Free run simulation","text":"<p>The predict method is use to generate the predictions. For now we only support free run simulation (also known as infinity steps ahead). Soon will let the user define a one-step ahead or k-step ahead prediction.</p> <pre><code>yhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#evaluate-the-model","title":"Evaluate the model","text":"<p>In this example we use the root_relative_squared_error metric because it is often used in System Identification. More metrics and information about it can be found on documentation.</p> <pre><code>rrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00017649882109753117\n</code></pre> <p>model_object.results return the selected model regressors, the estimated parameters and the ERR values. As shown below, the algorithm detect the exact model that was used for simulate the data.</p> <pre><code>r = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.57604864E-01\n1         y(k-1)  2.0000E-01  3.88976063E-02\n2  x1(k-1)y(k-1)  9.9992E-02  3.49749526E-03\n</code></pre> <p>In addition, you can access the residuals and plot_result methods to take a look at the prediction and two residual analysis. The extras and lam values below contain another residues analysis so you can plot it manually. This method will be improved soon. </p> <pre><code>plt.style.available\n</code></pre> <pre><code>['Solarize_Light2',\n '_classic_test_patch',\n '_mpl-gallery',\n '_mpl-gallery-nogrid',\n 'bmh',\n 'classic',\n 'dark_background',\n 'fast',\n 'fivethirtyeight',\n 'ggplot',\n 'grayscale',\n 'seaborn-v0_8',\n 'seaborn-v0_8-bright',\n 'seaborn-v0_8-colorblind',\n 'seaborn-v0_8-dark',\n 'seaborn-v0_8-dark-palette',\n 'seaborn-v0_8-darkgrid',\n 'seaborn-v0_8-deep',\n 'seaborn-v0_8-muted',\n 'seaborn-v0_8-notebook',\n 'seaborn-v0_8-paper',\n 'seaborn-v0_8-pastel',\n 'seaborn-v0_8-poster',\n 'seaborn-v0_8-talk',\n 'seaborn-v0_8-ticks',\n 'seaborn-v0_8-white',\n 'seaborn-v0_8-whitegrid',\n 'tableau-colorblind10']\n</code></pre> <pre><code>plot_results(\n    y=y_valid,\n    yhat=yhat,\n    n=1000,\n    title=\"test\",\n    xlabel=\"Samples\",\n    ylabel=r\"y, $\\hat{y}$\",\n    data_color=\"#1f77b4\",\n    model_color=\"#ff7f0e\",\n    marker=\"o\",\n    model_marker=\"*\",\n    linewidth=1.5,\n    figsize=(10, 6),\n    style=\"seaborn-v0_8-notebook\",\n    facecolor=\"white\",\n)\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(\n    data=ee, title=\"Residues\", ylabel=\"$e^2$\", style=\"seaborn-v0_8-notebook\"\n)\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(\n    data=x1e, title=\"Residues\", ylabel=\"$x_1e$\", style=\"seaborn-v0_8-notebook\"\n)\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"user-guide/tutorials/your-first-model/#setting-the-n_terms-parameter","title":"Setting the n_terms parameter","text":"<p>In the example above we let the number of terms to compose the final model to be defined as the minimum value of the information criteria. Once you ran the algorithm and choose the best number of parameters, you can turn order_selection to False and set the n_terms value (3 in this example). Here we have a small dataset, but in bigger data this can be critical because running information criteria algorithm is more computational expensive. Since we already know the best number of regressor, we set n_terms and we get the same result.</p> <p>However, this is not only critical because computational efficiency. In many situation, the minimum value of the information criteria can lead to overfitting. In some cases, the difference between choosing a model with 30 regressors or 10 is minimal, so you can take the model with 10 terms without loosing accuracy.</p> <p>In the following we use info_values to plot the information criteria values. As you can see, the minimum value relies where \\(xaxis = 5\\) </p> <pre><code>xaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <p></p> <pre><code> Here we are creating random samples with white noise and letting the algorithm choose\n the number of terms based on the minimum value of information criteria.\n This is not the best approach in System Identification, but serves as a simple example.\n The information criteria must be used as an __auxiliary tool__ to select *n_terms*.\n Plot the information values to help you on that!\n\n If you run the example above several times you might find some cases where the\n algorithm choose only the first two regressors, or four (depending on the information\n criteria method selected). This is because the minimum value of information criteria\n depends on residual variance (affected by noise) and have some limitations in nonlinear\n scenarios. However, if you check the ERR values (robust to noise) you will see that the\n ERR is ordering the regressors in the correct way!\n\n We have some examples on *information_criteria* notebook!\n</code></pre> <p>The n_info_values limits the number of regressors to apply the information criteria. We choose \\(n_y = n_x = \\ell = 2\\), so the candidate regressor is a list of 15 regressors. We can set n_info_values = 15 and see the information values for all regressors. This option can save some amount of computational resources when dealing with multiples inputs and large datasets.</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=True,\n    n_info_values=15,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\n\nmodel.fit(X=x_train, y=y_train)\n\nxaxis = np.arange(1, model.n_info_values + 1)\nplt.plot(xaxis, model.info_values)\nplt.xlabel(\"n_terms\")\nplt.ylabel(\"Information Criteria\")\n</code></pre> <pre><code>Text(0, 0.5, 'Information Criteria')\n</code></pre> <p></p> <p>Now running without executing information criteria methods (setting the n_terms) because we already know the optimal number of regressors</p> <pre><code>basis_function = Polynomial(degree=2)\nestimator = LeastSquares()\n\nmodel = FROLS(\n    order_selection=False,\n    n_info_values=15,\n    n_terms=3,\n    ylag=2,\n    xlag=2,\n    info_criteria=\"aic\",\n    estimator=estimator,\n    basis_function=basis_function,\n    err_tol=None,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n</code></pre> <pre><code>0.00017649882109753117\n      Regressors  Parameters             ERR\n0        x1(k-2)  9.0001E-01  9.57604864E-01\n1         y(k-1)  2.0000E-01  3.88976063E-02\n2  x1(k-1)y(k-1)  9.9992E-02  3.49749526E-03\n</code></pre>"},{"location":"user-guide/tutorials/your-first-model/#predict-method","title":"Predict method","text":"<p>One could ask why it is necessary to pass the test data on the predict method. The answers is: you don't need to pass the test data when you are running a infinity-steps ahead prediction, you just need to pass the initial conditions. However, if you wants to check how your model performs in a 1-step ahead prediction or n-step ahead prediction, you should provide the test data.</p> <p>To show you only need the initial condition, consider the following example using the previous trained model:</p> <pre><code>model.max_lag  # the number of initial conditions you should provide\n</code></pre> <pre><code>2\n</code></pre> <pre><code>yhat = model.predict(\n    X=x_valid, y=y_valid[: model.max_lag]\n)  # passing only the 2 initial values which will be used as initial conditions\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00017649882109753117\n</code></pre> <p>As you can see, the rrse obtained is the same as the one obtained when we input the full test data. That is due the fact that even in the cases you provide the full test data, the prediction method only uses the first values as initial conditions and drop the other values in the backend.</p> <p>In the 1-step ahead prediction or n-steps ahead prediction you should provide the full test data because we handle all the computations in the background so the users don't need to care to implement the loops themselves.</p> <p>If you wants the check how your model performs in a 3-steps ahead scenario using 200 samples in the test data, that means that at each 3 iterations the model feedback will use the real data as initial conditions. Thats why the full data test is necessary, because otherwise the model won't find the real value to use as feedback in the n-iteration.</p> <p>This is the case where you have access the historical data so you can check how your model performs in n-steps ahead prediction. If you choose to use a 3-steps ahead model prediction in real life, you have to predict the next 3 samples, wait the 3 iterations, collect the real data and use the new data as initial condition to predict the next 3 values and so on.</p> <p>In the infinity-steps ahead prediction scenario, if your model has a input it will be able to make predictions for all inputs by only providing the initial conditions. If your model has no input (a NAR model, for example), you can set the forecast horizon and the model will make predictions by using only the initial conditions. All the feedback will be the predicted values (this is, by the way, one of the reasons that usually n-steps ahead models are better than infinity-ahead models).</p> <p>It is worth to mention that changing the initial condition doesn't mean you are changing your model. The only thing changing is the initial condition and that could make a real difference in many cases.</p>"},{"location":"user-guide/tutorials/your-first-model/#extra-information","title":"Extra information","text":"<p>You can access some extra information like the list of all candidate regressors</p> <pre><code># for now the list is returned as a codification. Here, $0$ is the constant term, $[1001]=y{k-1}, [100n]=y_{k-n}, [200n] = x1_{k-n}, [300n]=x2_{k-n}$ and so on\nmodel.regressor_code  # list of all possible regressors given non_degree, n_y and n_x values\n</code></pre> <pre><code>array([[   0,    0],\n       [1001,    0],\n       [1002,    0],\n       [2001,    0],\n       [2002,    0],\n       [1001, 1001],\n       [1002, 1001],\n       [2001, 1001],\n       [2002, 1001],\n       [1002, 1002],\n       [2001, 1002],\n       [2002, 1002],\n       [2001, 2001],\n       [2002, 2001],\n       [2002, 2002]])\n</code></pre> <pre><code>print(model.err, \"\\n\\n\")  # err values for the selected terms\nprint(model.theta)  # estimated parameters for the final model structure\n</code></pre> <pre><code>[9.57604864e-01 3.88976063e-02 3.49749526e-03 1.43420284e-10\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n\n\n[[0.90000582]\n [0.20000142]\n [0.0999919 ]]\n</code></pre> <pre><code>\n</code></pre>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/","title":"Logistic Map","text":"<p>Tutorial created by Wilson Rocha</p>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/#the-logistic-map","title":"The Logistic Map","text":"<p>Chaotic systems are deterministic processes that exhibit unpredictable, seemingly random behavior due to their extreme sensitivity to initial conditions. Though governed by simple rules, these systems are notoriously difficult to model over long timescales, making them a fascinating challenge for system identification. The Logistic Map is a classic example of chaos, first popularized in ecology to model population growth. Its simple equation,</p> \\[ x_{n+1} = r x_n (1 - x_n) \\] <p>captures a rich variety of behaviors, from stable equilibria to periodic oscillations and full chaos, depending on the growth parameter \\(r\\). By studying this system, we gain insights into universal phenomena like the period-doubling route to chaos and the emergence of fractal structures in bifurcation diagrams. In this tutorial, we\u2019ll use SysIdentPy to model the Logistic Map and reconstruct its bifurcation diagram, showcasing how data-driven approaches can capture chaotic dynamics.</p>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/#visualizing-the-logistic-map","title":"Visualizing the Logistic Map","text":"<p>To understand the Logistic Map, let\u2019s first visualize its behavior across different regimes of \\(r\\):</p> <ul> <li>Stable Fixed Points \\(r &lt; 3\\): For low \\(r\\), the population converges to a steady value.</li> <li>Periodic Regimes \\(3 &lt; r &lt; 3.57\\): As \\(r\\) increases, the system undergoes period-doubling bifurcations, oscillating between 2, 4, 8, \u2026 states.</li> <li>Chaos \\(r \\geq 3.57\\): Beyond the accumulation point, the system behaves chaotically, with trajectories never repeating.</li> </ul> <p>A cobweb plot helps visualize the iteration process: starting from an initial \\(x_0\\), we alternate between evaluating the Logistic Map (vertical jumps) and updating \\(x_n\\) (horizontal moves to the diagonal). Chaotic regimes show erratic, non-repeating cobweb patterns, while periodic regimes trace stable cycles.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquares\n\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.display_results import results\n\n\ndef logistic_map(x, r):\n    \"\"\"Compute one iteration of the logistic map.\"\"\"\n    return r * x * (1 - x)\n\n\n# Parameters\nr = 3.7  # Growth rate (try 2.8, 3.2, 3.5, 3.9)\nn_iter = 50  # Number of iterations\nx0 = 0.2  # Initial condition\n\n# Create cobweb plot\nx = np.linspace(0, 1, 1000)\nf = logistic_map(x, r)\n\nplt.figure(figsize=(8, 6))\nplt.plot(x, f, \"b-\", label=f\"Logistic Map ($r={r}$)\")\nplt.plot(x, x, \"k--\", label=\"$y = x$\")\n\n# Simulate iterations\nxt = np.zeros(n_iter)\nxt[0] = x0\nfor i in range(n_iter - 1):\n    y = logistic_map(xt[i], r)\n    plt.plot([xt[i], xt[i]], [xt[i], y], \"r\", lw=0.5)  # Vertical line\n    plt.plot([xt[i], y], [y, y], \"r\", lw=0.5)  # Horizontal line\n    xt[i + 1] = y\n\nplt.xlabel(\"$x_n$\", fontsize=12)\nplt.ylabel(\"$x_{n+1}$\", fontsize=12)\nplt.title(\"Cobweb Plot for the Logistic Map\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.xlim(0, 1)\nplt.ylim(0, 1)\nplt.show()\n</code></pre> <p></p> <p>The plot above shows the Logistic Curve (Blue). The diagonal line (Black Dashed) represents \\(x_{n+1} = x_n\\) and shows the intersections with the logistic curve indicate fixed points. Finally, the vertical lines of the Cobweb trajectory (in red) apply the map \\(x_n \\to x_{n+1}\\) and the horizontal lines reset \\(x_n = x_{n+1}\\) for the next iteration.</p> <p>The interpretation by regime can be taken as follows:</p> <ul> <li> <p>Stable Fixed Point (e.g., \\(r = 2.8\\)):   The cobweb spirals inward to a single point.</p> </li> <li> <p>Periodic (e.g., \\(r = 3.5\\)):   The trajectory cycles between 4 points (period-4).</p> </li> <li> <p>Chaotic (e.g., \\(r = 3.9\\)):   The red lines never repeat, filling the space erratically.</p> </li> </ul>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/#3-generating-the-bifurcation-diagram","title":"3. Generating the Bifurcation Diagram","text":"<p>The bifurcation diagram summarizes how the Logistic Map\u2019s long-term behavior changes with \\(r\\). We will create it considering: 1. Sweeping \\(r\\) across a range (\\(3.5 \\leq r \\leq 4.0\\)). 2. Iterate the map: For each \\(r\\), discard transient iterations (first 500 steps) to focus on asymptotic behavior. 3. Plot \\(x\\) for the remaining iterations.</p> <p>The bifurcation diagram reveals several key features. Period-doubling cascades occur as successive bifurcations split stable points into pairs, leading to progressively longer periods. Chaotic regions emerge as dense bands of points, indicating a high sensitivity to initial conditions. Within these chaotic regimes, windows of order appear, such as near \\(r \\approx 3.83\\), where periodic behavior temporarily reemerges, highlighting the subtle structure underlying chaos.</p> <p>This diagram serves as a \u201cfingerprint\u201d of the system, which we\u2019ll later reconstruct using SysIdentPy models.</p> <pre><code>num = 1000\nN = 1000\nN_drop = 500\nr0 = 3.5\n\nrs = r0 + np.arange(num) / num * (4 - r0)\nxss = []\n\n# Generate bifurcation data\nfor r in rs:\n    x = 0.5  # Initial condition\n    xs = []\n\n    # Warmup iterations (discard transient)\n    for _ in range(N_drop):\n        x = logistic_map(x, r)\n\n    # Store stable iterations\n    for _ in range(N):\n        x = logistic_map(x, r)\n        xs.append(x)\n\n    xss.append(xs)\n\nplt.figure(figsize=(4, 4), dpi=100)\nfor r, xs in zip(rs, xss):\n    plt.plot([r] * len(xs), xs, \",\", alpha=0.1, c=\"black\", rasterized=True)\n\nplt.xlabel(\"$r$\")\nplt.ylabel(\"$x_n$\")\nplt.title(\"Logistic Map Bifurcation Diagram\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Let's start by building a model for the logistic map with \\(r = 3.5\\) to evaluate how SysIdentPy performs in this periodic scenario.</p> <pre><code>y = np.array(xss[0]).reshape(-1, 1)\ny_train = y[:800, :].copy()\ny_test = y[200:, :].copy()\n\nbasis_function = Polynomial(degree=3)\nmodel = FROLS(\n    ylag=1,\n    estimator=LeastSquares(),\n    basis_function=basis_function,\n    model_type=\"NAR\",\n)\nmodel.fit(y=y_train)\nyhat = model.predict(y=y_test[:2].reshape(-1, 1), forecast_horizon=len(y_test))\n\nr = pd.DataFrame(\n    results(\n        model.final_model,\n        model.theta,\n        model.err,\n        model.n_terms,\n        err_precision=8,\n        dtype=\"sci\",\n    ),\n    columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n)\nprint(r)\n\nplot_results(y=y_test[model.max_lag :], yhat=yhat[model.max_lag :])\n</code></pre> <pre><code>  Regressors   Parameters             ERR\n0          1  -2.4433E-14  9.05069788E-01\n1   y(k-1)^3   3.6415E-14  9.17450154E-02\n2     y(k-1)   3.5000E+00  3.13396079E-03\n3   y(k-1)^2  -3.5000E+00  5.12359326E-05\n\n\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 4\n  self.info_values = self.information_criterion(reg_matrix, y)\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/chaotic-systems/logistic-map/#5-reconstructing-the-bifurcation-diagram-with-models","title":"5. Reconstructing the Bifurcation Diagram with Models","text":"<p>Can a data-driven model like NARMAX replicate the Logistic Map\u2019s bifurcation behavior? Let\u2019s find out:</p> <p>The following steps will be executed:</p> <ol> <li>Loop over \\(r\\): For each value in the range (e.g., 2.5 to 4.0 in steps of 0.01).</li> <li>Train a model: Generate synthetic data for the current \\(r\\), split into training/testing sets, and fit a NARMAX model using SysIdentPy\u2019s <code>FROLS</code> algorithm.</li> <li>Prediction: Use the trained model to predict future states, starting from an initial condition.</li> <li>Plot: Collect the asymptotic states and overlay them on a bifurcation diagram.</li> </ol> <pre><code>def fit_model(y_train, y_test, degree=3, ylag=1, n_info_values=50):\n    \"\"\"Fit a NARX model and return predictions.\"\"\"\n    basis_function = Polynomial(degree=degree)\n    model = FROLS(\n        ylag=ylag,\n        n_info_values=n_info_values,\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n        model_type=\"NAR\",\n    )\n    model.fit(y=y_train)\n    return model.predict(\n        y=y_test[: model.max_lag].reshape(-1, 1), forecast_horizon=len(y_test)\n    )\n\n\ndef generate_bifurcation_data(rs, N, N_drop):\n    \"\"\"Generate logistic map data and model predictions for each r.\"\"\"\n    xss, yhat_bifurcation = [], []\n    for r in rs:\n        x = 0.5\n        # Warm-up iterations\n        for _ in range(N_drop):\n            x = logistic_map(x, r)\n\n        # Stable iterations (corrected loop)\n        xs = []\n        for _ in range(N):\n            x = logistic_map(x, r)\n            xs.append(x)\n\n        xss.append(xs)\n\n        # Prepare data for modeling\n        y = np.array(xs).reshape(-1, 1)\n        y_train, y_test = y[:800, :], y[200:, :]\n        yhat = fit_model(y_train, y_test)\n        yhat_bifurcation.append(np.array(yhat))\n\n    return xss, yhat_bifurcation\n\n\n# Parameters\nnum = 1000\nN, N_drop = 1000, 500\nr0 = 3.5\nrs = r0 + np.arange(num) / num * (4 - r0)\n\nxss, yhat_bifurcation = generate_bifurcation_data(rs, N, N_drop)\n\n# Plot predicted bifurcation diagram\nplt.figure(figsize=(4, 4), dpi=100)\nfor ind, r in enumerate(rs):\n    plt.plot(\n        [r] * len(yhat_bifurcation[ind]),\n        yhat_bifurcation[ind],\n        \",\",\n        alpha=0.1,\n        c=\"black\",\n        rasterized=True,\n    )\n\nplt.title(\"Bifurcation Diagram with FROLS Predictions\", fontsize=14)\nplt.xlabel(\"$r$\", fontsize=12)\nplt.ylabel(\"$x_n$\", fontsize=12)\nplt.grid(alpha=0.2)\nplt.show()\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 4\n  self.info_values = self.information_criterion(reg_matrix, y)\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\parameter_estimation\\estimators.py:75: UserWarning: Psi matrix might have linearly dependent rows.Be careful and check your data\n  self._check_linear_dependence_rows(psi)\n</code></pre> <p></p> <p>As shown in the plots above, the models accurately reproduce stable cycles in periodic regimes, with minimal prediction errors. In chaotic regimes, the models provide accurate short-term predictions, but long-term divergence occurs due to the sensitivity to initial conditions, highlighting the inherent limitations of chaotic system modeling. The generated bifurcation diagram retains the key features, such as period-doubling and chaotic transitions, although finer details may require a model with higher complexity. This experiment demonstrates SysIdentPy\u2019s ability to capture essential nonlinear dynamics, even in chaotic scenarios.</p>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/","title":"Lorenz System","text":"<p>Example created by Wilson Rocha</p> <p>Note: This example is based on the Lorenz system simulation reference from UF|Physics: Introduction to Biological Physics. Some of the code used for the Lorenz simulations and the bifurcation diagram is adapted from this resource.</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.integrate import odeint\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.parameter_estimation import LeastSquares\nfrom sysidentpy.utils.display_results import results\n</code></pre>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/#the-lorenz-system","title":"The Lorenz System","text":"<p>The Lorenz system is a well-known example in chaos theory, consisting of three ordinary differential equations that describe the behavior of convection currents in the atmosphere. First introduced by Edward Lorenz in 1963, this system is famous for exhibiting chaotic dynamics, where small changes in initial conditions lead to drastically different outcomes. Despite being deterministic, the system\u2019s behavior is highly unpredictable, making it a key example of chaos in mathematics and science.</p> <p>The equations that define the Lorenz system are:</p> \\[ \\begin{aligned} \\frac{dx}{dt} &amp;= \\sigma (y - x), \\\\ \\frac{dy}{dt} &amp;= x (\\rho - z) - y, \\\\ \\frac{dz}{dt} &amp;= xy - \\beta z, \\end{aligned} \\] <p>Where: - \\(x\\), \\(y\\), and \\(z\\) represent variables such as the rate of convection, the temperature difference between the rising and descending air, and the deviation of the temperature from average, - \\(\\sigma\\) is the Prandtl number (fluid viscosity/thermal diffusivity), \\(\\rho\\) is the Rayleigh number (drives convection), and \\(\\beta\\) is a system parameter related to the geometry of the system. Chaos emerges at \\(\\rho = 28\\), \\(\\sigma = 10\\), \\(\\beta = 8/3\\).</p> <p>In this tutorial, we will use SysIdentPy to create a model of the Lorenz system. SysIdentPy allows us to identify the system's parameters from data and explore its chaotic behavior in a structured way. We will first generate data by solving the Lorenz equations numerically and then apply SysIdentPy to model the system, capturing the dynamics of the attractor and estimating key system parameters.</p>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/#visualizing-the-lorenz-attractor","title":"Visualizing the Lorenz Attractor","text":"<p>The Lorenz attractor, a three-dimensional plot of the system\u2019s solutions, visually illustrates the chaotic behavior of the system, often resembling a \"butterfly\" shape. This chaotic structure reveals the system's sensitive dependence on initial conditions, meaning that even small differences in starting points can lead to vastly different trajectories. This is a striking example of non-periodic motion in dynamical systems.</p> <p>The parameter \\(\\rho\\) plays a critical role in shaping the behavior of the system. By varying \\(\\rho\\), it is possible to observe transitions from steady states to chaos. For lower values of \\(\\rho\\), the system remains stable, often exhibiting periodic behavior. As \\(\\rho\\) increases, the system becomes increasingly sensitive and begins to exhibit complex, chaotic dynamics. Exploring these transitions allows for a deeper understanding of how deterministic systems can evolve into chaotic systems and highlights the concept of bifurcation within nonlinear dynamics.</p> <pre><code>def lorenz(xyz, t, sigma, rho, beta):\n    x, y, z = xyz  # parse variables\n    dxdt = sigma * (y - x)\n    dydt = x * (rho - z) - y\n    dzdt = x * y - beta * z\n    return [dxdt, dydt, dzdt]\n\n\nsigma = 10.0\nbeta = 8 / 3\nrho = 28.0\n\nT = 50.0  # total time to run\ndt = 0.01  # time step\ntime_points = np.arange(0.0, T, dt)\n\n# Nontrivial steady state values for plotting\nx2 = y2 = np.sqrt(beta * (rho - 1))  # nontrivial steady state\nz2 = rho - 1\n\ninit = np.random.rand(3) * 2 - 1  # initial conditions between -1 and 1\nsol2 = odeint(lorenz, init, time_points, args=(sigma, rho, beta))\n\nplt.figure(figsize=(10, 6))\n# Plot nontrivial steady states as horizontal lines\nplt.axhline(x2, color=\"k\", lw=1, linestyle=\"--\", label=f\"Steady state: x={x2:.2f}\")\nplt.axhline(-x2, color=\"k\", lw=1, linestyle=\"--\", label=f\"Steady state: x={-x2:.2f}\")\n\n# Plot the single trajectory\nplt.plot(time_points, sol2[:, 0], lw=2, label=f\"Trajectory starting at {init}\")\n\nplt.xlabel(r\"$t$\")\nplt.ylabel(r\"$x$\")\nplt.title(\"Lorenz Attractor: Single Trajectory\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>plt.figure()\nplt.plot(sol2[:, 0], sol2[:, 2])  # plot trajectories projected onto the (x,z) plane\nplt.plot(\n    [sol2[0, 0]], [sol2[0, 2]], \"b^\"\n)  # blue trianble labels starting point of each trajectory\nplt.plot([x2], [z2], \"rx\")  # steady state\nplt.plot([-x2], [z2], \"rx\")  # steady state\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$z$\")\nplt.show()\n</code></pre> <p></p> <pre><code># Let us now visualize all the solutions as trajectories in a 3-d plot\n\nfig = plt.figure()\nax = fig.add_subplot(projection=\"3d\")\nax.plot(sol2[:, 0], sol2[:, 1], sol2[:, 2])\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$y$\")\nax.set_zlabel(r\"$z$\")\nplt.show()\n</code></pre> <p></p> <p>The bifurcation diagram shows the possible steady states or periodic orbits of the system for different values of a system parameter (like \\(\\rho\\)). This is a compact and powerful way to visualize how small changes in a parameter can drastically alter the system's behavior.</p> <pre><code>sigma = 10.0\nbeta = 8 / 3.0\n\nt1 = 20.0  # time to add perturbation\nT = 100.0\ndt = 0.01\ntime_points = np.arange(0.0, 50.0, 0.01)\n\nrho_list = np.geomspace(0.1, 1000.0, 401)  # list of rho values geometrically spaced\nsol_list = []  # list of solutions of Lorenz equations\n\nfor rho in rho_list:\n    init = np.random.rand(3) * 2 - 1  # random initial values\n    sol = odeint(lorenz, init, time_points, args=(sigma, rho, beta))\n    sol_list.append(sol)\n\n\nrP = 1  # onset of instability at the origin, pitchfork bifurcation\nrH = sigma * (sigma + beta + 3) / (sigma - beta - 1)  # onset of chaos, Hopf bifurcation\n\nplt.figure(figsize=(8, 6))\nplt.axvline(rP, lw=1, ls=\":\", label=r\"$\\rho = 1$\")\nplt.axvline(rH, lw=1, ls=\"--\", label=r\"$\\rho = %.3f$\" % rH)\nplt.axvline(28, lw=1, ls=\"-\", label=r\"$\\rho = 28$\")\nfor i in range(len(rho_list)):\n    rho = rho_list[i]\n    sol = sol_list[i]\n    y = sol[int(t1 / dt) :, 0]\n    x = [rho] * len(y)\n    plt.scatter(x, y, s=0.1, c=\"k\", marker=\".\", edgecolor=\"none\")\n\nplt.xscale(\"log\")\nplt.xlabel(r\"$\\rho$\")\nplt.ylabel(r\"$x$\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>The stable steady state at the origin undergoes a pitchfork bifurcation, splitting into two distinct steady states. As the parameter increases further, these two steady states experience a Hopf bifurcation, where one of the fixed points becomes a saddle point, marking the onset of chaotic behavior. For \\(\\rho &lt; 24.74\\), the system exhibits steady states or periodic orbits, while for \\(\\rho \\geq 24.74\\), chaos emerges, revealing a fractal structure.</p>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/#modeling-with-sysidentpy","title":"Modeling with SysIdentPy","text":"<p>Let's visualize the data that we need to model</p> <pre><code>plt.plot(sol2)\n</code></pre> <pre><code>[&lt;matplotlib.lines.Line2D at 0x27c7c2b5bb0&gt;,\n &lt;matplotlib.lines.Line2D at 0x27c7c177ce0&gt;,\n &lt;matplotlib.lines.Line2D at 0x27c75eb8da0&gt;]\n</code></pre> <p></p> <pre><code>def prepare_data(data, y_col, x_cols, train_size=4000):\n    \"\"\"Split and reshape the data for each model.\"\"\"\n    train_data, test_data = data[:train_size], data[train_size:]\n\n    # Extract y (target) and x (predictors)\n    y_train, y_test = train_data[:, y_col].reshape(-1, 1), test_data[:, y_col].reshape(\n        -1, 1\n    )\n    x_train, x_test = train_data[:, x_cols], test_data[:, x_cols]\n\n    return x_train, y_train, x_test, y_test\n\n\ndef train_and_predict(x_train, y_train, x_test, y_test):\n    \"\"\"Train the model and make predictions.\"\"\"\n    basis_function = Polynomial(degree=2)\n    model = FROLS(\n        ylag=1,\n        xlag=[[1], [1]],\n        estimator=LeastSquares(),\n        basis_function=basis_function,\n    )\n    model.fit(X=x_train, y=y_train)\n    yhat = model.predict(\n        X=x_test,\n        y=y_test[: model.max_lag].reshape(-1, 1),\n    )\n\n    r = pd.DataFrame(\n        results(\n            model.final_model,\n            model.theta,\n            model.err,\n            model.n_terms,\n            err_precision=8,\n            dtype=\"sci\",\n        ),\n        columns=[\"Regressors\", \"Parameters\", \"ERR\"],\n    )\n    print(r)\n    return yhat\n\n\n# First model: Using sol2[:, 0] for y and sol2[:, [1, 2]] for x\nx_train, y_train, x_test, y_test = prepare_data(\n    sol2, y_col=0, x_cols=[1, 2], train_size=4000\n)\nyhat_1 = train_and_predict(x_train, y_train, x_test, y_test)\n\n# set 1 because is the maximum lag. Can set as model.max_lag as well\nplot_results(y=y_test[1:], yhat=yhat_1[1:], n=1000)\n\n# Second model: Using sol2[:, 1] for y and sol2[:, [0, 2]] for x\nx_train, y_train, x_test, y_test = prepare_data(\n    sol2, y_col=1, x_cols=[0, 2], train_size=4000\n)\nyhat_2 = train_and_predict(x_train, y_train, x_test, y_test)\n\nplot_results(y=y_test[1:], yhat=yhat_2[1:], n=1000)\n\n# Third model: Using sol2[:, 2] for y and sol2[:, [0, 1]] for x\nx_train, y_train, x_test, y_test = prepare_data(\n    sol2, y_col=2, x_cols=[0, 1], train_size=4000\n)\nyhat_3 = train_and_predict(x_train, y_train, x_test, y_test)\n\nplot_results(y=y_test[1:], yhat=yhat_3[1:], n=1000)\n</code></pre> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 10\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n       Regressors   Parameters             ERR\n0          y(k-1)   9.1832E-01  9.98019711E-01\n1         x1(k-1)   9.4946E-02  1.97448493E-03\n2   x2(k-1)y(k-1)  -4.7263E-04  5.79130765E-06\n3  x2(k-1)x1(k-1)  -2.1239E-05  4.72613968E-09\n4         x2(k-1)   6.9084E-06  4.82674757E-10\n5       x1(k-1)^2  -1.1524E-06  4.27141670E-11\n6        y(k-1)^2  -8.8057E-06  3.31043247E-11\n7               1  -1.8509E-04  3.67821804E-11\n8   x1(k-1)y(k-1)   7.9252E-06  5.31217490E-11\n9       x2(k-1)^2   5.5901E-07  5.27580418E-12\n</code></pre> <p></p> <pre><code>       Regressors   Parameters             ERR\n0          y(k-1)   9.9829E-01  9.96084591E-01\n1   x2(k-1)y(k-1)  -6.5230E-04  3.09653629E-03\n2  x2(k-1)x1(k-1)  -9.6452E-03  4.62214305E-04\n3         x1(k-1)   2.7760E-01  3.50549821E-04\n4         x2(k-1)   2.8138E-04  3.86911341E-07\n5        y(k-1)^2  -3.2352E-05  3.41517241E-08\n6       x1(k-1)^2  -2.6343E-04  2.79943269E-08\n7               1  -6.2559E-03  3.02963912E-08\n8   x1(k-1)y(k-1)   2.3472E-04  4.11556688E-08\n9       x2(k-1)^2   1.5355E-05  3.32074748E-09\n\n\nc:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 10\n  self.info_values = self.information_criterion(reg_matrix, y)\n</code></pre> <p></p> <pre><code>c:\\Users\\wilso\\miniconda3\\envs\\sysidentpyv04\\Lib\\site-packages\\sysidentpy\\model_structure_selection\\forward_regression_orthogonal_least_squares.py:621: UserWarning: n_info_values is greater than the maximum number of all regressors space considering the chosen y_lag, u_lag, and non_degree. We set as 10\n  self.info_values = self.information_criterion(reg_matrix, y)\n\n\n       Regressors   Parameters             ERR\n0          y(k-1)   9.7932E-01  9.99326652E-01\n1  x2(k-1)x1(k-1)   8.7471E-03  6.70396643E-04\n2       x2(k-1)^2   9.5637E-04  2.13515631E-06\n3       x1(k-1)^2  -1.1022E-04  4.45532422E-07\n4         x1(k-1)   4.1767E-03  3.37917913E-08\n5        y(k-1)^2  -1.5200E-04  1.35172660E-08\n6               1  -3.4932E-02  2.45525697E-08\n7         x2(k-1)  -2.9565E-03  1.41491234E-09\n8   x2(k-1)y(k-1)   8.8849E-05  3.12544165E-09\n9   x1(k-1)y(k-1)  -1.0276E-04  6.02297621E-09\n</code></pre> <p></p>"},{"location":"user-guide/tutorials/chaotic-systems/lorenz-system/#results","title":"Results","text":"<p>Now we can visualize the results of each predicted trajectory</p> <pre><code># First model: Using predictions from yhat_1 (predicted x vs predicted z)\nplt.figure()\nplt.plot(yhat_1, yhat_3)  # plot predictions projected onto the (x, z) plane\nplt.plot(\n    [sol2[0, 0]], [sol2[0, 2]], \"b^\"\n)  # blue triangle labels starting point of each trajectory\nplt.plot([x2], [z2], \"rx\")  # steady state\nplt.plot([-x2], [z2], \"rx\")  # steady state\nplt.xlabel(r\"$x$\")\nplt.ylabel(r\"$z$\")\nplt.title(\"Butterfly Plot - Model 1 Predictions\")\nplt.show()\n\n# Second model: Using predictions from yhat_2 (predicted y vs predicted z)\nplt.figure()\nplt.plot(yhat_2, yhat_3)  # plot predictions projected onto the (y, z) plane\nplt.plot(\n    [sol2[0, 1]], [sol2[0, 2]], \"b^\"\n)  # blue triangle labels starting point of each trajectory\nplt.plot([x2], [z2], \"rx\")  # steady state\nplt.plot([-x2], [z2], \"rx\")  # steady state\nplt.xlabel(r\"$y$\")\nplt.ylabel(r\"$z$\")\nplt.title(\"Butterfly Plot - Model 2 Predictions\")\nplt.show()\n\n# Third model: Using predictions from yhat_3 (predicted z vs predicted x)\nplt.figure()\nplt.plot(yhat_3, yhat_1)  # plot predictions projected onto the (z, x) plane\nplt.plot(\n    [sol2[0, 2]], [sol2[0, 0]], \"b^\"\n)  # blue triangle labels starting point of each trajectory\nplt.plot([z2], [x2], \"rx\")  # steady state\nplt.plot([z2], [-x2], \"rx\")  # steady state\nplt.xlabel(r\"$z$\")\nplt.ylabel(r\"$x$\")\nplt.title(\"Butterfly Plot - Model 3 Predictions\")\nplt.show()\n</code></pre> <p></p> <p></p> <p></p> <pre><code>fig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\nax.plot(yhat_1, yhat_2, yhat_3)\nax.set_xlabel(r\"$x$\")\nax.set_ylabel(r\"$y$\")\nax.set_zlabel(r\"$z$\")\nax.set_title(\"3D Butterfly Plot with Predicted Data\")\nplt.show()\n</code></pre> <p></p> <p>As shown in the plots above, the models effectively capture the dynamics of the Lorenz system, reproducing the stable cycles in periodic regimes with minimal prediction errors. In chaotic regimes, the models maintain accurate short-term predictions but can exhibit long-term divergence, which is expected due to the sensitivity to initial conditions intrinsic to chaotic systems. Although finer details may require more sophisticated model configurations, SysIdentPy demonstrates its capability to model the nonlinear dynamics of the Lorenz system, even in the chaotic regime. This experiment highlights SysIdentPy's versatility in capturing both stable and chaotic behaviors in complex systems.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/","title":"Como Adicionar uma Tradu\u00e7\u00e3o","text":"<p>Guia para criar ou melhorar tradu\u00e7\u00f5es da documenta\u00e7\u00e3o em qualquer idioma.</p> <p>Usamos MkDocs + Material + <code>mkdocs-static-i18n</code>. Ingl\u00eas \u00e9 o fallback. Qualquer idioma novo replica a estrutura de pastas. Se faltar uma p\u00e1gina traduzida, aparece a vers\u00e3o inglesa.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#1-visao-geral","title":"1. Vis\u00e3o geral","text":"<p>Tr\u00eas cen\u00e1rios comuns:</p> <ol> <li>Melhorar uma p\u00e1gina j\u00e1 traduzida.</li> <li>Traduzir uma p\u00e1gina que s\u00f3 existe em ingl\u00eas.</li> <li>Adicionar um idioma totalmente novo.</li> </ol> <p>Tudo abaixo cobre esses casos.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#2-estrutura-de-pastas","title":"2. Estrutura de pastas","text":"<pre><code>/docs\n  en/\n    developer-guide/\n    getting-started/\n    user-guide/\n  &lt;locale&gt;/\n    (mesmos caminhos relativos)\n</code></pre> <p><code>&lt;locale&gt;</code> exemplos: <code>pt</code>, <code>es</code>, <code>fr</code>, <code>de</code>, <code>it</code>, <code>ja</code>, <code>zh</code>, <code>ru</code>. Use c\u00f3digos curtos (BCP\u201147). Evite variantes regionais salvo necessidade (<code>pt-BR</code>, <code>pt-PT</code>).</p> <p>Os caminhos relativos devem ser id\u00eanticos:</p> <pre><code>Ingl\u00eas: docs/en/developer-guide/how-to-add-a-translation.md\nEspanhol: docs/es/developer-guide/how-to-add-a-translation.md\nFranc\u00eas:  docs/fr/developer-guide/how-to-add-a-translation.md\n</code></pre>"},{"location":"pt/developer-guide/how-to-add-a-translation/#3-inicio-rapido-traduzir-ou-melhorar","title":"3. In\u00edcio r\u00e1pido (traduzir ou melhorar)","text":"<ol> <li>Fork e clone.</li> <li>Crie / ative ambiente virtual.</li> <li>Instale extras de docs:     <pre><code>pip install -e \".[docs]\"\n</code></pre></li> <li>Servidor local:     <pre><code>mkdocs serve\n</code></pre></li> <li>Abra a URL e use o seletor de idioma.</li> </ol> <p>Reinicie se arquivos novos n\u00e3o aparecerem.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#4-adicionando-um-novo-idioma-setup-inicial","title":"4. Adicionando um novo idioma (setup inicial)","text":"<p>Se a pasta j\u00e1 existe (ex: <code>pt/</code>), pule.</p> <ol> <li>Escolha c\u00f3digo (ex: <code>es</code>).</li> <li>Crie <code>docs/es/</code>.</li> <li>Copie <code>docs/en/index.md</code> para <code>docs/es/index.md</code> e traduza.</li> <li>(Opcional) Comece s\u00f3 com index + p\u00e1ginas principais para PR menor.</li> <li>Edite <code>mkdocs.yml</code> em <code>i18n.languages</code>:     <pre><code>- locale: es\n  name: Espa\u00f1ol\n  build: true\n  site_description: &lt;slogan traduzido&gt;\n  theme:\n    docs_dir: docs/es/\n    custom_dir: docs/es/\n    site_dir: site/es/\n    logo: overrides/assets/img/logotype-sysidentpy.svg\n</code></pre></li> <li>N\u00e3o duplique a navega\u00e7\u00e3o; o plugin mapeia automaticamente.</li> <li>Rode <code>mkdocs serve</code> e confirme o idioma no seletor.</li> </ol> <p>Para variantes regionais (ex: <code>pt-BR</code>) mantenha consist\u00eancia no nome da pasta e no <code>locale</code>.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#5-nova-pagina-em-ingles-fonte","title":"5. Nova p\u00e1gina em ingl\u00eas (fonte)","text":"<ol> <li>Crie em <code>docs/en/...</code>.</li> <li>Front matter:     <pre><code>---\ntemplate: overrides/main.html\ntitle: T\u00edtulo\n---\n</code></pre></li> <li>Adicione no <code>nav</code> do <code>mkdocs.yml</code> (apenas uma vez).</li> <li>Verifique build.</li> <li>(Opcional) Coment\u00e1rio para tradutores:     <pre><code>&lt;!-- Nota para tradu\u00e7\u00e3o: manter \"NARMAX\" em ingl\u00eas. --&gt;\n</code></pre></li> </ol>"},{"location":"pt/developer-guide/how-to-add-a-translation/#6-criando-o-arquivo-traduzido","title":"6. Criando o arquivo traduzido","text":"<ol> <li> <p>Caminho espelhado: <code>docs/&lt;locale&gt;/&lt;mesmo&gt;.md</code>.</p> </li> <li> <p>Copie o original.</p> </li> <li> <p>Traduza s\u00f3 texto natural. Preserve:</p> <ul> <li>Blocos de c\u00f3digo (coment\u00e1rios apenas se ajudar)</li> <li>Identificadores (fun\u00e7\u00f5es, classes, imports)</li> <li>Caminhos, chaves de config, URLs</li> <li>Alvos de links relativos</li> </ul> </li> <li> <p>Mantenha hierarquia de t\u00edtulos.</p> </li> <li> <p>Preserve tipos de admonitions (<code>!!! note</code>, etc.). T\u00edtulo interno pode ser traduzido.</p> </li> <li> <p>Parte pendente? Use:    <pre><code>!!! note \"Tradu\u00e7\u00e3o pendente\"\n    Este par\u00e1grafo ainda ser\u00e1 traduzido.\n</code></pre></p> </li> <li> <p>Remova notas pendentes antes de finalizar (se concluir).</p> </li> </ol>"},{"location":"pt/developer-guide/how-to-add-a-translation/#7-links-internos","title":"7. Links internos","text":"<p>Use links relativos: <pre><code>Veja o [guia de contribui\u00e7\u00e3o](contribute.md).\n</code></pre> O plugin resolve por idioma. Evite hardcode <code>/en/</code> ou outro prefixo.</p> <p>\u00c2ncoras: se traduzir t\u00edtulo, o slug muda; ajuste refer\u00eancias <code>(#ancora)</code>.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#8-imagens-e-midia","title":"8. Imagens e m\u00eddia","text":"<p>Se a imagem cont\u00e9m texto:</p> <ul> <li>Op\u00e7\u00e3o A: localizar imagem dentro de <code>docs/&lt;locale&gt;/assets/</code> com mesmo nome.</li> <li>Op\u00e7\u00e3o B: reutilizar imagem inglesa se o texto n\u00e3o atrapalha.</li> </ul> <p>SVG: manter s\u00edmbolos ou termos t\u00e9cnicos; traduzir r\u00f3tulos descritivos.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#9-formatacao-estilo","title":"9. Formata\u00e7\u00e3o &amp; estilo","text":"Aspecto Regra N\u00fameros Mantenha precis\u00e3o; separador decimal local \u00e9 opcional. Unidades N\u00e3o traduzir (ms, Hz, etc.). APIs Nunca traduzir identificadores. Aspas Use padr\u00e3o local sem quebrar Markdown. Capitaliza\u00e7\u00e3o Igual s\u00f3 para nomes pr\u00f3prios / APIs. Tom Neutro, direto. <p>Evite blocos n\u00e3o revisados de tradu\u00e7\u00e3o autom\u00e1tica. Prefira frases curtas.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#10-glossario-de-traducao","title":"10. Gloss\u00e1rio de tradu\u00e7\u00e3o","text":"<p>Mantenha estes termos consistentes. Adicione equivalentes para outros idiomas conforme necess\u00e1rio:</p> Termo em Ingl\u00eas Portugu\u00eas (pt) Conceitos centrais model structure estrutura do modelo parameter estimation estima\u00e7\u00e3o de par\u00e2metros residual analysis an\u00e1lise dos res\u00edduos time series s\u00e9rie temporal identification identifica\u00e7\u00e3o Termos t\u00e9cnicos basis function fun\u00e7\u00e3o de base regression regress\u00e3o algorithm algoritmo validation valida\u00e7\u00e3o simulation simula\u00e7\u00e3o Termos de desenvolvimento feature funcionalidade pull request (PR) pull request (PR) branch branch commit commit documentation documenta\u00e7\u00e3o <p>Para outros idiomas, siga padr\u00f5es similares. Prefira clareza \u00e0 tradu\u00e7\u00e3o literal.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#11-checklist-de-revisao-arquivo-traduzido","title":"11. Checklist de revis\u00e3o (arquivo traduzido)","text":"<ul> <li> Build local OK.</li> <li> Caminho espelhado correto.</li> <li> Links relativos sem <code>/en/</code> fixo.</li> <li> Blocos de c\u00f3digo intactos (coment\u00e1rios revisados).</li> <li> Terminologia consistente.</li> <li> Sem notas pendentes (ou marcadas claramente se parcial).</li> <li> Front matter com <code>title:</code> traduzido.</li> </ul>"},{"location":"pt/developer-guide/how-to-add-a-translation/#12-commit-pr","title":"12. Commit &amp; PR","text":"<p>Inclua arquivo ingl\u00eas + traduzido se a p\u00e1gina \u00e9 nova; caso contr\u00e1rio s\u00f3 o traduzido.</p> <p>Exemplo: <pre><code>git add docs/en/developer-guide/new-topic.md docs/es/developer-guide/new-topic.md\ngit commit -m \"docs: adicionar tradu\u00e7\u00e3o em espanhol de new-topic\"\n</code></pre></p> <p>Template de descri\u00e7\u00e3o de PR:</p> <ul> <li>Idioma: <code>&lt;locale&gt;</code></li> <li>P\u00e1ginas: lista</li> <li>Trechos ainda em ingl\u00eas: (se houver)</li> <li>Termos novos de gloss\u00e1rio: (se houver)</li> <li>Notas para revis\u00e3o: contexto, termos dif\u00edceis</li> </ul> <p>Tradu\u00e7\u00f5es parciais s\u00e3o aceit\u00e1veis \u2014 marque claramente.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#13-atualizando-traducoes","title":"13. Atualizando tradu\u00e7\u00f5es","text":"<p>Quando o ingl\u00eas mudar:</p> <ol> <li>Veja o diff.</li> <li>Aplique mudan\u00e7as equivalentes.</li> <li>Sem tempo para traduzir? Deixe em ingl\u00eas + nota tempor\u00e1ria.</li> <li>Remova a nota ao finalizar.</li> </ol> <p>Prefira PRs menores.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#14-problemas-comuns","title":"14. Problemas comuns","text":"Sintoma Causa Corre\u00e7\u00e3o P\u00e1gina s\u00f3 em ingl\u00eas Falta arquivo no locale Criar arquivo espelhado Erro de build Entrada <code>i18n</code> incorreta Corrigir <code>locale</code> / indenta\u00e7\u00e3o Link 404 Caminho diferente do ingl\u00eas Sincronizar caminho \u00c2ncora quebrada T\u00edtulo mudou Ajustar slug / t\u00edtulo Idioma n\u00e3o aparece Faltou adicionar em <code>mkdocs.yml</code> Adicionar e reiniciar"},{"location":"pt/developer-guide/how-to-add-a-translation/#15-automacao-opcional","title":"15. Automa\u00e7\u00e3o (opcional)","text":"<p>Scripts podem copiar estrutura base, mas revise manualmente termos t\u00e9cnicos. N\u00e3o sobrescreva tradu\u00e7\u00f5es existentes.</p>"},{"location":"pt/developer-guide/how-to-add-a-translation/#16-duvidas","title":"16. D\u00favidas","text":"<p>Abra Issue ou Discussion para confirmar termos antes de traduzir grandes trechos. Feedback cedo evita retrabalho.</p> <p>Obrigado por tornar a documenta\u00e7\u00e3o acess\u00edvel a mais pessoas.</p>"},{"location":"pt/getting-started/getting-started/","title":"Primeiros Passos","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o do SysIdentPy! Aprenda como come\u00e7ar a usar o SysIdentPy no seu projeto. Em seguida, explore os principais conceitos e descubra recursos adicionais para modelar sistemas din\u00e2micos e s\u00e9ries temporais.</p>          \ud83d\udcda Em busca de mais detalhes sobre modelos NARMAX? \u25bc <p>             Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e um conjunto de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>             Esse livro oferece uma orienta\u00e7\u00e3o detalhada para auxiliar no seu trabalho com o SysIdentPy.         </p> <p>             \ud83d\udee0\ufe0f Voc\u00ea tamb\u00e9m pode explorar os tutoriais na documenta\u00e7\u00e3o para exemplos pr\u00e1ticos.         </p>"},{"location":"pt/getting-started/getting-started/#o-que-e-o-sysidentpy","title":"O que \u00e9 o SysIdentPy","text":"<p>SysIdentPy \u00e9 uma biblioteca Python de c\u00f3digo aberto para Identifica\u00e7\u00e3o de Sistemas usando modelos NARMAX, constru\u00edda sobre o NumPy e distribu\u00edda sob a licen\u00e7a BSD de 3 cl\u00e1usulas. SysIdentPy disponibiliza uma estrutura flex\u00edvel e f\u00e1cil de usar para construir modelos din\u00e2micos n\u00e3o lineares para s\u00e9ries temporais e sistemas din\u00e2micos.</p> <p>Com o SysIdentPy, voc\u00ea pode:</p> <ul> <li>Construir e customizar modelos n\u00e3o lineares para previs\u00e3o de s\u00e9ries temporais e sistemas din\u00e2micos.</li> <li>Utilizar t\u00e9cnicas inovadoras para sele\u00e7\u00e3o de estrutura e estima\u00e7\u00e3o de par\u00e2metros do modelo.</li> <li>Experimentar modelos NARX neurais e outros algoritmos avan\u00e7ados.</li> </ul>"},{"location":"pt/getting-started/getting-started/#instalacao","title":"Instala\u00e7\u00e3o","text":"<p>SysIdentPy \u00e9 publicado como um pacote Python e pode ser instalado com <code>pip</code>, de prefer\u00eancia em um ambiente virtual. Caso n\u00e3o tenha experi\u00eancia, role a p\u00e1gina e expanda a caixa de ajuda. Instale com:</p> \u00daltima Vers\u00e3o <pre><code>pip install sysidentpy</code></pre> Suporte NARX Neural <pre><code>pip install sysidentpy[\"all\"]</code></pre> Vers\u00e3o Espec\u00edfica <pre><code>pip install sysidentpy==\"0.5.3\"</code></pre> Do Git <pre><code>pip install git+https://github.com/wilsonrljr/sysidentpy.git</code></pre>          \u2753 Como gerenciar as depend\u00eancias do meu projeto? \u25bc <p>             Se voc\u00ea n\u00e3o tem experi\u00eancia pr\u00e9via com Python, recomendamos a leitura de                              Using Python's pip to Manage Your Projects' Dependencies             , que \u00e9 uma excelente introdu\u00e7\u00e3o \u00e0 mec\u00e2nica de gerenciamento de pacotes em Python e ajuda na solu\u00e7\u00e3o de erros.         </p>"},{"location":"pt/getting-started/getting-started/#quais-sao-os-principais-recursos-do-sysidentpy","title":"Quais s\u00e3o os principais recursos do SysIdentPy?","text":"\ud83e\udde9 Filosofia NARMAX <p>Construa varia\u00e7\u00f5es como NARX, NAR, ARMA, NFIR e outras.</p> \ud83d\udcdd Sele\u00e7\u00e3o da Estrutura <p>Use m\u00e9todos como FROLS, MetaMSS e combina\u00e7\u00f5es com t\u00e9cnicas de estima\u00e7\u00e3o de par\u00e2metros.</p> \ud83d\udd17 Fun\u00e7\u00f5es Base <p>Escolha entre 8+ fun\u00e7\u00f5es base, combinando tipos lineares e n\u00e3o lineares para modelos NARMAX personalizados.</p> \ud83c\udfaf Estima\u00e7\u00e3o de Par\u00e2metros <p>Mais de 15 m\u00e9todos para explorar diferentes cen\u00e1rios em conjunto com t\u00e9cnicas de sele\u00e7\u00e3o de estrutura.</p> \u2696\ufe0f T\u00e9cnicas Multiobjetivo <p>Minimize diferentes fun\u00e7\u00f5es objetivo usando informa\u00e7\u00e3o afim para estima\u00e7\u00e3o de par\u00e2metros.</p> \ud83d\udd04 Simula\u00e7\u00e3o de Modelos <p>Reproduza resultados de artigos com SimulateNARMAX. Teste e compare modelos publicados em artigos.</p> \ud83e\udd16 NARX Neural (PyTorch) <p>Integre com PyTorch para arquiteturas NARX neurais usando qualquer otimizador e fun\u00e7\u00e3o de custo.</p> \ud83d\udee0\ufe0f Estimadores Gerais <p>Compat\u00edvel com scikit-learn, CatBoost e mais para criar modelos NARMAX.</p>"},{"location":"pt/getting-started/getting-started/#recursos-adicionais","title":"Recursos adicionais","text":"<ul> <li> \ud83e\udd1d Contribua com o SysIdentPy </li> <li> \ud83d\udcdc Informa\u00e7\u00f5es de Licen\u00e7a </li> <li> \ud83c\udd98 Ajuda &amp; Suporte </li> <li> \ud83d\udcc5 Palestras </li> <li> \ud83d\udc96 Torne-se um Patrocinador </li> <li> \ud83e\udde9 Explore o C\u00f3digo Fonte </li> </ul>"},{"location":"pt/getting-started/getting-started/#voce-gosta-do-sysidentpy","title":"Voc\u00ea gosta do SysIdentPy?","text":"<p>Gostaria de ajudar o SysIdentPy, outros usu\u00e1rios e o criador da biblioteca? Voc\u00ea pode \"dar uma estrela\" ao projeto no GitHub clicando no bot\u00e3o de estrela no canto superior direito da p\u00e1gina: https://github.com/wilsonrljr/sysidentpy. \u2b50\ufe0f</p> <p>Ao marcar um reposit\u00f3rio com estrela, voc\u00ea o encontra mais facilmente no futuro, recebe sugest\u00f5es de projetos relacionados no GitHub e ainda valoriza o trabalho do mantenedor.</p> <p>Considere, tamb\u00e9m, apoiar o projeto tornando-se um sponsor. Seu apoio ajuda a manter o desenvolvimento ativo e garante a evolu\u00e7\u00e3o cont\u00ednua do SysIdentPy.</p> <p> \u00a0 Seja um  Patrocinador no GitHub</p>"},{"location":"pt/getting-started/quickstart-guide/","title":"Uso B\u00e1sico","text":""},{"location":"pt/getting-started/quickstart-guide/#1-pre-requisitos","title":"1. Pr\u00e9-requisitos","text":"<p>Voc\u00ea precisa conhecer um pouco de Python.</p> <p>Para executar os exemplos, al\u00e9m do NumPy voc\u00ea precisar\u00e1 do <code>pandas</code> instalado.</p> <pre><code>pip install sysidentpy pandas\n# Opcional: Para redes neurais e recursos avan\u00e7ados\npip install sysidentpy[\"all\"]\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#2-principais-recursos","title":"2. Principais Recursos","text":"<p>SysIdentPy oferece uma estrutura flex\u00edvel para construir, validar e visualizar modelos n\u00e3o lineares de s\u00e9ries temporais e sistemas din\u00e2micos. O processo de modelagem envolve algumas etapas: definir a representa\u00e7\u00e3o matem\u00e1tica, escolher o algoritmo de estima\u00e7\u00e3o de par\u00e2metros, selecionar a estrutura do modelo e determinar o horizonte de previs\u00e3o.</p> <p>Os seguintes recursos est\u00e3o dispon\u00edveis no SysIdentPy:</p>"},{"location":"pt/getting-started/quickstart-guide/#classes-de-modelo","title":"Classes de Modelo","text":"<ul> <li>NARMAX, NARX, NARMA, NAR, NFIR, ARMAX, ARX, AR e suas variantes.</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#representacoes-matematicas","title":"Representa\u00e7\u00f5es Matem\u00e1ticas","text":"<ul> <li>Polynomial (Polinomial)</li> <li>Neural</li> <li>Fourier</li> <li>Laguerre</li> <li>Bernstein</li> <li>Bilinear</li> <li>Legendre</li> <li>Hermite</li> <li>HermiteNormalized</li> </ul> <p>Voc\u00ea tamb\u00e9m pode definir modelos NARX como Bayesian e Gradient Boosting usando a classe GeneralNARX, que oferece integra\u00e7\u00e3o direta com v\u00e1rios algoritmos de aprendizado de m\u00e1quina.</p>"},{"location":"pt/getting-started/quickstart-guide/#algoritmos-de-selecao-de-estrutura","title":"Algoritmos de Sele\u00e7\u00e3o de Estrutura","text":"<ul> <li>Forward Regression Orthogonal Least Squares (FROLS)</li> <li>Meta-model Structure Selection (MeMoSS / MetaMSS)</li> <li>Accelerated Orthogonal Least Squares (AOLS)</li> <li>Entropic Regression (ER)</li> <li>Ultra Orthogonal Least Squares (UOLS)</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#metodos-de-estimacao-de-parametros","title":"M\u00e9todos de Estima\u00e7\u00e3o de Par\u00e2metros","text":"<ul> <li>M\u00ednimos Quadrados (MQ)</li> <li>Total Least Squares (TLS)</li> <li>M\u00ednimos Quadrados Recursivos (MQR)</li> <li>Ridge Regression</li> <li>Non-Negative Least Squares (NNLS)</li> <li>Least Squares Minimal Residues (LSMR)</li> <li>Bounded Variable Least Squares (BVLS)</li> <li>Least Mean Squares (LMS) e suas variantes:</li> <li>Affine LMS</li> <li>LMS with Sign Error</li> <li>Normalized LMS</li> <li>LMS with Normalized Sign Error</li> <li>LMS with Sign Regressor</li> <li>Normalized LMS with Sign Sign</li> <li>Leaky LMS</li> <li>Fourth-Order LMS</li> <li>Mixed Norm LMS</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#criterios-de-selecao-de-ordem","title":"Crit\u00e9rios de Sele\u00e7\u00e3o de Ordem","text":"<ul> <li>Crit\u00e9rio de Informa\u00e7\u00e3o de Akaike (AIC)</li> <li>Crit\u00e9rio de Informa\u00e7\u00e3o de Akaike Corrigido (AICc)</li> <li>Crit\u00e9rio de Informa\u00e7\u00e3o Bayesiano (BIC)</li> <li>Final Prediction Error (FPE)</li> <li>Khundrin's Law of Iterated Logarithm Criterion (LILC)</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#metodos-de-previsao","title":"M\u00e9todos de Previs\u00e3o","text":"<ul> <li>Um passo \u00e0 frente (one-step ahead)</li> <li>n passos \u00e0 frente (n-steps ahead)</li> <li>Infinito passos \u00e0 frente / simula\u00e7\u00e3o livre (infinity-steps / free run simulation)</li> </ul>"},{"location":"pt/getting-started/quickstart-guide/#ferramentas-de-visualizacao","title":"Ferramentas de Visualiza\u00e7\u00e3o","text":"<ul> <li>Gr\u00e1ficos de previs\u00e3o</li> <li>An\u00e1lise de res\u00edduos</li> <li>Visualiza\u00e7\u00e3o da estrutura do modelo</li> <li>Visualiza\u00e7\u00e3o de par\u00e2metros</li> </ul> <p>Como voc\u00ea pode ver, o SysIdentPy suporta diversas combina\u00e7\u00f5es de modelos. N\u00e3o se preocupe em escolher todas as configura\u00e7\u00f5es logo no come\u00e7o. Vamos come\u00e7ar com as configura\u00e7\u00f5es padr\u00e3o.</p>          \ud83d\udcda Em busca de mais detalhes sobre modelos NARMAX? \u25bc <p>             Para informa\u00e7\u00f5es completas sobre modelos, m\u00e9todos e um conjunto de exemplos e benchmarks implementados no SysIdentPy, confira nosso livro:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy <p>             Esse livro oferece uma orienta\u00e7\u00e3o detalhada para auxiliar no seu trabalho com o SysIdentPy.         </p> <p>             \ud83d\udee0\ufe0f Voc\u00ea tamb\u00e9m pode explorar os tutoriais na documenta\u00e7\u00e3o para exemplos pr\u00e1ticos.         </p>"},{"location":"pt/getting-started/quickstart-guide/#3-guia-rapido","title":"3. Guia R\u00e1pido","text":"<p>Para manter as coisas simples, vamos carregar alguns dados simulados para os exemplos.</p> <pre><code>from sysidentpy.utils.generate_data import get_siso_data\n\n# Gera um conjunto de dados de um sistema din\u00e2mico simulado.\nx_train, x_valid, y_train, y_valid = get_siso_data(\n        n=300,\n        colored_noise=False,\n        sigma=0.0001,\n        train_percentage=80\n)\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#construa-seu-primeiro-modelo-narx","title":"Construa seu primeiro modelo NARX","text":"<p>Com os dados carregados, vamos construir um modelo NARX Polinomial. Usando as configura\u00e7\u00f5es padr\u00e3o, voc\u00ea precisa definir pelo menos o m\u00e9todo de sele\u00e7\u00e3o de estrutura e a representa\u00e7\u00e3o matem\u00e1tica (fun\u00e7\u00e3o base).</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\n</code></pre> <p>O m\u00e9todo de sele\u00e7\u00e3o de estrutura (MSS) habilita as opera\u00e7\u00f5es de \"treinamento\" e previs\u00e3o do modelo.</p> <p>Embora diferentes algoritmos tenham diferentes hiperpar\u00e2metros, esse n\u00e3o \u00e9 o foco aqui. Mostraremos como modific\u00e1-los, mas n\u00e3o discutiremos as melhores configura\u00e7\u00f5es nesse guia.</p> <pre><code>model.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Para avaliar o desempenho, voc\u00ea pode usar qualquer m\u00e9trica dispon\u00edvel na biblioteca. Exemplo com Root Relative Squared Error (RRSE):</p> <pre><code>from sysidentpy.metrics import root_relative_squared_error\n\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n</code></pre> <pre><code>0.00014\n</code></pre> <p>Para visualizar a equa\u00e7\u00e3o final do modelo polinomial, use a fun\u00e7\u00e3o <code>results</code>. Ela requer a seguinte configura\u00e7\u00e3o:</p> <ul> <li><code>final_model</code>: Regressoras selecionadas ap\u00f3s o ajuste</li> <li><code>theta</code>: Par\u00e2metros estimados</li> <li><code>err</code>: Error Reduction Ratio (ERR)</li> </ul> <pre><code>from sysidentpy.utils.display_results import results\n\nr = pd.DataFrame(\n        results(\n                model.final_model, model.theta, model.err,\n                model.n_terms, err_precision=8, dtype='sci'\n        ),\n        columns=['Regressores', 'Par\u00e2metros', 'ERR'])\nprint(r)\n</code></pre> <p>Resultado (exemplo):</p> <pre><code>Regressores     Par\u00e2metros        ERR\n0        x1(k-2)     0.9000  0.95556574\n1         y(k-1)     0.1999  0.04107943\n2  x1(k-1)y(k-1)     0.1000  0.00335113\n</code></pre> <p>Para visualizar o desempenho do modelo:</p> <p><pre><code>from sysidentpy.utils.plotting import plot_results\n\nplot_results(y=y_valid, yhat=yhat, n=1000)\n</code></pre> </p> <p>Analisar res\u00edduos de um modelo \u00e9 essencial. Podemos calcular a autocorrela\u00e7\u00e3o dos res\u00edduos e correla\u00e7\u00e3o cruzada entre res\u00edduos e entradas conforme exemplo abaixo:</p> <pre><code>from sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n        compute_residues_autocorrelation,\n        compute_cross_correlation,\n)\n\n# Autocorrela\u00e7\u00e3o\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Res\u00edduos\", ylabel=\"$e^2$\")\n\n# Correla\u00e7\u00e3o cruzada com uma entrada\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Res\u00edduos\", ylabel=\"$x_1e$\")\n</code></pre> <p>C\u00f3digo completo para refer\u00eancia:</p> <pre><code>import pandas as pd\n\nfrom sysidentpy.utils.generate_data import get_siso_data\nfrom sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.metrics import root_relative_squared_error\nfrom sysidentpy.utils.display_results import results\nfrom sysidentpy.utils.plotting import plot_results\nfrom sysidentpy.utils.plotting import plot_residues_correlation\nfrom sysidentpy.residues.residues_correlation import (\n        compute_residues_autocorrelation,\n        compute_cross_correlation,\n)\n\nx_train, x_valid, y_train, y_valid = get_siso_data(\n        n=300,\n        colored_noise=False,\n        sigma=0.0001,\n        train_percentage=80\n)\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n\nrrse = root_relative_squared_error(y_valid, yhat)\nprint(rrse)\n\nr = pd.DataFrame(\n        results(\n                model.final_model, model.theta, model.err,\n                model.n_terms, err_precision=8, dtype='sci'\n                ),\n        columns=['Regressores', 'Par\u00e2metros', 'ERR'])\nprint(r)\n\nplot_results(y=y_valid, yhat=yhat, n=1000, figsize=(15, 4))\nee = compute_residues_autocorrelation(y_valid, yhat)\nplot_residues_correlation(data=ee, title=\"Res\u00edduos\", ylabel=\"$e^2$\")\nx1e = compute_cross_correlation(y_valid, yhat, x_valid)\nplot_residues_correlation(data=x1e, title=\"Res\u00edduos\", ylabel=\"$x_1e$\")\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#personalizando-a-configuracao-do-modelo","title":"Personalizando a configura\u00e7\u00e3o do modelo","text":""},{"location":"pt/getting-started/quickstart-guide/#selecao-de-estrutura","title":"Sele\u00e7\u00e3o de Estrutura","text":"<p>Para usar o algoritmo AOLS em vez de <code>FROLS</code>:</p> <pre><code>from sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = AOLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Usando MetaMSS:</p> <pre><code>from sysidentpy.model_structure_selection import MetaMSS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = MetaMSS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Usando Entropic Regression (ER):</p> <pre><code>from sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#estimacao-de-parametros","title":"Estima\u00e7\u00e3o de Par\u00e2metros","text":"<p>Listar algoritmos dispon\u00edveis:</p> <pre><code>from sysidentpy import parameter_estimation\nprint(\"Algoritmos dispon\u00edveis:\", parameter_estimation.__all__)\n</code></pre> <p>Definir estimador espec\u00edfico (ex: LSMR):</p> <pre><code>from sysidentpy.model_structure_selection import ER\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.parameter_estimation import LeastSquaresMinimalResidual\n\nbasis_function = Polynomial(degree=2)\nmodel = ER(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        estimator=LeastSquaresMinimalResidual(),\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#funcao-base-representacao-matematica","title":"Fun\u00e7\u00e3o Base (Representa\u00e7\u00e3o Matem\u00e1tica)","text":"<p>Listar fun\u00e7\u00f5es base:</p> <pre><code>from sysidentpy import basis_function\nprint(\"Fun\u00e7\u00f5es base dispon\u00edveis:\", basis_function.__all__)\n</code></pre> <p>Exemplo com Fourier:</p> <pre><code>from sysidentpy.model_structure_selection import AOLS\nfrom sysidentpy.basis_function import Fourier\nfrom sysidentpy.parameter_estimation import LeastSquaresMinimalResidual\n\nbasis_function = Fourier(degree=2)\nmodel = AOLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        estimator=LeastSquaresMinimalResidual(),\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Note</p> <pre><code>O m\u00e9todo `results` suporta apenas a base **Polynomial** no momento. Suporte a todas as fun\u00e7\u00f5es de base est\u00e1 planejado para a vers\u00e3o 1.0.\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#customizando-o-tipo-de-modelo","title":"Customizando o Tipo de Modelo","text":"<p>Diferen\u00e7a entre NARX e ARX: presen\u00e7a de termos n\u00e3o lineares. <code>degree=2</code> (Polynomial) permite um modelo potencialmente NARX; <code>degree=1</code> resulta em ARX. Por\u00e9m, a linearidade final depende da equa\u00e7\u00e3o obtida pelo m\u00e9todo de sele\u00e7\u00e3o de estrutura.</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=1)  # ARX\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Para criar um NAR:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n        ylag=2,\n        basis_function=basis_function,\n        model_type=\"NAR\",\n)\nmodel.fit(y=y_train)\nyhat = model.predict(y=y_valid, forecast_horizon=23)\n</code></pre> <p>Para NFIR (apenas entradas):</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=1)\nmodel = FROLS(\n        xlag=2,\n        basis_function=basis_function,\n        model_type=\"NFIR\",\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre>          \ud83d\udcda Quer saber mais detalhes sobre condi\u00e7\u00f5es iniciais? \u25bc <p>             Veja o cap\u00edtulo 9 do nosso livro para entender por que modelos autorregressivos precisam de condi\u00e7\u00f5es iniciais:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy"},{"location":"pt/getting-started/quickstart-guide/#horizonte-de-previsao","title":"Horizonte de Previs\u00e3o","text":"<p>Por padr\u00e3o, <code>predict</code> realiza previs\u00e3o de infinitos passos a frente (ou simula\u00e7\u00e3o livre). Para um n\u00famero espec\u00edfico de passos \u00e0 frente, use <code>steps_ahead</code>:</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n)\nmodel.fit(X=x_train, y=y_train)\n\nyhat_1 = model.predict(X=x_valid, y=y_valid, steps_ahead=1)\nyhat_4 = model.predict(X=x_valid, y=y_valid, steps_ahead=4)\n</code></pre>          \ud83d\udcda Mais detalhes sobre previs\u00e3o com diferentes passos a frente? \u25bc <p>             Veja o cap\u00edtulo 9 do nosso livro para saber como funcionam previs\u00f5es um passo, n-passos e infinitos passos a frente:         </p> Nonlinear System Identification and Forecasting: Theory and Practice With SysIdentPy"},{"location":"pt/getting-started/quickstart-guide/#selecao-de-ordem","title":"Sele\u00e7\u00e3o de Ordem","text":"<p>A sele\u00e7\u00e3o de ordem \u00e9 uma abordagem cl\u00e1ssica para determinar automaticamente a ordem \u00f3tima do modelo ao utilizar o algoritmo FROLS. Esse processo auxilia na identifica\u00e7\u00e3o da melhor combina\u00e7\u00e3o dos atrasos e regressores por meio da avalia\u00e7\u00e3o de diferentes modelos com base em um crit\u00e9rio de informa\u00e7\u00e3o.</p> <p>Important</p> <pre><code>Crit\u00e9rios de informa\u00e7\u00e3o *s\u00f3 se aplicam* ao algoritmo **FROLS**.\n</code></pre> <p>Habilite com: 1. <code>order_selection=True</code> 2. <code>info_criteria=\"bic\"</code> (ou <code>\"aic\"</code>, <code>\"aicc\"</code>, <code>\"fpe\"</code>, <code>\"lilc\"</code>).</p> <pre><code>from sysidentpy.model_structure_selection import FROLS\nfrom sysidentpy.basis_function import Polynomial\n\nbasis_function = Polynomial(degree=2)\nmodel = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        order_selection=True,\n        info_criteria=\"bic\"\n)\nmodel.fit(X=x_train, y=y_train)\nyhat = model.predict(X=x_valid, y=y_valid)\n</code></pre> <p>Controlar n\u00famero de regressores testados: <code>n_info_values</code>.</p> <pre><code>model = FROLS(\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        order_selection=True,\n        info_criteria=\"bic\",\n        n_info_values=50\n)\n</code></pre> <p>Important</p> <pre><code>Aumentar `n_info_values` pode melhorar a precis\u00e3o, mas aumenta o tempo computacional.\n</code></pre>"},{"location":"pt/getting-started/quickstart-guide/#rede-neural-narx","title":"Rede Neural NARX","text":"<p>Exemplo com PyTorch:</p> <pre><code>from torch import nn\nfrom sysidentpy.neural_network import NARXNN\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\n\nbasis_function = Polynomial(degree=1)\n\nclass NARX(nn.Module):\n        def __init__(self):\n                super().__init__()\n                self.lin = nn.Linear(4, 10)\n                self.lin2 = nn.Linear(10, 10)\n                self.lin3 = nn.Linear(10, 1)\n                self.tanh = nn.Tanh()\n\n        def forward(self, xb):\n                z = self.lin(xb)\n                z = self.tanh(z)\n                z = self.lin2(z)\n                z = self.tanh(z)\n                z = self.lin3(z)\n                return z\n\nnarx_net = NARXNN(\n        net=NARX(),\n        ylag=2,\n        xlag=2,\n        basis_function=basis_function,\n        model_type=\"NARMAX\",\n        loss_func='mse_loss',\n        optimizer='Adam',\n        epochs=200,\n        verbose=False,\n        optim_params={'betas': (0.9, 0.999), 'eps': 1e-05}\n)\n\nnarx_net.fit(X=x_train, y=y_train)\nyhat = narx_net.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=1000, figsize=(15, 4))\n</code></pre> <p></p>"},{"location":"pt/getting-started/quickstart-guide/#estimadores-gerais","title":"Estimadores Gerais","text":"<p>Voc\u00ea pode integrar qualquer estimador (scikit-learn, xgboost, catboost etc.) desde que eles sigam o padr\u00e3o <code>fit</code> e <code>predict</code>.</p> <p>Exemplo CatBoost NARX:</p> <pre><code>from sysidentpy.general_estimators import NARX\nfrom catboost import CatBoostRegressor\nfrom sysidentpy.basis_function import Polynomial\nfrom sysidentpy.utils.plotting import plot_results\n\nbasis_function = Polynomial(degree=1)\ncatboost_narx = NARX(\n        base_estimator=CatBoostRegressor(\n                iterations=300,\n                learning_rate=0.1,\n                depth=6),\n        xlag=2,\n        ylag=2,\n        basis_function=basis_function,\n        model_type=\"NARMAX\",\n        fit_params={'verbose': False}\n)\n\ncatboost_narx.fit(X=x_train, y=y_train)\nyhat = catboost_narx.predict(X=x_valid, y=y_valid)\nplot_results(y=y_valid, yhat=yhat, n=200)\n</code></pre> <p></p> <p>Sem NARX (para compara\u00e7\u00e3o):</p> <pre><code>from catboost import CatBoostRegressor\nfrom sysidentpy.utils.plotting import plot_results\n\ncatboost = CatBoostRegressor(\n        iterations=300,\n        learning_rate=0.1,\n        depth=6\n)\ncatboost.fit(x_train, y_train, verbose=False)\nplot_results(y=y_valid, yhat=catboost.predict(x_valid), figsize=(15, 4))\n</code></pre> <p></p> <p>Voc\u00ea ainda pode explorar combina\u00e7\u00f5es: usar fun\u00e7\u00e3o base Fourier, previs\u00e3o multi-passos, diferentes estimadores etc.</p> <p>Este \u00e9 apenas um guia r\u00e1pido. Para tutoriais completos, guias passo a passo, explica\u00e7\u00f5es detalhadas e casos avan\u00e7ados, veja a documenta\u00e7\u00e3o e o livro.</p>"}]}